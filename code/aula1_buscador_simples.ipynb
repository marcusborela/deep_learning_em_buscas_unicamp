{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcN_5-RDWeqV"
      },
      "source": [
        "# Busca simples\n",
        "\n",
        "Desenvolvimento de um buscador Simples: Booleano, TF-IDF, BM25\n",
        "\n",
        "Tópicos abordados: Indexação, Bag-of-Words, TF-IDF, BM25\n",
        "\n",
        "Aula 1 - [Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)\n",
        "\n",
        "[Link para chat de apoio com WebChatGPT](https://github.com/marcusborela/deep_learning_em_buscas_unicamp/blob/main/chat/CG%20uso%20no%20buscador%20aula%201.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1aFWTVgejM"
      },
      "source": [
        "[![Open In Colab latest github version](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcusborela/deep_learning_em_buscas_unicamp/blob/main/code/aula1_buscador_simples.ipynb) [Open In Colab latest github version]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQxzYKGgMqce"
      },
      "source": [
        "## Enunciado exercício\n",
        "\n",
        "Aula 2 - Notebook: Buscador Booleano/bag-of-words e buscador com TF-IDF\n",
        "\n",
        "1. Usar o BM25 implementado pelo pyserini para buscar queries no TREC-DL 2020\n",
        "Documentação referencia: https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-passage.md\n",
        "2. Implementar um buscador booleano/bag-of-words.\n",
        "3. Implementar um buscador com TF-IDF\n",
        "4. Avaliar implementações 1, 2, e 3 no TREC-DL 2020 e calcular o nDCG@10\n",
        "Nos itens 2 e 3:\n",
        "\n",
        "Fazer uma implementação que suporta buscar eficientemente milhões de documentos.\n",
        "\n",
        "Não se pode usar bibliotecas como sklearn, que já implementam o BoW e TF-IDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalações de libraries"
      ],
      "metadata": {
        "id": "Y5MRuHo8md3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/castorini/pygaggle.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2af9dTbmPff",
        "outputId": "7019a6b5-2a9a-44f2-f11b-46e0e456260d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/castorini/pygaggle.git\n",
            "  Cloning https://github.com/castorini/pygaggle.git to /tmp/pip-req-build-onlghqtq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/castorini/pygaggle.git /tmp/pip-req-build-onlghqtq\n",
            "  Resolved https://github.com/castorini/pygaggle.git to commit c285f6084684367dd07b608ef19c2722b5b0637e\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs==14.0\n",
            "  Downloading coloredlogs-14.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 KB\u001b[0m \u001b[31m937.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.8/dist-packages (from pygaggle==0.0.3.1) (1.22.4)\n",
            "Collecting pydantic==1.7.4\n",
            "  Downloading pydantic-1.7.4-cp38-cp38-manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyserini>=0.16.0\n",
            "  Downloading pyserini-0.20.0-py3-none-any.whl (137.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.8/dist-packages (from pygaggle==0.0.3.1) (3.4.4)\n",
            "Collecting tensorboard==2.7.0\n",
            "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.10.2\n",
            "  Downloading tokenizers-0.10.2-cp38-cp38-manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.56.0\n",
            "  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.6.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers==2.0.0\n",
            "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from pygaggle==0.0.3.1) (1.13.1+cu116)\n",
            "Collecting humanfriendly>=7.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.24.2->pygaggle==0.0.3.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.24.2->pygaggle==0.0.3.1) (1.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers==2.0.0->pygaggle==0.0.3.1) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers==2.0.0->pygaggle==0.0.3.1) (3.7)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (1.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (2.25.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (0.38.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (1.51.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (2.16.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->pygaggle==0.0.3.1) (0.6.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (4.5.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (0.31.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (15.0.6.1)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->pygaggle==0.0.3.1) (3.3.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->pygaggle==0.0.3.1) (23.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->pygaggle==0.0.3.1) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.1->pygaggle==0.0.3.1) (3.9.0)\n",
            "Collecting pandas>=1.4.0\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightgbm>=3.3.2\n",
            "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.8/dist-packages (from pyserini>=0.16.0->pygaggle==0.0.3.1) (0.29.33)\n",
            "Collecting nmslib>=2.1.1\n",
            "  Downloading nmslib-2.1.1-cp38-cp38-manylinux2010_x86_64.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.8.1\n",
            "  Downloading onnxruntime-1.14.1-cp38-cp38-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjnius>=1.4.0\n",
            "  Downloading pyjnius-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (6.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (3.1.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (2.4.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (8.1.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pygaggle==0.0.3.1) (1.0.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->pygaggle==0.0.3.1) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->pygaggle==0.0.3.1) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->pygaggle==0.0.3.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->pygaggle==0.0.3.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard==2.7.0->pygaggle==0.0.3.1) (6.0.0)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from nmslib>=2.1.1->pyserini>=0.16.0->pygaggle==0.0.3.1) (5.4.8)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.8.1->pyserini>=0.16.0->pygaggle==0.0.3.1) (1.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.4.0->pyserini>=0.16.0->pygaggle==0.0.3.1) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.4.0->pyserini>=0.16.0->pygaggle==0.0.3.1) (2.8.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->pygaggle==0.0.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->pygaggle==0.0.3.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->pygaggle==0.0.3.1) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->pygaggle==0.0.3.1) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.1->pygaggle==0.0.3.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.1->pygaggle==0.0.3.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.2.1->pygaggle==0.0.3.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=0.11.15->tensorboard==2.7.0->pygaggle==0.0.3.1) (2.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers==2.0.0->pygaggle==0.0.3.1) (8.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.7.0->pygaggle==0.0.3.1) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.7.0->pygaggle==0.0.3.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->pygaggle==0.0.3.1) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini>=0.16.0->pygaggle==0.0.3.1) (1.2.1)\n",
            "Building wheels for collected packages: pygaggle, sentence_transformers, sacremoses\n",
            "  Building wheel for pygaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygaggle: filename=pygaggle-0.0.3.1-py3-none-any.whl size=75889 sha256=16d336e5486c44340bd157516c4ee44c591b3a68c6066787361d4f28c7df6a36\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k3gebtx_/wheels/3e/3f/f4/4b5a7cd54450cf49522bc9e76a0e57cea70a9f731fc4ff0257\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126709 sha256=83f8005bf1b59b58a8da1080df487368733756667cd3023db0fe9310a872e2f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/b7/50/451c9a52a337aac5521dbc10544a69e1447d28012feba30742\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=534dec5cdefc6249cb66506f7e23b195db33ab072682da191052c32944b9297b\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built pygaggle sentence_transformers sacremoses\n",
            "Installing collected packages: tokenizers, tensorflow-estimator, sentencepiece, keras, flatbuffers, tqdm, scipy, pyjnius, pydantic, pybind11, keras-preprocessing, humanfriendly, scikit-learn, sacremoses, pandas, nmslib, huggingface-hub, coloredlogs, transformers, onnxruntime, lightgbm, tensorboard, sentence_transformers, tensorflow, pyserini, pygaggle\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.1.21\n",
            "    Uninstalling flatbuffers-23.1.21:\n",
            "      Successfully uninstalled flatbuffers-23.1.21\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.5\n",
            "    Uninstalling pydantic-1.10.5:\n",
            "      Successfully uninstalled pydantic-1.10.5\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.1\n",
            "    Uninstalling scikit-learn-1.2.1:\n",
            "      Successfully uninstalled scikit-learn-1.2.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-14.0 flatbuffers-2.0.7 huggingface-hub-0.0.8 humanfriendly-10.0 keras-2.7.0 keras-preprocessing-1.1.2 lightgbm-3.3.5 nmslib-2.1.1 onnxruntime-1.14.1 pandas-1.5.3 pybind11-2.6.1 pydantic-1.7.4 pygaggle-0.0.3.1 pyjnius-1.4.2 pyserini-0.20.0 sacremoses-0.0.53 scikit-learn-0.24.2 scipy-1.5.4 sentence_transformers-2.0.0 sentencepiece-0.1.95 tensorboard-2.7.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tokenizers-0.10.2 tqdm-4.56.0 transformers-4.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyserini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTTVf-nYmc0w",
        "outputId": "3253f86a-fd3e-46df-b330-94265742eee8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyserini in /usr/local/lib/python3.8/dist-packages (0.20.0)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from pyserini) (1.5.3)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.8/dist-packages (from pyserini) (3.4.4)\n",
            "Requirement already satisfied: nmslib>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from pyserini) (2.1.1)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from pyserini) (4.6.1)\n",
            "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.8/dist-packages (from pyserini) (0.1.95)\n",
            "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.8/dist-packages (from pyserini) (0.29.33)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.8/dist-packages (from pyserini) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pyserini) (4.56.0)\n",
            "Requirement already satisfied: onnxruntime>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from pyserini) (1.14.1)\n",
            "Requirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.8/dist-packages (from pyserini) (3.3.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from pyserini) (1.5.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.8/dist-packages (from pyserini) (0.24.2)\n",
            "Requirement already satisfied: pyjnius>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from pyserini) (1.4.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from lightgbm>=3.3.2->pyserini) (0.38.4)\n",
            "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.8/dist-packages (from nmslib>=2.1.1->pyserini) (2.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from nmslib>=2.1.1->pyserini) (5.4.8)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.8.1->pyserini) (2.0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.19.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.7.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.8.1->pyserini) (14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.4.0->pyserini) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from pyjnius>=1.4.0->pyserini) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (2.25.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (1.7.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (2.4.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.2.1->pyserini) (8.1.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->pyserini) (0.0.53)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->pyserini) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->pyserini) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->pyserini) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->pyserini) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.1->pyserini) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.1->pyserini) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.3)\n",
            "Requirement already satisfied: humanfriendly>=7.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->onnxruntime>=1.8.1->pyserini) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3U_q5n1n_BW",
        "outputId": "cdb520e8-23bc-4adf-857f-dd22c9a1e16a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/17.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/17.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/17.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/17.0 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m16.2/17.0 MB\u001b[0m \u001b[31m222.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m212.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m212.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrFqBFx4MNeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vinculando pasta do google drive para salvar dados"
      ],
      "metadata": {
        "id": "r9xRdgUGMPgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ae-Iy2oz_9os",
        "outputId": "374bb904-9c6a-4705-c2c8-1cfa0e8c147a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/'"
      ],
      "metadata": {
        "id": "4TKTD0D__9jr",
        "outputId": "b7e42ea1-8834-4969-ef24-ec38b8ed30ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1_4911225524505804922.epub\n",
            "'Auto Call Recorder'\n",
            "'Cartão de embarque | LATAM Airlines no Brasil.pdf'\n",
            " Classroom\n",
            "'Colab Notebooks'\n",
            " dam0.tif\n",
            " dam1.tif\n",
            " dam2.tif\n",
            " dam3.tif\n",
            " dam4.tif\n",
            " dam5.tif\n",
            " dam6.tif\n",
            " dam7.tif\n",
            " dam8.tif\n",
            "'Exercícios - 20210718 - Marcus Vinícius Borela de Castro'\n",
            " foo.txt\n",
            " FunprespSeguro\n",
            " ia025\n",
            " Igreja\n",
            " marcus_pc_bkup\n",
            "'Pedido relativo ao Hurb.gdoc'\n",
            " Quorum_reuniões_anteriores.xlsx\n",
            " Relatório_Final_Projeto_exqa-complearning.gdoc\n",
            " Relatório_Final_Robustez_Consulta.gdoc\n",
            " Relatório_Final_Robustez_Query.gdoc\n",
            "'Sample file.txt'\n",
            " SEINT\n",
            " TCU\n",
            " temp\n",
            " teste.txt.gdoc\n",
            " treinamento\n",
            " Untitled0.ipynb\n",
            "'Untitled document.gdoc'\n",
            " ViagemBuzios.gsheet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive'"
      ],
      "metadata": {
        "id": "HlGfGbtnBP0K",
        "outputId": "360bd5aa-7c0f-42b3-d909-4510c088b8c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_dir = os.getcwd()\n",
        "print(\"Current directory:\", current_dir)"
      ],
      "metadata": {
        "id": "7GYGL4MV_yhQ",
        "outputId": "56f28de1-01a8-4812-f661-24d7c2e89ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baixando o repositório do pyserini para usara seus scripts"
      ],
      "metadata": {
        "id": "iMp1ME6UK8vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_pyserini = '/content/drive/MyDrive/treinamento/202301_IA368DD/code/pyserini'"
      ],
      "metadata": {
        "id": "aGyBaf54LwCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(path_pyserini):\n",
        "    os.makedirs(path_pyserini)\n",
        "    print('pasta criada')"
      ],
      "metadata": {
        "id": "vVBzuIuVMCqK",
        "outputId": "dd62be5d-b010-4c41-90a4-02fb786e183f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pasta criada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/castorini/pyserini/archive/refs/heads/master.zip -O pyserini.zip \n"
      ],
      "metadata": {
        "id": "AHkqDD0uK8Fi"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q pyserini.zip -d  {path_pyserini}"
      ],
      "metadata": {
        "id": "PSv1UUhfNUyH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_pyserini = path_pyserini + '/pyserini-master'"
      ],
      "metadata": {
        "id": "8Ncawo0rPMoC"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " os.path.exists(path_pyserini)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLHh0Nh8PW2E",
        "outputId": "9de04ffb-92c2-4899-d7fe-6c416f3fdf5b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixando tools que é um atalho para https://github.com/castorini/anserini-tools"
      ],
      "metadata": {
        "id": "GDxOYQasQiM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/castorini/anserini-tools/archive/refs/heads/master.zip -O anserini-tools.zip "
      ],
      "metadata": {
        "id": "cWOgSPElQivi"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q anserini-tools.zip -d  {path_pyserini}"
      ],
      "metadata": {
        "id": "30pi-eYwQ4zo"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_pyserini_tools = path_pyserini + '/anserini-tools-master'"
      ],
      "metadata": {
        "id": "uCAQ5rZERzvr"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " os.path.exists(path_pyserini_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-AK5u1mSC1f",
        "outputId": "aa5f1dd7-5868-4667-9d72-5082848a8493"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_pyserini_eval = path_pyserini + '/pyserini/eval'"
      ],
      "metadata": {
        "id": "kF1v31OuUzpM"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " os.path.exists(path_pyserini_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUSdXX25VAwb",
        "outputId": "a7c8b4d0-e1da-4cdc-b890-6501366e57e1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga dos dados da TREC 2020 "
      ],
      "metadata": {
        "id": "MBfxPDhaFq5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g4gKjhiICmAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtendo dados a partir do pyserini\n",
        "\n",
        "\n",
        "[Dicas aqui](https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-passage.md)"
      ],
      "metadata": {
        "id": "hGPQPU38MbZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_data = '/content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage'"
      ],
      "metadata": {
        "id": "2WGHMsGcB7Ep"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.path.exists(path_data):\n",
        "    os.makedirs(path_data)\n",
        "    print('pasta criada')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDb6qDbsA38j",
        "outputId": "3caa7c78-a77f-48c9-889f-fb46dbea9fcf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pasta criada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz -P {path_data}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR6vFxTu_ico",
        "outputId": "c1bf3dfe-aec1-47c1-a8d6-16d981b75259"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-04 00:30:37--  https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz\n",
            "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
            "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1057717952 (1009M) [application/gzip]\n",
            "Saving to: ‘/content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage/collectionandqueries.tar.gz’\n",
            "\n",
            "collectionandquerie 100%[===================>]   1009M  5.22MB/s    in 1m 53s  \n",
            "\n",
            "2023-03-04 00:32:30 (8.91 MB/s) - ‘/content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage/collectionandqueries.tar.gz’ saved [1057717952/1057717952]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvfz {path_data}/collectionandqueries.tar.gz -C {path_data}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJrteohm_iZz",
        "outputId": "ddd0fd3e-7a57-4dd3-aa92-a8b82de9a03c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collection.tsv\n",
            "qrels.dev.small.tsv\n",
            "qrels.train.tsv\n",
            "queries.dev.small.tsv\n",
            "queries.dev.tsv\n",
            "queries.eval.small.tsv\n",
            "queries.eval.tsv\n",
            "queries.train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to convert the MS MARCO tsv collection into Pyserini's jsonl files (which have one json object per line):"
      ],
      "metadata": {
        "id": "oiNtb0oVDTPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixando o programa do github do pyserini"
      ],
      "metadata": {
        "id": "QBvD2wMwEaVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python {path_pyserini_tools}/tools/scripts/msmarco/convert_collection_to_jsonl.py \\\n",
        " --collection-path {path_data}/collection.tsv \\\n",
        " --output-folder {path_data}/collection_jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cf9nEQS_iWs",
        "outputId": "601c3ea1-4e0a-4a06-bf0c-5324c509d3d2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting collection...\n",
            "Converted 0 docs, writing into file 1\n",
            "Converted 100,000 docs, writing into file 1\n",
            "Converted 200,000 docs, writing into file 1\n",
            "Converted 300,000 docs, writing into file 1\n",
            "Converted 400,000 docs, writing into file 1\n",
            "Converted 500,000 docs, writing into file 1\n",
            "Converted 600,000 docs, writing into file 1\n",
            "Converted 700,000 docs, writing into file 1\n",
            "Converted 800,000 docs, writing into file 1\n",
            "Converted 900,000 docs, writing into file 1\n",
            "Converted 1,000,000 docs, writing into file 2\n",
            "Converted 1,100,000 docs, writing into file 2\n",
            "Converted 1,200,000 docs, writing into file 2\n",
            "Converted 1,300,000 docs, writing into file 2\n",
            "Converted 1,400,000 docs, writing into file 2\n",
            "Converted 1,500,000 docs, writing into file 2\n",
            "Converted 1,600,000 docs, writing into file 2\n",
            "Converted 1,700,000 docs, writing into file 2\n",
            "Converted 1,800,000 docs, writing into file 2\n",
            "Converted 1,900,000 docs, writing into file 2\n",
            "Converted 2,000,000 docs, writing into file 3\n",
            "Converted 2,100,000 docs, writing into file 3\n",
            "Converted 2,200,000 docs, writing into file 3\n",
            "Converted 2,300,000 docs, writing into file 3\n",
            "Converted 2,400,000 docs, writing into file 3\n",
            "Converted 2,500,000 docs, writing into file 3\n",
            "Converted 2,600,000 docs, writing into file 3\n",
            "Converted 2,700,000 docs, writing into file 3\n",
            "Converted 2,800,000 docs, writing into file 3\n",
            "Converted 2,900,000 docs, writing into file 3\n",
            "Converted 3,000,000 docs, writing into file 4\n",
            "Converted 3,100,000 docs, writing into file 4\n",
            "Converted 3,200,000 docs, writing into file 4\n",
            "Converted 3,300,000 docs, writing into file 4\n",
            "Converted 3,400,000 docs, writing into file 4\n",
            "Converted 3,500,000 docs, writing into file 4\n",
            "Converted 3,600,000 docs, writing into file 4\n",
            "Converted 3,700,000 docs, writing into file 4\n",
            "Converted 3,800,000 docs, writing into file 4\n",
            "Converted 3,900,000 docs, writing into file 4\n",
            "Converted 4,000,000 docs, writing into file 5\n",
            "Converted 4,100,000 docs, writing into file 5\n",
            "Converted 4,200,000 docs, writing into file 5\n",
            "Converted 4,300,000 docs, writing into file 5\n",
            "Converted 4,400,000 docs, writing into file 5\n",
            "Converted 4,500,000 docs, writing into file 5\n",
            "Converted 4,600,000 docs, writing into file 5\n",
            "Converted 4,700,000 docs, writing into file 5\n",
            "Converted 4,800,000 docs, writing into file 5\n",
            "Converted 4,900,000 docs, writing into file 5\n",
            "Converted 5,000,000 docs, writing into file 6\n",
            "Converted 5,100,000 docs, writing into file 6\n",
            "Converted 5,200,000 docs, writing into file 6\n",
            "Converted 5,300,000 docs, writing into file 6\n",
            "Converted 5,400,000 docs, writing into file 6\n",
            "Converted 5,500,000 docs, writing into file 6\n",
            "Converted 5,600,000 docs, writing into file 6\n",
            "Converted 5,700,000 docs, writing into file 6\n",
            "Converted 5,800,000 docs, writing into file 6\n",
            "Converted 5,900,000 docs, writing into file 6\n",
            "Converted 6,000,000 docs, writing into file 7\n",
            "Converted 6,100,000 docs, writing into file 7\n",
            "Converted 6,200,000 docs, writing into file 7\n",
            "Converted 6,300,000 docs, writing into file 7\n",
            "Converted 6,400,000 docs, writing into file 7\n",
            "Converted 6,500,000 docs, writing into file 7\n",
            "Converted 6,600,000 docs, writing into file 7\n",
            "Converted 6,700,000 docs, writing into file 7\n",
            "Converted 6,800,000 docs, writing into file 7\n",
            "Converted 6,900,000 docs, writing into file 7\n",
            "Converted 7,000,000 docs, writing into file 8\n",
            "Converted 7,100,000 docs, writing into file 8\n",
            "Converted 7,200,000 docs, writing into file 8\n",
            "Converted 7,300,000 docs, writing into file 8\n",
            "Converted 7,400,000 docs, writing into file 8\n",
            "Converted 7,500,000 docs, writing into file 8\n",
            "Converted 7,600,000 docs, writing into file 8\n",
            "Converted 7,700,000 docs, writing into file 8\n",
            "Converted 7,800,000 docs, writing into file 8\n",
            "Converted 7,900,000 docs, writing into file 8\n",
            "Converted 8,000,000 docs, writing into file 9\n",
            "Converted 8,100,000 docs, writing into file 9\n",
            "Converted 8,200,000 docs, writing into file 9\n",
            "Converted 8,300,000 docs, writing into file 9\n",
            "Converted 8,400,000 docs, writing into file 9\n",
            "Converted 8,500,000 docs, writing into file 9\n",
            "Converted 8,600,000 docs, writing into file 9\n",
            "Converted 8,700,000 docs, writing into file 9\n",
            "Converted 8,800,000 docs, writing into file 9\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above script should generate 9 jsonl files in collections/msmarco-passage/collection_jsonl, each with 1M lines (except for the last one, which should have 841,823 lines).\n",
        "\n",
        "We can now index these docs as a JsonCollection using Pyserini:"
      ],
      "metadata": {
        "id": "u4snYshcDmXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.index.lucene \\\n",
        "  --collection JsonCollection \\\n",
        "  --input {path_data}/collection_jsonl \\\n",
        "  --index indexes/lucene-index-msmarco-passage \\\n",
        "  --generator DefaultLuceneDocumentGenerator \\\n",
        "  --threads 9 \\\n",
        "  --storePositions --storeDocvectors --storeRaw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exN_9Ree_iTS",
        "outputId": "b0e4614c-65c9-4a80-c86e-ca7b88fc9c5d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2023-03-04 00:42:27,081 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Setting log level to INFO\n",
            "2023-03-04 00:42:27,083 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Starting indexer...\n",
            "2023-03-04 00:42:27,083 INFO  [main] index.IndexCollection (IndexCollection.java:395) - ============ Loading Parameters ============\n",
            "2023-03-04 00:42:27,083 INFO  [main] index.IndexCollection (IndexCollection.java:396) - DocumentCollection path: /content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage/collection_jsonl\n",
            "2023-03-04 00:42:27,084 INFO  [main] index.IndexCollection (IndexCollection.java:397) - CollectionClass: JsonCollection\n",
            "2023-03-04 00:42:27,084 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Generator: DefaultLuceneDocumentGenerator\n",
            "2023-03-04 00:42:27,084 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Threads: 9\n",
            "2023-03-04 00:42:27,085 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Language: en\n",
            "2023-03-04 00:42:27,085 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Stemmer: porter\n",
            "2023-03-04 00:42:27,085 INFO  [main] index.IndexCollection (IndexCollection.java:402) - Keep stopwords? false\n",
            "2023-03-04 00:42:27,085 INFO  [main] index.IndexCollection (IndexCollection.java:403) - Stopwords: null\n",
            "2023-03-04 00:42:27,086 INFO  [main] index.IndexCollection (IndexCollection.java:404) - Store positions? true\n",
            "2023-03-04 00:42:27,086 INFO  [main] index.IndexCollection (IndexCollection.java:405) - Store docvectors? true\n",
            "2023-03-04 00:42:27,086 INFO  [main] index.IndexCollection (IndexCollection.java:406) - Store document \"contents\" field? false\n",
            "2023-03-04 00:42:27,087 INFO  [main] index.IndexCollection (IndexCollection.java:407) - Store document \"raw\" field? true\n",
            "2023-03-04 00:42:27,087 INFO  [main] index.IndexCollection (IndexCollection.java:408) - Additional fields to index: []\n",
            "2023-03-04 00:42:27,087 INFO  [main] index.IndexCollection (IndexCollection.java:409) - Optimize (merge segments)? false\n",
            "2023-03-04 00:42:27,087 INFO  [main] index.IndexCollection (IndexCollection.java:410) - Whitelist: null\n",
            "2023-03-04 00:42:27,088 INFO  [main] index.IndexCollection (IndexCollection.java:411) - Pretokenized?: false\n",
            "2023-03-04 00:42:27,088 INFO  [main] index.IndexCollection (IndexCollection.java:412) - Index path: indexes/lucene-index-msmarco-passage\n",
            "2023-03-04 00:42:27,093 INFO  [main] index.IndexCollection (IndexCollection.java:450) - ============ Indexing Collection ============\n",
            "2023-03-04 00:42:27,513 INFO  [main] index.IndexCollection (IndexCollection.java:565) - Thread pool with 9 threads initialized.\n",
            "2023-03-04 00:42:27,513 INFO  [main] index.IndexCollection (IndexCollection.java:567) - Initializing collection in /content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage/collection_jsonl\n",
            "2023-03-04 00:42:27,520 INFO  [main] index.IndexCollection (IndexCollection.java:576) - 9 files found\n",
            "2023-03-04 00:42:27,520 INFO  [main] index.IndexCollection (IndexCollection.java:577) - Starting to index...\n",
            "2023-03-04 00:43:27,537 INFO  [main] index.IndexCollection (IndexCollection.java:591) - 0.00% of files completed, 1,660,000 documents indexed\n",
            "2023-03-04 00:44:27,538 INFO  [main] index.IndexCollection (IndexCollection.java:591) - 0.00% of files completed, 3,360,000 documents indexed\n",
            "2023-03-04 00:45:27,539 INFO  [main] index.IndexCollection (IndexCollection.java:591) - 0.00% of files completed, 4,950,000 documents indexed\n",
            "2023-03-04 00:46:27,541 INFO  [main] index.IndexCollection (IndexCollection.java:591) - 0.00% of files completed, 6,530,000 documents indexed\n",
            "2023-03-04 00:46:57,354 DEBUG [pool-2-thread-9] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs08.json: 841823 docs added.\n",
            "2023-03-04 00:47:27,545 INFO  [main] index.IndexCollection (IndexCollection.java:591) - 11.11% of files completed, 8,061,823 documents indexed\n",
            "2023-03-04 00:47:51,416 DEBUG [pool-2-thread-8] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs07.json: 1000000 docs added.\n",
            "2023-03-04 00:47:51,743 DEBUG [pool-2-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs01.json: 1000000 docs added.\n",
            "2023-03-04 00:47:52,473 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs00.json: 1000000 docs added.\n",
            "2023-03-04 00:47:53,180 DEBUG [pool-2-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs02.json: 1000000 docs added.\n",
            "2023-03-04 00:47:53,393 DEBUG [pool-2-thread-5] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs04.json: 1000000 docs added.\n",
            "2023-03-04 00:47:54,154 DEBUG [pool-2-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs03.json: 1000000 docs added.\n",
            "2023-03-04 00:47:55,088 DEBUG [pool-2-thread-6] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs05.json: 1000000 docs added.\n",
            "2023-03-04 00:47:55,733 DEBUG [pool-2-thread-7] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - collection_jsonl/docs06.json: 1000000 docs added.\n",
            "2023-03-04 00:48:51,569 INFO  [main] index.IndexCollection (IndexCollection.java:633) - Indexing Complete! 8,841,823 documents indexed\n",
            "2023-03-04 00:48:51,570 INFO  [main] index.IndexCollection (IndexCollection.java:634) - ============ Final Counter Values ============\n",
            "2023-03-04 00:48:51,570 INFO  [main] index.IndexCollection (IndexCollection.java:635) - indexed:        8,841,823\n",
            "2023-03-04 00:48:51,570 INFO  [main] index.IndexCollection (IndexCollection.java:636) - unindexable:            0\n",
            "2023-03-04 00:48:51,570 INFO  [main] index.IndexCollection (IndexCollection.java:637) - empty:                  0\n",
            "2023-03-04 00:48:51,571 INFO  [main] index.IndexCollection (IndexCollection.java:638) - skipped:                0\n",
            "2023-03-04 00:48:51,571 INFO  [main] index.IndexCollection (IndexCollection.java:639) - errors:                 0\n",
            "2023-03-04 00:48:51,584 INFO  [main] index.IndexCollection (IndexCollection.java:642) - Total 8,841,823 documents indexed in 00:06:24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -hs './indexes/lucene-index-msmarco-passage'"
      ],
      "metadata": {
        "id": "u9LG2KrpGpaO",
        "outputId": "0e5829a2-5915-4155-df7c-b38777f3c008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.2G\t./indexes/lucene-index-msmarco-passage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Retrieval on the Dev Queries\n",
        "\n",
        "The 6980 queries in the development set are already stored in the repo. Let's take a peek:"
      ],
      "metadata": {
        "id": "sTn273M0FTTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head {path_pyserini_tools}/topics-and-qrels/topics.msmarco-passage.dev-subset.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WOE8VZPfzf",
        "outputId": "44b3d2ce-1b90-4a0b-a2f0-b3b8aab5352b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1048585\twhat is paula deen's brother\n",
            "2\t Androgen receptor define\n",
            "524332\ttreating tension headaches without medication\n",
            "1048642\twhat is paranoid sc\n",
            "524447\ttreatment of varicose veins in legs\n",
            "786674\twhat is prime rate in canada\n",
            "1048876\twho plays young dr mallard on ncis\n",
            "1048917\twhat is operating system misconfiguration\n",
            "786786\twhat is priority pass\n",
            "524699\ttricare service number\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line contains a tab-delimited (query id, query) pair. Conveniently, Pyserini already knows how to load and iterate through these pairs. We can now perform retrieval using these queries:"
      ],
      "metadata": {
        "id": "-9XSzW8uHtic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.search.lucene \\\n",
        "  --index indexes/lucene-index-msmarco-passage \\\n",
        "  --topics msmarco-passage-dev-subset \\\n",
        "  --output runs/run.msmarco-passage.bm25tuned.txt \\\n",
        "  --output-format msmarco \\\n",
        "  --hits 1000 \\\n",
        "  --bm25 --k1 0.82 --b 0.68"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWYHSXe3Hw9o",
        "outputId": "067539cc-5557-4ee7-de6f-e6d2f002ccd2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pre-defined topic order for msmarco-passage-dev-subset\n",
            "Setting BM25 parameters: k1=0.82, b=0.68\n",
            "Running msmarco-passage-dev-subset topics, saving to runs/run.msmarco-passage.bm25tuned.txt...\n",
            "100% 6980/6980 [09:21<00:00, 12.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we set the BM25 parameters to k1=0.82, b=0.68 (tuned by grid search). The option --output-format msmarco says to generate output in the MS MARCO output format. The option --hits specifies the number of documents to return per query. Thus, the output file should have approximately 6980 × 1000 = 6.9M lines.\n",
        "\n",
        "Retrieval speed will vary by hardware: On a reasonably modern CPU with an SSD, we might get around 13 qps (queries per second), and so the entire run should finish in under ten minutes (using a single thread). We can perform multi-threaded retrieval by using the --threads and --batch-size arguments. For example, setting --threads 16 --batch-size 64 on a CPU with sufficient cores, the entire run will finish in a couple of minutes."
      ],
      "metadata": {
        "id": "gfXHH9upIVrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run finishes, we can evaluate the results using the official MS MARCO evaluation script:"
      ],
      "metadata": {
        "id": "QxKtnfGoIZ6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python {path_pyserini_tools}/scripts/msmarco/msmarco_passage_eval.py \\\n",
        "    {path_pyserini_tools}/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt runs/run.msmarco-passage.bm25tuned.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmbx8gXJIhwk",
        "outputId": "ef1e8943-d6f6-4b3b-91dc-850d46a42d09"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################\n",
            "MRR @10: 0.18741227770955546\n",
            "QueriesRanked: 6980\n",
            "#####################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use the official TREC evaluation tool, trec_eval, to compute metrics other than MRR@10. For that we first need to convert the run file into TREC format:\n",
        "\n"
      ],
      "metadata": {
        "id": "cYrRnzZ7Sn3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.eval.convert_msmarco_run_to_trec_run \\\n",
        "   --input runs/run.msmarco-passage.bm25tuned.txt \\\n",
        "   --output runs/run.msmarco-passage.bm25tuned.trec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srD7sPpQIhk8",
        "outputId": "76eba321-cb2b-4831-e844-247d92b71be2"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python {path_pyserini_tools}/scripts/msmarco/convert_msmarco_to_trec_qrels.py \\\n",
        "   --input {path_pyserini_tools}/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt \\\n",
        "   --output {path_data}/qrels.dev.small.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqDiOD9jSraq",
        "outputId": "9825b1c2-21a1-4308-d717-2bd5af35924f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then run the trec_eval tool:\n",
        "\n"
      ],
      "metadata": {
        "id": "acky68SpTUj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python {path_pyserini_eval}/trec_eval.py -c -mrecall.1000 -mmap \\\n",
        "   {path_data}/qrels.dev.small.trec runs/run.msmarco-passage.bm25tuned.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "526K0oJWIhV8",
        "outputId": "c65e6bb2-9656-4a5f-acfd-6bdee62c62e1"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-mrecall.1000', '-mmap', '/content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage/qrels.dev.small.trec', 'runs/run.msmarco-passage.bm25tuned.trec']\n",
            "Results:\n",
            "map                   \tall\t0.1957\n",
            "recall_1000           \tall\t0.8573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculando ndcg@10 pelo pyserini"
      ],
      "metadata": {
        "id": "c-EQ6_F2VktR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python {path_pyserini_eval}/trec_eval.py -c -mndcg_cut.10 \\\n",
        "   {path_data}/qrels.dev.small.trec runs/run.msmarco-passage.bm25tuned.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPto96LVFZU_",
        "outputId": "1d893b5f-a261-46dc-881e-35be0720d79b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-mndcg_cut.10', '/content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage/qrels.dev.small.trec', 'runs/run.msmarco-passage.bm25tuned.trec']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.2340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mx54C_BPFZSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxnDyCr6WxIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD CODE"
      ],
      "metadata": {
        "id": "H5PP9YCdF5rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eaTvJd3MF5ni"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L6Y2H8MH14s"
      },
      "source": [
        "## Carga dos dados da TREC 2020 \n",
        "\n",
        "Fonte: [Repositório do projeto Robustez query](https://github.com/leonardo3108/robustez-query)\n",
        "\n",
        "\n",
        "Final Project at Discipline IA376, Deep Learning for NLP, Turma E - Tópicos em Engenharia de Computação VII - 2021.2S\n",
        "\n",
        "Authors: Leonardo Augusto da Silva Pacheco e Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Mais sobre o dataset](https://microsoft.github.io/msmarco/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extração dos julgamentos\n",
        "\n",
        "\n",
        "Obtidos dados de teste do arquivo 2020qrels-pass.txt\n",
        "\n",
        "Só para ciência, também há arquivos com relevância: \n",
        "\n",
        ". Dev: qrels.dev.tsv\n",
        "\n",
        ". Train: qrels.train.tsv "
      ],
      "metadata": {
        "id": "qXX-w6KeydjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -nc https://trec.nist.gov/data/deep/2020qrels-pass.txt"
      ],
      "metadata": {
        "id": "0FH8TFArxdO_",
        "outputId": "640208e8-abb8-40b8-f31a-4e2d4b6657ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-03 23:16:29--  https://trec.nist.gov/data/deep/2020qrels-pass.txt\n",
            "Resolving trec.nist.gov (trec.nist.gov)... 132.163.4.175, 2610:20:6b01:4::175\n",
            "Connecting to trec.nist.gov (trec.nist.gov)|132.163.4.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 218617 (213K) [text/plain]\n",
            "Saving to: ‘2020qrels-pass.txt’\n",
            "\n",
            "2020qrels-pass.txt  100%[===================>] 213.49K  1.19MB/s    in 0.2s    \n",
            "\n",
            "2023-03-03 23:16:29 (1.19 MB/s) - ‘2020qrels-pass.txt’ saved [218617/218617]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passages** were judged on a four-point scale of:\n",
        "* Not Relevant (0), \n",
        "* Related (1), \n",
        "* Highly Relevant (2), and \n",
        "* Perfect (3), \n",
        "\n",
        "where 'Related' is actually NOT Relevant---it means that the passage was on the same general topic, but did not answer the question. \n",
        "\n",
        "Thus, for Passage Ranking task runs (only), to compute evaluation measures that use binary relevance judgments using \n",
        "trec_eval, you either need to use \n",
        "```\n",
        "trec_eval's -l option [trec_eval -l 2 qrelsfile runfile]\n",
        "```\n",
        "or modify the qrels file to change all 1 judgments to 0."
      ],
      "metadata": {
        "id": "5oRR--L5ynlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('query_number passage_identifier judging')\n",
        "queries = []\n",
        "pids = []\n",
        "for i, line in enumerate(open('2020qrels-pass.txt')):\n",
        "    query_nr, _, pid, judging = line.rstrip().split()\n",
        "    if query_nr not in queries:\n",
        "        queries.append(query_nr)\n",
        "    if pid not in pids:\n",
        "        pids.append(pid)\n",
        "    if i < 20: print(query_nr, pid, judging)"
      ],
      "metadata": {
        "id": "MfxH4AphyjX9",
        "outputId": "af07df75-496e-4113-ef41-0d0c41411bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_number passage_identifier judging\n",
            "23849 1020327 2\n",
            "23849 1034183 3\n",
            "23849 1120730 0\n",
            "23849 1139571 1\n",
            "23849 1143724 0\n",
            "23849 1147202 0\n",
            "23849 1150311 0\n",
            "23849 1158886 2\n",
            "23849 1175024 1\n",
            "23849 1201385 0\n",
            "23849 1215556 0\n",
            "23849 1220759 0\n",
            "23849 1221770 0\n",
            "23849 1333480 1\n",
            "23849 1381453 2\n",
            "23849 1414114 2\n",
            "23849 1414115 0\n",
            "23849 1414120 2\n",
            "23849 1449780 0\n",
            "23849 146754 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('{} queries:'.format(len(queries)))\n",
        "print(queries)"
      ],
      "metadata": {
        "id": "9Adf84JAyj_9",
        "outputId": "d95ccf5a-1d76-4650-9baa-85e821edf7bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54 queries:\n",
            "['23849', '42255', '47210', '67316', '118440', '121171', '135802', '141630', '156498', '169208', '174463', '258062', '324585', '330975', '332593', '336901', '390360', '405163', '555530', '583468', '640502', '673670', '701453', '730539', '768208', '877809', '911232', '914916', '938400', '940547', '997622', '1030303', '1037496', '1043135', '1051399', '1064670', '1071750', '1105792', '1106979', '1108651', '1109707', '1110678', '1113256', '1115210', '1116380', '1121353', '1122767', '1127540', '1131069', '1132532', '1133579', '1136043', '1136047', '1136962']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extração das Queries\n",
        "\n",
        "Datasets extraídos a partir de: [TREC 2020 Deep Learning Track Guidelines](https://microsoft.github.io/msmarco/TREC-Deep-Learning-2020)"
      ],
      "metadata": {
        "id": "1_k4m3xzy440"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -nc https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz"
      ],
      "metadata": {
        "id": "lyyaFzFsvVmJ",
        "outputId": "fef6eac1-8067-467d-ac09-2e6d3351a449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-03 23:23:09--  https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz\n",
            "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
            "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18882551 (18M) [application/gzip]\n",
            "Saving to: ‘queries.tar.gz’\n",
            "\n",
            "queries.tar.gz      100%[===================>]  18.01M  9.58MB/s    in 1.9s    \n",
            "\n",
            "2023-03-03 23:23:11 (9.58 MB/s) - ‘queries.tar.gz’ saved [18882551/18882551]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "\n",
        "query_text = {}\n",
        "\n",
        "for i, line in enumerate(gzip.open('queries.tar.gz', mode='rt')):\n",
        "    fields = line.strip().split()\n",
        "    #if i not in [0, 101093, 202185]:\n",
        "    #if i > 202185: print(i)\n",
        "    number = fields[0]\n",
        "    text = ' '.join(fields[1:])\n",
        "    if number in queries:\n",
        "        query_text[number] = text\n",
        "for query in queries:\n",
        "    print(query, query_text.get(query, '?????'))"
      ],
      "metadata": {
        "id": "lNLkcLXGzFLA",
        "outputId": "4e88c9ba-5444-4ee5-dfd2-5aab8bbf4703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23849 are naturalization records public information\n",
            "42255 average salary for dental hygienist in nebraska\n",
            "47210 average wedding dress alteration cost\n",
            "67316 can fever cause miscarriage early pregnancy\n",
            "118440 define bmt medical\n",
            "121171 define etruscans\n",
            "135802 definition of laudable\n",
            "141630 describe how muscles and bones work together to produce movement\n",
            "156498 do google docs auto save\n",
            "169208 does mississippi have an income tax\n",
            "174463 dog day afternoon meaning\n",
            "258062 how long does it take to remove wisdom tooth\n",
            "324585 how much money do motivational speakers make\n",
            "330975 how much would it cost to install my own wind turbine\n",
            "332593 how often to button quail lay eggs\n",
            "336901 how old is vanessa redgrave\n",
            "390360 ia suffix meaning\n",
            "405163 is caffeine an narcotic\n",
            "555530 what are best foods to lower cholesterol\n",
            "583468 what carvedilol used for\n",
            "640502 what does it mean if your tsh is low\n",
            "673670 what is a alm\n",
            "701453 what is a statutory deed\n",
            "730539 what is chronometer who invented it\n",
            "768208 what is mamey\n",
            "877809 what metal are hip replacements made of\n",
            "911232 what type of conflict does della face in o, henry the gift of the magi\n",
            "914916 what type of tissue are bronchioles\n",
            "938400 when did family feud come out?\n",
            "940547 when did rock n roll begin?\n",
            "997622 where is the show shameless filmed\n",
            "1030303 who is aziz hashim\n",
            "1037496 who is rep scalise?\n",
            "1043135 who killed nicholas ii of russia\n",
            "1051399 who sings monk theme song\n",
            "1064670 why do hunters pattern their shotguns?\n",
            "1071750 why is pete rose banned from hall of fame\n",
            "1105792 define: geon\n",
            "1106979 define pareto chart in statistics\n",
            "1108651 what the best way to get clothes white\n",
            "1109707 what medium do radio waves travel through\n",
            "1110678 what is the un fao\n",
            "1113256 what is reba mcentire's net worth\n",
            "1115210 what is chaff and flare\n",
            "1116380 what is a nonconformity? earth science\n",
            "1121353 what can you do about discrimination in the workplace in oklahoma city\n",
            "1122767 what amino produces carnitine\n",
            "1127540 meaning of shebang\n",
            "1131069 how many sons robert kraft has\n",
            "1132532 average annual income data analyst\n",
            "1133579 how does granulation tissue start\n",
            "1136043 difference between a hotel and motel\n",
            "1136047 difference between a company's strategy and business model is\n",
            "1136962 why did the ancient egyptians call their land kemet, or black land?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(queries), len(query_text)"
      ],
      "metadata": {
        "id": "tRon5GAKzGwU",
        "outputId": "a6fa715b-303b-4eb6-d9a8-c3f602144c6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54, 54)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extração das passagens do MsMarco"
      ],
      "metadata": {
        "id": "qirqA5Y0zep9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -nc https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz"
      ],
      "metadata": {
        "id": "SmOnTax9zXAU",
        "outputId": "ea0417a8-1590-46dd-b0f7-7b46603db72d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-03 23:25:32--  https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz\n",
            "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
            "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1035009698 (987M) [application/octet-stream]\n",
            "Saving to: ‘collection.tar.gz’\n",
            "\n",
            "collection.tar.gz   100%[===================>] 987.06M  6.73MB/s    in 1m 57s  \n",
            "\n",
            "2023-03-03 23:27:30 (8.44 MB/s) - ‘collection.tar.gz’ saved [1035009698/1035009698]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "61qyv5ob2hqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "passage_text = {}\n",
        "\n",
        "for line in tqdm(gzip.open('collection.tar.gz', mode='rt'), unit='B', unit_scale=True, total=os.path.getsize('collection.tar.gz')):\n",
        "    fields = line.strip().split()\n",
        "    pid = fields[0]\n",
        "    text = ' '.join(fields[1:])\n",
        "    if pid in pids:\n",
        "        passage_text[pid] = text\n",
        "for ndx, pid in enumerate(pids):\n",
        "    print(pid, passage_text.get(pid, '?????'))\n",
        "    if ndx > 10: \n",
        "      break"
      ],
      "metadata": {
        "id": "EvKBXl4-zxvQ",
        "outputId": "1cd8355f-bcc1-4da3-9671-fa5c80b8e2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 5.39M/1.04G [13:33<43:11:55, 6.62kB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pids), len(passage_text)"
      ],
      "metadata": {
        "id": "edStuwPBzoNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e007dYSoJ-zT"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}