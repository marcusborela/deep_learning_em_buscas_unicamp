{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcN_5-RDWeqV"
      },
      "source": [
        "# Aula3 - Resolvendo Tarefas com LLM (Large Language Model) de Maneira Zero e Few-shot\n",
        "\n",
        "[Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)\n",
        "\n",
        "[Link para chat de apoio com WebChatGPT](https://github.com/marcusborela/deep_learning_em_buscas_unicamp/blob/main/chat/aula3_resolvendo_tarefas_com_llm_de_maneira_zero_e_few_shot.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1aFWTVgejM"
      },
      "source": [
        "[![Open In Colab latest github version](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcusborela/deep_learning_em_buscas_unicamp/blob/main/code/aula_2_classificacao_de_texto_e_reranqueador.ipynb) [Open In Colab latest github version]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQxzYKGgMqce"
      },
      "source": [
        "# Enunciado exercício\n",
        "O aluno irá escolher uma tarefa para resolver de maneira zero ou few-shot. Sugestões:\n",
        "- Classificação de textos (ex: análise de sentimos (IMDB))\n",
        "- Predizer se uma passagem/parágrafo é relevante para uma pergunta/query\n",
        "- Se uma resposta predita por um sistema de QA ou sumarizador é semanticamente igual à resposta ground-truth\n",
        "\n",
        "\n",
        "É importante ter uma função de avaliação da qualidade das respostas do modelo few-shot. Por exemplo, acurácia.\n",
        "\n",
        "\n",
        "É possível criar um pequeno dataset de teste manualmente (ex: com 10 à 100 exemplos)\n",
        "\n",
        "\n",
        "- Usar a API do LLAMA fornecida por nós (licença exclusiva para pesquisa). [Colab demo da API do LLAMA](https://colab.research.google.com/drive/1zZ-ch29LTicNPA62t2MaOwMROywnqUxf?usp=sharing) (obrigado, Thales Rogério)\n",
        "- Opcionalmente, usar a API do code-davinci-002, que é de graça e trás resultados muito bons.\n",
        "CUIDADO: NÃO USAR O TEXT-DAVINCI-002/003, que é pago\n",
        "\n",
        "- Opcionalmente, usar a API do ChatGPT (gpt-3.5-turbo) que é barata: ~1 centavo de real por 1000 tokens (uma página)\n",
        "- Opcionalmente, usar o Alpaca: https://alpaca-ai.ngrok.io/\n",
        "\n",
        "\n",
        "Dicas:\n",
        "- Teste com zero-shot E few-shot.\n",
        "- No few-shot, faça testes com e sem instruções no cabeçalho (explicação da tarefa, ex: \"Traduza de Ingles para Portugues\"). Pode ser que sem a instrução o modelo até funcione melhor.\n",
        "- Siga sempre um padrão ao criar os exemplos few-shot. Aqui tem uma pagina com dicas para prompt engineering: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dicas\n",
        "\n",
        "[Exemplos de prompt](https://platform.openai.com/examples) \n"
      ],
      "metadata": {
        "id": "r_EIoey0Rqi2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmRLgbyi_Dvg"
      },
      "source": [
        "# Organizando o ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "a8bba3e5-9ad2-42e7-ad79-5a9a84bdd63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DKAZ8CWCAM3-"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria(lista_mem=['cpu']):\n",
        "  \"\"\"\n",
        "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
        "\n",
        "  Parâmetros:\n",
        "  -----------\n",
        "  lista_mem : list, opcional\n",
        "      Lista com strings 'cpu' e/ou 'gpu'. \n",
        "      'cpu' - exibe informações de memória da CPU.\n",
        "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
        "      O valor padrão é ['cpu'].\n",
        "\n",
        "  Saída:\n",
        "  -------\n",
        "  A função não retorna nada, apenas exibe as informações na tela.\n",
        "\n",
        "  Exemplo de uso:\n",
        "  ---------------\n",
        "  Para exibir informações de memória da CPU:\n",
        "      mostra_memoria(['cpu'])\n",
        "\n",
        "  Para exibir informações de memória da CPU e GPU:\n",
        "      mostra_memoria(['cpu', 'gpu'])\n",
        "  \n",
        "  Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "  \"\"\"  \n",
        "  if 'cpu' in lista_mem:\n",
        "    vm = virtual_memory()\n",
        "    ram={}\n",
        "    ram['total']=round(vm.total / 1e9,2)\n",
        "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
        "    print('/nGPU')\n",
        "    gpu_info = !nvidia-smi\n",
        "  if 'gpu' in lista_mem:\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "9e657f51-db71-4f32-805a-2183563fe8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 13.62\n",
            " available 12.44\n",
            " used 0.88\n",
            " free 9.32\n",
            " cached 3.06\n",
            " buffers 0.37\n",
            "/nGPU\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xRdgUGMPgh"
      },
      "source": [
        "### Vinculando pasta do google drive para salvar dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IsJiN6H8K6pe"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae-Iy2oz_9os",
        "outputId": "8487de23-8a50-4083-d99b-c899ff1668af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5y1AUxJ_ZfzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "achvQ78sa3p3"
      },
      "source": [
        "## Fixando as seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "AG9RjMb8Qlot"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bkETIyWGkbOf"
      },
      "outputs": [],
      "source": [
        "def inicializa_seed(num_semente:int=123):\n",
        "  \"\"\"\n",
        "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
        "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
        "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
        "  \n",
        "  Args:\n",
        "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
        "  \n",
        "  References:\n",
        "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
        "  \"\"\"\n",
        "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
        "  random.seed(num_semente)\n",
        "  np.random.seed(num_semente)\n",
        "  torch.manual_seed(num_semente)\n",
        "  \n",
        "  # Define as sementes da GPU\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  #torch.cuda.manual_seed(num_semente)\n",
        "  #Cuda algorithms\n",
        "  #torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "UJGxQIrdaVp4"
      },
      "outputs": [],
      "source": [
        "num_semente=123\n",
        "inicializa_seed(num_semente)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2gtkEPhA0t"
      },
      "source": [
        "## Preparando para debug e display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "BJ6S4P5Hw4iG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kebsl1uQDFUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1ed269-722b-4a2a-f1a0-908d1683cc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnR2kDS_2FgZ"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZEqQ7mKg5fs"
      },
      "source": [
        "Dicas em https://zohaib.me/debugging-in-google-collab-notebook/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjrlXHq1hC8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d94254-4bf8-4632-ef84-012727944746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.3/793.3 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 KB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "# %pdb off # desativa debug em exceção\n",
        "# %pdb on  # ativa debug em exceção\n",
        "# ipdb.set_trace(context=8)  para execução nesse ponto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wQ5pmlOHxHhk"
      },
      "outputs": [],
      "source": [
        "def config_display():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de display do Pandas.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurando formato saída Pandas\n",
        "  # define o número máximo de colunas que serão exibidas\n",
        "  pd.options.display.max_columns = None\n",
        "\n",
        "  # define a largura máxima de uma linha\n",
        "  pd.options.display.width = 1000\n",
        "\n",
        "  # define o número máximo de linhas que serão exibidas\n",
        "  pd.options.display.max_rows = 100\n",
        "\n",
        "  # define o número máximo de caracteres por coluna\n",
        "  pd.options.display.max_colwidth = 50\n",
        "\n",
        "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
        "  pd.options.display.show_dimensions = True\n",
        "\n",
        "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
        "  pd.options.display.precision = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2tDy72ATNHs"
      },
      "outputs": [],
      "source": [
        "def config_debug():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
        "  transformers e datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define opções de impressão de tensores para o modo científico\n",
        "  torch.set_printoptions(sci_mode=True) \n",
        "  \"\"\"\n",
        "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
        "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
        "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
        "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
        "    uma melhor compreensão dos números envolvidos.  \n",
        "  \"\"\"\n",
        "\n",
        "  # Habilita detecção de anomalias no autograd do PyTorch\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  \"\"\"\n",
        "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
        "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
        "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
        "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
        "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
        "    antes que se torne um problema maior.\n",
        "\n",
        "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
        "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
        "    ela deve ser usada com cautela e apenas para depuração.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "  \"\"\"\n",
        "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
        "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
        "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
        "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
        "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
        "    e removê-lo após a solução do problema.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define o nível de verbosity do pacote transformers para info\n",
        "  transformers.utils.logging.set_verbosity_info() \n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
        "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
        "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
        "\n",
        "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
        "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
        "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
        "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
        "    necessidade de cada tarefa.\n",
        "  \n",
        "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
        "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
        "  \n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define o modo verbose do xmode, que é utilizado no debug\n",
        "  %xmode Verbose \n",
        "\n",
        "  \"\"\"\n",
        "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
        "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
        "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
        "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
        "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
        "\n",
        "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
        "    comentar a linha acima e descomentar a linha abaixo:\n",
        "    %xmode Plain\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "    Dica:\n",
        "    1.  pdb (Python Debugger)\n",
        "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
        "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
        "\n",
        "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
        "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
        "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
        "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
        "\n",
        "\n",
        "    2. ipdb\n",
        "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
        "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
        "      \n",
        "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
        "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
        "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
        "      comandos para investigar o comportamento.\n",
        "\n",
        "      Durante a depuração, você pode usar comandos:\n",
        "        next (para executar a próxima linha de código), \n",
        "        step (para entrar em uma função chamada na próxima linha de código) \n",
        "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
        "\n",
        "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
        "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
        "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Tb4aqtcExR84"
      },
      "outputs": [],
      "source": [
        "config_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Bq4043fkfh",
        "outputId": "fa8e5db1-1feb-4393-fb66-d394d1ad693c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception reporting mode: Verbose\n"
          ]
        }
      ],
      "source": [
        "# config_debug()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentando chamadas aos LLM"
      ],
      "metadata": {
        "id": "4WJ9VOUMHFz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_modelos = {'gpt-3.5-turbo': {'max_tokens': 4096},  # $0.002 / 1K tokens\n",
        "                'code-davinci-002': {'max_tokens': 8001}} # gratuito"
      ],
      "metadata": {
        "id": "DJPeXy97SC1P"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat GPT (Modelo gpt-3.5-turbo)"
      ],
      "metadata": {
        "id": "RSL3JJSRHPgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para uso do gpt-3.5-turbo, usamos como referência o caderno da [openai: How_to_format_inputs_to_ChatGPT_models.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)"
      ],
      "metadata": {
        "id": "J2gFIL0xIPk0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPPjYkUTCG3X"
      },
      "source": [
        "### How to format inputs to ChatGPT models\n",
        "\n",
        "ChatGPT is powered by `gpt-3.5-turbo`, OpenAI's most advanced model.\n",
        "\n",
        "You can build your own applications with `gpt-3.5-turbo` using the OpenAI API.\n",
        "\n",
        "Chat models take a series of messages as input, and return an AI-written message as output.\n",
        "\n",
        "This guide illustrates the chat format with a few example API calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9bepRrfCG3a"
      },
      "source": [
        "### Import the openai library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UA9ergb1CG3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2878ab3e-92c4-4d7f-b241-169de850f14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
        "%pip install --upgrade openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2e7MPhAdCG3c"
      },
      "outputs": [],
      "source": [
        "# import the OpenAI Python library for calling the OpenAI API\n",
        "import openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42EWgSy0CG3d"
      },
      "source": [
        "### An example chat API call\n",
        "\n",
        "A chat API call has two required inputs:\n",
        "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`)\n",
        "- `messages`: a list of message objects, where each object has at least two fields:\n",
        "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
        "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
        "\n",
        "Typically, a conversation will start with a system message, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
        "\n",
        "Let's look at an example chat API calls to see how the chat format works in practice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass"
      ],
      "metadata": {
        "id": "4fhQJQgkEAfY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = getpass.getpass(\"Entre a OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrefch9xDnso",
        "outputId": "c3d42e7a-5bb2-4147-d2f1-d86192b0a3b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entre a OPENAI_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WSgy_fuXCG3d"
      },
      "outputs": [],
      "source": [
        "MODEL = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Marcus.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")"
      ],
      "metadata": {
        "id": "u4sbukooCcYr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhCsPcVqCa_g",
        "outputId": "7f47abdf-c5da-4de9-d2cb-81ab97a1a0d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-6vrjUfjDVqeIcZV49F7tHbcgFMiEx at 0x7ff7b51548b0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"Marcus who?\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1679249264,\n",
              "  \"id\": \"chatcmpl-6vrjUfjDVqeIcZV49F7tHbcgFMiEx\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 4,\n",
              "    \"prompt_tokens\": 38,\n",
              "    \"total_tokens\": 42\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll0CNznzCG3e"
      },
      "source": [
        "As you can see, the response object has a few fields:\n",
        "- `id`: the ID of the request\n",
        "- `object`: the type of object returned (e.g., `chat.completion`)\n",
        "- `created`: the timestamp of the request\n",
        "- `model`: the full name of the model used to generate the response\n",
        "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
        "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
        "    - `message`: the message object generated by the model, with `role` and `content`\n",
        "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
        "    - `index`: the index of the completion in the list of choices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPknWoscCG3f"
      },
      "source": [
        "Extract just the reply with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RzJWlWJQCG3f",
        "outputId": "13317c64-1857-454f-cccf-4dcf5defe00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Marcus who?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tZSva3FCG3g"
      },
      "source": [
        "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
        "\n",
        "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBxLujRCCG3g",
        "outputId": "a0c96c44-da7b-4c05-b571-e9c4b2714f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ahoy matey! Asynchronous programming be like havin' a crew o' pirates workin' on different tasks at the same time. Ye see, instead o' waitin' for one task to be completed before startin' the next, we can have multiple tasks runnin' at once. It be like havin' me crew hoistin' the sails while others be swabbin' the deck and loadin' the cannons. Each task be workin' independently, but they all be contributin' to the overall success o' the ship. And just like how me crew communicates with each other to make sure everything be runnin' smoothly, asynchronous programming uses callbacks and promises to coordinate the different tasks and make sure they all be finished in the right order. Arrr, it be a powerful tool for any programmer lookin' to optimize their code and make it run faster.\n"
          ]
        }
      ],
      "source": [
        "# example with a system message\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw0pEd9NCG3g",
        "outputId": "6830a352-cd8c-4cd7-a4f6-b6bd359bce88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Ahoy mateys! Let me tell ye about asynchronous programming, arrr! \n",
            "\n",
            "Ye see, in the world of programming, sometimes we need to wait for things to happen before we can move on to the next task. But with asynchronous programming, we can keep working on other tasks while we wait for those things to happen. \n",
            "\n",
            "It's like when we're sailing the high seas and we need to wait for the wind to change direction. We don't just sit there twiddling our thumbs, do we? No, we keep busy with other tasks like repairing the ship or checking the maps. \n",
            "\n",
            "In programming, we use something called callbacks or promises to keep track of those things we're waiting for. And while we wait for those callbacks or promises to be fulfilled, we can keep working on other parts of our code. \n",
            "\n",
            "So, me hearties, asynchronous programming is like being a pirate on the high seas, always ready to tackle the next task while we wait for the winds to change. Arrr!\n"
          ]
        }
      ],
      "source": [
        "# example without a system message\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "824RhtrqCG3h"
      },
      "source": [
        "### Tips for instructing gpt-3.5-turbo-0301\n",
        "\n",
        "Best practices for instructing models may change from model version to model version. The advice that follows applies to `gpt-3.5-turbo-0301` and may not apply to future models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-lPZSqfCG3h"
      },
      "source": [
        "#### System messages\n",
        "\n",
        "The system message can be used to prime the assistant with different personalities or behaviors.\n",
        "\n",
        "However, the model does not generally pay as much attention to the system message, and therefore we recommend placing important instructions in the user message instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdoD0GmzCG3h",
        "outputId": "3fb4af7c-0e5c-4792-b4b7-918b5d0a9bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Fractions are a way of representing a part of a whole. The top number of a fraction is called the numerator, and it represents how many parts of the whole we are talking about. The bottom number is called the denominator, and it represents how many equal parts the whole is divided into.\n",
            "\n",
            "For example, if we have a pizza that is divided into 8 equal slices, and we take 3 slices, we can represent that as the fraction 3/8. The numerator is 3 because we took 3 slices, and the denominator is 8 because the pizza was divided into 8 slices.\n",
            "\n",
            "To add or subtract fractions, we need to have a common denominator. This means that the denominators of the fractions need to be the same. To do this, we can find the least common multiple (LCM) of the denominators and then convert each fraction to an equivalent fraction with the LCM as the denominator.\n",
            "\n",
            "To multiply fractions, we simply multiply the numerators together and the denominators together. To divide fractions, we multiply the first fraction by the reciprocal of the second fraction (flip the second fraction upside down).\n",
            "\n",
            "Now, here's a question to check for understanding: If we have a pizza that is divided into 12 equal slices, and we take 4 slices, what is the fraction that represents how much of the pizza we took?\n"
          ]
        }
      ],
      "source": [
        "# An example of a system message that primes the assistant to explain concepts in great depth\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_VhnkQ0CG3i",
        "outputId": "5b242939-5a9b-4944-e6f7-8f93886474b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fractions represent a part of a whole. They consist of a numerator (top number) and a denominator (bottom number) separated by a line. The numerator represents how many parts of the whole are being considered, while the denominator represents the total number of equal parts that make up the whole.\n"
          ]
        }
      ],
      "source": [
        "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUWsG7RCG3i"
      },
      "source": [
        "#### Few-shot prompting\n",
        "\n",
        "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
        "\n",
        "One way to show the model what you want is with faked example messages.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgWLtb92CG3i",
        "outputId": "679560f2-f767-4e2c-dd92-aa0f24a8f0e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We don't have enough time to complete everything perfectly for the client.\n"
          ]
        }
      ],
      "source": [
        "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
        "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfC1CF9bCG3i"
      },
      "source": [
        "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can instead set the `name` field of `system` messages to `example_user` and `example_assistant`.\n",
        "\n",
        "Transforming the few-shot example above, we could write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hULcxYX8CG3j",
        "outputId": "d0a41e8d-3fbb-44a2-f86a-2ae0b5f909d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This sudden change in plans means we don't have enough time to do everything for the client's project.\n"
          ]
        }
      ],
      "source": [
        "# The business jargon translation example, but with example names for the example messages\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KzutVliCG3j"
      },
      "source": [
        "Not every attempt at engineering conversations will succeed at first.\n",
        "\n",
        "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
        "\n",
        "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
        "\n",
        "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbCTtNFWCG3j"
      },
      "source": [
        "## Counting tokens OpenAI Models\n",
        "\n",
        "Mais detalhes em [OpenAI: How_to_count_tokens_with_tiktoken.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
        "\n",
        "When you submit your request, the API transforms the messages into a sequence of tokens.\n",
        "\n",
        "The number of tokens used affects:\n",
        "- the cost of the request\n",
        "- the time it takes to generate the response\n",
        "- when the reply gets cut off from hitting the maximum token limit (4096 for `gpt-3.5-turbo`)\n",
        "\n",
        "As of Mar 01, 2023, you can use the following function to count the number of tokens that a list of messages will use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G0Sd0Y5FQn2",
        "outputId": "80f6f9e1-571c-4f37-e1e1-de31535e2bde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xxl5YAOjCG3j"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":  # if there's a name, the role is omitted\n",
        "                    num_tokens += -1  # role is always required and always 1 token\n",
        "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n"
      ],
      "metadata": {
        "id": "CYfsV-1AFKYu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DU955UeOCG3j"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
        "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "    {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fh0gAS6zCG3k",
        "outputId": "bcf92f77-5a9a-4b93-cba9-9192a1056a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126 prompt tokens counted.\n"
          ]
        }
      ],
      "source": [
        "# example token count from the function defined above\n",
        "print(f\"{num_tokens_from_messages(messages)} prompt tokens counted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yFVFizlyCG3k"
      },
      "outputs": [],
      "source": [
        "# example token count from the OpenAI API\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ncUkSxFql1",
        "outputId": "316e76e6-dd5a-4681-92b5-2cbba4781fc2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-6vrpIjOeqDsCjjiymvv9NEtaRvphY at 0x7ff7b4fc58b0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"This sudden change in plans means we don't have enough time to do everything for the client's project.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1679249624,\n",
              "  \"id\": \"chatcmpl-6vrpIjOeqDsCjjiymvv9NEtaRvphY\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 22,\n",
              "    \"prompt_tokens\": 126,\n",
              "    \"total_tokens\": 148\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goDW5l1jFpGl",
        "outputId": "c530c03d-7ab5-4bfb-ef06-58bb5fe519b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126 prompt tokens used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WJ3ZmEDkCG3k"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"tiktoken is great!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjtWUNbFUusD",
        "outputId": "5aab5f88-f51f-4d4c-b379-296ef626556f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 1609, 5963, 374, 2294, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBFhOkAVRZq",
        "outputId": "ca437d9e-a100-4df3-ef0a-5dbd3a66482c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b't', b'ik', b'token', b' is', b' great', b'!']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "rOXZ3gibUycK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuCoumBTU1hy",
        "outputId": "9bca10b4-a27e-41ba-d9c8-1b66b9df0c4b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code-davinci-002"
      ],
      "metadata": {
        "id": "8rE1ZbFvH-fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para uso do code-davinci-00, usamos como referência o caderno da [openai: Unit_test_writing_using_a_multi-step_prompt.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt.ipynb)"
      ],
      "metadata": {
        "id": "VlmQYjBHIS57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dicas para iteração com esse modelo em https://platform.openai.com/docs/guides/code/best-practices"
      ],
      "metadata": {
        "id": "SYFNiURiMb3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"code-davinci-002\""
      ],
      "metadata": {
        "id": "UBi552vwIF6K"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_teste = 'Write a function in python that calculates fibonacci'"
      ],
      "metadata": {
        "id": "QdF_JFnAMmhQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens:int = 1000\n",
        "temperature:float = 1.0"
      ],
      "metadata": {
        "id": "ufDazzgSM23n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "        model=MODEL,\n",
        "        prompt=prompt_teste,\n",
        "        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\", \"```\"],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stream=False)"
      ],
      "metadata": {
        "id": "dRmTl06rLgFz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6-judNTNWew",
        "outputId": "6c65da47-2517-49e1-913a-fa14b9bd2f3b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-6vsLlR8hqWEuay0f6FIulqpWnBIrD at 0x7ff7ae8b2c20> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \" sequence. Like f(n) = f(n-1) + f(n-2) + ... + f(3) + f(2) + f(1). For example   f(6) = f(5) + f(4) = 21.\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1679251637,\n",
              "  \"id\": \"cmpl-6vsLlR8hqWEuay0f6FIulqpWnBIrD\",\n",
              "  \"model\": \"code-davinci-002\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 58,\n",
              "    \"prompt_tokens\": 10,\n",
              "    \"total_tokens\": 68\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_teste = 'Meu nome é Marcus. Moro no Brasil. A capital do Brasil é: '"
      ],
      "metadata": {
        "id": "6OY-hayXNjfG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "        model=MODEL,\n",
        "        prompt=prompt_teste,\n",
        "        # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\", \"```\"],\n",
        "        max_tokens=10,\n",
        "        temperature=temperature,\n",
        "        stream=False)"
      ],
      "metadata": {
        "id": "GbGiI_q9NiUY"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f96891-ade9-47dc-ba29-dd49a368586c",
        "id": "1nQ4ctVzNiUY"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-6vsOFHPFFhUpS9RcKHeatc55S8Duh at 0x7ff7b51544f0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"Bras\\u00edlia. Meu e-mail \\u00e9\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1679251791,\n",
              "  \"id\": \"cmpl-6vsOFHPFFhUpS9RcKHeatc55S8Duh\",\n",
              "  \"model\": \"code-davinci-002\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 10,\n",
              "    \"prompt_tokens\": 20,\n",
              "    \"total_tokens\": 30\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLAMA "
      ],
      "metadata": {
        "id": "TKh6Zj9FH5Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Colab demo da API do LLAMA](https://colab.research.google.com/drive/1zZ-ch29LTicNPA62t2MaOwMROywnqUxf?usp=sharing) (obrigado, Thales Rogério)"
      ],
      "metadata": {
        "id": "x2C983lKXdIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "N3k64gPuH7yc"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url=\"http://143.106.167.108/api\""
      ],
      "metadata": {
        "id": "9xyqOr8MH9jK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhyvNKBkXoXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={\n",
        "\t\"prompt\":\"\"\"Given table, specify which rows have repeated values for both \"Item number\" and \"Local\". If no row is repeated say \"no repeats\".\n",
        "\n",
        "Example 1:\n",
        "|Row | Item number | Local |\n",
        "|1 |  3 5 7 | New York |\n",
        "|2|  5 8 2 | Madagascar |\n",
        "|3|  3 4 5 | New York |\n",
        "|4|  3 4 5 | Paris |\n",
        "\n",
        "Explanation: Rows 1 and 3 have the same local \"New York\" and the same item number \"3 4 5\". Therefore they are repeated.\n",
        "\n",
        "Answer: (1,3).\n",
        "\n",
        "Example 2:\n",
        "|Row | Item number | Local |\n",
        "|1 |  0 9 2 4 | Amsterdam |\n",
        "|2|  9 4 2 4 | Barcelona |\n",
        "|3|  7 3 2 | London |\n",
        "|4|  7 3 1 | London |\n",
        "|5|  7 3 2 | London |\n",
        "|6|  7 3 2 | London |\n",
        "|7|  7 3 2 | London |\n",
        "|8|  7 3  2 |  New York |\n",
        "|9 |  0 9 2 4 | Amsterdam |\n",
        "\n",
        "Explanation:\"\"\",\n",
        "\n",
        "\"temperature\": 0.0,\n",
        "\"top_p\": 1,\n",
        "\"max_length\": 250\n",
        "}\n",
        "\n",
        "r=requests.post(f\"{base_url}/complete\", json=data)"
      ],
      "metadata": {
        "id": "XO3YNLXsIG2l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if r.ok:\n",
        "  response=r.json()\n",
        "  print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71W7rBZXYOV4",
        "outputId": "07067a5c-d1ea-495f-c6fe-7f085a19fc97"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'Given table, specify which rows have repeated values for both \"Item number\" and \"Local\". If no row is repeated say \"no repeats\".\\n\\nExample 1:\\n|Row | Item number | Local |\\n|1 |  3 5 7 | New York |\\n|2|  5 8 2 | Madagascar |\\n|3|  3 4 5 | New York |\\n|4|  3 4 5 | Paris |\\n\\nExplanation: Rows 1 and 3 have the same local \"New York\" and the same item number \"3 4 5\". Therefore they are repeated.\\n\\nAnswer: (1,3).\\n\\nExample 2:\\n|Row | Item number | Local |\\n|1 |  0 9 2 4 | Amsterdam |\\n|2|  9 4 2 4 | Barcelona |\\n|3|  7 3 2 | London |\\n|4|  7 3 1 | London |\\n|5|  7 3 2 | London |\\n|6|  7 3 2 | London |\\n|7|  7 3 2 | London |\\n|8|  7 3  2 |  New York |\\n|9 |  0 9 2 4 | Amsterdam |\\n\\nExplanation:', 'temperature': 0.0, 'top_p': 1.0, 'max_length': 250, 'stopping_tokens': [], 'request_uuid': '6b526e94-986c-4519-adc5-2519f7e6705a'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiNo-zmnYLmo",
        "outputId": "253bcfd7-9478-4821-9cc0-b94ead909f9a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': 'Given table, specify which rows have repeated values for both \"Item number\" and \"Local\". If no row is repeated say \"no repeats\".\\n\\nExample 1:\\n|Row | Item number | Local |\\n|1 |  3 5 7 | New York |\\n|2|  5 8 2 | Madagascar |\\n|3|  3 4 5 | New York |\\n|4|  3 4 5 | Paris |\\n\\nExplanation: Rows 1 and 3 have the same local \"New York\" and the same item number \"3 4 5\". Therefore they are repeated.\\n\\nAnswer: (1,3).\\n\\nExample 2:\\n|Row | Item number | Local |\\n|1 |  0 9 2 4 | Amsterdam |\\n|2|  9 4 2 4 | Barcelona |\\n|3|  7 3 2 | London |\\n|4|  7 3 1 | London |\\n|5|  7 3 2 | London |\\n|6|  7 3 2 | London |\\n|7|  7 3 2 | London |\\n|8|  7 3  2 |  New York |\\n|9 |  0 9 2 4 | Amsterdam |\\n\\nExplanation:',\n",
              " 'temperature': 0.0,\n",
              " 'top_p': 1.0,\n",
              " 'max_length': 250,\n",
              " 'stopping_tokens': [],\n",
              " 'request_uuid': '6b526e94-986c-4519-adc5-2519f7e6705a'}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the request_uuid to check if the completion job is done"
      ],
      "metadata": {
        "id": "H-kEgjHqImqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request_uuid=response[\"request_uuid\"]"
      ],
      "metadata": {
        "id": "Minx0TGWIu-D"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "lkLZZTcOIRIB"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ready = False\n",
        "while not ready:\n",
        "    r = requests.get(f\"{base_url}/get_result/{request_uuid}\")\n",
        "    response = r.json()\n",
        "    ready = response['ready']\n",
        "    if ready:\n",
        "        print(response['generated_text'])\n",
        "        break\n",
        "    # Wait 10 seconds before checking again\n",
        "    time.sleep(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr0aQCoNYfmQ",
        "outputId": "34c13dec-0848-4be1-d53a-6d2e36dde333"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Rows 3-7 all have the same local \"London\", but their item numbers differ. Therefore there are no repeats in this example.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "when consulting the result you may find 3 scenarios\n",
        "- Your job did not run yet, you should try again in a couple of seconds (Ready=False, message=None)\n",
        "- Your job did run and everything worked (Ready=True, message=your response)\n",
        "- Your job did run but it failed (Ready=True, message=None)"
      ],
      "metadata": {
        "id": "GkwfJwtnI3eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rate limiting\n",
        "\n",
        "We may adjust this during the week, but due to computational constrains we will apply a rate limit of about 2 requests per 5 seconds. If you exceed this limit you will receive an error 429. You should adjust your code accordingly.\n",
        "\n",
        "Please remember that the whole class is using a shared resource, so avoid excessive requests even if they are under the rate limit.\n",
        "\n",
        "If you encounter any errors or problems, let us know in the classroom."
      ],
      "metadata": {
        "id": "ShA1aBfNJUI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30):\n",
        "  r=requests.get(f\"{base_url}/get_result/{request_uuid}\")\n",
        "  print(i, \"->\", r.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z3s-PFPJSVm",
        "outputId": "ec8252e2-0d3d-4331-a9df-629f36677b46"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -> 200\n",
            "1 -> 200\n",
            "2 -> 200\n",
            "3 -> 429\n",
            "4 -> 429\n",
            "5 -> 429\n",
            "6 -> 429\n",
            "7 -> 429\n",
            "8 -> 429\n",
            "9 -> 429\n",
            "10 -> 429\n",
            "11 -> 429\n",
            "12 -> 200\n",
            "13 -> 429\n",
            "14 -> 429\n",
            "15 -> 429\n",
            "16 -> 429\n",
            "17 -> 429\n",
            "18 -> 429\n",
            "19 -> 429\n",
            "20 -> 429\n",
            "21 -> 429\n",
            "22 -> 429\n",
            "23 -> 200\n",
            "24 -> 429\n",
            "25 -> 429\n",
            "26 -> 429\n",
            "27 -> 429\n",
            "28 -> 429\n",
            "29 -> 429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGpuNTU3ZHMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4WJ9VOUMHFz1",
        "P9bepRrfCG3a",
        "42EWgSy0CG3d",
        "824RhtrqCG3h",
        "u-lPZSqfCG3h",
        "cTUWsG7RCG3i",
        "8rE1ZbFvH-fL"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}