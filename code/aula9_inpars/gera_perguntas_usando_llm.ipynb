{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CcN_5-RDWeqV"
      },
      "source": [
        "# Aula 9 - InPars\n",
        "\n",
        "[Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VQxzYKGgMqce"
      },
      "source": [
        "# Enunciado exercício\n",
        "\n",
        "\n",
        "Objetivo: gerar dataset para treino de modelos de buscas usando a técnica do InPars e avaliar um modelo reranqueador treinado neste dataset no TREC-COVID:\n",
        "\n",
        "Entrada: 3-5 exemplos few-shot + documento amostrado da coleção do TREC-COVID\n",
        "\n",
        "Saída: query que seja relevante para o documento amostrado\n",
        "\n",
        "É opcional fazer a etapa de filtragem usando as queries de maior prob descrita no Artigo.\n",
        "\n",
        "Como modelo gerador, use um dos seguintes modelos:\n",
        "ChatGPT-3.5-turbo: ~1 USD para cada 1k exemplos\n",
        "FLAN-T5 (base, large ou XL), LLAMA-(7,13B), Alpaca-(7/13B), que são possiveis de rodar no Colab Pro.\n",
        "\n",
        "Também tem a inference-api da HF: https://huggingface.co/inference-api.\n",
        "\n",
        "Com exceção do LLAMA, é possivel usar zero-shot ao inves de few-shot.\n",
        "Dado 1k-10k pares <query sintética; documento>, treinar um modelo reranqueador miniLM igual ao da aula 2/3.\n",
        "\n",
        "Exemplos negativos (i.e., <query sintética; doc não relevant) vem do BM25: dado a query sintetica, retornar top 1000 com o BM25, e amostrar aleatoriamente alguns documentos como negativo\n",
        "\n",
        "Começar treino do miniLM já treinado no MS MARCO\n",
        "\n",
        "Avaliar no TREC-COVID e comparar com o reranqueador apenas treinado no MSMARCO\n",
        "\n",
        "Nota: Também usar o dataset dos colegas para obter diversidade de exemplos: Assim que tiver gerado o dataset sintético, favor colocar na planilha, assim outras pessoas podem usa-lo.\n",
        "- Para aumentar a aleatoriedade, seed usada deve o seu numero na planilha.\n",
        "\n",
        "Colocar dataset no formato jsonlines:\n",
        "{\"query\": query, \"positive_doc_id\": doc_id, \"negative_doc_ids\": [opcional]}\\n \n",
        "\n",
        "\n",
        "\n",
        "Dicas: (do exercício da aula 2)\n",
        "\n",
        "- Siga sempre um padrão ao criar os exemplos few-shot. Aqui tem uma pagina com dicas para prompt engineering: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api\n",
        "\n",
        "\n",
        "- Usar a API do LLAMA fornecida por nós (licença exclusiva para pesquisa). [Colab demo da API do LLAMA](https://colab.research.google.com/drive/1zZ-ch29LTicNPA62t2MaOwMROywnqUxf?usp=sharing) (obrigado, Thales Rogério)\n",
        "- Opcionalmente, usar a API do code-davinci-002, que é de graça e trás resultados muito bons.\n",
        "CUIDADO: NÃO USAR O TEXT-DAVINCI-002/003, que é pago\n",
        "\n",
        "- Opcionalmente, usar a API do ChatGPT (gpt-3.5-turbo) que é barata: ~1 centavo de real por 1000 tokens (uma página)\n",
        "  \n",
        "- Opcionalmente, usar o Alpaca: https://alpaca-ai.ngrok.io/\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este caderno contempla os passos 1 (criar prompt) e 2 (gerar queries) do [fluxo de processamento](https://github.com/marcusborela/deep_learning_em_buscas_unicamp/blob/main/presentations/articles/Aula%208%20-%20InPars%20Process.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmRLgbyi_Dvg"
      },
      "source": [
        "# Organizando o ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCY6qqxCXrd"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gHPHUE5PCZdu"
      },
      "outputs": [],
      "source": [
        "import requests  # para Llama\n",
        "import time # para Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AEUf1Hk5dQba"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6pLmlN-NipyL"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2e7MPhAdCG3c"
      },
      "outputs": [],
      "source": [
        "# import the OpenAI Python library for calling the OpenAI API\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xxl5YAOjCG3j"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4fhQJQgkEAfY"
      },
      "outputs": [],
      "source": [
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DKAZ8CWCAM3-"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BJ6S4P5Hw4iG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AG9RjMb8Qlot"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "mp.set_start_method('spawn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rnR2kDS_2FgZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# para limpar texto\n",
        "import ftfy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19yOgi9OMjD"
      },
      "source": [
        "## Definindo paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "DIRETORIO_LOCAL = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local'\n",
        "DIRETORIO_TRABALHO = F'{DIRETORIO_LOCAL}/inpars'\n",
        "DIRETORIO_TREC_COVID = F'{DIRETORIO_LOCAL}/trec_covid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pasta já existia!\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(DIRETORIO_LOCAL):\n",
        "    print('pasta já existia!')\n",
        "else:\n",
        "    os.makedirs(DIRETORIO_LOCAL)\n",
        "    print('pasta criada!')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Função de verificação de memória"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "8c340509-fb4b-4264-d29c-98fa9313e8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May  1 02:50:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
            "| 58%   48C    P8    28W / 370W |     58MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1229      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1371      G   /usr/bin/gnome-shell                9MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria(lista_mem=['cpu']):\n",
        "  \"\"\"\n",
        "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
        "\n",
        "  Parâmetros:\n",
        "  -----------\n",
        "  lista_mem : list, opcional\n",
        "      Lista com strings 'cpu' e/ou 'gpu'. \n",
        "      'cpu' - exibe informações de memória da CPU.\n",
        "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
        "      O valor padrão é ['cpu'].\n",
        "\n",
        "  Saída:\n",
        "  -------\n",
        "  A função não retorna nada, apenas exibe as informações na tela.\n",
        "\n",
        "  Exemplo de uso:\n",
        "  ---------------\n",
        "  Para exibir informações de memória da CPU:\n",
        "      mostra_memoria(['cpu'])\n",
        "\n",
        "  Para exibir informações de memória da CPU e GPU:\n",
        "      mostra_memoria(['cpu', 'gpu'])\n",
        "  \n",
        "  Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "  \"\"\"  \n",
        "  if 'cpu' in lista_mem:\n",
        "    vm = virtual_memory()\n",
        "    ram={}\n",
        "    ram['total']=round(vm.total / 1e9,2)\n",
        "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
        "    print('/nGPU')\n",
        "    gpu_info = !nvidia-smi\n",
        "  if 'gpu' in lista_mem:\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "347c9e3b-238b-46f4-f530-a00030c343c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 59.13\n",
            " used 7.17\n",
            " free 58.39\n",
            " cached 1.68\n",
            " buffers 0.11\n",
            "/nGPU\n",
            "Mon May  1 02:50:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
            "| 58%   47C    P8    28W / 370W |     58MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1229      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1371      G   /usr/bin/gnome-shell                9MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xRdgUGMPgh"
      },
      "source": [
        "### Vinculando pasta do google drive para salvar dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "achvQ78sa3p3"
      },
      "source": [
        "## Fixando as seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bkETIyWGkbOf"
      },
      "outputs": [],
      "source": [
        "def inicializa_seed(num_semente:int=123):\n",
        "  \"\"\"\n",
        "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
        "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
        "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
        "  \n",
        "  Args:\n",
        "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
        "  \n",
        "  References:\n",
        "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
        "  \"\"\"\n",
        "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
        "  random.seed(num_semente)\n",
        "  np.random.seed(num_semente)\n",
        "  torch.manual_seed(num_semente)\n",
        "  \n",
        "  # Define as sementes da GPU\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  #torch.cuda.manual_seed(num_semente)\n",
        "  #Cuda algorithms\n",
        "  #torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A semente dever ser meu número na planilha da tarefa no [classoom](https://docs.google.com/spreadsheets/u/0/d/1mvA8ZrN2FjPxIDwJkYJGsVdZNH49Z1w1L0_3pIAZhQo/htmlview#gid=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_semente = 13 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UJGxQIrdaVp4"
      },
      "outputs": [],
      "source": [
        "inicializa_seed(num_semente)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2gtkEPhA0t"
      },
      "source": [
        "## Preparando para debug e display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wQ5pmlOHxHhk"
      },
      "outputs": [],
      "source": [
        "def config_display():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de display do Pandas.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurando formato saída Pandas\n",
        "  # define o número máximo de colunas que serão exibidas\n",
        "  pd.options.display.max_columns = None\n",
        "\n",
        "  # define a largura máxima de uma linha\n",
        "  pd.options.display.width = 1000\n",
        "\n",
        "  # define o número máximo de linhas que serão exibidas\n",
        "  pd.options.display.max_rows = 100\n",
        "\n",
        "  # define o número máximo de caracteres por coluna\n",
        "  pd.options.display.max_colwidth = 50\n",
        "\n",
        "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
        "  pd.options.display.show_dimensions = True\n",
        "\n",
        "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
        "  pd.options.display.precision = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "b2tDy72ATNHs"
      },
      "outputs": [],
      "source": [
        "def config_debug():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
        "  transformers e datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define opções de impressão de tensores para o modo científico\n",
        "  torch.set_printoptions(sci_mode=True) \n",
        "  \"\"\"\n",
        "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
        "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
        "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
        "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
        "    uma melhor compreensão dos números envolvidos.  \n",
        "  \"\"\"\n",
        "\n",
        "  # Habilita detecção de anomalias no autograd do PyTorch\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  \"\"\"\n",
        "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
        "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
        "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
        "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
        "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
        "    antes que se torne um problema maior.\n",
        "\n",
        "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
        "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
        "    ela deve ser usada com cautela e apenas para depuração.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "  \"\"\"\n",
        "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
        "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
        "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
        "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
        "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
        "    e removê-lo após a solução do problema.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define o nível de verbosity do pacote transformers para info\n",
        "  # transformers.utils.logging.set_verbosity_info() \n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
        "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
        "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
        "\n",
        "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
        "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
        "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
        "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
        "    necessidade de cada tarefa.\n",
        "  \n",
        "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
        "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
        "  \n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define o modo verbose do xmode, que é utilizado no debug\n",
        "  # %xmode Verbose \n",
        "\n",
        "  \"\"\"\n",
        "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
        "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
        "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
        "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
        "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
        "\n",
        "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
        "    comentar a linha acima e descomentar a linha abaixo:\n",
        "    %xmode Plain\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "    Dica:\n",
        "    1.  pdb (Python Debugger)\n",
        "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
        "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
        "\n",
        "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
        "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
        "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
        "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
        "\n",
        "\n",
        "    2. ipdb\n",
        "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
        "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
        "      \n",
        "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
        "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
        "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
        "      comandos para investigar o comportamento.\n",
        "\n",
        "      Durante a depuração, você pode usar comandos:\n",
        "        next (para executar a próxima linha de código), \n",
        "        step (para entrar em uma função chamada na próxima linha de código) \n",
        "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
        "\n",
        "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
        "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
        "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Tb4aqtcExR84"
      },
      "outputs": [],
      "source": [
        "config_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Bq4043fkfh",
        "outputId": "fa8e5db1-1feb-4393-fb66-d394d1ad693c"
      },
      "outputs": [],
      "source": [
        "config_debug()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baixando o dataset para geração de queries (trec-covid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Já existia a pasta\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(f\"{DIRETORIO_TREC_COVID}/corpus.jsonl.gz\"):\n",
        "    !wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz\n",
        "    !mv corpus.jsonl.gz {DIRETORIO_TREC_COVID}\n",
        "    print('Baixado')\n",
        "else:\n",
        "    print('Já existia a pasta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descompacte o arquivo para a memória\n",
        "with gzip.open(f'{DIRETORIO_TREC_COVID}/corpus.jsonl.gz', 'rt') as f:\n",
        "    # Leia o conteúdo do arquivo descompactado\n",
        "    corpus = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> len(corpus): 171332 corpus[0] {'_id': 'ug7v899j', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia', 'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'metadata': {'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35282/', 'pubmed_id': '11472636'}}\n"
          ]
        }
      ],
      "source": [
        "# Exiba os dados carregados\n",
        "print(f\"{type(corpus)} len(corpus): {len(corpus)} corpus[0] {corpus[0]}\" )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construção do prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot_example_1 = random.randint(0, len(corpus))\n",
        "few_shot_example_2 = random.randint(0, len(corpus))\n",
        "few_shot_example_3 = random.randint(0, len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[67897, 76220, 48686]\n"
          ]
        }
      ],
      "source": [
        "lista_few_shot_example = [few_shot_example_1, few_shot_example_2, few_shot_example_3]\n",
        "print(lista_few_shot_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert few_shot_example_1 == 67897, f\"Números gerados não repetem o prompt construído\"\n",
        "assert few_shot_example_2 == 76220, f\"Números gerados não repetem o prompt construído\"\n",
        "assert few_shot_example_3 == 48686, f\"Números gerados não repetem o prompt construído\"\n",
        "# Abaixo lista construída para prompt (original)\n",
        "# lista_few_shot_example = [67897, 76220, 48686]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(67897, {'_id': 'i138ax5u', 'title': 'Oncology during the COVID-19 pandemic: challenges, dilemmas and the psychosocial impact on cancer patients', 'text': 'COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have outlined our clinical experience in managing patients with gastrointestinal, hematological, gynaecological, dermatological, neurological, thyroid, lung and paediatric cancers in the COVID-19 era and have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.', 'metadata': {'url': 'https://www.ncbi.nlm.nih.gov/pubmed/32565968/; https://doi.org/10.3892/ol.2020.11599', 'pubmed_id': '32565968'}}), (76220, {'_id': '3mu0576h', 'title': 'Frequency of Coronavirus NL63 Infection in Children with Upper Respiratory Infection by Real-Time PCR', 'text': 'Background: Human coronavirus NL63 (HCoV-NL63) is a new respiratory virus associated with acute respiratory infection in children Infection with this virus is usually accompanied by upper and lower infections of the respiratory tract in adults Objectives: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR Results: Out of 138 samples, 33 (23 9%) were positive for coronavirus NL63, including 21 (63 6%) male samples and 12 (36 4%) female samples There was no significant correlation between gender and positivity for coronavirus infection (P &gt; 0 05) However, the association of clinical symptoms with the virus was statistically significant (P &lt; 0 05) Conclusions: This study was conducted for the first time in Kerman Province In this study, the frequency of coronavirus NL63 was evaluated among children with acute respiratory infection with a highly sensitive method, real-time PCR The prevalence of this virus was 33%, which was more frequent than in similar studies', 'metadata': {'url': 'https://doi.org/10.5812/ijp.101905', 'pubmed_id': ''}}), (48686, {'_id': 'ephwco98', 'title': 'Acute Respiratory Distress Syndrome From an Infectious Disease Perspective.', 'text': 'Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes. This article encompasses the most common infectious etiologies of ARDS and their clinical presentations and management, along with commonly encountered infectious complications in such patients.', 'metadata': {'url': 'https://doi.org/10.1097/cnq.0000000000000283; https://www.ncbi.nlm.nih.gov/pubmed/31449153/', 'pubmed_id': '31449153'}})]\n"
          ]
        }
      ],
      "source": [
        "print([(id, corpus[id]) for id in lista_few_shot_example])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geração de perguntas usando em paralelo, via browser, o [chatgpt](https://chat.openai.com/)\n",
        "\n",
        "Prompt usado:\n",
        "\n",
        "        \"\"\"Consider the text below: \n",
        "\n",
        "        Text: <text>\n",
        "\n",
        "        Generate a question such that the answer is inside the text above. Suppose you are a <patient/doctor>.\n",
        "\n",
        "        Question:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_shot_example_1_doctor = \"What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\"\n",
        "question_shot_example_1_patient = \"what does the text suggest are the challenges faced by vulnerable cancer patients during the COVID-19 pandemic?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_shot_example_1 = question_shot_example_1_doctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_shot_example_2_doctor = \"What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\"\n",
        "question_shot_example_2_patient = \"What is the prevalence of coronavirus NL63 among children under the age of five years with acute respiratory infection according to the study?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_shot_example_2 = question_shot_example_2_doctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_shot_example_3_patient = 'What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?'\n",
        "question_shot_example_3_doctor = 'What does this article cover regarding acute respiratory distress syndrome (ARDS)?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_shot_example_3 = question_shot_example_3_patient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortened_text_shot_example_1 = \"COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortened_text_shot_example_2 = \"In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "shortened_text_shot_example_3 = \"Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vevzNkrIuH8r"
      },
      "outputs": [],
      "source": [
        "instrucao = 'Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\\n\\n'\n",
        "exemplo1 = f'Text: {shortened_text_shot_example_1}\\n\\nQuestion: {question_shot_example_1}\\n\\n'\n",
        "exemplo2 = f'Text: {shortened_text_shot_example_2}\\n\\nQuestion: {question_shot_example_2}\\n\\n'\n",
        "exemplo3 = f'Text: {shortened_text_shot_example_3}\\n\\nQuestion: {question_shot_example_3}\\n\\n'\n",
        "texto_a_completar = 'Text: {context}\\n\\nQuestion: '\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_for_question = instrucao + exemplo1 + exemplo2 + exemplo3 + texto_a_completar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\n",
            "\n",
            "Text: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\n",
            "\n",
            "Question: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\n",
            "\n",
            "Text: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\n",
            "\n",
            "Question: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\n",
            "\n",
            "Text: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\n",
            "\n",
            "Question: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\n",
            "\n",
            "Text: {context}\n",
            "\n",
            "Question: \n"
          ]
        }
      ],
      "source": [
        "print(prompt_for_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The thiol-disulfide oxidoreductase thioredoxin-1 (Trx1) is known to be secreted by leukocytes and to exhibit cytokine-like properties. Extracellular effects of Trx1 require a functional active site, suggesting a redox-based mechanism of action. However, specific cell surface proteins and pathways co\n"
          ]
        }
      ],
      "source": [
        "text_test = corpus[100]['text'][:300]\n",
        "print(text_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_teste = prompt_for_question.replace('{context}', text_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\n",
            "\n",
            "Text: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\n",
            "\n",
            "Question: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\n",
            "\n",
            "Text: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\n",
            "\n",
            "Question: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\n",
            "\n",
            "Text: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\n",
            "\n",
            "Question: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\n",
            "\n",
            "Text: The thiol-disulfide oxidoreductase thioredoxin-1 (Trx1) is known to be secreted by leukocytes and to exhibit cytokine-like properties. Extracellular effects of Trx1 require a functional active site, suggesting a redox-based mechanism of action. However, specific cell surface proteins and pathways co\n",
            "\n",
            "Question: \n"
          ]
        }
      ],
      "source": [
        "print(prompt_teste)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4WJ9VOUMHFz1"
      },
      "source": [
        "# Gerando queries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Com modelo opensource EleutherAI/gpt-j-6B"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fonte de apoio para construção do código: [Projeto Extractive Q&A - Performance Comparison between Learning Methods: Context and Transfer - exqa-complearning - Borela e Léo em diciplina anterior](https://github.com/marcusborela/exqa-complearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPTJForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import  List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KeywordsStoppingCriteria(StoppingCriteria):\n",
        "    \"\"\"\n",
        "      Fonte: https://stackoverflow.com/questions/69403613/how-to-early-stop-autoregressive-model-with-a-list-of-stop-words\n",
        "    \"\"\"\n",
        "    def __init__(self, keywords_ids:list):\n",
        "        self.keywords = keywords_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        if input_ids[0][-1] in self.keywords:\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextGeneration(): # pylint: disable=missing-class-docstring\n",
        "\n",
        "    \"\"\"\n",
        "        Fonte dos parâmetros\n",
        "\n",
        "            https://github.com/huggingface/transformers/blob/main/src/transformers/generation_utils.py\n",
        "            linha: 920\n",
        "\n",
        "                length_penalty (`float`, *optional*, defaults to 1.0):\n",
        "                 Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.\n",
        "                 0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer\n",
        "                 sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n",
        "\n",
        "                temperature (`float`, *optional*, defaults to 1.0):\n",
        "                The value used to module the next token probabilities.\n",
        "\n",
        "                num_return_sequences(`int`, *optional*, defaults to 1):\n",
        "                The number of independently computed returned sequences for each element in the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    _dict_parameters_example = {\n",
        "                    # parâmetros tarefa\n",
        "                    \"num_top_k\": 1,\n",
        "                    \"num_max_answer_length\":64,\n",
        "                    # parâmetros context - text generation\n",
        "                    'list_stop_words': ['.', '\\n', '!'],\n",
        "                    \"val_length_penalty\":2,\n",
        "                    'if_do_sample': False,\n",
        "                    'val_temperature': 1,\n",
        "}\n",
        "\n",
        "    def __init__(self,\n",
        "                 parm_name_model: str,\n",
        "                 parm_dict_config:Dict):\n",
        "\n",
        "\n",
        "        # para evitar estouro memória gpu\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Since we are using our model only for inference, switch to `eval` mode:\n",
        "        self.name_model = parm_name_model\n",
        "        self.path_model = \"/home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/\"+self.name_model\n",
        "        # print(self.path_model)\n",
        "\n",
        "        self.name_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        self.device = torch.device(self.name_device)\n",
        "\n",
        "\n",
        "        self.pipe = pipeline(\"text-generation\", model=self.get_model(self.path_model).to(self.device).eval(),\\\n",
        "                              tokenizer=self.get_tokenizer(self.path_model),\\\n",
        "                              device=self.device,framework='pt')\n",
        "\n",
        "        # setting to avoid warning msg\n",
        "        #   Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
        "        #   The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior.\n",
        "        #   Please pass your input's `attention_mask` to obtain reliable results.\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.atualiza_parametros_resposta(parm_dict_config)\n",
        "\n",
        "        # Se desejar salvar o modelo, para ficar mais rápido a carga\n",
        "        #   gptj6b: muda de 4 minutos para 12 segundos\n",
        "        # torch.save(self.pipe.model, self.path_model+\"/pytorch_model_saved.pt\")\n",
        "        self.max_seq_len = self.pipe.model.config.max_position_embeddings\n",
        "\n",
        "\n",
        "    def atualiza_parametros_resposta(self, parm_dict_config:Dict):\n",
        "\n",
        "        if parm_dict_config is None:\n",
        "            parm_dict_config = deepcopy(Reader._dict_parameters_example)\n",
        "\n",
        "        self.num_top_k = parm_dict_config[\"num_top_k\"]\n",
        "        # self.num_doc_stride = parm_dict_config[\"num_doc_stride\"]\n",
        "        self.num_max_answer_length = parm_dict_config[\"num_max_answer_length\"]\n",
        "        self.if_do_sample = parm_dict_config['if_do_sample']\n",
        "        self.val_temperature = parm_dict_config['val_temperature']\n",
        "        self.list_stop_words = parm_dict_config[\"list_stop_words\"]\n",
        "        self.val_length_penalty = parm_dict_config[\"val_length_penalty\"]\n",
        "        self.prompt = parm_dict_config[\"prompt_missing_context\"]\n",
        "        assert '{context}' in self.prompt, \"Prompt must have {context} in the text.\"\n",
        "        self.prompt_length = len(self.pipe.tokenizer(self.prompt)['input_ids'])\n",
        "        stop_ids = [self.tokenizer.encode(w)[0] for w in self.list_stop_words]\n",
        "        self.stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
        "        # para evitar gerar máscar com \"___\"\n",
        "        # self.bad_words = ['_','__','___','____', '______', '________',  '________________', ]\n",
        "        self.bad_words = [self.tokenizer.decode(pos) for pos in range(self.tokenizer.vocab_size) if '_' in self.tokenizer.decode(pos)]\n",
        "        self.bad_words_ids = [[self.tokenizer.get_vocab()[word]] \\\n",
        "                                    for word in self.bad_words \\\n",
        "                                      if word in self.tokenizer.get_vocab()]\n",
        "        # self.bad_words.append('\\xa0') # id 1849\n",
        "        self.bad_words_ids += [[1849]]\n",
        "\n",
        "    @property\n",
        "    def info(self):\n",
        "        return {\"name\":self.name_model,\n",
        "                \"device\": self.name_device,\n",
        "                \"top_k\": self.num_top_k,\n",
        "                \"if_do_sample\": self.if_do_sample,\n",
        "                \"val_temperature\": self.val_temperature,\n",
        "                \"list_stop_words\": self.list_stop_words,\n",
        "                \"val_length_penalty\": self.val_length_penalty,\n",
        "                \"length_penalty\": self.val_length_penalty,\n",
        "                \"num_max_answer_length\":self.num_max_answer_length,\n",
        "                \"max_seq_len\": self.max_seq_len,\n",
        "                \"bad_words\": self.bad_words,\n",
        "                \"prompt_length\": self.prompt_length,\n",
        "                \"prompt\" : self.prompt\n",
        "                }\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        return self.pipe.tokenizer\n",
        "\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return self.pipe.model\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model(path_model: str,\n",
        "                  *args, **kwargs):\n",
        "        # return GPTJForCausalLM.from_pretrained(path_model, *args, **kwargs)\n",
        "        # return GPTNeoForCausalLM.from_pretrained(path_model, *args, **kwargs)\n",
        "        if 'neo' in path_model:\n",
        "            return GPTNeoForCausalLM.from_pretrained(\n",
        "                path_model)\n",
        "        else:\n",
        "            return GPTJForCausalLM.from_pretrained(\n",
        "            path_model,\n",
        "            revision=\"float16\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_tokenizer(path_model: str,\n",
        "                      *args, batch_size: int = 128, **kwargs) -> GPT2Tokenizer:\n",
        "        return GPT2Tokenizer.from_pretrained(path_model, use_fast=False, *args, **kwargs)\n",
        "\n",
        "    def generate_text(self,\n",
        "                      texto_contexto: str,\n",
        "                      parm_dict_config_generation:Dict=None,\n",
        "                      parm_print:bool=False) -> List[Dict]:\n",
        "\n",
        "        assert isinstance(texto_contexto, str), f\"Expected just one context, not {type(texto_contexto)}\"\n",
        "\n",
        "\n",
        "        prompt = self.prompt.replace('{context}', texto_contexto)\n",
        "\n",
        "        #print('Tokenizing...')\n",
        "        # tokenized = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        # prompt_len = len(# .input_ids[0])\n",
        "        # print(f\"chars(prompt): {len(prompt)}\")\n",
        "        # print(f\"tokens(prompt): {prompt_len}\")\n",
        "\n",
        "        val_temperature = self.val_temperature\n",
        "        if_do_sample = self.if_do_sample\n",
        "        num_top_k = self.num_top_k\n",
        "        val_length_penalty = self.val_length_penalty\n",
        "        list_stop_words = self.list_stop_words\n",
        "        stop_criteria = self.stop_criteria\n",
        "        bad_words = self.bad_words\n",
        "        bad_words_ids = self.bad_words_ids\n",
        "        num_max_answer_length = self.num_max_answer_length\n",
        "\n",
        "        if parm_dict_config_generation:\n",
        "            if \"val_temperature\" in parm_dict_config_generation:\n",
        "                val_temperature = parm_dict_config_generation[\"val_temperature\"]\n",
        "\n",
        "            if \"if_do_sample\" in parm_dict_config_generation:\n",
        "                if_do_sample = parm_dict_config_generation[\"if_do_sample\"]\n",
        "\n",
        "            if \"num_top_k\" in parm_dict_config_generation:\n",
        "                num_top_k = parm_dict_config_generation[\"num_top_k\"]\n",
        "\n",
        "            if \"val_length_penalty\" in parm_dict_config_generation:\n",
        "                val_length_penalty = parm_dict_config_generation[\"val_length_penalty\"]\n",
        "\n",
        "            if \"list_stop_words\" in parm_dict_config_generation:\n",
        "                list_stop_words = parm_dict_config_generation[\"list_stop_words\"]\n",
        "                stop_ids = [self.tokenizer.encode(w)[0] for w in list_stop_words]\n",
        "                stop_criteria = KeywordsStoppingCriteria(stop_ids)                \n",
        "\n",
        "            if \"bad_words\" in parm_dict_config_generation:\n",
        "                bad_words = parm_dict_config_generation['bad_words']\n",
        "                bad_words_ids = [self.pipe.tokenizer.get_vocab()[word] for word in bad_words]\n",
        "\n",
        "            if \"num_max_answer_length\" in parm_dict_config_generation:\n",
        "                num_max_answer_length = parm_dict_config_generation[\"num_max_answer_length\"]\n",
        "\n",
        "        length_text_input = len(self.tokenizer(texto_contexto)['input_ids'])\n",
        "        max_length = length_text_input + self.prompt_length + num_max_answer_length\n",
        "\n",
        "        if max_length > self.max_seq_len:\n",
        "            pos_truncar = self.max_seq_len-num_max_answer_length-3\n",
        "            prompt = prompt[:pos_truncar]\n",
        "            print(f\"Max_length desejado {max_length} é maior do que o tratado pelo modelo {self.max_seq_len}. Truncado em {pos_truncar}.\")\n",
        "\n",
        "\n",
        "        if parm_print:\n",
        "            print({\"num_top_k\": num_top_k,\n",
        "                    \"if_do_sample\": if_do_sample,\n",
        "                    \"val_temperature\": val_temperature,\n",
        "                    \"val_length_penalty\": val_length_penalty,\n",
        "                    \"list_stop_words\": list_stop_words,\n",
        "                    \"bad_words\": bad_words,\n",
        "                    \"num_max_answer_length\":num_max_answer_length,\n",
        "                    \"max_length\": max_length,\n",
        "                    \"prompt\" : prompt\n",
        "                    })\n",
        "\n",
        "\n",
        "\n",
        "        respostas = self.pipe(prompt,\n",
        "            return_tensors=False,\n",
        "            return_text=True,\n",
        "            return_full_text=False,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "            # max_question_len = len(texto_pergunta), # número de tokens.. #chars>#tokens\n",
        "            max_length= max_length,\n",
        "            # min_length = 2,\n",
        "            num_return_sequences = num_top_k,\n",
        "            num_beams =  num_top_k,\n",
        "            do_sample = if_do_sample,\n",
        "            temperature = val_temperature,\n",
        "            length_penalty = val_length_penalty, # self.num_length_penalty \\\n",
        "            stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
        "            bad_words_ids=bad_words_ids\n",
        "            )\n",
        "\n",
        "        if not isinstance(respostas, list):\n",
        "            lista_respostas = [respostas]\n",
        "        else:\n",
        "            lista_respostas = respostas\n",
        "\n",
        "        if len(lista_respostas) < self.num_top_k:\n",
        "            print(f\"Warning: #answers=len(lista_respostas)<self.num_top_k {len(lista_respostas)} < {self.num_top_k}. \")\n",
        "\n",
        "        # retira caracteres inválidos\n",
        "        for resp in lista_respostas:\n",
        "            resp['generated_text'] = ftfy.fix_text(resp['generated_text']).strip()\n",
        "\n",
        "        return lista_respostas[:self.num_top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_config_model = {\n",
        "                # parâmetros complementares\n",
        "                \"num_top_k\": 1,\n",
        "                \"num_max_answer_length\":80,\n",
        "                \"prompt_missing_context\" : prompt_for_question,\n",
        "                # parâmetros context\n",
        "                'list_stop_words': ['\\n', '?', 'Text:', '\\n\\n'],\n",
        "                'if_do_sample': False,\n",
        "                \"val_length_penalty\":0.,\n",
        "                'val_temperature': 0.1,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_model = \"EleutherAI/gpt-j-6B\"\n",
        "dict_config_model['list_stop_words'] = ['.', '\\n','!']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto_teste = prompt_for_question.replace('{context}', corpus[100]['text'][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\n",
            "\n",
            "Text: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\n",
            "\n",
            "Question: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\n",
            "\n",
            "Text: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\n",
            "\n",
            "Question: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\n",
            "\n",
            "Text: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\n",
            "\n",
            "Question: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\n",
            "\n",
            "Text: The thiol-disulfide oxidoreductase thioredoxin-1 (Trx1) is known to be secreted by leukocytes and to exhibit cytokine-like properties. Extracellular effects of Trx1 require a functional active site, suggesting a redox-based mechanism of action. However, specific cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses have not been identified so far. Using a mechanism-based kinetic trapping technique to identify disulfide exchange interactions on the in\n",
            "\n",
            "Question: \n"
          ]
        }
      ],
      "source": [
        "print(texto_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "parar aqui",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mparar aqui\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mException\u001b[0m: parar aqui"
          ]
        }
      ],
      "source": [
        "raise Exception(\"parar aqui\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 20 s, sys: 26.6 s, total: 46.6 s\n",
            "Wall time: 2min 39s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generator = TextGeneration(parm_name_model=name_model, \n",
        "        parm_dict_config=dict_config_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(generator.tokenizer(texto_teste)['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generator.info()\n",
            " {'name': 'EleutherAI/gpt-j-6B', 'device': 'cuda:0', 'top_k': 1, 'if_do_sample': False, 'val_temperature': 0.1, 'list_stop_words': ['.', '\\n', '!'], 'val_length_penalty': 0.0, 'length_penalty': 0.0, 'num_max_answer_length': 80, 'max_seq_len': 2048, 'bad_words': ['_', '__', '____', '________', ' _', '________________', '________________________________', ' __', '._', '___', '_-', '_{', '______', '________________________________________________________________', '(_', '_____', '[_', '_-_', '________________________', '_______', ' $_', '_(', ' (_', ' ______', '_.', ' \"_', ' ___', '._', '/_', '_>'], 'prompt_length': 371, 'prompt': 'Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\\n\\nText: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\\n\\nQuestion: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\\n\\nText: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\\n\\nQuestion: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\\n\\nText: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\\n\\nQuestion: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\\n\\nText: {context}\\n\\nQuestion: '}\n"
          ]
        }
      ],
      "source": [
        "print(f\"generator.info()\\n {generator.info}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'num_top_k': 1, 'if_do_sample': False, 'val_temperature': 0.1, 'val_length_penalty': 0.0, 'list_stop_words': ['.', '\\n', '!'], 'bad_words': ['_', '__', '____', '________', ' _', '________________', '________________________________', ' __', '._', '___', '_-', '_{', '______', '________________________________________________________________', '(_', '_____', '[_', '_-_', '________________________', '_______', ' $_', '_(', ' (_', ' ______', '_.', ' \"_', ' ___', '._', '/_', '_>'], 'num_max_answer_length': 80, 'max_length': 562, 'prompt': 'Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\\n\\nText: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\\n\\nQuestion: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\\n\\nText: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\\n\\nQuestion: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\\n\\nText: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\\n\\nQuestion: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\\n\\nText: The thiol-disulfide oxidoreductase thioredoxin-1 (Trx1) is known to be secreted by leukocytes and to exhibit cytokine-like properties. Extracellular effects of Trx1 require a functional active site, suggesting a redox-based mechanism of action. However, specific cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses have not been identified so far. Using a mechanism-based kinetic trapping technique to identify disulfide exchange interactions on the in\\n\\nQuestion: '}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "resultado = generator.generate_text(texto_contexto=corpus[100]['text'][:500], parm_print=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'What is the significance of identifying cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses?'}]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_config_generation = {\n",
        "                # parâmetros complementares\n",
        "                \"num_top_k\": 2,\n",
        "                \"num_max_answer_length\":50, # o prompt tem 402\n",
        "                \"prompt_missing_context\" : prompt_for_question,\n",
        "                'list_stop_words': ['\\n', '?', 'Text:', '\\n\\n'],\n",
        "                'if_do_sample': True,\n",
        "                \"val_length_penalty\":0.,\n",
        "                'val_temperature': 1.,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "resultado = generator.generate_text(texto_contexto=corpus[100]['text'][:500], \n",
        "                                                parm_print=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'What is the significance of identifying cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses?'}]"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpa o cache da memória da GPU\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "resultado = generator.pipe(texto_teste,\n",
        "            return_tensors=True,\n",
        "            output_scores=True,\n",
        "            return_text=True,\n",
        "            return_full_text=False,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "            max_length= 550, # prompt_len: 402 + self.num_max_answer_length,\n",
        "            num_return_sequences = 1,\n",
        "            num_beams =  1,\n",
        "            do_sample = False,\n",
        "            temperature = 0.1,\n",
        "            bad_words_ids=generator.bad_words_ids,\n",
        "            stopping_criteria=StoppingCriteriaList([generator.stop_criteria])\n",
        "            \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': '\\xa0What is the significance of identifying cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses?'}]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gerar queries para trecc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'EleutherAI/gpt-j-6B',\n",
              " 'device': 'cuda:0',\n",
              " 'top_k': 1,\n",
              " 'if_do_sample': False,\n",
              " 'val_temperature': 0.1,\n",
              " 'list_stop_words': ['.', '\\n', '!'],\n",
              " 'val_length_penalty': 0.0,\n",
              " 'length_penalty': 0.0,\n",
              " 'num_max_answer_length': 80,\n",
              " 'max_seq_len': 2048,\n",
              " 'bad_words': ['_',\n",
              "  '__',\n",
              "  '____',\n",
              "  '________',\n",
              "  ' _',\n",
              "  '________________',\n",
              "  '________________________________',\n",
              "  ' __',\n",
              "  '._',\n",
              "  '___',\n",
              "  '_-',\n",
              "  '_{',\n",
              "  '______',\n",
              "  '________________________________________________________________',\n",
              "  '(_',\n",
              "  '_____',\n",
              "  '[_',\n",
              "  '_-_',\n",
              "  '________________________',\n",
              "  '_______',\n",
              "  ' $_',\n",
              "  '_(',\n",
              "  ' (_',\n",
              "  ' ______',\n",
              "  '_.',\n",
              "  ' \"_',\n",
              "  ' ___',\n",
              "  '._',\n",
              "  '/_',\n",
              "  '_>'],\n",
              " 'prompt_length': 371,\n",
              " 'prompt': 'Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\\n\\nText: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\\n\\nQuestion: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\\n\\nText: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\\n\\nQuestion: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\\n\\nText: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\\n\\nQuestion: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\\n\\nText: {context}\\n\\nQuestion: '}"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [00:02,  1.33s/it]/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "2333it [56:18,  1.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max_length desejado 10449 é maior do que o tratado pelo modelo 2048. Truncado em 1965.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2782it [1:07:49,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max_length desejado 25243 é maior do que o tratado pelo modelo 2048. Truncado em 1965.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4863it [1:59:02,  1.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max_length desejado 2095 é maior do que o tratado pelo modelo 2048. Truncado em 1965.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "9999it [3:19:17,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max_length desejado 2473 é maior do que o tratado pelo modelo 2048. Truncado em 1965.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11493it [3:46:48,  1.02s/it]"
          ]
        }
      ],
      "source": [
        "dict_queries_geradas = {}\n",
        "dict_erros_doctos = {}\n",
        "for cnt, docto in tqdm(enumerate(corpus)):\n",
        "    try:\n",
        "        if len(docto['text']) > 20:\n",
        "            if ('title' in docto) and len(docto['title']) >= 5 :\n",
        "                texto = docto['title'] + '. ' + docto['text'] \n",
        "            else:\n",
        "                texto = docto['text'] \n",
        "            resultado = generator.generate_text(texto_contexto=texto)\n",
        "            dict_queries_geradas[docto['_id']] = resultado[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        dict_erros_doctos[docto['_id']] = str(e)\n",
        "        print(f\"Erro ao gerar query para o documento {docto['_id']}: {e}\")            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Foram geradas queries para 6 documentos. Deixados sem queries: 171326\n"
          ]
        }
      ],
      "source": [
        "print(f\"Foram geradas queries para {len(dict_queries_geradas)} documentos. Deixados sem queries: {len(corpus) - len(dict_queries_geradas)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pickle' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDIRETORIO_TRABALHO\u001b[39m}\u001b[39;00m\u001b[39m/queries_geradas.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m outputFile:\n\u001b[0;32m----> 2\u001b[0m     pickle\u001b[39m.\u001b[39mdump(dict_queries_geradas, outputFile, pickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ],
      "source": [
        "with open(f\"{DIRETORIO_TRABALHO}/queries_geradas.pickle\", 'wb') as outputFile:\n",
        "    pickle.dump(dict_queries_geradas, outputFile, pickle.HIGHEST_PROTOCOL)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f\"{DIRETORIO_TRABALHO}/erros_doctos_geracao.pickle\", 'wb') as outputFile:\n",
        "    pickle.dump(dict_erros_doctos, outputFile, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ug7v899j': 'What is the significance of the finding that infections were more common in infants and preschool children?',\n",
              " '02tnwd4m': 'What is the significance of the presumed contribution of NO• to inflammatory diseases of the lung?',\n",
              " 'ejv2xln0': 'What is the significance of SP-D in the innate response to inhaled microorganisms and organic antigens?',\n",
              " '2b73a28n': 'What is the significance of ET-1 in lung disease?',\n",
              " '9785vg6d': 'What is the significance of the mucosal responses in the respiratory epithelium in response to pneumovirus infection?',\n",
              " 'zjufx4fo': 'What is the significance of the body TRS in discontinuous RNA synthesis?'}"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_queries_geradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Para cada docto que teve query gerada:\n",
        "   Calcular índice de relevância conforme rankeador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Ordenar pelos primeiros 10 mil documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Outras opções de modelo para geração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSL3JJSRHPgT"
      },
      "source": [
        "## Chat GPT (Modelo gpt-3.5-turbo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2gFIL0xIPk0"
      },
      "source": [
        "Para uso do gpt-3.5-turbo, usamos como referência o caderno da [openai: How_to_format_inputs_to_ChatGPT_models.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPPjYkUTCG3X"
      },
      "source": [
        "### How to format inputs to ChatGPT models\n",
        "\n",
        "ChatGPT is powered by `gpt-3.5-turbo`, OpenAI's most advanced model.\n",
        "\n",
        "You can build your own applications with `gpt-3.5-turbo` using the OpenAI API.\n",
        "\n",
        "Chat models take a series of messages as input, and return an AI-written message as output.\n",
        "\n",
        "This guide illustrates the chat format with a few example API calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42EWgSy0CG3d"
      },
      "source": [
        "### An example chat API call\n",
        "\n",
        "A chat API call has two required inputs:\n",
        "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`)\n",
        "- `messages`: a list of message objects, where each object has at least two fields:\n",
        "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
        "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
        "\n",
        "Typically, a conversation will start with a system message, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
        "\n",
        "Let's look at an example chat API calls to see how the chat format works in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSgy_fuXCG3d"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME_CHAT_GPT = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrefch9xDnso",
        "outputId": "8ef6d95c-ce90-48d6-e01b-07b4ab2adb74"
      },
      "outputs": [],
      "source": [
        "openai.api_key = getpass.getpass(\"Entre a OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['OPENAI_DISABLE_SSL_CERT_VERIFICATION'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "str expected, not int",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mverify_ssl_certs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/os.py:684\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key, value):\n\u001b[1;32m    683\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)\n\u001b[0;32m--> 684\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencodevalue(value)\n\u001b[1;32m    685\u001b[0m     putenv(key, value)\n\u001b[1;32m    686\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[key] \u001b[39m=\u001b[39m value\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/os.py:756\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(value):\n\u001b[1;32m    755\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 756\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mstr expected, not \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    757\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39mencode(encoding, \u001b[39m'\u001b[39m\u001b[39msurrogateescape\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: str expected, not int"
          ]
        }
      ],
      "source": [
        "os.environ['verify_ssl_certs']=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4sbukooCcYr"
      },
      "outputs": [
        {
          "ename": "APIConnectionError",
          "evalue": "Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/ssl.py:501\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    498\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    499\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    502\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    503\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    504\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    505\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    506\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    507\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    508\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    509\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1042\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/ssl.py:1310\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1311\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/adapters.py:563\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    565\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
            "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mMODEL_NAME_CHAT_GPT,\n\u001b[1;32m      3\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      4\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful question generator.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      5\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt_teste},\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m      8\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_requestor.py:529\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 529\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[1;32m    530\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[1;32m    531\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    532\u001b[0m util\u001b[39m.\u001b[39mlog_debug(\n\u001b[1;32m    533\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpenAI API response\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    534\u001b[0m     path\u001b[39m=\u001b[39mabs_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     request_id\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Request-Id\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    538\u001b[0m )\n\u001b[1;32m    539\u001b[0m \u001b[39m# Don't read the whole stream for debug logging unless necessary.\u001b[39;00m\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))"
          ]
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL_NAME_CHAT_GPT,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful question generator.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_teste},\n",
        "    ],\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhCsPcVqCa_g",
        "outputId": "50d18048-932d-4dbd-8664-6871b23a48f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7B7PZvfnx1ejDQaycSOYTMfWLNh2C at 0x7f3d88a36a40> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"What is the suggested mechanism of action for the extracellular effects of thioredoxin-1 (Trx1)?\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1682884453,\n",
              "  \"id\": \"chatcmpl-7B7PZvfnx1ejDQaycSOYTMfWLNh2C\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 24,\n",
              "    \"prompt_tokens\": 436,\n",
              "    \"total_tokens\": 460\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "APIConnectionError",
          "evalue": "Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/ssl.py:501\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    498\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    499\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    502\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    503\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    504\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    505\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    506\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    507\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    508\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    509\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1042\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/ssl.py:1310\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1311\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/adapters.py:563\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    565\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
            "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mMODEL_NAME_CHAT_GPT,\n\u001b[1;32m      3\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      4\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are an assistant who will help me answer questions.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      5\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt_teste},\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m      8\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/openai/api_requestor.py:529\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 529\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[1;32m    530\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[1;32m    531\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    532\u001b[0m util\u001b[39m.\u001b[39mlog_debug(\n\u001b[1;32m    533\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpenAI API response\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    534\u001b[0m     path\u001b[39m=\u001b[39mabs_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     request_id\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Request-Id\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    538\u001b[0m )\n\u001b[1;32m    539\u001b[0m \u001b[39m# Don't read the whole stream for debug logging unless necessary.\u001b[39;00m\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))"
          ]
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL_NAME_CHAT_GPT,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant who will help me answer questions.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_teste},\n",
        "    ],\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll0CNznzCG3e"
      },
      "source": [
        "As you can see, the response object has a few fields:\n",
        "- `id`: the ID of the request\n",
        "- `object`: the type of object returned (e.g., `chat.completion`)\n",
        "- `created`: the timestamp of the request\n",
        "- `model`: the full name of the model used to generate the response\n",
        "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
        "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
        "    - `message`: the message object generated by the model, with `role` and `content`\n",
        "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
        "    - `index`: the index of the completion in the list of choices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPknWoscCG3f"
      },
      "source": [
        "Extract just the reply with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RzJWlWJQCG3f",
        "outputId": "c5f47a3c-097a-494f-b34f-25fd2a14f060"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What is the suggested mechanism of action for the extracellular effects of thioredoxin-1 (Trx1)?'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tZSva3FCG3g"
      },
      "source": [
        "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
        "\n",
        "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "824RhtrqCG3h"
      },
      "source": [
        "### Tips for instructing gpt-3.5-turbo-0301\n",
        "\n",
        "Best practices for instructing models may change from model version to model version. The advice that follows applies to `gpt-3.5-turbo-0301` and may not apply to future models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-lPZSqfCG3h"
      },
      "source": [
        "#### System messages\n",
        "\n",
        "The system message can be used to prime the assistant with different personalities or behaviors.\n",
        "\n",
        "However, the model does not generally pay as much attention to the system message, and therefore we recommend placing important instructions in the user message instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdoD0GmzCG3h"
      },
      "source": [
        "An example of a system message that primes the assistant to explain concepts in great depth\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fbCTtNFWCG3j"
      },
      "source": [
        "### Counting tokens OpenAI Models\n",
        "\n",
        "Mais detalhes em [OpenAI: How_to_count_tokens_with_tiktoken.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
        "\n",
        "When you submit your request, the API transforms the messages into a sequence of tokens.\n",
        "\n",
        "The number of tokens used affects:\n",
        "- the cost of the request\n",
        "- the time it takes to generate the response\n",
        "- when the reply gets cut off from hitting the maximum token limit (4096 for `gpt-3.5-turbo`)\n",
        "\n",
        "As of Mar 01, 2023, you can use the following function to count the number of tokens that a list of messages will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goDW5l1jFpGl",
        "outputId": "011b152c-1015-475f-bac8-1db45dfa3415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "436 prompt tokens used.\n"
          ]
        }
      ],
      "source": [
        "print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ3ZmEDkCG3k"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjtWUNbFUusD",
        "outputId": "03937d03-824b-4704-c23b-aba117885a55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[83, 1609, 5963, 374, 2294, 0]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.encode(\"tiktoken is great!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBFhOkAVRZq",
        "outputId": "f942e49d-13cd-428f-95c9-ac3495380a29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b't', b'ik', b'token', b' is', b' great', b'!']"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOXZ3gibUycK"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuCoumBTU1hy",
        "outputId": "28591c78-1bd4-4464-ced7-32c2b7ad6a35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKh6Zj9FH5Zz"
      },
      "source": [
        "## LLAMA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2C983lKXdIY"
      },
      "source": [
        "[Colab demo da API do LLAMA](https://colab.research.google.com/drive/1zZ-ch29LTicNPA62t2MaOwMROywnqUxf?usp=sharing) (obrigado, Thales Rogério)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xyqOr8MH9jK"
      },
      "outputs": [],
      "source": [
        "base_url=\"http://143.106.167.108/api\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO3YNLXsIG2l"
      },
      "outputs": [
        {
          "ename": "ConnectionError",
          "evalue": "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
            "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/packages/six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
            "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[82], line 34\u001b[0m\n\u001b[1;32m      1\u001b[0m data\u001b[39m=\u001b[39m{\n\u001b[1;32m      2\u001b[0m \t\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\"\"\u001b[39m\u001b[39mGiven table, specify which rows have repeated values for both \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mItem number\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLocal\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. If no row is repeated say \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno repeats\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m250\u001b[39m\n\u001b[1;32m     32\u001b[0m }\n\u001b[0;32m---> 34\u001b[0m r\u001b[39m=\u001b[39mrequests\u001b[39m.\u001b[39;49mpost(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mbase_url\u001b[39m}\u001b[39;49;00m\u001b[39m/complete\u001b[39;49m\u001b[39m\"\u001b[39;49m, json\u001b[39m=\u001b[39;49mdata)\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/requests/adapters.py:547\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 547\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    549\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    551\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
            "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
          ]
        }
      ],
      "source": [
        "data={\n",
        "\t\"prompt\":\"\"\"Given table, specify which rows have repeated values for both \"Item number\" and \"Local\". If no row is repeated say \"no repeats\".\n",
        "\n",
        "Example 1:\n",
        "|Row | Item number | Local |\n",
        "|1 |  3 5 7 | New York |\n",
        "|2|  5 8 2 | Madagascar |\n",
        "|3|  3 4 5 | New York |\n",
        "|4|  3 4 5 | Paris |\n",
        "\n",
        "Explanation: Rows 1 and 3 have the same local \"New York\" and the same item number \"3 4 5\". Therefore they are repeated.\n",
        "\n",
        "Answer: (1,3).\n",
        "\n",
        "Example 2:\n",
        "|Row | Item number | Local |\n",
        "|1 |  0 9 2 4 | Amsterdam |\n",
        "|2|  9 4 2 4 | Barcelona |\n",
        "|3|  7 3 2 | London |\n",
        "|4|  7 3 1 | London |\n",
        "|5|  7 3 2 | London |\n",
        "|6|  7 3 2 | London |\n",
        "|7|  7 3 2 | London |\n",
        "|8|  7 3  2 |  New York |\n",
        "|9 |  0 9 2 4 | Amsterdam |\n",
        "\n",
        "Explanation:\"\"\",\n",
        "\n",
        "\"temperature\": 0.0,\n",
        "\"top_p\": 1,\n",
        "\"max_length\": 250\n",
        "}\n",
        "\n",
        "r=requests.post(f\"{base_url}/complete\", json=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71W7rBZXYOV4",
        "outputId": "919ddf7e-fc3c-4905-d198-c716eb1a8f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': 'Given table, specify which rows have repeated values for both \"Item number\" and \"Local\". If no row is repeated say \"no repeats\".\\n\\nExample 1:\\n|Row | Item number | Local |\\n|1 |  3 5 7 | New York |\\n|2|  5 8 2 | Madagascar |\\n|3|  3 4 5 | New York |\\n|4|  3 4 5 | Paris |\\n\\nExplanation: Rows 1 and 3 have the same local \"New York\" and the same item number \"3 4 5\". Therefore they are repeated.\\n\\nAnswer: (1,3).\\n\\nExample 2:\\n|Row | Item number | Local |\\n|1 |  0 9 2 4 | Amsterdam |\\n|2|  9 4 2 4 | Barcelona |\\n|3|  7 3 2 | London |\\n|4|  7 3 1 | London |\\n|5|  7 3 2 | London |\\n|6|  7 3 2 | London |\\n|7|  7 3 2 | London |\\n|8|  7 3  2 |  New York |\\n|9 |  0 9 2 4 | Amsterdam |\\n\\nExplanation:', 'temperature': 0.0, 'top_p': 1.0, 'max_length': 250, 'stopping_tokens': [], 'request_uuid': '403647f2-fd6d-40e6-a21c-098ea4870703'}\n"
          ]
        }
      ],
      "source": [
        "if r.ok:\n",
        "  response=r.json()\n",
        "  print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiNo-zmnYLmo",
        "outputId": "46824015-ae4a-4724-db6e-5509d21cccb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'Given table, specify which rows have repeated values for both \"Item number\" and \"Local\". If no row is repeated say \"no repeats\".\\n\\nExample 1:\\n|Row | Item number | Local |\\n|1 |  3 5 7 | New York |\\n|2|  5 8 2 | Madagascar |\\n|3|  3 4 5 | New York |\\n|4|  3 4 5 | Paris |\\n\\nExplanation: Rows 1 and 3 have the same local \"New York\" and the same item number \"3 4 5\". Therefore they are repeated.\\n\\nAnswer: (1,3).\\n\\nExample 2:\\n|Row | Item number | Local |\\n|1 |  0 9 2 4 | Amsterdam |\\n|2|  9 4 2 4 | Barcelona |\\n|3|  7 3 2 | London |\\n|4|  7 3 1 | London |\\n|5|  7 3 2 | London |\\n|6|  7 3 2 | London |\\n|7|  7 3 2 | London |\\n|8|  7 3  2 |  New York |\\n|9 |  0 9 2 4 | Amsterdam |\\n\\nExplanation:',\n",
              " 'temperature': 0.0,\n",
              " 'top_p': 1.0,\n",
              " 'max_length': 250,\n",
              " 'stopping_tokens': [],\n",
              " 'request_uuid': '403647f2-fd6d-40e6-a21c-098ea4870703'}"
            ]
          },
          "execution_count": 257,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-kEgjHqImqX"
      },
      "source": [
        "We will use the request_uuid to check if the completion job is done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Minx0TGWIu-D"
      },
      "outputs": [],
      "source": [
        "request_uuid=response[\"request_uuid\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkLZZTcOIRIB"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr0aQCoNYfmQ",
        "outputId": "b5f08b26-b973-40de-971a-600773e6ecc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Rows 3-7 all have the same local \"London\", but their item numbers differ. Therefore there are no repeats in this example.\n"
          ]
        }
      ],
      "source": [
        "ready = False\n",
        "while not ready:\n",
        "    r = requests.get(f\"{base_url}/get_result/{request_uuid}\")\n",
        "    response = r.json()\n",
        "    ready = response['ready']\n",
        "    if ready:\n",
        "        print(response['generated_text'])\n",
        "        break\n",
        "    # Wait 10 seconds before checking again\n",
        "    print(f\"Aguardando 10 segundos\")\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkwfJwtnI3eC"
      },
      "source": [
        "when consulting the result you may find 3 scenarios\n",
        "- Your job did not run yet, you should try again in a couple of seconds (Ready=False, message=None)\n",
        "- Your job did run and everything worked (Ready=True, message=your response)\n",
        "- Your job did run but it failed (Ready=True, message=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShA1aBfNJUI4"
      },
      "source": [
        "### Rate limiting\n",
        "\n",
        "We may adjust this during the week, but due to computational constrains we will apply a rate limit of about 2 requests per 5 seconds. If you exceed this limit you will receive an error 429. You should adjust your code accordingly.\n",
        "\n",
        "Please remember that the whole class is using a shared resource, so avoid excessive requests even if they are under the rate limit.\n",
        "\n",
        "If you encounter any errors or problems, let us know in the classroom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z3s-PFPJSVm",
        "outputId": "ec8252e2-0d3d-4331-a9df-629f36677b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 -> 200\n",
            "1 -> 200\n",
            "2 -> 200\n",
            "3 -> 429\n",
            "4 -> 429\n",
            "5 -> 429\n",
            "6 -> 429\n",
            "7 -> 429\n",
            "8 -> 429\n",
            "9 -> 429\n",
            "10 -> 429\n",
            "11 -> 429\n",
            "12 -> 200\n",
            "13 -> 429\n",
            "14 -> 429\n",
            "15 -> 429\n",
            "16 -> 429\n",
            "17 -> 429\n",
            "18 -> 429\n",
            "19 -> 429\n",
            "20 -> 429\n",
            "21 -> 429\n",
            "22 -> 429\n",
            "23 -> 200\n",
            "24 -> 429\n",
            "25 -> 429\n",
            "26 -> 429\n",
            "27 -> 429\n",
            "28 -> 429\n",
            "29 -> 429\n"
          ]
        }
      ],
      "source": [
        "for i in range(30):\n",
        "  r=requests.get(f\"{base_url}/get_result/{request_uuid}\")\n",
        "  print(i, \"->\", r.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaDzORFIJjZl"
      },
      "outputs": [],
      "source": [
        "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf-int4\")\n",
        "# llama-7b-hf, llama-65b-hf, llama-smallint-pt, llama-13b-hf-int4, llama-7b-hf-int4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rascunho"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternativa a chamar pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurando os parâmetros\n",
        "model_inputs = generator.tokenizer(texto_teste, return_tensors='pt')\n",
        "\n",
        "model_kwargs = {\n",
        "    'max_length': 550, # prompt_len: 402 + self.num_max_answer_length,\n",
        "    'num_return_sequences': generator.num_top_k,\n",
        "    'num_beams': generator.num_top_k,\n",
        "    'do_sample': generator.if_do_sample,\n",
        "    'temperature': generator.val_temperature,\n",
        "    'length_penalty': generator.val_length_penalty,\n",
        "    'bad_words_ids': generator.bad_words_ids,\n",
        "    'stopping_criteria':StoppingCriteriaList([generator.stop_criteria])\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# gerando as sequências de texto\n",
        "output = generator.model.generate(**model_inputs.to(generator.device), **model_kwargs, return_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 505])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convertendo para texto e calculando as probabilidades\n",
        "texts = generator.tokenizer.batch_decode(output, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Instruction: Based on the text, generate just one question succinctly, answered by the text, avoiding repeating words. See examples below:\\n\\nText: COVID-19 has caused unprecedented societal turmoil, triggering a rapid, still ongoing, transformation of healthcare provision on a global level. In this new landscape, it is highly important to acknowledge the challenges this pandemic poses on the care of the particularly vulnerable cancer patients and the subsequent psychosocial impact on them. We have reviewed the emerging literature around barriers to care of oncology patients and how this crisis affects them. Moreover, evolving treatment strategies and novel ways of addressing the needs of oncology patients in the new context of the pandemic are discussed.\\n\\nQuestion: What is the importance of acknowledging the challenges posed by the COVID-19 pandemic on vulnerable cancer patients?\\n\\nText: In a retrospective study, we investigated the incidence of coronavirus infection in children under the age of five years Methods: We collected 138 specimens (nasal and throat swabs) from children less than five-years-old with acute respiratory infection from October 2018 to December 2019 Then, HCoV-NL63 was investigated using real-time PCR\\n\\nQuestion: What method was used to investigate the presence of HCoV-NL63 in the collected specimens from children with acute respiratory infection?\\n\\nText: Acute respiratory distress syndrome (ARDS) is an inflammatory form of lung injury in response to various clinical entities or inciting events, quite frequently due to an underlying infection. Morbidity and mortality associated with ARDS are significant. Hence, early recognition and targeted treatment are crucial to improve clinical outcomes.\\n\\nQuestion: What is the significance of early recognition and targeted treatment in improving clinical outcomes in ARDS?\\n\\nText: The thiol-disulfide oxidoreductase thioredoxin-1 (Trx1) is known to be secreted by leukocytes and to exhibit cytokine-like properties. Extracellular effects of Trx1 require a functional active site, suggesting a redox-based mechanism of action. However, specific cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses have not been identified so far. Using a mechanism-based kinetic trapping technique to identify disulfide exchange interactions on the in\\n\\nQuestion: Â What is the significance of identifying cell surface proteins and pathways coupling extracellular Trx1 redox activity to cellular responses?']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "forward() got an unexpected keyword argument 'max_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m probabilities \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpipe\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs\u001b[39m.\u001b[39;49mto(generator\u001b[39m.\u001b[39;49mdevice), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m'\u001b[39m\u001b[39mprobs\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'max_length'"
          ]
        }
      ],
      "source": [
        "probabilities = generator.pipe.model(**model_inputs.to(generator.device), **model_kwargs, return_dict=True)['probs']\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tentando rodar com multiprocessamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_docto(docto, generator):\n",
        "    try:\n",
        "        if len(docto['text']) > 20:\n",
        "            if len(docto['title']) >= 5 :\n",
        "                texto = docto['title'] + '. ' + docto['text'] \n",
        "            else:\n",
        "                texto = docto['text'] \n",
        "            resultado = generator.generate_text(texto_contexto=texto)\n",
        "            return (docto['_id'], 'ok', resultado[0]['generated_text'])\n",
        "    except Exception as e:\n",
        "        return (docto['_id'], 'erro', str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import repeat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/171332 [00:00<?, ?it/s]Process SpawnPoolWorker-324:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'process_docto' on <module '__main__' (built-in)>\n",
            "  0%|          | 0/171332 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n",
            "\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[0;32m--> 853\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n",
            "\u001b[1;32m    854\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
            "\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[91], line 6\u001b[0m\n",
            "\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m mp\u001b[39m.\u001b[39mPool(processes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mas\u001b[39;00m pool, tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(corpus)) \u001b[39mas\u001b[39;00m pbar:\n",
            "\u001b[1;32m      5\u001b[0m     results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mimap_unordered(process_docto, \u001b[39mzip\u001b[39m(corpus[\u001b[39m0\u001b[39m:\u001b[39m100\u001b[39m], [generator]))    \n",
            "\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results:\n",
            "\u001b[1;32m      7\u001b[0m         \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;32m      8\u001b[0m             \u001b[39mif\u001b[39;00m res[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mok\u001b[39m\u001b[39m'\u001b[39m:\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n",
            "\u001b[1;32m    856\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "\u001b[0;32m--> 858\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n",
            "\u001b[1;32m    859\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[1;32m    860\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n",
            "\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n",
            "\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n",
            "\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "dict_queries_geradas = {}\n",
        "dict_erros_doctos = {}\n",
        "\n",
        "with mp.Pool(processes=1) as pool, tqdm(total=len(corpus)) as pbar:\n",
        "    results = pool.imap_unordered(process_docto, zip(corpus[0:100], [generator]))    \n",
        "    for res in results:\n",
        "        if res is not None:\n",
        "            if res[1] == 'ok':\n",
        "                dict_queries_geradas[res[0]] = res[2]\n",
        "            else:\n",
        "                dict_erros_doctos[res[0]] = res[2]\n",
        "                print(f\"Erro ao gerar query para o documento {res[0]}: {res[2]}\")\n",
        "        pbar.update()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "P9bepRrfCG3a",
        "v19yOgi9OMjD",
        "4WJ9VOUMHFz1",
        "Mebj-4wbn9k7",
        "jOGRYPn-3bmp",
        "YygV4s372xYS",
        "lcf4EzA9EhC8",
        "-LBQrhPb7gnh",
        "6tEsBGVO7NWF",
        "f3jLQOCm7Vnj",
        "F3sDzZyQoe4Z",
        "UmzDpf8FEIWz"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "treinapython39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
