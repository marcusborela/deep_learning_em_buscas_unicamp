{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmRLgbyi_Dvg"
   },
   "source": [
    "# Organizando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRETORIO_TRABALHO = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DIRETORIO_TRABALHO), f\"Path para {DIRETORIO_TRABALHO} não existe!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DKAZ8CWCAM3-"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9XgIWvkkH-kn"
   },
   "outputs": [],
   "source": [
    "def mostra_memoria(lista_mem=['cpu']):\n",
    "  \"\"\"\n",
    "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
    "\n",
    "  Parâmetros:\n",
    "  -----------\n",
    "  lista_mem : list, opcional\n",
    "      Lista com strings 'cpu' e/ou 'gpu'. \n",
    "      'cpu' - exibe informações de memória da CPU.\n",
    "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
    "      O valor padrão é ['cpu'].\n",
    "\n",
    "  Saída:\n",
    "  -------\n",
    "  A função não retorna nada, apenas exibe as informações na tela.\n",
    "\n",
    "  Exemplo de uso:\n",
    "  ---------------\n",
    "  Para exibir informações de memória da CPU:\n",
    "      mostra_memoria(['cpu'])\n",
    "\n",
    "  Para exibir informações de memória da CPU e GPU:\n",
    "      mostra_memoria(['cpu', 'gpu'])\n",
    "  \n",
    "  Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "  \"\"\"  \n",
    "  if 'cpu' in lista_mem:\n",
    "    vm = virtual_memory()\n",
    "    ram={}\n",
    "    ram['total']=round(vm.total / 1e9,2)\n",
    "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
    "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
    "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
    "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
    "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
    "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
    "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
    "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
    "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
    "    print('/nGPU')\n",
    "    gpu_info = !nvidia-smi\n",
    "  if 'gpu' in lista_mem:\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "      print('Not connected to a GPU')\n",
    "    else:\n",
    "      print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dri9iiMAvCT",
    "outputId": "53aebd5a-e29f-4c8e-d233-5221aae9f9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 56.65\n",
      " used 9.39\n",
      " free 35.1\n",
      " cached 21.28\n",
      " buffers 1.59\n",
      "/nGPU\n",
      "Sun Apr  9 14:30:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 67%   50C    P8    27W / 370W |     61MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1241      G   /usr/lib/xorg/Xorg                 45MiB |\n",
      "|    0   N/A  N/A      1381      G   /usr/bin/gnome-shell               14MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achvQ78sa3p3"
   },
   "source": [
    "## Fixando as seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AG9RjMb8Qlot"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bkETIyWGkbOf"
   },
   "outputs": [],
   "source": [
    "def inicializa_seed(num_semente:int=123):\n",
    "  \"\"\"\n",
    "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
    "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
    "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
    "  \n",
    "  Args:\n",
    "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
    "  \n",
    "  References:\n",
    "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
    "  \"\"\"\n",
    "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
    "  random.seed(num_semente)\n",
    "  np.random.seed(num_semente)\n",
    "  torch.manual_seed(num_semente)\n",
    "  \n",
    "  # Define as sementes da GPU\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  #torch.cuda.manual_seed(num_semente)\n",
    "  #Cuda algorithms\n",
    "  #torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ViMcw_kVkbOf"
   },
   "outputs": [],
   "source": [
    "num_semente=123\n",
    "inicializa_seed(num_semente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v2gtkEPhA0t"
   },
   "source": [
    "## Preparando para debug e display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BJ6S4P5Hw4iG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kebsl1uQDFUf",
    "outputId": "4b1ed269-722b-4a2a-f1a0-908d1683cc62"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rnR2kDS_2FgZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZEqQ7mKg5fs"
   },
   "source": [
    "https://zohaib.me/debugging-in-google-collab-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjrlXHq1hC8n",
    "outputId": "18d94254-4bf8-4632-ef84-012727944746"
   },
   "outputs": [],
   "source": [
    "# !pip install -Uqq ipdb\n",
    "import ipdb\n",
    "# %pdb off # desativa debug em exceção\n",
    "# %pdb on  # ativa debug em exceção\n",
    "# ipdb.set_trace(context=8)  para execução nesse ponto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wQ5pmlOHxHhk"
   },
   "outputs": [],
   "source": [
    "def config_display():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de display do Pandas.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configurando formato saída Pandas\n",
    "  # define o número máximo de colunas que serão exibidas\n",
    "  pd.options.display.max_columns = None\n",
    "\n",
    "  # define a largura máxima de uma linha\n",
    "  pd.options.display.width = 1000\n",
    "\n",
    "  # define o número máximo de linhas que serão exibidas\n",
    "  pd.options.display.max_rows = 100\n",
    "\n",
    "  # define o número máximo de caracteres por coluna\n",
    "  pd.options.display.max_colwidth = 50\n",
    "\n",
    "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
    "  pd.options.display.show_dimensions = True\n",
    "\n",
    "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
    "  pd.options.display.precision = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b2tDy72ATNHs"
   },
   "outputs": [],
   "source": [
    "def config_debug():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
    "  transformers e datasets.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define opções de impressão de tensores para o modo científico\n",
    "  torch.set_printoptions(sci_mode=True) \n",
    "  \"\"\"\n",
    "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
    "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
    "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
    "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
    "    uma melhor compreensão dos números envolvidos.  \n",
    "  \"\"\"\n",
    "\n",
    "  # Habilita detecção de anomalias no autograd do PyTorch\n",
    "  torch.autograd.set_detect_anomaly(True)\n",
    "  \"\"\"\n",
    "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
    "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
    "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
    "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
    "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
    "    antes que se torne um problema maior.\n",
    "\n",
    "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
    "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
    "    ela deve ser usada com cautela e apenas para depuração.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
    "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "  \"\"\"\n",
    "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
    "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
    "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
    "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
    "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
    "    e removê-lo após a solução do problema.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define o nível de verbosity do pacote transformers para info\n",
    "  # transformers.utils.logging.set_verbosity_info() \n",
    "  \n",
    "  \n",
    "  \"\"\"\n",
    "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
    "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
    "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
    "\n",
    "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
    "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
    "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
    "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
    "    necessidade de cada tarefa.\n",
    "  \n",
    "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
    "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
    "  \n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Define o modo verbose do xmode, que é utilizado no debug\n",
    "  # %xmode Verbose \n",
    "\n",
    "  \"\"\"\n",
    "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
    "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
    "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
    "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
    "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
    "\n",
    "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
    "    comentar a linha acima e descomentar a linha abaixo:\n",
    "    %xmode Plain\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "    Dica:\n",
    "    1.  pdb (Python Debugger)\n",
    "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
    "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
    "\n",
    "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
    "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
    "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
    "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
    "\n",
    "\n",
    "    2. ipdb\n",
    "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
    "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
    "      \n",
    "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
    "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
    "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
    "      comandos para investigar o comportamento.\n",
    "\n",
    "      Durante a depuração, você pode usar comandos:\n",
    "        next (para executar a próxima linha de código), \n",
    "        step (para entrar em uma função chamada na próxima linha de código) \n",
    "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
    "\n",
    "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
    "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
    "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Tb4aqtcExR84"
   },
   "outputs": [],
   "source": [
    "config_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5Bq4043fkfh",
    "outputId": "fa8e5db1-1feb-4393-fb66-d394d1ad693c"
   },
   "outputs": [],
   "source": [
    "config_debug()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga dos dados msmarco_triples.train.tiny.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(f\"{DIRETORIO_TRABALHO}/msmarco_triples.train.tiny.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIRETORIO_TRABALHO}/msmarco_triples.train.tiny.tsv\"):\n",
    "    !wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
    "    !mv msmarco_triples.train.tiny.tsv {DIRETORIO_TRABALHO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{DIRETORIO_TRABALHO}/msmarco_triples.train.tiny.tsv\", delimiter=\"\\t\", \n",
    "                 header=None, names=[\"query\", \"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "igtPldcHPwCS",
    "outputId": "594683c5-90e1-43d7-c00e-8d066b3fe161"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is a little caffeine ok during pregnancy</td>\n",
       "      <td>We donât know a lot about the effects of caf...</td>\n",
       "      <td>It is generally safe for pregnant women to eat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what fruit is native to australia</td>\n",
       "      <td>Passiflora herbertiana. A rare passion fruit n...</td>\n",
       "      <td>The kola nut is the fruit of the kola tree, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how large is the canadian military</td>\n",
       "      <td>The Canadian Armed Forces. 1  The first large-...</td>\n",
       "      <td>The Canadian Physician Health Institute (CPHI)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>types of fruit trees</td>\n",
       "      <td>Cherry. Cherry trees are found throughout the ...</td>\n",
       "      <td>The kola nut is the fruit of the kola tree, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how many calories a day are lost breastfeeding</td>\n",
       "      <td>Not only is breastfeeding better for the baby,...</td>\n",
       "      <td>However, you still need some niacin each day; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query                                           positive                                           negative\n",
       "0        is a little caffeine ok during pregnancy  We donât know a lot about the effects of caf...  It is generally safe for pregnant women to eat...\n",
       "1               what fruit is native to australia  Passiflora herbertiana. A rare passion fruit n...  The kola nut is the fruit of the kola tree, a ...\n",
       "2              how large is the canadian military  The Canadian Armed Forces. 1  The first large-...  The Canadian Physician Health Institute (CPHI)...\n",
       "3                            types of fruit trees  Cherry. Cherry trees are found throughout the ...  The kola nut is the fruit of the kola tree, a ...\n",
       "4  how many calories a day are lost breastfeeding  Not only is breastfeeding better for the baby,...  However, you still need some niacin each day; ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ua7i1BCTRTGx",
    "outputId": "7e8162d8-a630-4014-9e2c-057d66a94768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query       object\n",
       "positive    object\n",
       "negative    object\n",
       "Length: 3, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ3S90BBTrKJ"
   },
   "source": [
    "Verificando correção do arquivo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qm7ss2BfTlOu",
    "outputId": "a7140122-cf21-46cd-cb04-decdcf3cb959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query       0\n",
      "positive    0\n",
      "negative    0\n",
      "Length: 3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q ftfy\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'We donât know a lot about the effects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We don't know a lot about the effects\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftfy.fix_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query        34.2256364\n",
       "positive    353.7535455\n",
       "negative    340.4646364\n",
       "Length: 3, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.applymap(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pois treinaremos doc2query apenas para geração de queries relevantes\n",
    "del df['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Lg0b1wj0SEsY"
   },
   "outputs": [],
   "source": [
    "df['query'] = df['query'].apply(ftfy.fix_text)\n",
    "df['positive'] = df['positive'].apply(ftfy.fix_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "igtPldcHPwCS",
    "outputId": "594683c5-90e1-43d7-c00e-8d066b3fe161"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is a little caffeine ok during pregnancy</td>\n",
       "      <td>We don't know a lot about the effects of caffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what fruit is native to australia</td>\n",
       "      <td>Passiflora herbertiana. A rare passion fruit n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how large is the canadian military</td>\n",
       "      <td>The Canadian Armed Forces. 1  The first large-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>types of fruit trees</td>\n",
       "      <td>Cherry. Cherry trees are found throughout the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how many calories a day are lost breastfeeding</td>\n",
       "      <td>Not only is breastfeeding better for the baby,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query                                           positive\n",
       "0        is a little caffeine ok during pregnancy  We don't know a lot about the effects of caffe...\n",
       "1               what fruit is native to australia  Passiflora herbertiana. A rare passion fruit n...\n",
       "2              how large is the canadian military  The Canadian Armed Forces. 1  The first large-...\n",
       "3                            types of fruit trees  Cherry. Cherry trees are found throughout the ...\n",
       "4  how many calories a day are lost breastfeeding  Not only is breastfeeding better for the baby,...\n",
       "\n",
       "[5 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisão em treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(df['positive'].values, \n",
    "                                                      df['query'].values,\n",
    "                                                      test_size=1000, \n",
    "                                                      random_state=num_semente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (1000,), 'how many fitbit steps equal a mile')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_valid), Y_valid.shape, Y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (10000,),\n",
       " 'There are 40 weeks in a school year - and 12 weeks of holidays. Unless you live somewhere snowy and you have to take snow days. These can extend the length by another week or so. There are 40 weeks in a school year - and 12 weeks of holidays. Unless you live somewhere snowy and you have to take snow days.')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), X_train.shape, X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando o tokenizador do t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, None, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id,tokenizer.cls_token_id,tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='it is just a test!'\n",
    "y='it is just a continuation!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 19, 131, 3, 9, 794, 55, 1, 34, 19, 131, 3, 9, 25192, 55, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode_plus(x,y)['input_ids']\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'just', '', 'a', 'test', '!', '</s>', 'it', 'is', 'just', '', 'a', 'continuation', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34, 19, 131, 3, 9, 794, 55, 1], [34, 19, 131, 3, 9, 25192, 55, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([x,y])['input_ids']\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '<unk>', '<pad>', '<extra_id_0>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511, 512)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence, tokenizer.model_max_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "      Classe para criar um dataset de texto e query.\n",
    "    \"\"\"  \n",
    "    def __init__(self, texts: np.ndarray, queries:np.ndarray, tokenizer):\n",
    "      \"\"\"\n",
    "      Inicializa um novo objeto MyDataset.\n",
    "\n",
    "      Args:\n",
    "          texts (np.ndarray): um array com as strings de texto. Cada linha deve ter 2 strings.\n",
    "          tokenizer: um objeto tokenizer do Hugging Face Transformers.\n",
    "          max_seq_length (int): o tamanho máximo da sequência a ser considerado.\n",
    "      Raises:\n",
    "          AssertionError: se os parâmetros não estiverem no formato esperado.\n",
    "      \"\"\"\n",
    "      # Verifica se os parâmetros são do tipo esperado\n",
    "      assert isinstance(texts, np.ndarray), f\"Parâmetro texts deve ser do tipo np.ndarray e não {type(texts)}\"\n",
    "      assert isinstance(queries, np.ndarray), f\"Parâmetro queries deve ser do tipo np.ndarray e não {type(queries)}\"\n",
    "      for row in texts:\n",
    "          assert isinstance(row, str), f\"Each element in texts.row must be a string e não {type(row)}\"\n",
    "          break\n",
    "\n",
    "      self.max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "      # Salvar os dados dos tensores para adiantar o tempo de processamento\n",
    "      self.tokenized_texts = tokenizer.batch_encode_plus(texts, return_length=True)\n",
    "      self.tokenized_queries = tokenizer.batch_encode_plus(queries, return_attention_mask=False, return_length=True)\n",
    "      \n",
    "      print(\"tokenized_texts size stats:\\n{}\\n\".format(stats.describe(self.tokenized_texts['length'])))\n",
    "      print(\"tokenized_queries size stats:\\n{}\\n\".format(stats.describe(self.tokenized_queries['length']))) \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "          Retorna o tamanho do dataset (= tamanho do array texts)\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "          Retorna um dicionário com os dados do texto e sua classe correspondente, em um formato que pode \n",
    "          ser usado pelo dataloader do PyTorch para alimentar um modelo de aprendizado de máquina.\n",
    "        \"\"\"\n",
    "        # print(f\"getitem index={index} self.tokenized_texts['input_ids'][index] {self.tokenized_texts['input_ids'][index]}\")\n",
    "        saida = {\"input_ids\": self.tokenized_texts[\"input_ids\"][index], \n",
    "                \"attention_mask\": self.tokenized_texts[\"attention_mask\"][index], \n",
    "                \"labels\": self.tokenized_texts[\"input_ids\"][index]} \n",
    "        # print(f\"saida {saida}\")\n",
    "        return saida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17AcNhe2C4tb"
   },
   "source": [
    "#### Testando o MyDataset e o Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_tNkc8kmC8ie"
   },
   "outputs": [],
   "source": [
    "# Cria dados fictícios\n",
    "texts = np.array(['This is the first text',\n",
    "                  'This is text 2.1',\n",
    "                  'This is text 3.1',\n",
    "                  'This is text 4.1',\n",
    "                  'This is text 5.1',\n",
    "                  'This is text 6.1',\n",
    "                  'This is text 7.1'])\n",
    "queries = np.array(['This is the first query',\n",
    "                  'This is query 2.1',\n",
    "                  'This is query 3.1',\n",
    "                  'This is query 4.1',\n",
    "                  'This is query 5.1',\n",
    "                  'This is query 6.1',\n",
    "                  'This is query 7.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 19, 8, 166, 1499, 1], [100, 19, 1499, 3, 14489, 1], [100, 19, 1499, 3, 18495, 1], [100, 19, 1499, 3, 19708, 1], [100, 19, 1499, 3, 20519, 1], [100, 19, 1499, 3, 23769, 1], [100, 19, 1499, 3, 25059, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbaCMoKRDy20",
    "outputId": "1853f452-f5a4-4803-d33b-b0dd58c8ba65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_texts size stats:\n",
      "DescribeResult(nobs=7, minmax=(6, 6), mean=6.0, variance=0.0, skewness=nan, kurtosis=nan)\n",
      "\n",
      "tokenized_queries size stats:\n",
      "DescribeResult(nobs=7, minmax=(6, 6), mean=6.0, variance=0.0, skewness=nan, kurtosis=nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110915/1543394617.py:29: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  print(\"tokenized_texts size stats:\\n{}\\n\".format(stats.describe(self.tokenized_texts['length'])))\n",
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/scipy/stats/_stats_py.py:1522: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  sk = skew(a, axis, bias=bias)\n",
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/scipy/stats/_stats_py.py:1523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurt = kurtosis(a, axis, bias=bias)\n",
      "/tmp/ipykernel_110915/1543394617.py:30: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  print(\"tokenized_queries size stats:\\n{}\\n\".format(stats.describe(self.tokenized_queries['length'])))\n"
     ]
    }
   ],
   "source": [
    "# Cria um objeto da classe MyDataset\n",
    "dummy_dataset = MyDataset(texts=texts, queries=queries, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFNuWyMYDz7v",
    "outputId": "f74ff3bd-84d6-42a8-a7de-88501b882de9"
   },
   "outputs": [],
   "source": [
    "# Testa o método __len__()\n",
    "assert len(dummy_dataset) == 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testa o método __getitem__()\n",
    "sample = dummy_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert set(sample.keys()) == {'input_ids', 'attention_mask', 'labels'} # \n",
    "assert isinstance(sample['input_ids'], list)\n",
    "assert isinstance(sample['attention_mask'], list)\n",
    "assert isinstance(sample['labels'], list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9ng05CvFu1t",
    "outputId": "476a8515-b00e-4df7-b7f4-be4382ae8917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [100, 19, 8, 166, 1499, 1], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [100, 19, 8, 166, 1499, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando dataset e dataloaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5INVpV1RGuau"
   },
   "outputs": [],
   "source": [
    "dummy_loader = DataLoader(dummy_dataset, batch_size=3, shuffle=False, num_workers=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(dummy_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([100, 100, 100]),\n",
       "  tensor([19, 19, 19]),\n",
       "  tensor([   8, 1499, 1499]),\n",
       "  tensor([166,   3,   3]),\n",
       "  tensor([ 1499, 14489, 18495]),\n",
       "  tensor([1, 1, 1])],\n",
       " 'attention_mask': [tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1])],\n",
       " 'labels': [tensor([100, 100, 100]),\n",
       "  tensor([19, 19, 19]),\n",
       "  tensor([   8, 1499, 1499]),\n",
       "  tensor([166,   3,   3]),\n",
       "  tensor([ 1499, 14489, 18495]),\n",
       "  tensor([1, 1, 1])]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_texts size stats:\n",
      "DescribeResult(nobs=1000, minmax=(20, 299), mean=86.623, variance=1287.2781491491492, skewness=1.203754702156062, kurtosis=1.888567910351541)\n",
      "\n",
      "tokenized_queries size stats:\n",
      "DescribeResult(nobs=1000, minmax=(3, 25), mean=9.517, variance=10.392103103103105, skewness=0.8108317830729576, kurtosis=1.3237936614062669)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = MyDataset(X_train, Y_train, tokenizer)\n",
    "val_dataset = MyDataset(X_valid, Y_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_texts size stats:\n",
      "DescribeResult(nobs=10000, minmax=(14, 326), mean=86.9556, variance=1262.5688855285528, skewness=1.138437592548724, kurtosis=1.5776621817771685)\n",
      "\n",
      "tokenized_queries size stats:\n",
      "DescribeResult(nobs=10000, minmax=(3, 57), mean=9.5016, variance=12.030400480048003, skewness=1.7924405763231719, kurtosis=10.845883054521773)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(X_train, Y_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código fonte em https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    print(f\"Em compute_metrics: result={result}\")\n",
    "\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM #T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainerCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Fonte: Eduardo Seiti \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, best_validation_yet=99999, model=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.best_validation_metric = best_validation_yet\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
    "        print(f'CustomTrainerCallback.on_evaluate - Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} metrics.keys()' )\n",
    "\n",
    "        print(f\"metrics['eval_loss']={metrics['eval_loss']} metrics['eval_bleu']={metrics['eval_bleu']} self.best_validation_metric={self.best_validation_metric}\")\n",
    "\n",
    "        # caso queira salvar por aqui:\n",
    "        #if metrics['eval_bleu'] > self.best_validation_metric:\n",
    "        #    nome_arquivo = f\"{DIRETORIO_TRABALHO}/model-checkpoint-{state.global_step}-{metrics['eval_bleu']:.4f}\"\n",
    "        #    print(f\"vou salvar {nome_arquivo}\")\n",
    "        #    self.model.save_pretrained(nome_arquivo)\n",
    "        #    self.best_validation_metric = metrics['eval_bleu']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parm_model, parm_optimizer, parm_lr_scheduler,\n",
    "          num_batch_size:int=24, num_epochs:int=3, num_acum_steps:int=8):   # , num_steps_eval:int=100):\n",
    "    \"\"\"\n",
    "    Função auxiliar de treinamento. \n",
    "    parm_model: o modelo Seq2Seq que será treinado.\n",
    "    num_batch_size: o tamanho do lote (batch size) para treinamento e avaliação. Padrão é 24, o que significa que o valor será determinado pelo parâmetro per_device_train_batch_size em training_args.\n",
    "    num_epochs: o número de épocas de treinamento. Padrão é 3.\n",
    "    num_acum_steps: o número de passos de acumulação de gradiente. Padrão é 8.\n",
    "    num_steps_eval: o número de passos para avaliação durante o treinamento. Padrão é 100.\n",
    "    Fonte versão base: colega Monique\n",
    "\n",
    "    \"\"\"\n",
    "    global steps, diretorio, tokenizer, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "    num_training_steps = num_epochs * int(len(train_dataset) // (num_batch_size * num_acum_steps))\n",
    "    # será avaliado a cada époica\n",
    "    if num_epochs >= 1:\n",
    "        num_steps_eval = math.ceil(num_training_steps / num_epochs)  \n",
    "    else:\n",
    "        num_steps_eval = math.ceil(num_training_steps)\n",
    "    print(f\"num_training_steps = {num_training_steps} batch size = {num_batch_size} num_steps_eval={num_steps_eval}\")\n",
    "\n",
    "    trainer_callback = CustomTrainerCallback(best_validation_yet=-1, model=parm_model)\n",
    "\n",
    "    # Argumentos de treinamento do modelo Seq2Seq\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=DIRETORIO_TRABALHO, # Onde os modelos são salvos\n",
    "        logging_dir = DIRETORIO_TRABALHO+\"/logs\",\n",
    "        # logging_strategy=\"steps\",  # Especifique a estratégia de registro (por passos)\n",
    "        logging_strategy=\"steps\",  # Especifique a estratégia de registro (por passos)\n",
    "        logging_steps=num_steps_eval, # Número de etapas para registrar os logs\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=num_batch_size, # Tamanho do batch por dispositivo durante o treinamento\n",
    "        per_device_eval_batch_size=num_batch_size, # Tamanho do batch por dispositivo durante a avaliação\n",
    "        gradient_accumulation_steps=num_acum_steps, # Número de etapas de acumulação de gradiente\n",
    "        evaluation_strategy='steps', # Estratégia de avaliação durante o treinamento\n",
    "        eval_steps=num_steps_eval, # Número de etapas para realizar a avaliação\n",
    "        save_steps=num_steps_eval, # Em cada avaliação\n",
    "        predict_with_generate=True, # Permitir geração de predições com o modelo\n",
    "        # lr_scheduler_type=None,  # Set to None to disable the default scheduler\n",
    "        fp16=True, # Usar precisão mista (half-precision) para acelerar o treinamento   \n",
    "        num_train_epochs=num_epochs, # Número de épocas de treinamento\n",
    "        report_to=\"neptune\",\n",
    "        dataloader_pin_memory = True, # os dados carregados em memória pelo DataLoader são fixados (pinned) na memória do sistema \n",
    "                                      # Pode acelerar a transferência dos dados para a GPU,\n",
    "                                      #  uma vez que a GPU pode ler os dados diretamente da memória fixada sem precisar fazer uma cópia adicional dos dados\n",
    "        load_best_model_at_end=True, # Carregar o melhor modelo ao final do treinamento\n",
    "        metric_for_best_model='bleu', # Métrica usada para selecionar o melhor modelo\n",
    "        # save_total_limit=2 # Número máximo de checkpoints a serem salvos\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Objeto de collator de dados para ajustar os dados de treinamento\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=parm_model,\n",
    "        label_pad_token_id=-100, # Token de padding para os rótulos\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None, # Valor de padding múltiplo de 8 para aproveitar otimizações com FP16\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Inicialização do treinador Seq2Seq\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=parm_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset, # Conjunto de dados de treinamento\n",
    "        eval_dataset=val_dataset, # Conjunto de dados de avaliação\n",
    "        data_collator=data_collator, # Collator de dados\n",
    "        tokenizer=tokenizer, # Tokenizer\n",
    "        callbacks=[trainer_callback], \n",
    "        optimizers=(parm_optimizer, parm_lr_scheduler),\n",
    "        compute_metrics=compute_metrics, # Função para calcular as métricas de avaliação\n",
    "    )\n",
    "\n",
    "    # Treinamento do modelo\n",
    "    train_results = trainer.train()\n",
    "    \n",
    "    return trainer, train_results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integração com Neptune (não deu certo)\n",
    "\n",
    "Mais em https://docs.neptune.ai/integrations/transformers/#__tabbed_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neptune.new as neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEPTUNE_ALLOW_SELF_SIGNED_CERTIFICATE'] = 'TRUE'\n",
    "os.environ['NEPTUNE_PROJECT'] = 'marcusborela/IA386DD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEPTUNE_API_TOKEN'] = getpass.getpass('Informe NEPTUNE_API_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch_size=8\n",
    "num_epochs=0.1\n",
    "num_acum_steps=8\n",
    "num_steps_eval=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonte original: Eduardo Seiti \n",
    "\"\"\"\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)\n",
    "lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, \n",
    "                                                               num_warmup_steps=10,\n",
    "                                                               num_training_steps=num_training_steps, \n",
    "                                                               num_cycles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adafactor(model.parameters(), relative_step=False,lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps = 15.600000000000001 batch size = 8 num_steps_eval=16\n",
      "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-55\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:38<00:00,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0045, 'learning_rate': 0.0, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 16/16 [03:33<00:00,  6.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6629, 'gen_len': 18.997}\n",
      "{'eval_loss': 0.0030814576894044876, 'eval_bleu': 2.6629, 'eval_gen_len': 18.997, 'eval_runtime': 115.5457, 'eval_samples_per_second': 8.655, 'eval_steps_per_second': 1.082, 'epoch': 0.1}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 16:10:17] metrics.keys()\n",
      "metrics['eval_loss']=0.0030814576894044876 metrics['eval_bleu']=2.6629 self.best_validation_metric=-1\n",
      "vou salvar /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/model-checkpoint-16-2.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [03:37<00:00,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 218.8266, 'train_samples_per_second': 4.57, 'train_steps_per_second': 0.073, 'train_loss': 0.004458750132471323, 'epoch': 0.1}\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-55\n",
      "CustomTrainerCallback.on_train_end - Momento: [2023-Apr-09 16:10:22] metrics.keys()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer, train_results \u001b[39m=\u001b[39m train(parm_model\u001b[39m=\u001b[39;49mmodel, parm_optimizer \u001b[39m=\u001b[39;49m optimizer, parm_lr_scheduler \u001b[39m=\u001b[39;49m lr_scheduler, num_batch_size\u001b[39m=\u001b[39;49mnum_batch_size, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, num_acum_steps\u001b[39m=\u001b[39;49mnum_acum_steps)\n",
      "Cell \u001b[0;32mIn[170], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(parm_model, parm_optimizer, parm_lr_scheduler, num_batch_size, num_epochs, num_acum_steps)\u001b[0m\n\u001b[1;32m     64\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     65\u001b[0m     model\u001b[39m=\u001b[39mparm_model,\n\u001b[1;32m     66\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics, \u001b[39m# Função para calcular as métricas de avaliação\u001b[39;00m\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[39m# Treinamento do modelo\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     78\u001b[0m \u001b[39m#metrics = trainer.evaluate()\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# print(metrics)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m trainer, train_results\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:2049\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m             logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDeleting older checkpoint [\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint\u001b[39m}\u001b[39;00m\u001b[39m] due to args.save_total_limit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2047\u001b[0m             shutil\u001b[39m.\u001b[39mrmtree(checkpoint)\n\u001b[0;32m-> 2049\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_end(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[1;32m   2051\u001b[0m \u001b[39mreturn\u001b[39;00m TrainOutput(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, train_loss, metrics)\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer_callback.py:356\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_end\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer_callback.py:397\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    396\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 397\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[1;32m    398\u001b[0m             args,\n\u001b[1;32m    399\u001b[0m             state,\n\u001b[1;32m    400\u001b[0m             control,\n\u001b[1;32m    401\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    402\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    403\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    404\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[1;32m    405\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    406\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[1;32m    407\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[116], line 30\u001b[0m, in \u001b[0;36mCustomTrainerCallback.on_train_end\u001b[0;34m(self, args, state, control, model, metrics, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_end\u001b[39m(\u001b[39mself\u001b[39m, args, state, control,  model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, metrics\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCustomTrainerCallback.on_train_end - Momento: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS]\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m metrics.keys()\u001b[39m\u001b[39m'\u001b[39m )\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmetrics[\u001b[39m\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]=\u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39;49m\u001b[39meval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m metrics[\u001b[39m\u001b[39m'\u001b[39m\u001b[39meval_bleu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]=\u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39m\u001b[39meval_bleu\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m self.best_validation_metric=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_validation_metric\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m metrics[\u001b[39m'\u001b[39m\u001b[39meval_bleu\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_validation_metric:\n\u001b[1;32m     34\u001b[0m         nome_arquivo \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDIRETORIO_TRABALHO\u001b[39m}\u001b[39;00m\u001b[39m/model-checkpoint-\u001b[39m\u001b[39m{\u001b[39;00mstate\u001b[39m.\u001b[39mglobal_step\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39m\u001b[39meval_bleu\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer, train_results = train(parm_model=model, parm_optimizer = optimizer, parm_lr_scheduler = lr_scheduler, num_batch_size=num_batch_size, num_epochs=num_epochs, num_acum_steps=num_acum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/checkpoint-450/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/checkpoint-450/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/checkpoint-450.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/checkpoint-450/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = T5ForConditionalGeneration.from_pretrained(f\"{DIRETORIO_TRABALHO}/checkpoint-450\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "100%|██████████| 125/125 [01:53<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008388221031054854, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 113.5132, 'eval_samples_per_second': 8.81, 'eval_steps_per_second': 1.101, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query\n",
      "Configuration saved in /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/config.json\n",
      "Configuration saved in /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/generation_config.json\n",
      "Model weights saved in /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/pytorch_model.bin\n",
      "tokenizer config file saved in /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/tokenizer_config.json\n",
      "Special tokens file saved in /home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 468\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size =  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/468 [1:08:22<132:12:11, 1025.72s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.70 GiB total capacity; 20.39 GiB already allocated; 50.25 MiB free; 22.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[39m=\u001b[39m train(model, \u001b[39m32\u001b[39;49m, \u001b[39m12\u001b[39;49m)\n        trainer \u001b[0;34m= <transformers.trainer_seq2seq.Seq2SeqTrainer object at 0x7f58aca8bee0>\u001b[0m\u001b[0;34m\n        \u001b[0mmodel \u001b[0;34m= T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)\u001b[0m\n",
      "Cell \u001b[0;32mIn[77], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model=T5ForConditionalGeneration(\n  (shared): Embeddin...n_features=768, out_features=32128, bias=False)\n), batch_size=32, epochs=12)\u001b[0m\n\u001b[1;32m     30\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForSeq2Seq( \n\u001b[1;32m     31\u001b[0m     tokenizer,\n\u001b[1;32m     32\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m     label_pad_token_id\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     34\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m \u001b[39mif\u001b[39;00m training_args\u001b[39m.\u001b[39mfp16 \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     38\u001b[0m                         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     39\u001b[0m                         train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     44\u001b[0m                         )\n\u001b[0;32m---> 46\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n        trainer \u001b[0;34m= <transformers.trainer_seq2seq.Seq2SeqTrainer object at 0x7f58c429afd0>\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self=<transformers.trainer_seq2seq.Seq2SeqTrainer object>, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None, **kwargs={})\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n        inner_training_loop \u001b[0;34m= functools.partial(<bound method Trainer._inner_training_loop of <transformers.trainer_seq2seq.Seq2SeqTrainer object at 0x7f58c429afd0>>, batch_size=32)\u001b[0m\u001b[0;34m\n        \u001b[0margs \u001b[0;34m= Seq2SeqTrainingArguments(output_dir='/home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=12, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query/runs/Apr08_23-13-39_borela-wks', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=2, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='cuda_amp', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='/home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='bleu', greater_is_better=True, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, sortish_sampler=False, predict_with_generate=True, generation_max_length=None, generation_num_beams=None)\u001b[0m\u001b[0;34m\n        \u001b[0mresume_from_checkpoint \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mtrial \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mignore_keys_for_eval \u001b[0;34m= None\u001b[0m\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self=<transformers.trainer_seq2seq.Seq2SeqTrainer object>, batch_size=32, args=Seq2SeqTrainingArguments(output_dir='/home/borel...ation_max_length=None, generation_num_beams=None), resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n        model \u001b[0;34m= T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= <transformers.trainer_seq2seq.Seq2SeqTrainer object at 0x7f58c429afd0>\u001b[0m\u001b[0;34m\n        \u001b[0minputs \u001b[0;34m= {'input_ids': tensor([[ 1129,   503,    25,  ...,     0,     0,     0],\n        [   71,  1911,     7,  ...,     0,     0,     0],\n        [17355,   127,     7,  ...,     0,     0,     0],\n        ...,\n        [ 6069,     3, 26306,  ...,     0,     0,     0],\n        [15476,    13, 22986,  ...,     0,     0,     0],\n        [   27,  1310,  2944,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 1129,   503,    25,  ...,  -100,  -100,  -100],\n        [   71,  1911,     7,  ...,  -100,  -100,  -100],\n        [17355,   127,     7,  ...,  -100,  -100,  -100],\n        ...,\n        [ 6069,     3, 26306,  ...,  -100,  -100,  -100],\n        [15476,    13, 22986,  ...,  -100,  -100,  -100],\n        [   27,  1310,  2944,  ...,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[    0,  1129,   503,  ...,     0,     0,     0],\n        [    0,    71,  1911,  ...,     0,     0,     0],\n        [    0, 17355,   127,  ...,     0,     0,     0],\n        ...,\n        [    0,  6069,     3,  ...,     0,     0,     0],\n        [    0, 15476,    13,  ...,     0,     0,     0],\n        [    0,    27,  1310,  ...,     0,     0,     0]])}\u001b[0m\u001b[0;34m\n        \u001b[0mtr_loss_step \u001b[0;34m= tensor(3.6756e-04, device='cuda:0')\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self=<transformers.trainer_seq2seq.Seq2SeqTrainer object>, model=T5ForConditionalGeneration(\n  (shared): Embeddin...n_features=768, out_features=32128, bias=False)\n), inputs={'input_ids': tensor([[ 1129,   503,    25,  ......0,  ...,     0,     0,     0]], device='cuda:0')})\u001b[0m\n\u001b[1;32m   2642\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2644\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2645\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n        inputs \u001b[0;34m= {'input_ids': tensor([[ 1129,   503,    25,  ...,     0,     0,     0],\n        [   71,  1911,     7,  ...,     0,     0,     0],\n        [17355,   127,     7,  ...,     0,     0,     0],\n        ...,\n        [ 6069,     3, 26306,  ...,     0,     0,     0],\n        [15476,    13, 22986,  ...,     0,     0,     0],\n        [   27,  1310,  2944,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ 1129,   503,    25,  ...,  -100,  -100,  -100],\n        [   71,  1911,     7,  ...,  -100,  -100,  -100],\n        [17355,   127,     7,  ...,  -100,  -100,  -100],\n        ...,\n        [ 6069,     3, 26306,  ...,  -100,  -100,  -100],\n        [15476,    13, 22986,  ...,  -100,  -100,  -100],\n        [   27,  1310,  2944,  ...,  -100,  -100,  -100]], device='cuda:0'), 'decoder_input_ids': tensor([[    0,  1129,   503,  ...,     0,     0,     0],\n        [    0,    71,  1911,  ...,     0,     0,     0],\n        [    0, 17355,   127,  ...,     0,     0,     0],\n        ...,\n        [    0,  6069,     3,  ...,     0,     0,     0],\n        [    0, 15476,    13,  ...,     0,     0,     0],\n        [    0,    27,  1310,  ...,     0,     0,     0]], device='cuda:0')}\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= <transformers.trainer_seq2seq.Seq2SeqTrainer object at 0x7f58c429afd0>\u001b[0m\u001b[0;34m\n        \u001b[0mmodel \u001b[0;34m= T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2648\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self=<transformers.trainer_seq2seq.Seq2SeqTrainer object>, model=T5ForConditionalGeneration(\n  (shared): Embeddin...n_features=768, out_features=32128, bias=False)\n), inputs={'input_ids': tensor([[ 1129,   503,    25,  ......0,  ...,     0,     0,     0]], device='cuda:0')}, return_outputs=False)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2676\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2677\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n        model \u001b[0;34m= T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0minputs \u001b[0;34m= {'input_ids': tensor([[ 1129,   503,    25,  ...,     0,     0,     0],\n        [   71,  1911,     7,  ...,     0,     0,     0],\n        [17355,   127,     7,  ...,     0,     0,     0],\n        ...,\n        [ 6069,     3, 26306,  ...,     0,     0,     0],\n        [15476,    13, 22986,  ...,     0,     0,     0],\n        [   27,  1310,  2944,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ 1129,   503,    25,  ...,  -100,  -100,  -100],\n        [   71,  1911,     7,  ...,  -100,  -100,  -100],\n        [17355,   127,     7,  ...,  -100,  -100,  -100],\n        ...,\n        [ 6069,     3, 26306,  ...,  -100,  -100,  -100],\n        [15476,    13, 22986,  ...,  -100,  -100,  -100],\n        [   27,  1310,  2944,  ...,  -100,  -100,  -100]], device='cuda:0'), 'decoder_input_ids': tensor([[    0,  1129,   503,  ...,     0,     0,     0],\n        [    0,    71,  1911,  ...,     0,     0,     0],\n        [    0, 17355,   127,  ...,     0,     0,     0],\n        ...,\n        [    0,  6069,     3,  ...,     0,     0,     0],\n        [    0, 15476,    13,  ...,     0,     0,     0],\n        [    0,    27,  1310,  ...,     0,     0,     0]], device='cuda:0')}\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self=T5ForConditionalGeneration(\n  (shared): Embeddin...n_features=768, out_features=32128, bias=False)\n), *args=(), **kwargs={'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1,...      [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'decoder_input_ids': tensor([[    0,  1129,   503,  ...,     0,     0...10,  ...,     0,     0,     0]], device='cuda:0'), 'input_ids': tensor([[ 1129,   503,    25,  ...,     0,     0...44,  ...,     0,     0,     0]], device='cuda:0'), 'labels': tensor([[ 1129,   503,    25,  ...,  -100,  -100...44,  ...,  -100,  -100,  -100]], device='cuda:0')})\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n        forward_call \u001b[0;34m= <bound method T5ForConditionalGeneration.forward of T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)>\u001b[0m\u001b[0;34m\n        \u001b[0margs \u001b[0;34m= ()\u001b[0m\u001b[0;34m\n        \u001b[0mkwargs \u001b[0;34m= {'input_ids': tensor([[ 1129,   503,    25,  ...,     0,     0,     0],\n        [   71,  1911,     7,  ...,     0,     0,     0],\n        [17355,   127,     7,  ...,     0,     0,     0],\n        ...,\n        [ 6069,     3, 26306,  ...,     0,     0,     0],\n        [15476,    13, 22986,  ...,     0,     0,     0],\n        [   27,  1310,  2944,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ 1129,   503,    25,  ...,  -100,  -100,  -100],\n        [   71,  1911,     7,  ...,  -100,  -100,  -100],\n        [17355,   127,     7,  ...,  -100,  -100,  -100],\n        ...,\n        [ 6069,     3, 26306,  ...,  -100,  -100,  -100],\n        [15476,    13, 22986,  ...,  -100,  -100,  -100],\n        [   27,  1310,  2944,  ...,  -100,  -100,  -100]], device='cuda:0'), 'decoder_input_ids': tensor([[    0,  1129,   503,  ...,     0,     0,     0],\n        [    0,    71,  1911,  ...,     0,     0,     0],\n        [    0, 17355,   127,  ...,     0,     0,     0],\n        ...,\n        [    0,  6069,     3,  ...,     0,     0,     0],\n        [    0, 15476,    13,  ...,     0,     0,     0],\n        [    0,    27,  1310,  ...,     0,     0,     0]], device='cuda:0')}\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1704\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self=T5ForConditionalGeneration(\n  (shared): Embeddin...n_features=768, out_features=32128, bias=False)\n), input_ids=tensor([[ 1129,   503,    25,  ...,     0,     0...44,  ...,     0,     0,     0]], device='cuda:0'), attention_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1,...      [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), decoder_input_ids=tensor([[    0,  1129,   503,  ...,     0,     0...10,  ...,     0,     0,     0]], device='cuda:0'), decoder_attention_mask=None, head_mask=None, decoder_head_mask=None, cross_attn_head_mask=None, encoder_outputs=BaseModelOutputWithPastAndCrossAttentions(last_h...tes=None, attentions=None, cross_attentions=None), past_key_values=None, inputs_embeds=None, decoder_inputs_embeds=None, labels=tensor([[ 1129,   503,    25,  ...,  -100,  -100...44,  ...,  -100,  -100,  -100]], device='cuda:0'), use_cache=True, output_attentions=None, output_hidden_states=None, return_dict=True)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1703\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1704\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n        self \u001b[0;34m= T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0mdecoder_input_ids \u001b[0;34m= tensor([[    0,  1129,   503,  ...,     0,     0,     0],\n        [    0,    71,  1911,  ...,     0,     0,     0],\n        [    0, 17355,   127,  ...,     0,     0,     0],\n        ...,\n        [    0,  6069,     3,  ...,     0,     0,     0],\n        [    0, 15476,    13,  ...,     0,     0,     0],\n        [    0,    27,  1310,  ...,     0,     0,     0]], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0mdecoder_attention_mask \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mdecoder_inputs_embeds \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mpast_key_values \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mhidden_states \u001b[0;34m= tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,  ...,  7.3756e-01,\n          -6.4471e-01, -4.3072e-02],\n         [-0.0000e+00,  1.4130e-01, -2.8108e-01,  ...,  3.3907e-01,\n          -3.6488e-02, -3.6908e-01],\n         [ 9.3195e-02,  3.7150e-01,  9.4190e-02,  ..., -6.1819e-02,\n           3.5049e-03, -0.0000e+00],\n         ...,\n         [-2.3926e-02, -4.8247e-01,  2.3497e-01,  ...,  3.9880e-01,\n          -8.4972e-02, -9.8002e-02],\n         [ 3.5569e-02, -2.3768e-01,  2.7225e-01,  ...,  3.4104e-01,\n           3.3176e-01, -5.0014e-01],\n         [ 1.6381e-01,  5.2310e-02, -7.1547e-02,  ...,  3.7553e-01,\n           5.6716e-01, -0.0000e+00]],\n\n        [[ 6.2732e-01,  3.2355e-01,  0.0000e+00,  ...,  2.2136e-01,\n           0.0000e+00,  5.1227e-01],\n         [-7.4636e-02,  6.8215e-01,  1.2985e-01,  ..., -8.1269e-02,\n          -2.4353e-02, -1.7799e-01],\n         [-0.0000e+00,  3.3901e-01,  1.7466e-01,  ...,  0.0000e+00,\n           1.4631e-01,  1.6036e-02],\n         ...,\n         [ 7.9134e-02, -0.0000e+00, -9.6235e-02,  ...,  6.9691e-01,\n           3.6416e-01, -0.0000e+00],\n         [-1.3400e-01, -3.2879e-01,  2.6364e-01,  ...,  7.1581e-01,\n           2.3241e-01, -2.1390e-01],\n         [ 2.8726e-01, -1.8657e-01,  1.5836e-01,  ...,  0.0000e+00,\n           2.7848e-01, -0.0000e+00]],\n\n        [[ 8.7178e-02,  9.1433e-02,  2.7344e-01,  ...,  1.7490e-02,\n          -2.9121e-01, -1.4335e-01],\n         [ 1.5637e-01, -4.9306e-01,  2.6517e-01,  ...,  3.8128e-01,\n          -3.1244e-01, -1.5314e-01],\n         [ 1.7885e-01, -0.0000e+00, -1.0513e-03,  ...,  6.7242e-02,\n          -1.3838e-01, -3.2072e-01],\n         ...,\n         [ 2.0394e-01, -9.1664e-02,  3.0847e-03,  ...,  6.7082e-01,\n           5.4445e-01, -8.3819e-02],\n         [-2.3183e-01, -7.7485e-03, -5.5931e-01,  ...,  5.3906e-01,\n           1.6026e-01, -0.0000e+00],\n         [ 3.8757e-01, -6.0742e-02, -8.9720e-02,  ...,  3.7971e-01,\n           5.8543e-01,  6.9683e-02]],\n\n        ...,\n\n        [[ 2.5694e-01,  3.8836e-01,  3.6472e-01,  ...,  1.9078e-01,\n           1.1912e-01,  4.2067e-01],\n         [ 1.7438e-01,  1.1089e-01,  5.0688e-01,  ...,  1.4332e-01,\n          -1.7266e-01,  4.9860e-01],\n         [-5.2431e-01,  1.1103e+00,  5.6405e-02,  ..., -3.1441e-02,\n          -2.7494e-01,  2.1520e-01],\n         ...,\n         [-3.7092e-02, -2.6356e-01, -2.1277e-01,  ...,  4.0784e-01,\n           0.0000e+00,  2.2286e-01],\n         [ 2.4721e-01,  1.2555e-01, -2.7395e-01,  ..., -1.1099e-01,\n           7.4805e-02,  4.2801e-01],\n         [ 4.3440e-03, -0.0000e+00, -1.2907e-02,  ..., -7.4690e-03,\n           9.0333e-03,  2.5710e-03]],\n\n        [[ 0.0000e+00, -1.9124e-01,  0.0000e+00,  ...,  0.0000e+00,\n           3.4049e-01, -2.3081e-01],\n         [ 1.1040e-01, -1.6056e-01,  3.6804e-01,  ..., -4.7069e-01,\n          -0.0000e+00,  2.2921e-01],\n         [-2.3070e-01,  2.6098e-01,  2.6266e-01,  ..., -6.6238e-01,\n          -4.0395e-01, -2.9121e-01],\n         ...,\n         [-3.7796e-01, -7.3537e-02, -1.4169e-01,  ...,  3.3294e-01,\n           7.2322e-02, -2.4653e-01],\n         [-1.8759e-02, -1.0135e-01, -1.6968e-01,  ..., -4.5800e-03,\n           1.7808e-01, -3.0931e-01],\n         [-2.0037e-01, -3.4233e-01,  1.8222e-01,  ...,  2.3297e-02,\n           1.6224e-01, -0.0000e+00]],\n\n        [[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  7.6713e-02,\n          -2.7339e-01,  2.0604e-01],\n         [-3.8789e-02,  1.4853e-01,  2.4096e-01,  ...,  9.6284e-02,\n          -0.0000e+00,  3.6521e-01],\n         [ 0.0000e+00,  4.6811e-01, -2.0381e-01,  ..., -3.3839e-01,\n           2.3055e-01,  4.2210e-01],\n         ...,\n         [ 2.2385e-01, -7.3387e-02, -5.2777e-01,  ...,  1.3698e-01,\n           5.1136e-01,  1.2732e-01],\n         [ 2.8012e-01, -1.1424e-01, -5.9984e-01,  ...,  3.8852e-01,\n           6.4370e-01, -4.2405e-01],\n         [ 1.0257e-01, -6.2944e-01, -0.0000e+00,  ...,  3.9602e-01,\n           3.1288e-01, -4.1677e-01]]], device='cuda:0',\n       grad_fn=<NativeDropoutBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mattention_mask \u001b[0;34m= tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0mdecoder_head_mask \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mcross_attn_head_mask \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0muse_cache \u001b[0;34m= True\u001b[0m\u001b[0;34m\n        \u001b[0moutput_attentions \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0moutput_hidden_states \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mreturn_dict \u001b[0;34m= True\u001b[0m\n\u001b[1;32m   1705\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1706\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1707\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1708\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1709\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1710\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1711\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1712\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1713\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1714\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1715\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1716\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1717\u001b[0m )\n\u001b[1;32m   1719\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1721\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self=T5Stack(\n  (embed_tokens): Embedding(32128, 768)...rm()\n  (dropout): Dropout(p=0.1, inplace=False)\n), *args=(), **kwargs={'attention_mask': None, 'cross_attn_head_mask': None, 'encoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1,...      [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'encoder_hidden_states': tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,...cuda:0',\n       grad_fn=<NativeDropoutBackward0>), 'head_mask': None, 'input_ids': tensor([[    0,  1129,   503,  ...,     0,     0...10,  ...,     0,     0,     0]], device='cuda:0'), 'inputs_embeds': None, 'output_attentions': None, 'output_hidden_states': None, 'past_key_values': None, ...})\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n        forward_call \u001b[0;34m= <bound method T5Stack.forward of T5Stack(\n  (embed_tokens): Embedding(32128, 768)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n            (relative_attention_bias): Embedding(32, 12)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1-11): 11 x T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)>\u001b[0m\u001b[0;34m\n        \u001b[0margs \u001b[0;34m= ()\u001b[0m\u001b[0;34m\n        \u001b[0mkwargs \u001b[0;34m= {'input_ids': tensor([[    0,  1129,   503,  ...,     0,     0,     0],\n        [    0,    71,  1911,  ...,     0,     0,     0],\n        [    0, 17355,   127,  ...,     0,     0,     0],\n        ...,\n        [    0,  6069,     3,  ...,     0,     0,     0],\n        [    0, 15476,    13,  ...,     0,     0,     0],\n        [    0,    27,  1310,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': None, 'inputs_embeds': None, 'past_key_values': None, 'encoder_hidden_states': tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,  ...,  7.3756e-01,\n          -6.4471e-01, -4.3072e-02],\n         [-0.0000e+00,  1.4130e-01, -2.8108e-01,  ...,  3.3907e-01,\n          -3.6488e-02, -3.6908e-01],\n         [ 9.3195e-02,  3.7150e-01,  9.4190e-02,  ..., -6.1819e-02,\n           3.5049e-03, -0.0000e+00],\n         ...,\n         [-2.3926e-02, -4.8247e-01,  2.3497e-01,  ...,  3.9880e-01,\n          -8.4972e-02, -9.8002e-02],\n         [ 3.5569e-02, -2.3768e-01,  2.7225e-01,  ...,  3.4104e-01,\n           3.3176e-01, -5.0014e-01],\n         [ 1.6381e-01,  5.2310e-02, -7.1547e-02,  ...,  3.7553e-01,\n           5.6716e-01, -0.0000e+00]],\n\n        [[ 6.2732e-01,  3.2355e-01,  0.0000e+00,  ...,  2.2136e-01,\n           0.0000e+00,  5.1227e-01],\n         [-7.4636e-02,  6.8215e-01,  1.2985e-01,  ..., -8.1269e-02,\n          -2.4353e-02, -1.7799e-01],\n         [-0.0000e+00,  3.3901e-01,  1.7466e-01,  ...,  0.0000e+00,\n           1.4631e-01,  1.6036e-02],\n         ...,\n         [ 7.9134e-02, -0.0000e+00, -9.6235e-02,  ...,  6.9691e-01,\n           3.6416e-01, -0.0000e+00],\n         [-1.3400e-01, -3.2879e-01,  2.6364e-01,  ...,  7.1581e-01,\n           2.3241e-01, -2.1390e-01],\n         [ 2.8726e-01, -1.8657e-01,  1.5836e-01,  ...,  0.0000e+00,\n           2.7848e-01, -0.0000e+00]],\n\n        [[ 8.7178e-02,  9.1433e-02,  2.7344e-01,  ...,  1.7490e-02,\n          -2.9121e-01, -1.4335e-01],\n         [ 1.5637e-01, -4.9306e-01,  2.6517e-01,  ...,  3.8128e-01,\n          -3.1244e-01, -1.5314e-01],\n         [ 1.7885e-01, -0.0000e+00, -1.0513e-03,  ...,  6.7242e-02,\n          -1.3838e-01, -3.2072e-01],\n         ...,\n         [ 2.0394e-01, -9.1664e-02,  3.0847e-03,  ...,  6.7082e-01,\n           5.4445e-01, -8.3819e-02],\n         [-2.3183e-01, -7.7485e-03, -5.5931e-01,  ...,  5.3906e-01,\n           1.6026e-01, -0.0000e+00],\n         [ 3.8757e-01, -6.0742e-02, -8.9720e-02,  ...,  3.7971e-01,\n           5.8543e-01,  6.9683e-02]],\n\n        ...,\n\n        [[ 2.5694e-01,  3.8836e-01,  3.6472e-01,  ...,  1.9078e-01,\n           1.1912e-01,  4.2067e-01],\n         [ 1.7438e-01,  1.1089e-01,  5.0688e-01,  ...,  1.4332e-01,\n          -1.7266e-01,  4.9860e-01],\n         [-5.2431e-01,  1.1103e+00,  5.6405e-02,  ..., -3.1441e-02,\n          -2.7494e-01,  2.1520e-01],\n         ...,\n         [-3.7092e-02, -2.6356e-01, -2.1277e-01,  ...,  4.0784e-01,\n           0.0000e+00,  2.2286e-01],\n         [ 2.4721e-01,  1.2555e-01, -2.7395e-01,  ..., -1.1099e-01,\n           7.4805e-02,  4.2801e-01],\n         [ 4.3440e-03, -0.0000e+00, -1.2907e-02,  ..., -7.4690e-03,\n           9.0333e-03,  2.5710e-03]],\n\n        [[ 0.0000e+00, -1.9124e-01,  0.0000e+00,  ...,  0.0000e+00,\n           3.4049e-01, -2.3081e-01],\n         [ 1.1040e-01, -1.6056e-01,  3.6804e-01,  ..., -4.7069e-01,\n          -0.0000e+00,  2.2921e-01],\n         [-2.3070e-01,  2.6098e-01,  2.6266e-01,  ..., -6.6238e-01,\n          -4.0395e-01, -2.9121e-01],\n         ...,\n         [-3.7796e-01, -7.3537e-02, -1.4169e-01,  ...,  3.3294e-01,\n           7.2322e-02, -2.4653e-01],\n         [-1.8759e-02, -1.0135e-01, -1.6968e-01,  ..., -4.5800e-03,\n           1.7808e-01, -3.0931e-01],\n         [-2.0037e-01, -3.4233e-01,  1.8222e-01,  ...,  2.3297e-02,\n           1.6224e-01, -0.0000e+00]],\n\n        [[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  7.6713e-02,\n          -2.7339e-01,  2.0604e-01],\n         [-3.8789e-02,  1.4853e-01,  2.4096e-01,  ...,  9.6284e-02,\n          -0.0000e+00,  3.6521e-01],\n         [ 0.0000e+00,  4.6811e-01, -2.0381e-01,  ..., -3.3839e-01,\n           2.3055e-01,  4.2210e-01],\n         ...,\n         [ 2.2385e-01, -7.3387e-02, -5.2777e-01,  ...,  1.3698e-01,\n           5.1136e-01,  1.2732e-01],\n         [ 2.8012e-01, -1.1424e-01, -5.9984e-01,  ...,  3.8852e-01,\n           6.4370e-01, -4.2405e-01],\n         [ 1.0257e-01, -6.2944e-01, -0.0000e+00,  ...,  3.9602e-01,\n           3.1288e-01, -4.1677e-01]]], device='cuda:0',\n       grad_fn=<NativeDropoutBackward0>), 'encoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'head_mask': None, 'cross_attn_head_mask': None, 'use_cache': True, 'output_attentions': None, 'output_hidden_states': None, 'return_dict': True}\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1074\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self=T5Stack(\n  (embed_tokens): Embedding(32128, 768)...rm()\n  (dropout): Dropout(p=0.1, inplace=False)\n), input_ids=tensor([[    0,  1129,   503,  ...,     0,     0...10,  ...,     0,     0,     0]], device='cuda:0'), attention_mask=tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  .....1.0000e+00, 1.0000e+00]],\n       device='cuda:0'), encoder_hidden_states=tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,...cuda:0',\n       grad_fn=<NativeDropoutBackward0>), encoder_attention_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1,...      [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), inputs_embeds=tensor([[[-7.5536e-01, 5.9922e-01, -2.4394e+00, ...   device='cuda:0', grad_fn=<EmbeddingBackward0>), head_mask=[None, None, None, None, None, None, None, None, None, None, None, None], cross_attn_head_mask=[None, None, None, None, None, None, None, None, None, None, None, None], past_key_values=[None, None, None, None, None, None, None, None, None, None, None, None], use_cache=True, output_attentions=False, output_hidden_states=False, return_dict=True)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1062\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1063\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n        layer_module \u001b[0;34m= T5Block(\n  (layer): ModuleList(\n    (0): T5LayerSelfAttention(\n      (SelfAttention): T5Attention(\n        (q): Linear(in_features=768, out_features=768, bias=False)\n        (k): Linear(in_features=768, out_features=768, bias=False)\n        (v): Linear(in_features=768, out_features=768, bias=False)\n        (o): Linear(in_features=768, out_features=768, bias=False)\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (1): T5LayerCrossAttention(\n      (EncDecAttention): T5Attention(\n        (q): Linear(in_features=768, out_features=768, bias=False)\n        (k): Linear(in_features=768, out_features=768, bias=False)\n        (v): Linear(in_features=768, out_features=768, bias=False)\n        (o): Linear(in_features=768, out_features=768, bias=False)\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (2): T5LayerFF(\n      (DenseReluDense): T5DenseActDense(\n        (wi): Linear(in_features=768, out_features=3072, bias=False)\n        (wo): Linear(in_features=3072, out_features=768, bias=False)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (act): ReLU()\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n)\u001b[0m\u001b[0;34m\n        \u001b[0mlayer_outputs \u001b[0;34m= (tensor([[[-3.9580e+02,  2.4767e+02, -7.2931e+01,  ...,  3.8053e+02,\n          -6.4637e+02, -4.8657e+02],\n         [-1.2749e+02, -8.8088e+01,  2.4900e+01,  ..., -3.7472e+02,\n          -1.8831e+02, -8.1284e+02],\n         [ 1.0088e+01, -1.3811e+02, -3.5234e+02,  ..., -3.7094e+02,\n           5.3798e+01, -5.3244e+02],\n         ...,\n         [-6.0796e+01,  1.0092e+02, -2.5755e+01,  ...,  2.6453e+02,\n          -5.0002e+01, -3.0047e+02],\n         [-1.4893e+02,  5.1579e+01,  3.0008e+01,  ...,  2.4587e+02,\n          -5.1689e+02, -2.8850e+02],\n         [-1.2358e+02,  6.0058e+01,  2.5703e+01,  ...,  1.1384e+02,\n          -4.2387e+02, -2.3618e+02]],\n\n        [[ 4.1202e+00,  5.0162e+01, -4.1346e+01,  ...,  7.5681e+01,\n           2.3860e+02,  2.3550e+02],\n         [-2.0577e+02,  1.1997e+02,  2.5339e+02,  ..., -5.0824e+02,\n          -2.7210e+02,  4.6938e+01],\n         [-5.9783e+02,  1.1258e+00, -6.1282e+02,  ..., -3.6052e+02,\n          -1.2050e+00,  6.6538e+02],\n         ...,\n         [ 6.0902e+01,  3.5652e+01,  1.4630e+02,  ..., -1.2697e+02,\n           1.4870e+02,  7.8078e+01],\n         [ 1.0076e+02,  5.9857e+01,  1.1207e+02,  ...,  4.1839e+01,\n           2.5267e+02,  2.6653e+02],\n         [ 6.1666e+01,  1.8881e+01,  1.1667e+02,  ..., -1.4297e+02,\n           1.3583e+02,  2.1588e+02]],\n\n        [[ 1.4321e+02,  2.6521e+01, -1.9856e+02,  ...,  3.9943e+01,\n          -2.3000e+02,  2.9039e+00],\n         [ 5.1894e+02, -2.8780e+02, -6.5460e+02,  ...,  3.2682e+02,\n          -3.2717e+02, -5.6882e+02],\n         [-1.2565e+02, -6.4311e+02, -7.1021e+02,  ..., -2.0594e+02,\n           2.6969e+02,  3.0860e+02],\n         ...,\n         [ 1.1608e+02, -1.0791e+02,  1.7948e+01,  ..., -1.2139e+02,\n          -7.3849e+01, -1.9148e+02],\n         [ 1.3399e+02, -1.5552e+02,  7.6590e+01,  ...,  2.9785e+01,\n          -1.5029e+02, -1.1276e+02],\n         [ 1.2852e+02, -1.3101e+02,  4.2857e+01,  ..., -1.2577e+02,\n          -1.3094e+02, -1.1035e+02]],\n\n        ...,\n\n        [[-1.9247e+02,  2.0719e+02, -2.4518e+02,  ..., -2.2249e+02,\n          -1.6554e+02,  3.8623e+01],\n         [-3.0558e+02, -1.1055e+02, -2.2065e+02,  ...,  8.8692e+01,\n          -2.2254e+02, -6.0727e+01],\n         [-4.4662e+01,  5.9589e+02, -6.2137e+02,  ..., -1.3274e+02,\n          -6.1440e+02, -4.6574e+02],\n         ...,\n         [-4.7629e+00,  4.4491e+01, -8.1681e+01,  ..., -1.3646e+02,\n          -8.2333e+01,  3.0740e+01],\n         [-6.2734e+01,  2.8587e+01, -8.2016e+01,  ..., -2.3431e+02,\n          -1.7656e+02,  2.2520e+01],\n         [-7.0461e+01,  8.4793e+01, -4.4015e+01,  ..., -1.9447e+02,\n          -1.3073e+02,  4.1494e+01]],\n\n        [[ 2.4631e+00,  2.9960e+02, -1.4315e+01,  ..., -1.5654e+01,\n           2.9429e+02,  1.6030e+01],\n         [-1.7511e+02, -5.4824e+02, -2.2807e+02,  ...,  1.5704e+02,\n          -2.3582e+02, -2.0936e+02],\n         [-3.1996e+02,  1.5875e+02,  5.1828e+02,  ..., -2.4916e+02,\n          -4.6174e+01, -4.3444e+02],\n         ...,\n         [ 4.7171e+01,  9.3918e+01, -1.4850e+01,  ..., -1.3771e+02,\n           1.8458e+01,  8.3231e+01],\n         [ 5.3622e+01,  4.9918e+01, -5.2572e+01,  ..., -1.2729e+02,\n           1.3833e+01,  1.3946e+01],\n         [ 5.8013e+01,  8.5568e+01, -2.7452e+01,  ..., -9.1226e+01,\n           1.0243e+02, -1.7468e-01]],\n\n        [[ 7.4680e+01,  1.5303e+02, -5.8707e+01,  ..., -7.5620e+01,\n          -3.2263e+02,  3.5384e+00],\n         [-2.3415e+01,  9.2403e+01, -3.2122e+02,  ..., -1.4288e+02,\n          -4.9064e+02,  6.4593e+01],\n         [-2.3628e+02,  2.8563e+01,  2.7882e+02,  ..., -3.6215e+02,\n          -1.5245e+02, -6.6334e+01],\n         ...,\n         [ 1.7690e+02, -1.0148e+02,  3.3293e+01,  ..., -9.4638e+01,\n          -1.6065e+02, -1.0381e+02],\n         [ 1.6100e+02, -6.9087e+01,  6.1568e+01,  ..., -1.2287e+02,\n          -1.1618e+02, -5.3916e+01],\n         [ 1.1145e+02,  8.4470e+01,  5.4469e+01,  ..., -1.8014e+02,\n          -1.3798e+02, -2.0567e+01]]], device='cuda:0', grad_fn=<AddBackward0>), (tensor([[[[ 3.9185e-02,  2.4524e-01,  3.6768e-01,  ..., -3.7866e-01,\n           -5.7178e-01,  1.8237e-01],\n          [-1.6152e+00, -6.1893e-04,  1.6514e+00,  ..., -1.2148e+00,\n            6.5381e-01,  2.3999e-01],\n          [ 1.4883e+00,  1.8018e+00,  2.3047e+00,  ..., -1.0771e+00,\n           -4.7827e-01, -4.7729e-01],\n          ...,\n          [-2.5497e-02, -1.0974e-01, -1.1340e-01,  ..., -6.6833e-02,\n           -2.2449e-01,  1.5027e-01],\n          [ 1.0187e-01,  7.5378e-02,  4.9896e-02,  ..., -2.5757e-01,\n           -2.3242e-01, -1.3904e-01],\n          [ 1.5771e-01, -8.3876e-04, -7.9346e-02,  ..., -1.2830e-01,\n           -2.3230e-01, -1.0919e-01]],\n\n         [[ 5.1416e-01,  2.1826e-01, -8.8379e-02,  ..., -6.2842e-01,\n            1.1455e+00,  1.8320e+00],\n          [-1.0371e+00,  9.8096e-01,  3.8428e-01,  ..., -2.3711e+00,\n           -5.6854e-02, -4.9170e-01],\n          [ 2.7100e-01,  2.1914e+00, -2.3645e-01,  ...,  1.9844e+00,\n           -2.3438e+00, -7.5146e-01],\n          ...,\n          [ 5.1465e-01,  3.3887e-01, -4.3121e-02,  ..., -3.5693e-01,\n            1.4492e+00,  5.6494e-01],\n          [ 7.4707e-01,  6.5820e-01, -2.1887e-01,  ..., -4.2603e-01,\n            1.3936e+00,  1.3953e-01],\n          [ 6.1816e-01,  7.5342e-01, -3.5059e-01,  ..., -6.6113e-01,\n            1.5400e+00,  5.3369e-01]],\n\n         [[-5.3369e-01, -1.3013e-01,  5.1392e-02,  ...,  3.1226e-01,\n            3.9819e-01, -3.2910e-01],\n          [-4.0405e-01,  2.4902e-01, -2.7754e+00,  ..., -2.2168e+00,\n           -7.4121e-01,  7.3877e-01],\n          [ 2.3301e+00,  1.1152e+00, -2.1172e+00,  ..., -2.8027e+00,\n           -1.8984e+00,  3.8730e+00],\n          ...,\n          [ 2.0361e-01,  1.7120e-02, -2.7930e-01,  ...,  2.3547e-01,\n            3.7012e-01,  2.2156e-01],\n          [-7.4463e-03, -3.1421e-01, -2.5952e-01,  ...,  3.2227e-01,\n            2.7612e-01,  5.4150e-01],\n          [ 5.7275e-01,  5.8624e-02, -5.4297e-01,  ...,  1.3477e-01,\n            5.5713e-01,  4.6826e-01]],\n\n         ...,\n\n         [[-6.4893e-01, -4.6973e-01,  6.5918e-01,  ..., -2.7930e-01,\n           -5.0781e-01, -2.3425e-01],\n          [ 3.7549e-01, -1.5889e+00,  1.2129e+00,  ...,  3.8794e-01,\n            3.8340e+00, -9.2969e-01],\n          [ 2.4609e-01, -2.7070e+00,  1.0732e+00,  ...,  1.8252e+00,\n            5.0547e+00, -5.9229e-01],\n          ...,\n          [-1.2952e-01, -5.3369e-01,  8.9502e-01,  ..., -1.2463e-01,\n           -7.2168e-01,  4.5685e-02],\n          [-2.1399e-01, -3.5205e-01,  8.4570e-01,  ..., -1.1731e-01,\n           -4.1992e-01,  1.3599e-01],\n          [-3.8745e-01, -6.3037e-01,  5.9570e-01,  ..., -3.2471e-02,\n           -3.5645e-01,  1.0986e-01]],\n\n         [[ 3.1055e-01,  2.1448e-01,  2.5098e-01,  ..., -5.1117e-02,\n            9.5947e-02,  1.9861e-01],\n          [ 1.2529e+00,  1.0928e+00, -1.4990e+00,  ...,  3.0469e-01,\n            1.0234e+00,  5.9619e-01],\n          [ 1.8105e+00,  1.3955e+00,  2.0813e-01,  ...,  2.4731e-01,\n            1.3633e+00, -8.9990e-01],\n          ...,\n          [-1.3440e-01,  2.8732e-02,  1.9629e-01,  ..., -5.3833e-02,\n            4.1772e-01,  2.0667e-01],\n          [ 7.3486e-02,  3.5547e-01,  2.4658e-01,  ...,  3.2532e-02,\n            5.4736e-01,  4.0918e-01],\n          [-4.0161e-02,  1.4404e-01,  2.8857e-01,  ...,  6.5430e-02,\n            7.2900e-01,  2.2595e-01]],\n\n         [[-6.3721e-01,  2.5537e-01,  5.3906e-01,  ..., -6.3232e-02,\n            1.1559e-02,  5.4102e-01],\n          [ 2.4643e-02, -1.8933e-01,  6.0645e-01,  ...,  9.5093e-02,\n           -1.0215e+00, -1.1201e+00],\n          [-3.7964e-01, -1.5625e+00, -4.1821e-01,  ...,  1.0371e+00,\n           -1.7168e+00,  1.0342e+00],\n          ...,\n          [-7.5391e-01, -5.0720e-02,  2.4585e-01,  ...,  1.8958e-01,\n           -2.7515e-01,  4.7534e-01],\n          [-6.0693e-01,  6.7017e-02, -1.5549e-02,  ...,  2.7344e-01,\n           -2.0264e-02,  2.1802e-01],\n          [-6.3574e-01, -1.1957e-01,  5.8250e-03,  ...,  1.5723e-01,\n           -1.1853e-01,  4.2969e-01]]],\n\n\n        [[[ 3.4644e-01, -1.5656e-02,  9.9670e-02,  ..., -2.3218e-01,\n           -1.1833e-02,  8.3374e-02],\n          [ 2.2383e+00, -1.9426e-03, -6.8359e-01,  ..., -6.4990e-01,\n           -4.4983e-02,  1.4072e+00],\n          [ 1.5479e+00, -5.2686e-01,  1.5393e-01,  ...,  8.4521e-01,\n           -1.1584e-01,  7.9297e-01],\n          ...,\n          [ 4.6356e-02, -5.6738e-01, -8.9539e-02,  ..., -8.4778e-02,\n            2.8271e-01, -3.6743e-01],\n          [ 2.5244e-01, -4.9023e-01,  2.3682e-02,  ..., -3.1494e-01,\n            2.4304e-01, -3.5840e-01],\n          [ 6.6345e-02, -1.2817e-01,  5.8075e-02,  ..., -2.5684e-01,\n            2.8906e-01, -2.8052e-01]],\n\n         [[ 4.5898e-01,  2.7124e-01, -9.5703e-02,  ...,  4.5135e-02,\n            1.0957e+00,  4.1333e-01],\n          [ 5.7910e-01,  2.8247e-01,  4.0088e-01,  ..., -4.9976e-01,\n           -1.2383e+00, -1.0439e+00],\n          [ 3.4004e+00,  1.0608e-01,  2.1533e-01,  ..., -1.5566e+00,\n           -6.7090e-01, -1.2363e+00],\n          ...,\n          [ 5.6152e-01,  6.5918e-01, -3.1909e-01,  ...,  1.1322e-01,\n            7.5488e-01, -4.5605e-01],\n          [ 5.8203e-01,  5.3027e-01, -3.4985e-01,  ...,  1.8408e-01,\n            9.6143e-01, -3.0054e-01],\n          [ 3.6450e-01,  6.1670e-01, -5.7227e-01,  ...,  1.5149e-01,\n            9.5508e-01, -4.0601e-01]],\n\n         [[-2.5439e-01,  2.5049e-01, -2.0129e-01,  ...,  3.5181e-01,\n           -1.0168e-01,  2.0355e-02],\n          [ 7.1436e-01, -1.4087e-01, -3.1172e+00,  ..., -1.1543e+00,\n           -2.6035e+00,  1.0537e+00],\n          [ 1.8398e+00,  7.8125e-01, -1.5664e+00,  ..., -1.0834e-01,\n           -1.1963e+00,  4.5776e-01],\n          ...,\n          [ 3.4570e-01, -1.5808e-01, -7.5146e-01,  ...,  7.9834e-01,\n            3.7012e-01,  1.0371e+00],\n          [ 5.9961e-01, -1.3965e-01, -6.0400e-01,  ...,  7.7344e-01,\n            3.9404e-01,  8.7012e-01],\n          [ 3.4912e-01, -2.2659e-02, -8.2715e-01,  ...,  5.7861e-01,\n            4.4287e-01,  8.6230e-01]],\n\n         ...,\n\n         [[-5.1807e-01, -7.2852e-01,  1.5723e-01,  ...,  3.2764e-01,\n           -1.1885e+00,  4.7534e-01],\n          [-9.0723e-01, -1.4590e+00,  1.2012e+00,  ...,  1.3584e+00,\n            4.8398e+00,  6.7236e-01],\n          [ 5.3418e-01, -1.8418e+00,  1.6758e+00,  ...,  1.4932e+00,\n            4.3477e+00,  6.8555e-01],\n          ...,\n          [-2.8516e-01, -9.2822e-01,  3.7305e-01,  ...,  3.9282e-01,\n           -2.6782e-01,  6.7822e-01],\n          [-1.7700e-01, -1.4277e+00,  4.4922e-01,  ...,  6.6406e-01,\n           -4.6143e-01,  5.4053e-01],\n          [-1.2077e-02, -1.1670e+00,  6.1035e-01,  ...,  4.5801e-01,\n           -2.3206e-01,  6.9824e-01]],\n\n         [[-2.8320e-01, -8.5297e-03,  1.9043e-01,  ..., -2.9126e-01,\n            3.5840e-01,  1.2036e-01],\n          [ 1.5479e+00,  1.6025e+00,  8.0420e-01,  ...,  8.3789e-01,\n            1.1123e+00,  1.8320e+00],\n          [ 1.4678e+00,  2.1348e+00,  7.7344e-01,  ...,  1.2881e+00,\n            1.2490e+00, -5.0812e-02],\n          ...,\n          [-7.0459e-01, -2.4585e-01,  1.2146e-01,  ..., -7.0459e-01,\n            1.2773e+00,  2.7051e-01],\n          [-7.5928e-01, -2.2925e-01, -6.2744e-02,  ..., -4.0381e-01,\n            1.1738e+00,  1.7603e-01],\n          [-6.4453e-01, -1.2769e-01,  1.4351e-02,  ..., -4.0234e-01,\n            1.3555e+00,  2.4060e-01]],\n\n         [[-3.1055e-01, -1.8848e-01,  3.5938e-01,  ..., -1.8201e-01,\n           -1.4648e-01,  2.4231e-01],\n          [-1.1396e+00, -2.6855e-01,  1.0605e+00,  ...,  9.0527e-01,\n           -9.0771e-01, -5.0879e-01],\n          [-8.3398e-01, -4.1748e-01,  2.0215e+00,  ..., -1.0010e+00,\n            8.5596e-01,  5.4004e-01],\n          ...,\n          [-1.4062e-01,  3.1494e-02, -2.4939e-01,  ...,  1.4307e-01,\n           -2.9224e-01,  3.1006e-01],\n          [-1.2781e-01,  4.5166e-02, -2.5439e-01,  ...,  3.2861e-01,\n           -6.8298e-02,  4.7339e-01],\n          [-1.0358e-01,  1.3770e-01,  2.0096e-02,  ...,  2.3267e-01,\n           -3.5693e-01,  1.4734e-01]]],\n\n\n        [[[ 2.8564e-01,  4.4952e-02,  1.3086e-01,  ..., -3.6102e-02,\n           -6.7041e-01,  2.4155e-02],\n          [ 1.4980e+00,  2.1738e+00,  2.0059e+00,  ...,  1.1250e+00,\n           -2.2578e+00,  2.1934e+00],\n          [ 8.2178e-01,  3.9551e-01,  2.3523e-01,  ..., -2.6221e-01,\n           -2.2773e+00,  1.5552e-01],\n          ...,\n          [ 1.2231e-01, -2.2656e-01,  1.4050e-01,  ..., -1.1572e-01,\n            3.9795e-01, -3.4033e-01],\n          [ 1.9934e-01, -4.2627e-01, -2.5940e-02,  ...,  6.4270e-02,\n            2.0752e-01, -4.1138e-01],\n          [-9.1309e-02, -3.1958e-01, -4.1931e-02,  ..., -5.9265e-02,\n            1.1426e-01, -4.1919e-01]],\n\n         [[ 3.4644e-01, -7.2083e-02, -2.2498e-01,  ...,  6.9397e-02,\n            5.3516e-01,  8.9990e-01],\n          [-4.3945e-01, -6.4258e-01,  6.8701e-01,  ..., -1.4092e+00,\n            1.0703e+00,  2.0859e+00],\n          [ 1.8564e+00,  2.0374e-01,  7.0605e-01,  ...,  6.8652e-01,\n           -2.6035e+00,  2.8613e+00],\n          ...,\n          [ 2.9297e-01,  1.2168e+00, -2.4097e-01,  ..., -4.3823e-02,\n            5.8301e-01, -2.2949e-01],\n          [ 3.0444e-01,  9.2334e-01, -2.3743e-01,  ...,  1.0612e-02,\n            5.8008e-01, -3.2812e-01],\n          [ 4.6948e-01,  8.1836e-01, -1.8286e-01,  ..., -2.6917e-02,\n            4.6631e-01, -2.7002e-01]],\n\n         [[-2.8760e-01,  2.2986e-01, -8.6060e-02,  ...,  5.6787e-01,\n            2.3291e-01, -4.4141e-01],\n          [-2.6719e+00,  3.5229e-01,  8.7402e-01,  ...,  1.0754e-01,\n           -1.1875e+00, -1.6528e-01],\n          [-9.8926e-01,  2.2087e-03, -1.1826e+00,  ...,  1.7061e+00,\n           -7.7539e-01,  2.6465e+00],\n          ...,\n          [ 3.1152e-01, -4.8413e-01, -8.6670e-01,  ...,  2.8076e-01,\n            3.8940e-01,  1.2510e+00],\n          [ 2.5610e-01, -4.1309e-01, -7.8711e-01,  ...,  3.2300e-01,\n            5.5518e-01,  1.2637e+00],\n          [ 1.0449e-01, -4.5581e-01, -7.7051e-01,  ...,  3.0127e-01,\n            3.5815e-01,  1.3535e+00]],\n\n         ...,\n\n         [[-7.8711e-01, -7.0166e-01, -1.5381e-01,  ...,  1.3660e-01,\n           -9.0137e-01,  1.1176e-01],\n          [ 6.7773e-01, -1.6895e+00,  1.9287e-01,  ...,  1.4807e-01,\n            4.2812e+00, -2.9551e+00],\n          [-1.4561e+00, -1.9150e+00,  4.0820e-01,  ..., -1.0107e+00,\n            5.6250e+00,  1.5613e-01],\n          ...,\n          [-2.5513e-02, -8.9697e-01, -2.5684e-01,  ...,  4.6045e-01,\n           -2.5146e-01,  2.3486e-01],\n          [-6.3660e-02, -7.8662e-01, -3.1250e-01,  ...,  4.4604e-01,\n           -1.7163e-01,  1.5356e-01],\n          [-2.1887e-01, -9.5850e-01, -2.6758e-01,  ...,  2.0386e-01,\n           -1.5845e-01,  4.7095e-01]],\n\n         [[-1.9934e-01, -9.9365e-02,  3.6377e-01,  ..., -1.0931e-01,\n            2.5488e-01,  1.5283e-01],\n          [-1.2002e+00,  9.1553e-01, -6.0547e-01,  ...,  1.6113e+00,\n           -5.7373e-01, -1.3311e+00],\n          [ 1.4434e+00,  1.6953e+00, -7.6611e-01,  ...,  5.3564e-01,\n            6.7041e-01, -1.0518e+00],\n          ...,\n          [-5.7666e-01, -3.4204e-01, -1.5918e-01,  ..., -1.2744e-01,\n            1.4268e+00, -1.0040e-02],\n          [-5.9229e-01, -5.0488e-01, -1.3708e-01,  ..., -1.9727e-01,\n            1.3643e+00,  1.7969e-01],\n          [-4.5801e-01, -3.0469e-01,  2.0740e-01,  ..., -5.0232e-02,\n            1.3057e+00,  1.4734e-01]],\n\n         [[-4.6777e-01, -2.4402e-01,  5.4590e-01,  ..., -2.1637e-02,\n           -7.1899e-02,  1.5295e-01],\n          [ 1.3428e+00, -2.6392e-01,  2.3145e+00,  ...,  7.8711e-01,\n           -2.7012e+00, -3.1426e+00],\n          [ 2.0679e-01, -2.2383e+00,  2.8105e+00,  ...,  6.1670e-01,\n           -5.3564e-01, -5.5664e-01],\n          ...,\n          [-3.3887e-01,  2.5049e-01, -9.3445e-02,  ...,  5.2246e-01,\n            2.4036e-01,  3.2568e-01],\n          [-2.3828e-01, -1.0693e-01, -1.6117e-03,  ...,  1.7603e-01,\n            2.1326e-01,  4.2163e-01],\n          [-3.5474e-01, -2.9068e-02,  5.6915e-02,  ...,  2.7124e-01,\n           -6.5369e-02,  4.7705e-01]]],\n\n\n        ...,\n\n\n        [[[ 1.5063e-01, -6.0596e-01,  1.8753e-02,  ..., -2.0325e-01,\n           -5.8350e-01,  2.3041e-03],\n          [ 9.5166e-01,  1.1719e+00,  9.2871e-01,  ..., -3.3472e-01,\n            7.2510e-01, -5.4150e-01],\n          [ 4.0649e-01,  1.3438e+00,  2.1436e-01,  ..., -7.6318e-01,\n            4.2261e-01, -2.0813e-01],\n          ...,\n          [ 6.3843e-02, -5.4297e-01, -6.0501e-03,  ..., -9.3018e-02,\n           -1.9507e-01, -2.9224e-01],\n          [ 1.2286e-01, -3.5791e-01, -3.1174e-02,  ..., -1.6541e-01,\n           -2.6416e-01, -1.4929e-01],\n          [ 1.1253e-02, -6.2256e-01,  3.5645e-02,  ..., -2.1338e-01,\n           -4.9072e-02, -2.8857e-01]],\n\n         [[ 5.6592e-01, -2.9785e-01,  2.2253e-01,  ...,  1.9363e-02,\n            1.2109e+00,  3.2886e-01],\n          [-1.5808e-01, -4.3335e-01,  7.0215e-01,  ...,  1.8027e+00,\n            9.5068e-01, -1.1328e+00],\n          [ 5.6934e-01,  5.2734e-01,  7.4341e-02,  ..., -5.6592e-01,\n           -1.6445e+00, -9.6582e-01],\n          ...,\n          [ 5.0146e-01,  1.6321e-01,  1.7676e-01,  ..., -1.3477e-01,\n            1.1904e+00, -4.8169e-01],\n          [ 3.1348e-01,  2.2180e-01,  6.2561e-02,  ...,  2.0728e-01,\n            1.1963e+00, -3.3252e-01],\n          [ 4.6411e-01,  3.4326e-01, -1.3864e-04,  ..., -1.2866e-01,\n            1.2100e+00, -4.4946e-01]],\n\n         [[-2.1814e-01,  4.9048e-01, -3.3276e-01,  ...,  2.5537e-01,\n           -1.6956e-01,  6.0303e-02],\n          [ 3.6230e-01,  9.6240e-01, -1.9639e+00,  ...,  2.4365e-01,\n           -2.7324e+00, -1.8750e-01],\n          [ 6.3232e-01,  2.4570e+00, -2.3477e+00,  ..., -1.4424e+00,\n           -1.6963e+00,  1.3467e+00],\n          ...,\n          [ 7.4609e-01,  2.4338e-02, -7.8906e-01,  ...,  5.2881e-01,\n            4.3640e-02,  6.1279e-01],\n          [ 8.1348e-01, -3.5645e-02, -6.0938e-01,  ...,  5.3076e-01,\n            6.3171e-02,  5.1660e-01],\n          [ 8.2568e-01, -5.2734e-02, -7.9639e-01,  ...,  5.9766e-01,\n           -7.9590e-02,  6.6357e-01]],\n\n         ...,\n\n         [[-7.9199e-01, -9.5654e-01, -3.7872e-02,  ...,  4.3945e-01,\n           -6.4209e-01,  3.1689e-01],\n          [-1.1797e+00, -2.2109e+00,  1.3193e+00,  ...,  2.9902e+00,\n            3.3926e+00,  1.3604e+00],\n          [ 6.0352e-01, -3.9404e-01,  7.7881e-01,  ...,  2.5684e+00,\n            4.1758e+00,  1.7422e+00],\n          ...,\n          [-2.6709e-01, -1.0625e+00, -1.4221e-01,  ...,  6.4111e-01,\n           -4.9121e-01,  4.9463e-01],\n          [-3.5083e-01, -1.1504e+00, -5.7037e-02,  ...,  5.7568e-01,\n           -5.6689e-01,  5.6787e-01],\n          [-2.2644e-01, -1.1172e+00, -4.3884e-02,  ...,  5.5273e-01,\n           -4.1382e-01,  5.5566e-01]],\n\n         [[ 3.1338e-03, -6.8604e-02,  6.3843e-02,  ..., -4.7290e-01,\n            2.7759e-01,  3.6499e-01],\n          [ 4.0308e-01,  8.1152e-01,  1.3623e+00,  ...,  5.0928e-01,\n            8.8037e-01,  1.0052e-01],\n          [ 1.7981e-01,  4.9707e-01,  1.2354e+00,  ...,  1.0771e+00,\n            1.4023e+00, -3.8062e-01],\n          ...,\n          [-6.5576e-01, -4.0833e-02,  1.3403e-01,  ..., -6.2354e-01,\n            9.9951e-01,  7.8430e-02],\n          [-6.4990e-01,  1.0681e-02,  4.1168e-02,  ..., -4.5459e-01,\n            1.1270e+00,  1.8542e-01],\n          [-6.0840e-01, -2.0386e-01,  1.1853e-01,  ..., -4.6191e-01,\n            8.7842e-01,  1.6516e-01]],\n\n         [[-1.5161e-01, -2.6782e-01,  4.0112e-01,  ..., -3.1055e-01,\n            3.6987e-01,  9.0637e-02],\n          [ 5.7715e-01,  1.0803e-01,  4.5166e-01,  ..., -1.3184e-01,\n            1.6855e+00, -7.5500e-02],\n          [ 1.4756e+00,  2.5757e-01,  4.4922e-01,  ..., -8.2373e-01,\n            8.6865e-01, -7.9053e-01],\n          ...,\n          [-3.9258e-01, -6.2805e-02, -7.5317e-02,  ..., -1.2561e-01,\n            1.3135e-01,  2.6050e-01],\n          [-2.5342e-01,  5.2368e-02,  7.7759e-02,  ...,  3.9856e-02,\n            1.2964e-01,  2.5781e-01],\n          [-8.3069e-02, -1.4880e-01,  3.1396e-01,  ..., -4.1809e-03,\n            2.3474e-01,  2.1497e-01]]],\n\n\n        [[[ 4.3140e-01, -1.8774e-01,  1.8408e-01,  ...,  1.5236e-02,\n           -2.1838e-01,  1.4275e-02],\n          [-5.2393e-01,  2.2734e+00,  2.8931e-01,  ...,  8.5791e-01,\n           -1.2588e+00,  7.0703e-01],\n          [-1.3320e+00, -3.5815e-01, -7.1191e-01,  ..., -2.9614e-01,\n           -5.9375e-01,  1.5221e-02],\n          ...,\n          [ 1.3110e-01, -2.8345e-01, -6.3843e-02,  ...,  1.0077e-01,\n            4.6234e-03, -3.1494e-01],\n          [ 2.7148e-01, -3.3130e-01,  8.0811e-02,  ...,  1.7273e-01,\n           -2.2339e-01, -3.1445e-01],\n          [ 2.3181e-01, -3.7329e-01,  9.1187e-02,  ...,  8.2397e-02,\n           -6.2561e-02, -2.6807e-01]],\n\n         [[ 3.9551e-01,  1.0208e-02,  1.4233e-01,  ..., -6.3477e-02,\n            8.6670e-01,  5.1074e-01],\n          [ 1.1113e+00,  8.1982e-01,  1.1191e+00,  ..., -1.0615e+00,\n           -9.4434e-01,  7.5537e-01],\n          [-4.6704e-01,  1.9727e+00, -5.7031e-01,  ..., -3.4155e-01,\n           -3.2461e+00, -8.2227e-01],\n          ...,\n          [ 2.1729e-01,  3.5962e-01, -2.7100e-01,  ..., -1.3351e-02,\n            6.9775e-01, -1.4001e-01],\n          [ 1.9604e-01,  2.7905e-01, -1.2585e-01,  ...,  8.6243e-02,\n            8.2227e-01, -2.0065e-02],\n          [ 2.4353e-01,  4.6606e-01, -1.7627e-01,  ...,  1.9775e-02,\n            1.0156e+00, -1.5894e-01]],\n\n         [[-4.8145e-01, -1.1322e-01, -4.2627e-01,  ...,  2.9517e-01,\n           -1.7627e-01, -8.2214e-02],\n          [-1.1992e+00, -5.8008e-01, -8.1396e-01,  ...,  9.2529e-02,\n           -1.6191e+00,  2.4570e+00],\n          [-6.6895e-01,  4.7485e-01, -1.4180e+00,  ..., -1.1768e+00,\n           -2.6978e-01,  1.6836e+00],\n          ...,\n          [ 7.2876e-02,  8.5388e-02, -8.1006e-01,  ...,  6.8018e-01,\n            4.0771e-02,  5.6836e-01],\n          [-1.3733e-01, -1.5210e-01, -6.2744e-01,  ...,  6.2158e-01,\n           -1.9974e-02,  5.7422e-01],\n          [-2.2888e-02,  3.9948e-02, -6.9629e-01,  ...,  5.9717e-01,\n            1.0760e-01,  7.1240e-01]],\n\n         ...,\n\n         [[-7.9639e-01, -7.8564e-01, -3.5107e-01,  ...,  1.3721e-01,\n           -7.1777e-01,  1.9482e-01],\n          [ 8.9404e-01, -5.9717e-01,  1.0195e+00,  ..., -3.2031e-01,\n            5.5391e+00, -3.7280e-01],\n          [-9.0430e-01, -1.9141e+00,  1.5371e+00,  ...,  1.5225e+00,\n            5.6328e+00,  9.2725e-01],\n          ...,\n          [-4.2944e-01, -7.7588e-01, -1.9482e-01,  ...,  1.5576e-01,\n           -6.9189e-01,  5.0684e-01],\n          [-3.1445e-01, -7.9102e-01, -2.2876e-01,  ...,  1.2378e-01,\n           -8.7549e-01,  5.1367e-01],\n          [-1.9934e-01, -6.6162e-01, -4.4702e-01,  ...,  2.0679e-01,\n           -8.2471e-01,  2.6318e-01]],\n\n         [[ 7.7271e-02, -1.1133e-01,  5.9961e-01,  ...,  3.3417e-02,\n           -3.2257e-02,  1.9983e-01],\n          [ 2.1270e+00,  1.0459e+00,  1.3906e+00,  ...,  4.3848e-01,\n            1.6924e+00, -7.2388e-02],\n          [ 2.6191e+00,  1.5205e+00,  2.1328e+00,  ...,  4.5996e-01,\n            1.8184e+00,  2.2668e-01],\n          ...,\n          [-3.7817e-01, -1.7554e-01,  2.3560e-01,  ..., -6.2256e-02,\n            7.1143e-01,  5.1575e-02],\n          [-3.6255e-01, -2.1729e-01,  3.2593e-01,  ..., -2.3792e-01,\n            6.8262e-01,  2.8763e-02],\n          [-2.8516e-01, -1.6028e-01,  2.3499e-01,  ..., -6.8604e-02,\n            8.3936e-01,  1.4099e-01]],\n\n         [[-4.8926e-01, -3.8269e-02,  3.8843e-01,  ..., -2.7344e-01,\n           -8.3435e-02,  1.4417e-01],\n          [-5.4443e-02, -1.4961e+00,  3.2056e-01,  ...,  3.0811e-01,\n           -9.3555e-01,  3.0225e-01],\n          [-2.3496e+00, -1.5303e+00,  6.1377e-01,  ..., -8.8135e-01,\n           -1.0342e+00, -2.7319e-01],\n          ...,\n          [-3.4521e-01,  1.2708e-01,  1.5527e-01,  ...,  2.8320e-02,\n            1.1574e-02,  5.3497e-02],\n          [-3.5620e-01, -3.5126e-02,  2.0435e-01,  ...,  1.9604e-01,\n           -1.0272e-01,  4.4678e-02],\n          [-4.1479e-01, -3.9902e-03,  1.4514e-01,  ...,  9.2407e-02,\n           -3.9978e-02,  7.0190e-02]]],\n\n\n        [[[ 2.7393e-01, -1.9360e-01,  2.6294e-01,  ...,  4.7180e-02,\n           -2.0068e-01,  7.8003e-02],\n          [-2.3984e+00,  1.8799e+00,  1.8516e+00,  ...,  8.4961e-01,\n           -1.7603e-01,  5.0488e-01],\n          [-1.9824e+00,  1.0410e+00, -8.0469e-01,  ...,  6.9031e-02,\n           -1.4082e+00, -3.6011e-01],\n          ...,\n          [-1.1676e-01, -2.9346e-01,  1.3513e-01,  ..., -7.0410e-01,\n            8.1299e-01, -2.0312e-01],\n          [-2.7496e-02, -1.5503e-01, -1.0687e-01,  ..., -5.1123e-01,\n            8.8379e-01, -2.6807e-01],\n          [ 7.1289e-02, -2.2729e-01,  9.7412e-02,  ..., -3.5278e-01,\n            5.5713e-01, -1.6150e-01]],\n\n         [[ 5.9717e-01,  1.9263e-01, -1.9604e-01,  ...,  2.5073e-01,\n            7.0654e-01,  8.2227e-01],\n          [ 2.5586e-01,  2.5215e+00,  2.0972e-01,  ..., -1.8574e+00,\n           -2.3223e+00,  5.4102e-01],\n          [-1.4014e+00,  1.7373e+00,  2.4768e-01,  ...,  2.0154e-01,\n           -1.2178e+00,  8.8037e-01],\n          ...,\n          [ 2.2668e-01,  1.2100e+00,  1.5967e-01,  ..., -2.4280e-01,\n            3.1885e-01, -2.6416e-01],\n          [ 3.7817e-01,  1.2021e+00, -2.4521e-02,  ..., -3.5986e-01,\n            6.1035e-01, -4.5752e-01],\n          [ 4.8022e-01,  1.0332e+00,  9.1125e-02,  ..., -2.2583e-01,\n            5.2197e-01, -1.1780e-01]],\n\n         [[-3.2886e-01, -2.4719e-02,  8.2581e-02,  ...,  2.1802e-01,\n           -6.3171e-03, -8.3313e-02],\n          [-2.9414e+00,  4.2236e-02, -1.9834e+00,  ..., -1.4209e+00,\n           -5.1709e-01,  9.6582e-01],\n          [-1.2988e+00,  1.7227e+00, -2.3184e+00,  ..., -1.2227e+00,\n            4.1943e-01,  2.6543e+00],\n          ...,\n          [ 3.7231e-01, -3.9209e-01, -8.5205e-01,  ...,  5.4297e-01,\n            2.6978e-01,  1.3682e+00],\n          [ 3.3276e-01, -3.9160e-01, -1.0664e+00,  ...,  8.4082e-01,\n            3.5156e-01,  1.1650e+00],\n          [ 9.6497e-02, -2.4768e-01, -7.5928e-01,  ...,  5.9570e-01,\n            4.3945e-01,  1.0547e+00]],\n\n         ...,\n\n         [[-5.0781e-01, -4.6460e-01,  4.0894e-03,  ...,  3.7817e-01,\n           -1.1953e+00,  4.2065e-01],\n          [-1.1792e-01, -7.7881e-01, -1.0223e-01,  ...,  2.2656e+00,\n            5.1641e+00,  2.2510e-01],\n          [ 4.7729e-01,  9.4833e-03, -3.8379e-01,  ..., -7.6904e-01,\n            5.9062e+00,  1.0225e+00],\n          ...,\n          [ 3.5352e-01, -9.9219e-01, -2.6566e-02,  ...,  5.7861e-01,\n           -1.5247e-01,  2.5903e-01],\n          [ 4.8145e-01, -6.4111e-01,  1.3416e-01,  ...,  3.7329e-01,\n           -3.3398e-01,  2.5513e-01],\n          [ 2.3804e-01, -1.1895e+00,  2.4280e-01,  ...,  5.0879e-01,\n           -1.4526e-01,  2.2217e-01]],\n\n         [[-3.8477e-01,  1.3159e-01,  3.1323e-01,  ..., -2.8345e-01,\n           -1.8387e-02,  4.0381e-01],\n          [ 1.6377e+00,  4.9731e-01,  1.9502e+00,  ...,  1.5986e+00,\n           -8.0444e-02, -4.0967e-01],\n          [ 2.1348e+00,  7.3486e-01,  1.4062e+00,  ...,  7.9248e-01,\n            1.2930e+00,  3.5645e-01],\n          ...,\n          [-9.2285e-01,  3.4570e-01, -1.9348e-01,  ..., -4.5850e-01,\n            1.0225e+00,  4.3115e-01],\n          [-8.6816e-01,  5.1849e-02, -1.7273e-01,  ..., -3.1128e-01,\n            1.1455e+00,  6.2988e-01],\n          [-9.4336e-01,  3.8428e-01, -5.5115e-02,  ..., -2.6587e-01,\n            9.5020e-01,  3.5425e-01]],\n\n         [[-3.9844e-01,  3.7384e-03,  2.6929e-01,  ...,  2.2552e-02,\n           -3.2446e-01,  3.7573e-01],\n          [-1.1836e+00, -1.0830e+00,  8.5645e-01,  ...,  7.7246e-01,\n           -7.6758e-01, -1.1963e+00],\n          [ 1.2659e-01, -4.6533e-01,  9.0256e-03,  ...,  3.8789e+00,\n           -1.8672e+00, -1.8232e+00],\n          ...,\n          [-6.1670e-01,  4.7729e-01, -3.4302e-02,  ...,  1.7957e-01,\n           -1.0645e-01,  6.7432e-01],\n          [-1.6028e-01,  2.2754e-01, -3.8757e-02,  ...,  2.7515e-01,\n           -1.1438e-01,  5.1514e-01],\n          [-3.1689e-01,  2.5635e-01, -2.2168e-01,  ...,  3.5083e-01,\n            1.3464e-01,  4.4653e-01]]]], device='cuda:0', dtype=torch.float16,\n       grad_fn=<TransposeBackward0>), tensor([[[[-1.2441e+00,  4.5459e-01,  3.5571e-01,  ..., -2.3880e-02,\n            1.3896e+00, -1.3147e-01],\n          [-3.2402e+00,  3.1523e+00,  2.2773e+00,  ...,  3.3262e+00,\n           -2.3789e+00,  5.0117e+00],\n          [ 1.0400e+00, -4.3433e-01,  7.8438e+00,  ...,  8.1250e+00,\n           -4.3984e+00,  6.7305e+00],\n          ...,\n          [-1.6123e+00,  8.5754e-02,  1.4648e+00,  ...,  4.0845e-01,\n            1.1328e+00,  1.4609e+00],\n          [-1.4707e+00,  4.5215e-01,  1.7617e+00,  ...,  1.0040e-01,\n            1.6953e+00, -2.6953e-01],\n          [-1.3926e+00,  1.0127e+00,  2.6270e+00,  ...,  7.9639e-01,\n            1.8994e+00,  2.7222e-01]],\n\n         [[-2.3965e+00,  3.1406e+00,  6.5723e-01,  ..., -1.6240e+00,\n            9.9869e-03,  2.8691e+00],\n          [ 5.1221e-01,  3.9771e-01,  1.0469e+00,  ..., -1.6494e+00,\n            4.7422e+00,  4.5195e+00],\n          [-8.8438e+00, -5.9424e-01,  3.8379e+00,  ...,  9.0674e-01,\n            5.8516e+00, -3.7598e+00],\n          ...,\n          [-1.6201e+00,  4.3906e+00,  2.2246e+00,  ..., -2.7383e+00,\n            1.4561e+00,  2.9121e+00],\n          [-1.9043e+00,  4.1758e+00,  2.6406e+00,  ..., -2.2598e+00,\n            4.7998e-01,  3.2520e+00],\n          [-1.0332e+00,  3.8438e+00,  2.2637e+00,  ..., -2.4316e+00,\n            6.0303e-01,  3.0820e+00]],\n\n         [[-7.4023e-01,  2.7852e+00,  6.1182e-01,  ..., -9.1992e-01,\n            1.9248e+00,  4.5312e-01],\n          [-4.6914e+00,  2.7129e+00, -1.8809e+00,  ...,  2.8027e+00,\n           -4.1260e-01, -1.4678e+00],\n          [ 2.1270e+00,  2.0254e+00, -5.4922e+00,  ...,  5.1172e+00,\n           -4.6606e-01,  2.1973e-01],\n          ...,\n          [-7.7344e-01,  2.3496e+00,  2.0352e+00,  ..., -1.0967e+00,\n            1.5371e+00, -6.8604e-02],\n          [-8.5889e-01,  1.7637e+00,  1.8271e+00,  ..., -1.2119e+00,\n            2.6484e+00,  2.7930e-01],\n          [-7.2754e-01,  2.6875e+00,  2.0430e+00,  ..., -7.5195e-01,\n            2.2754e+00,  3.4473e-01]],\n\n         ...,\n\n         [[-1.5479e+00, -7.6318e-01, -1.1309e+00,  ..., -4.2432e-01,\n            1.7871e+00, -1.4160e-01],\n          [-1.5732e+00, -5.1367e+00,  1.4258e+00,  ..., -3.3838e-01,\n            3.5278e-01, -2.1582e+00],\n          [ 2.1445e+00, -5.4688e-01, -2.3848e+00,  ...,  1.9297e+00,\n           -3.6011e-01, -2.7832e+00],\n          ...,\n          [-9.8535e-01, -8.3838e-01, -2.3027e+00,  ..., -1.1633e-01,\n            1.8730e+00, -5.4248e-01],\n          [-9.7266e-01, -2.1660e+00, -1.6982e+00,  ..., -9.1736e-02,\n            1.7959e+00, -5.6885e-01],\n          [-9.7900e-01, -1.2676e+00, -1.4531e+00,  ..., -5.3223e-01,\n            2.1836e+00, -6.1084e-01]],\n\n         [[ 1.3232e+00,  1.0358e-01, -4.6851e-01,  ..., -1.9697e+00,\n            2.1899e-01, -1.1841e-01],\n          [ 4.6016e+00,  3.6875e+00,  1.5967e+00,  ...,  1.3555e+00,\n           -5.1406e+00, -1.4922e+00],\n          [ 1.9775e+00,  1.6699e+00,  4.6875e-02,  ..., -2.1504e+00,\n           -9.3408e-01, -6.0352e-01],\n          ...,\n          [ 1.0352e+00, -4.1504e-01,  9.4727e-01,  ..., -2.9023e+00,\n            1.7119e+00, -1.8591e-01],\n          [ 1.9751e-01, -1.5906e-01,  6.6357e-01,  ..., -3.0488e+00,\n            1.5000e+00,  5.4138e-02],\n          [ 5.5029e-01,  5.9631e-02,  5.7422e-01,  ..., -2.5527e+00,\n            1.7080e+00,  3.7695e-01]],\n\n         [[ 4.8047e-01,  1.0254e+00,  9.6631e-01,  ...,  1.0830e+00,\n            6.3525e-01, -4.0283e-01],\n          [ 4.3867e+00,  6.2109e+00, -2.9761e-01,  ...,  5.0547e+00,\n            1.6172e+00, -1.9932e+00],\n          [-2.6191e+00,  2.8574e+00,  3.0054e-01,  ...,  3.0371e+00,\n            4.0508e+00, -3.7383e+00],\n          ...,\n          [ 7.6514e-01, -1.1896e-01,  5.2148e-01,  ...,  3.3423e-01,\n           -9.3213e-01, -9.8145e-01],\n          [-1.4648e-01, -2.9099e-02,  2.1924e-01,  ...,  4.2163e-01,\n           -1.4395e+00, -5.0244e-01],\n          [ 5.8252e-01, -1.4551e-01,  4.3213e-01,  ...,  2.9321e-01,\n           -6.9189e-01, -1.3506e+00]]],\n\n\n        [[[-1.1615e-01, -4.0479e-01,  8.3984e-01,  ...,  4.1479e-01,\n           -1.1890e-01,  4.6069e-01],\n          [ 2.4785e+00, -5.9180e-01,  3.1602e+00,  ...,  3.4629e+00,\n            4.8633e-01,  3.1816e+00],\n          [-1.7480e-01,  2.5430e+00, -3.4644e-01,  ...,  4.0000e+00,\n            7.5537e-01,  7.4297e+00],\n          ...,\n          [-1.0605e+00,  4.8584e-01,  7.7246e-01,  ...,  8.0957e-01,\n            1.1494e+00, -6.6260e-01],\n          [-1.7939e+00,  1.6150e-01,  1.3193e+00,  ...,  1.1621e+00,\n            6.2646e-01,  4.3701e-01],\n          [-2.2734e+00,  3.3716e-01,  1.6846e+00,  ...,  1.2783e+00,\n            1.5117e+00,  1.5894e-01]],\n\n         [[-9.3848e-01,  1.0785e-01, -1.4148e-01,  ..., -2.2156e-01,\n           -1.2588e+00,  1.4141e+00],\n          [-3.8613e+00,  4.7388e-01,  5.8594e+00,  ...,  5.9326e-01,\n            6.3984e+00,  3.6738e+00],\n          [-5.0977e+00,  1.9941e+00, -4.8867e+00,  ...,  1.9053e+00,\n            7.0391e+00,  3.1758e+00],\n          ...,\n          [-9.7510e-01,  1.5293e+00,  7.2168e-01,  ..., -4.6802e-01,\n            6.8115e-01,  1.6836e+00],\n          [-2.0273e+00,  9.1211e-01,  1.4727e+00,  ..., -6.8970e-02,\n           -4.0503e-01,  1.1064e+00],\n          [-1.4707e+00,  1.7344e+00,  1.0850e+00,  ...,  5.2917e-02,\n           -1.1096e-01,  1.3691e+00]],\n\n         [[ 2.2278e-01,  4.8999e-01, -2.4500e-01,  ..., -5.3906e-01,\n            2.4002e-02, -1.4180e+00],\n          [-1.4355e-01,  3.6621e+00,  3.2715e-01,  ..., -4.5234e+00,\n           -1.5518e+00,  2.0825e-01],\n          [ 1.6748e-01, -2.5840e+00, -5.2539e-01,  ..., -1.1680e+00,\n           -2.4375e+00,  2.2461e+00],\n          ...,\n          [-8.2422e-01,  5.6689e-01,  2.7954e-01,  ..., -7.2021e-01,\n            5.0195e-01, -9.1992e-01],\n          [-4.4165e-01, -8.0383e-02,  9.4543e-02,  ..., -1.0225e+00,\n            2.5537e-01, -9.0332e-01],\n          [-6.6650e-01,  3.8379e-01,  2.0691e-01,  ..., -1.3311e+00,\n            1.9629e-01, -1.3174e+00]],\n\n         ...,\n\n         [[-1.0264e+00, -4.2847e-01, -8.2080e-01,  ..., -4.8291e-01,\n            4.3311e-01,  2.2571e-01],\n          [-2.7305e+00,  1.0977e+00,  3.1445e+00,  ..., -1.6348e+00,\n           -4.4727e+00,  2.6543e+00],\n          [ 2.0078e+00, -3.9395e+00, -2.0898e+00,  ..., -1.2170e-01,\n            1.7615e-01, -2.0156e+00],\n          ...,\n          [ 1.3611e-01, -2.0156e+00, -1.9365e+00,  ...,  4.6948e-01,\n            4.0649e-01,  7.5586e-01],\n          [-1.6431e-01, -1.7178e+00, -1.9062e+00,  ...,  3.3594e-01,\n            3.0249e-01,  5.0684e-01],\n          [-1.1566e-01, -1.3584e+00, -2.0352e+00,  ...,  7.5684e-01,\n            7.1680e-01,  1.3513e-01]],\n\n         [[-3.5309e-02,  2.5537e-01,  4.6094e-01,  ..., -5.0635e-01,\n            8.8721e-01,  3.4937e-01],\n          [-5.3555e+00,  3.1113e+00, -1.6729e+00,  ...,  1.8047e+00,\n            2.3691e+00, -2.5508e+00],\n          [-6.8672e+00,  1.9102e+00, -5.5371e-01,  ..., -4.1211e-01,\n           -1.2959e+00,  1.8486e+00],\n          ...,\n          [ 7.4805e-01, -7.2461e-01,  8.2959e-01,  ..., -6.5039e-01,\n            1.0176e+00,  3.3545e-01],\n          [ 2.8687e-01, -2.6660e-01,  1.0938e+00,  ..., -9.0967e-01,\n            1.7930e+00,  6.4795e-01],\n          [-2.4072e-01, -3.3154e-01,  1.0137e+00,  ..., -7.3242e-01,\n            1.6484e+00,  1.2705e+00]],\n\n         [[-2.5610e-01,  2.1252e-01,  9.6436e-02,  ...,  9.9854e-01,\n            4.3896e-01, -5.7617e-01],\n          [-3.2246e+00,  4.0898e+00,  6.6650e-01,  ..., -2.0645e+00,\n           -1.9668e+00,  1.7285e+00],\n          [-5.4962e-02,  5.7578e+00,  1.8506e+00,  ...,  3.7598e-01,\n           -1.7891e+00,  9.7266e-01],\n          ...,\n          [ 6.2842e-01,  1.2952e-01, -8.3496e-01,  ...,  2.5366e-01,\n           -2.3682e-01, -1.7627e+00],\n          [ 4.1309e-01,  6.6711e-02, -3.4521e-01,  ...,  7.2363e-01,\n           -7.2852e-01, -1.1924e+00],\n          [ 3.8550e-01,  2.8296e-01, -1.2909e-02,  ...,  2.1509e-01,\n           -1.4551e-01, -1.9326e+00]]],\n\n\n        [[[ 2.8516e-01,  4.3427e-02, -3.6102e-02,  ...,  2.2424e-01,\n            1.1582e+00, -4.3262e-01],\n          [ 7.3516e+00,  1.6003e-01,  1.8105e+00,  ..., -6.7227e+00,\n            9.2500e+00,  6.5781e+00],\n          [ 3.6108e-01, -3.1211e+00, -2.3125e+00,  ...,  1.0586e+00,\n           -1.1406e+00,  2.5801e+00],\n          ...,\n          [-2.4062e+00,  1.5723e+00,  1.2100e+00,  ...,  8.8867e-01,\n            2.0176e+00,  1.6089e-01],\n          [-1.5947e+00,  1.4043e+00,  1.6475e+00,  ...,  7.8271e-01,\n            2.1289e+00, -5.9766e-01],\n          [-2.2148e+00,  1.6309e+00,  1.3740e+00,  ...,  6.8994e-01,\n            2.2227e+00,  2.6901e-02]],\n\n         [[ 1.7163e-01,  4.1504e-01, -3.1494e-01,  ...,  5.6494e-01,\n           -9.5557e-01,  6.4209e-01],\n          [-5.9297e+00,  1.1484e+00,  4.0156e+00,  ...,  5.4531e+00,\n            4.5352e+00,  4.2993e-01],\n          [-5.2227e+00, -6.8750e-01,  4.0186e-01,  ...,  7.9956e-02,\n            4.0781e+00,  5.8516e+00],\n          ...,\n          [-3.7451e-01,  7.4121e-01,  9.0381e-01,  ...,  9.3918e-03,\n            7.3975e-01,  1.0068e+00],\n          [-3.7305e-01,  9.1650e-01,  1.6162e+00,  ..., -1.6602e-01,\n            1.3806e-01,  1.0029e+00],\n          [-3.7842e-01,  9.4922e-01,  9.7754e-01,  ...,  7.6904e-02,\n            6.1401e-02,  1.4219e+00]],\n\n         [[ 9.3945e-01,  1.2031e+00, -2.5610e-01,  ..., -5.2002e-01,\n           -1.6708e-02, -2.2107e-01],\n          [ 3.1484e+00, -1.8723e-02,  3.8037e-01,  ...,  2.7305e+00,\n            3.2227e-01,  2.0195e+00],\n          [ 2.3594e+00, -1.7695e+00, -2.6934e+00,  ...,  2.4844e+00,\n            1.5244e+00, -1.6387e+00],\n          ...,\n          [-5.0879e-01,  9.7461e-01,  7.8906e-01,  ..., -4.7046e-01,\n            1.1841e-01, -6.6748e-01],\n          [-6.4160e-01,  1.2100e+00,  1.0039e+00,  ..., -4.7632e-01,\n            7.6660e-01, -1.7920e-01],\n          [-1.2578e+00,  1.3311e+00,  5.2612e-02,  ..., -4.7388e-01,\n            1.2512e-01, -3.8910e-02]],\n\n         ...,\n\n         [[-1.0166e+00,  4.9536e-01, -4.1626e-01,  ..., -1.8274e-01,\n            3.6865e-01,  7.3779e-01],\n          [-2.4609e+00, -4.8359e+00, -4.6836e+00,  ...,  5.1211e+00,\n            4.3164e+00,  3.2500e+00],\n          [ 1.1670e+00, -1.6396e+00, -5.2695e+00,  ...,  2.8379e+00,\n            4.5547e+00,  3.8105e+00],\n          ...,\n          [ 2.3730e-01, -2.3691e+00, -1.9668e+00,  ...,  2.1035e+00,\n            9.5215e-01,  1.8053e-03],\n          [ 5.6348e-01, -2.0723e+00, -1.1416e+00,  ...,  1.9180e+00,\n            7.8027e-01,  7.7576e-02],\n          [-7.2119e-01, -1.9707e+00, -1.4932e+00,  ...,  2.4453e+00,\n            6.3574e-01, -4.5685e-02]],\n\n         [[ 1.9128e-01,  6.1475e-01,  4.2773e-01,  ...,  2.6440e-01,\n           -5.0830e-01, -3.5278e-01],\n          [-2.1309e+00,  5.2070e+00, -1.5928e+00,  ...,  2.9023e+00,\n            1.8086e+00, -8.0391e+00],\n          [-1.7773e+00,  3.9238e+00, -1.3672e+00,  ...,  1.2090e+00,\n            9.8291e-01, -4.2734e+00],\n          ...,\n          [ 7.0801e-01,  3.9185e-01,  2.4854e-01,  ..., -5.7031e-01,\n           -1.5649e-01, -8.1836e-01],\n          [-2.1896e-02,  1.1514e+00,  6.5063e-02,  ..., -8.1934e-01,\n           -3.7872e-02, -9.4043e-01],\n          [ 9.7168e-01,  1.3672e-01,  1.6785e-01,  ..., -4.5044e-01,\n           -1.1530e-01, -5.1709e-01]],\n\n         [[-7.2754e-02,  1.5249e-03, -1.0254e+00,  ...,  2.7588e-01,\n           -3.0640e-01, -3.4766e-01],\n          [ 3.0781e+00,  2.3340e+00,  3.3789e-01,  ...,  1.0518e+00,\n           -2.1660e+00, -4.8413e-01],\n          [ 3.6465e+00,  3.4453e+00,  8.3984e-01,  ...,  1.0352e+00,\n           -4.4883e+00,  3.2148e+00],\n          ...,\n          [ 3.0319e-02,  1.2500e-01, -1.1172e+00,  ..., -8.9941e-01,\n           -2.1621e+00, -1.3389e+00],\n          [-4.1284e-01, -2.5708e-01, -8.9209e-01,  ..., -6.2695e-01,\n           -2.0430e+00, -1.9570e+00],\n          [-1.1688e-01, -1.1091e-03, -7.2656e-01,  ..., -4.4653e-01,\n           -1.9688e+00, -1.9072e+00]]],\n\n\n        ...,\n\n\n        [[[ 1.0144e-01,  1.1533e+00, -3.7781e-02,  ...,  1.0449e+00,\n            1.6650e-01, -4.1771e-03],\n          [-4.6133e+00,  3.5625e+00,  4.9180e+00,  ...,  3.1680e+00,\n           -6.6260e-01,  1.2360e-01],\n          [-1.0098e+00,  2.5801e+00,  2.8926e+00,  ...,  2.8906e+00,\n            2.5859e+00,  4.3164e+00],\n          ...,\n          [-1.3105e+00,  1.4062e+00,  1.3574e+00,  ...,  2.4582e-02,\n            1.4834e+00,  3.2495e-01],\n          [-1.4180e+00,  1.2646e+00,  1.1943e+00,  ...,  7.0020e-01,\n            1.8291e+00,  4.4678e-01],\n          [-1.9385e+00,  1.5107e+00,  1.0146e+00,  ...,  2.5146e-01,\n            1.3662e+00,  3.2397e-01]],\n\n         [[-7.7930e-01,  3.1958e-01,  1.3271e+00,  ...,  9.8096e-01,\n           -1.1279e+00,  1.2128e-01],\n          [ 4.1553e-01,  4.8242e+00, -2.0762e+00,  ...,  3.9414e+00,\n           -3.6328e+00,  5.1797e+00],\n          [ 9.9219e-01,  8.9014e-01, -3.1016e+00,  ...,  2.2949e+00,\n           -2.6582e+00, -1.5176e+00],\n          ...,\n          [-5.3369e-01,  1.3252e+00,  1.6943e+00,  ..., -4.6362e-01,\n           -2.3828e-01,  3.6157e-01],\n          [-8.2715e-01,  1.2197e+00,  1.9648e+00,  ...,  2.1553e-03,\n            1.3458e-02,  6.2842e-01],\n          [-1.1279e+00,  9.1895e-01,  2.2598e+00,  ..., -1.6223e-01,\n           -7.7271e-02,  6.8115e-01]],\n\n         [[-7.3535e-01,  7.3853e-02, -5.9033e-01,  ..., -1.4580e+00,\n           -4.8755e-01, -7.3193e-01],\n          [ 2.5488e+00,  3.7480e+00, -1.5566e+00,  ..., -2.6113e+00,\n           -1.4756e+00, -1.4131e+00],\n          [-3.1230e+00,  2.9062e+00, -1.0596e+00,  ...,  7.6562e-01,\n           -1.2354e+00,  1.8398e+00],\n          ...,\n          [-5.6836e-01, -3.9893e-01,  1.6907e-01,  ..., -1.4697e+00,\n           -3.1958e-01, -3.2617e-01],\n          [-7.1387e-01,  8.0371e-01,  7.8809e-01,  ..., -1.7422e+00,\n           -2.4365e-01, -1.0322e+00],\n          [-1.1572e+00,  1.8860e-01,  6.5332e-01,  ..., -1.7363e+00,\n           -1.0283e+00, -1.3818e+00]],\n\n         ...,\n\n         [[-4.7168e-01,  9.6826e-01, -1.6973e+00,  ...,  4.3359e-01,\n            4.0088e-01,  2.4817e-01],\n          [ 2.6152e+00, -2.4590e+00,  1.6357e-01,  ...,  4.3994e-01,\n            2.6641e+00, -3.5488e+00],\n          [ 9.0674e-01, -6.9531e+00, -4.0742e+00,  ...,  2.6738e+00,\n            2.6504e+00, -3.6279e-01],\n          ...,\n          [-6.2439e-02, -6.3525e-01, -1.4658e+00,  ...,  1.1318e+00,\n            2.1130e-01,  5.9961e-01],\n          [-1.1816e-03, -7.8418e-01, -1.3213e+00,  ...,  1.0029e+00,\n            6.7688e-02,  1.3916e-01],\n          [-3.2593e-02, -1.3623e+00, -1.6260e+00,  ...,  1.1289e+00,\n            1.6223e-01,  5.4443e-01]],\n\n         [[ 4.1846e-01,  1.3340e+00, -6.5527e-01,  ..., -6.6748e-01,\n           -5.4932e-01, -1.3025e-01],\n          [ 5.7910e-01,  8.6084e-01,  4.3242e+00,  ..., -3.4395e+00,\n            2.8359e+00, -5.2539e+00],\n          [ 4.5239e-01,  9.3628e-02, -3.2349e-01,  ...,  1.1045e+00,\n            2.1680e+00, -6.8594e+00],\n          ...,\n          [ 8.2666e-01,  3.0762e-01,  5.6055e-01,  ..., -7.4280e-02,\n            3.1763e-01, -9.5444e-03],\n          [ 8.7354e-01, -2.6505e-02,  2.7905e-01,  ..., -1.0312e+00,\n            1.9153e-01, -8.0566e-01],\n          [ 1.1270e+00,  2.6108e-02,  1.8835e-01,  ..., -6.6455e-01,\n           -1.2830e-01, -5.3271e-01]],\n\n         [[-1.0352e+00,  3.5425e-01,  2.2571e-01,  ...,  6.1865e-01,\n            1.1270e+00, -3.4985e-01],\n          [ 1.8086e+00, -4.2109e+00,  1.4639e+00,  ...,  9.8779e-01,\n            2.3652e+00,  1.8643e+00],\n          [ 1.1523e+00,  4.9883e+00,  3.4180e-01,  ...,  8.2275e-01,\n            3.3418e+00,  1.2832e+00],\n          ...,\n          [-2.3926e-01,  8.2373e-01,  5.1318e-01,  ..., -1.3879e-01,\n           -3.4546e-01, -1.0098e+00],\n          [-2.3022e-01,  6.7773e-01,  2.1033e-01,  ..., -2.6550e-02,\n           -3.1812e-01, -1.0791e+00],\n          [ 2.7115e-02,  9.2236e-01,  8.3008e-02,  ..., -4.3457e-01,\n            3.8159e-01, -1.4092e+00]]],\n\n\n        [[[-7.4280e-02,  1.8555e-01,  1.4771e-01,  ...,  2.6807e-01,\n            6.8701e-01,  2.4622e-01],\n          [-1.1709e+00,  2.2773e+00,  3.5859e+00,  ...,  4.8657e-01,\n            3.2266e+00,  2.0781e+00],\n          [-3.0508e+00,  2.7676e+00,  6.5039e+00,  ...,  4.1992e+00,\n            3.4082e+00,  7.7197e-01],\n          ...,\n          [-1.2500e+00,  6.6895e-01,  1.0234e+00,  ..., -3.6869e-03,\n            1.9150e+00,  5.9473e-01],\n          [-1.1035e+00,  5.9619e-01,  9.5361e-01,  ..., -4.3872e-01,\n            1.1562e+00,  1.2842e-01],\n          [-9.4580e-01,  9.6143e-01,  8.4277e-01,  ..., -6.4850e-03,\n            1.5420e+00, -9.7412e-02]],\n\n         [[-5.7275e-01, -6.8848e-02, -6.0742e-01,  ...,  1.2178e+00,\n           -1.3818e+00, -3.4351e-01],\n          [-9.5703e+00,  1.1484e+01,  9.3359e-01,  ..., -7.5898e+00,\n            7.3438e+00,  1.0805e+01],\n          [-3.2305e+00,  2.0645e+00,  1.9697e+00,  ..., -2.0527e+00,\n            5.0273e+00,  4.1562e+00],\n          ...,\n          [-5.5859e-01,  2.4023e-01,  8.9648e-01,  ..., -2.1924e-01,\n           -3.4961e-01,  4.2627e-01],\n          [-7.1533e-01,  9.4360e-02,  5.4443e-01,  ..., -5.0293e-01,\n           -3.8086e-01,  7.8076e-01],\n          [-4.4800e-01,  6.8848e-01,  6.3281e-01,  ..., -4.8901e-01,\n           -4.2554e-01,  7.5586e-01]],\n\n         [[-7.4707e-02,  1.1113e+00, -2.6147e-01,  ..., -9.4531e-01,\n           -4.0332e-01, -9.4092e-01],\n          [-4.0742e+00,  5.6719e+00,  3.4375e-01,  ..., -3.8496e+00,\n            2.7637e+00,  3.0312e+00],\n          [-5.5566e-01,  3.6152e+00, -1.5996e+00,  ...,  5.8252e-01,\n            4.6729e-01, -3.3691e-01],\n          ...,\n          [-5.7520e-01,  1.0703e+00,  1.2749e-02,  ..., -6.0938e-01,\n            2.5317e-01, -1.0508e+00],\n          [-4.0820e-01,  5.3467e-01,  4.3848e-01,  ..., -8.2275e-01,\n            6.5869e-01, -1.4209e+00],\n          [ 3.4515e-02,  7.8857e-01, -7.3059e-02,  ..., -5.5078e-01,\n            5.1562e-01, -1.1455e+00]],\n\n         ...,\n\n         [[-6.2012e-01,  3.9917e-02, -3.9282e-01,  ..., -8.1006e-01,\n            2.8857e-01,  3.3423e-01],\n          [ 3.8398e+00, -9.5410e-01, -4.3945e+00,  ...,  4.5746e-02,\n            4.3242e+00, -2.5918e+00],\n          [ 4.3042e-01, -1.5449e+00, -2.0234e+00,  ...,  1.4365e+00,\n           -3.6289e+00, -3.1719e+00],\n          ...,\n          [ 6.3477e-02, -7.5537e-01, -8.0322e-01,  ..., -1.7273e-01,\n            1.3110e-01, -2.9126e-01],\n          [-1.5979e-01, -6.1719e-01, -9.8926e-01,  ..., -5.1709e-01,\n            3.8232e-01, -3.5034e-02],\n          [-5.8167e-02, -1.6807e+00, -8.2422e-01,  ..., -4.8950e-01,\n           -2.1765e-01,  9.4971e-02]],\n\n         [[ 4.5752e-01,  1.8628e-01,  5.4639e-01,  ..., -7.5391e-01,\n            3.2300e-01, -1.1221e+00],\n          [ 1.0781e+00,  1.1514e+00, -2.8418e-01,  ..., -2.6562e+00,\n           -1.7900e+00, -2.1582e+00],\n          [ 1.4629e+00,  8.7012e-01,  2.8438e+00,  ...,  3.5938e+00,\n           -1.2764e+00, -3.6934e+00],\n          ...,\n          [ 6.6309e-01,  3.7207e-01,  4.9902e-01,  ..., -9.8584e-01,\n           -2.3474e-01, -1.0186e+00],\n          [ 6.6553e-01,  1.3464e-01,  6.7139e-01,  ..., -7.8027e-01,\n           -4.8828e-01, -7.9590e-01],\n          [ 1.2480e+00,  5.0146e-01,  4.8364e-01,  ..., -8.0127e-01,\n            7.0068e-02, -7.5684e-01]],\n\n         [[-6.1963e-01,  1.3008e+00,  4.1138e-01,  ..., -1.9226e-01,\n           -5.8655e-02, -3.0566e-01],\n          [-3.6230e+00,  4.3281e+00,  3.1152e-01,  ...,  5.6641e+00,\n            2.6621e+00, -1.4336e+00],\n          [-2.4453e+00,  3.1758e+00, -5.8887e-01,  ...,  5.4023e+00,\n            1.8982e-01, -3.2871e+00],\n          ...,\n          [-4.0503e-01,  7.7588e-01, -2.5781e-01,  ..., -2.2388e-01,\n           -1.8896e-01, -1.4219e+00],\n          [-5.6250e-01,  8.7500e-01, -1.7969e-01,  ..., -1.0358e-01,\n           -1.2817e-01, -7.2510e-01],\n          [-7.5439e-01,  7.8076e-01, -4.4531e-01,  ...,  4.1443e-02,\n           -5.2197e-01, -9.3555e-01]]],\n\n\n        [[[-9.8022e-02,  8.3313e-02,  4.3848e-01,  ..., -3.9917e-02,\n            5.9668e-01, -9.9365e-01],\n          [ 3.0020e+00,  1.7148e+00,  3.7441e+00,  ...,  1.5332e+00,\n            4.1235e-01,  3.2690e-01],\n          [ 8.3740e-01,  4.8164e+00,  4.8867e+00,  ...,  1.4678e+00,\n            6.1953e+00,  3.2617e+00],\n          ...,\n          [-1.7393e+00,  9.7949e-01,  1.0986e+00,  ...,  2.7979e-01,\n            1.3115e+00, -2.5830e-01],\n          [-1.3037e+00,  1.3555e+00,  5.2295e-01,  ..., -6.1615e-02,\n            1.5615e+00,  2.6001e-01],\n          [-1.7793e+00,  1.0176e+00,  1.0625e+00,  ...,  1.2170e-01,\n            1.7773e+00, -3.5767e-01]],\n\n         [[-1.1064e+00, -3.7308e-03,  1.0020e+00,  ...,  1.3799e+00,\n           -2.8467e-01,  1.1104e+00],\n          [-5.4492e-01,  1.1289e+01,  3.1113e+00,  ..., -2.8809e+00,\n            5.2295e-01,  4.8516e+00],\n          [ 5.1797e+00,  5.4541e-01,  7.4375e+00,  ..., -5.7910e-01,\n            5.8125e+00,  2.3770e+00],\n          ...,\n          [-1.6104e+00,  6.9629e-01,  2.0098e+00,  ...,  5.2832e-01,\n           -6.6992e-01,  1.1885e+00],\n          [-1.6406e+00,  3.1592e-01,  1.3184e+00,  ...,  3.7500e-01,\n           -5.5322e-01,  8.7939e-01],\n          [-1.8750e+00,  1.9763e-01,  1.7627e+00,  ...,  6.9141e-01,\n           -1.5222e-01,  1.1953e+00]],\n\n         [[ 5.6787e-01, -3.1177e-01, -3.7793e-01,  ...,  5.5359e-02,\n            2.2900e-01, -1.3223e+00],\n          [-1.6631e+00,  1.3818e+00, -2.6309e+00,  ...,  4.6016e+00,\n           -3.5527e+00, -1.4053e+00],\n          [-6.4453e-02, -9.8096e-01,  2.9395e-01,  ...,  1.6709e+00,\n            5.6875e+00, -3.5864e-01],\n          ...,\n          [-4.5459e-01,  8.6035e-01,  9.0234e-01,  ..., -1.1729e+00,\n           -1.6418e-01, -1.4697e+00],\n          [-7.9395e-01,  8.9258e-01,  1.0674e+00,  ..., -1.1465e+00,\n           -6.3477e-01, -1.2217e+00],\n          [-6.5527e-01,  1.0272e-01,  1.1631e+00,  ..., -6.5869e-01,\n           -5.3955e-01, -1.7637e+00]],\n\n         ...,\n\n         [[-8.0176e-01,  1.9214e-01, -9.0088e-02,  ..., -1.9861e-01,\n           -2.1399e-01,  4.7974e-01],\n          [ 9.3164e-01, -5.1602e+00, -2.2559e+00,  ...,  2.1250e+00,\n           -2.1406e+00,  3.4790e-01],\n          [ 1.7227e+00, -3.4414e+00, -2.2986e-01,  ...,  1.1162e+00,\n            1.8762e-01, -3.7578e+00],\n          ...,\n          [-7.7197e-01, -3.5625e+00, -1.0410e+00,  ..., -5.5615e-01,\n            2.0703e-01, -8.6328e-01],\n          [-2.3706e-01, -3.2812e+00, -8.1348e-01,  ..., -1.3164e+00,\n           -1.8994e-01, -9.3750e-01],\n          [-9.3945e-01, -2.4082e+00, -7.0508e-01,  ..., -7.9956e-02,\n           -1.6296e-02, -5.3271e-01]],\n\n         [[ 2.1045e-01,  5.7648e-02,  4.8169e-01,  ..., -7.5928e-01,\n            5.8838e-01, -2.0715e-01],\n          [ 2.9277e+00, -1.0195e+00, -7.0605e-01,  ..., -3.0059e+00,\n            4.0039e+00, -3.3750e+00],\n          [-1.0625e+00,  3.9551e+00,  3.0811e-01,  ...,  4.9297e+00,\n            4.2334e-01, -1.2822e+00],\n          ...,\n          [ 2.0142e-01, -1.2756e-01,  1.2080e+00,  ...,  8.4900e-02,\n            1.5088e+00, -1.4424e+00],\n          [ 2.9028e-01,  3.4277e-01,  6.3037e-01,  ..., -6.4014e-01,\n            8.4912e-01, -3.5278e-01],\n          [ 2.2388e-01, -3.0859e-01,  3.9209e-01,  ..., -4.8828e-01,\n            8.3154e-01, -3.8257e-01]],\n\n         [[-9.6973e-01, -5.0195e-01, -4.3970e-01,  ...,  7.2607e-01,\n           -1.2329e-01, -3.0688e-01],\n          [ 1.6875e+00,  2.3203e+00, -9.2236e-01,  ..., -8.0176e-01,\n            6.8867e+00, -3.7383e+00],\n          [-5.0879e-01,  3.0879e+00, -2.3697e-02,  ...,  2.7649e-02,\n            1.3154e+00,  3.4424e-01],\n          ...,\n          [-4.9976e-01,  8.5010e-01, -6.0742e-01,  ..., -1.6445e+00,\n           -1.6162e+00, -1.2695e+00],\n          [-1.0242e-01,  4.5166e-01, -7.6807e-01,  ..., -1.1191e+00,\n           -5.8789e-01, -1.4268e+00],\n          [-7.0459e-01,  5.2295e-01, -5.2441e-01,  ..., -8.9600e-01,\n           -1.1406e+00, -8.0176e-01]]]], device='cuda:0', dtype=torch.float16,\n       grad_fn=<TransposeBackward0>), tensor([[[[-3.6523e+00,  7.5938e+00, -7.8857e-01,  ...,  2.9375e+00,\n            1.0656e+01, -3.9922e+00],\n          [ 1.0430e+00,  5.6797e+00,  1.7510e+00,  ...,  4.6328e+00,\n            8.5625e+00, -3.0508e+00],\n          [-4.7314e-01,  1.0156e+01,  6.6641e+00,  ..., -4.1836e+00,\n            6.5469e+00, -1.2832e+00],\n          ...,\n          [-1.0410e+00,  2.6211e+00,  3.0371e+00,  ...,  2.7246e+00,\n           -3.6367e+00, -2.2266e+00],\n          [-5.6758e+00,  2.3262e+00,  4.8438e+00,  ...,  3.0020e+00,\n            3.2168e+00, -8.4922e+00],\n          [-2.1484e+00,  3.9609e+00,  7.4297e+00,  ...,  4.6406e+00,\n           -3.9771e-01, -1.2998e+00]],\n\n         [[ 4.8477e+00, -3.1582e+00, -2.9434e+00,  ..., -3.5293e+00,\n           -2.6348e+00, -2.9004e+00],\n          [-3.3164e+00, -2.1113e+00, -6.1094e+00,  ...,  1.0000e+01,\n            1.3094e+01, -1.8623e+00],\n          [-3.0059e+00,  4.6328e+00,  1.6318e+00,  ...,  1.8203e+00,\n            6.8672e+00,  2.0332e+00],\n          ...,\n          [-3.2598e+00,  1.4854e+00,  4.3477e+00,  ...,  1.6689e+00,\n           -2.1484e+00, -6.9883e+00],\n          [-3.0605e+00, -1.3271e+00,  3.4512e+00,  ..., -6.1758e+00,\n           -3.8125e+00, -2.4688e+00],\n          [-2.6211e+00, -2.9590e+00,  4.9258e+00,  ..., -9.9316e-01,\n           -4.7656e+00, -7.0234e+00]],\n\n         [[-5.1328e+00,  3.9023e+00, -4.6484e+00,  ...,  4.2266e+00,\n            6.7852e+00, -3.1250e+00],\n          [-1.1230e+00, -2.5938e+00,  1.7051e+00,  ...,  1.5088e+00,\n            6.6172e+00, -3.6641e+00],\n          [ 3.3618e-01, -6.0234e+00,  2.6094e+00,  ...,  7.3750e+00,\n            3.7891e+00, -3.0312e+00],\n          ...,\n          [ 8.9697e-01,  4.9683e-01,  3.7422e+00,  ...,  2.0527e+00,\n           -1.8066e+00, -2.0312e+00],\n          [ 5.0781e+00, -6.1133e-01,  3.0469e+00,  ...,  3.8691e+00,\n           -1.9688e+00, -3.0508e+00],\n          [ 5.8281e+00, -1.7617e+00,  3.4980e+00,  ...,  4.9648e+00,\n            1.2021e+00, -3.1992e+00]],\n\n         ...,\n\n         [[-2.4492e+00, -8.4375e-01, -6.0859e+00,  ..., -6.4531e+00,\n           -6.7734e+00,  6.9102e+00],\n          [-4.9707e-01, -6.6162e-01, -5.4375e+00,  ..., -7.0039e+00,\n           -9.8047e+00,  5.6562e+00],\n          [ 1.4082e+00, -8.4717e-01, -2.7539e+00,  ..., -3.2402e+00,\n           -1.3516e+00, -3.9590e+00],\n          ...,\n          [-2.2676e+00,  2.6211e+00, -7.4365e-01,  ..., -9.0078e+00,\n           -3.7671e-01, -2.3633e+00],\n          [ 7.6709e-01, -6.2061e-01,  8.3281e+00,  ..., -9.5000e+00,\n           -2.7695e+00, -3.3008e+00],\n          [ 3.4688e+00,  2.5254e+00,  4.0859e+00,  ..., -1.0844e+01,\n           -1.3245e-01, -1.9688e+00]],\n\n         [[-1.2285e+00, -7.4219e+00, -1.7012e+00,  ...,  1.1230e+00,\n           -2.7285e+00, -4.8672e+00],\n          [ 3.2852e+00, -3.3926e+00,  1.8242e+00,  ...,  1.9580e+00,\n           -3.2402e+00, -3.3379e+00],\n          [-9.9170e-01,  4.6082e-02, -2.2070e+00,  ...,  3.4277e+00,\n            1.4092e+00,  4.7773e+00],\n          ...,\n          [-6.0303e-01,  2.3398e+00,  3.7148e+00,  ...,  2.2969e+00,\n           -6.7344e+00,  6.9885e-02],\n          [ 1.9814e+00,  3.5571e-01, -2.1484e+00,  ...,  2.0374e-01,\n           -3.8711e+00, -6.6113e-01],\n          [ 4.1953e+00, -6.2549e-01,  4.5430e+00,  ..., -5.3320e-01,\n           -8.7695e-01, -1.7305e+00]],\n\n         [[ 1.3340e+00, -2.5078e+00, -1.0518e+00,  ...,  2.7075e-01,\n            1.7688e-01, -5.1797e+00],\n          [ 2.7852e+00,  7.5684e-01, -1.1743e-01,  ..., -1.0879e+00,\n            3.2148e+00, -9.3438e+00],\n          [ 2.3328e-01,  4.5664e+00,  7.0117e-01,  ..., -3.3438e+00,\n            2.6973e+00, -5.1094e+00],\n          ...,\n          [-4.7188e+00, -1.6104e+00, -2.9570e+00,  ...,  9.4609e+00,\n           -8.0391e+00,  8.8281e+00],\n          [-5.4297e+00, -1.9258e+00, -7.8467e-01,  ...,  9.8096e-01,\n            9.4629e-01,  1.3779e+00],\n          [-3.0605e+00, -6.9141e-01, -1.6465e+00,  ...,  6.7891e+00,\n           -5.5859e+00,  2.4219e+00]]],\n\n\n        [[[-8.5078e+00,  1.2953e+01, -1.6494e+00,  ...,  2.3164e+00,\n            1.3703e+01, -8.2344e+00],\n          [ 1.4463e+00,  9.0234e+00,  4.2539e+00,  ...,  2.8965e+00,\n           -3.7539e+00, -5.8633e+00],\n          [ 4.3516e+00,  7.8633e+00,  1.1875e+01,  ...,  1.6846e+00,\n            1.1656e+01, -8.8828e+00],\n          ...,\n          [-6.4805e+00,  3.8965e+00, -6.6553e-01,  ...,  1.2871e+00,\n           -1.3320e+00, -6.2305e+00],\n          [-6.0703e+00,  1.7842e+00,  6.3711e+00,  ...,  6.5137e-01,\n           -1.4551e+00, -7.5977e+00],\n          [-3.3809e+00, -5.4590e-01,  3.8477e+00,  ...,  3.3711e+00,\n           -3.4492e+00, -4.1367e+00]],\n\n         [[-2.1270e+00, -8.2188e+00,  3.0117e+00,  ..., -4.5742e+00,\n            1.8525e+00, -2.0820e+00],\n          [ 4.7539e+00, -2.8398e+00,  4.5664e+00,  ..., -1.8242e+00,\n            2.2773e+00, -2.4082e+00],\n          [-1.9326e+00,  3.0293e+00,  5.0586e+00,  ..., -2.7227e+00,\n            6.9336e+00,  3.8359e+00],\n          ...,\n          [-4.1328e+00,  4.9854e-01,  3.1113e+00,  ..., -5.8711e+00,\n           -1.2910e+00, -1.0732e+00],\n          [ 1.9805e+00,  2.4453e+00,  1.5303e+00,  ..., -4.7969e+00,\n           -3.6016e+00, -1.7188e+00],\n          [-2.3965e+00, -3.6367e+00,  6.4805e+00,  ..., -2.5762e+00,\n           -6.9375e+00, -6.7305e+00]],\n\n         [[-3.1934e+00,  4.3477e+00, -6.4014e-01,  ...,  2.7129e+00,\n            2.6250e+00, -3.0781e+00],\n          [-5.0859e+00,  3.0762e+00,  1.3086e+00,  ..., -2.0957e+00,\n           -1.2148e+00,  2.1602e+00],\n          [-6.9258e+00, -9.8877e-02, -1.8936e+00,  ...,  5.4727e+00,\n           -3.5527e+00, -6.6289e+00],\n          ...,\n          [ 4.4414e+00,  9.6094e-01, -2.1777e+00,  ...,  2.1562e+00,\n           -2.9238e+00, -2.0371e+00],\n          [ 3.6074e+00,  1.6289e+00,  7.3877e-01,  ...,  3.2090e+00,\n           -1.8838e+00,  4.4678e-01],\n          [ 3.9336e+00, -2.0227e-01,  5.6758e+00,  ...,  4.6680e+00,\n            2.1621e+00, -1.2280e-01]],\n\n         ...,\n\n         [[-3.5762e+00,  1.6055e+00, -8.4131e-01,  ..., -4.6133e+00,\n           -8.1875e+00, -2.0233e-02],\n          [-4.0508e+00,  6.7725e-01,  1.2695e+00,  ..., -3.0566e+00,\n           -3.6113e+00, -2.3457e+00],\n          [-4.8906e+00, -1.6270e+00, -4.0859e+00,  ..., -5.1992e+00,\n           -8.7422e+00, -5.0312e+00],\n          ...,\n          [ 1.2256e+00,  2.5970e-02,  6.1484e+00,  ..., -6.8672e+00,\n           -1.4404e+00, -6.4219e+00],\n          [ 1.3223e+00,  1.8057e+00,  6.6602e+00,  ..., -5.8867e+00,\n           -4.1367e+00, -7.7852e+00],\n          [ 2.6152e+00, -8.5831e-03,  7.2266e+00,  ..., -1.0875e+01,\n           -2.6406e+00, -8.6953e+00]],\n\n         [[ 2.8926e+00, -6.1484e+00, -4.8828e+00,  ...,  1.0925e-01,\n           -2.1797e+00,  7.8438e+00],\n          [-7.8369e-01, -3.1758e+00,  1.3105e+00,  ...,  6.2988e-01,\n           -6.4926e-03, -1.4150e+00],\n          [ 5.0430e+00, -6.4609e+00, -5.8125e+00,  ...,  2.7832e+00,\n           -8.6875e+00,  1.0117e+00],\n          ...,\n          [ 1.8057e+00,  7.3672e+00, -5.5234e+00,  ..., -8.8135e-01,\n           -4.2773e+00, -9.7656e-01],\n          [ 2.6309e+00,  6.2227e+00, -3.8516e+00,  ..., -2.3945e+00,\n           -4.5469e+00,  4.7305e+00],\n          [ 4.5898e+00,  2.9551e+00,  3.2056e-01,  ..., -1.5439e+00,\n           -1.6299e+00, -1.4609e+00]],\n\n         [[-4.0352e+00, -3.7559e+00, -5.8320e+00,  ..., -5.1562e-01,\n           -6.5332e-01, -8.2188e+00],\n          [-7.4658e-01, -2.4453e+00, -3.3809e+00,  ...,  7.5703e+00,\n            2.4297e+00, -2.2617e+00],\n          [-6.8633e+00, -4.0820e+00, -1.4336e+00,  ..., -5.1914e+00,\n           -2.3730e+00, -1.1562e+01],\n          ...,\n          [-3.0977e+00, -5.3242e+00,  1.3506e+00,  ...,  7.1758e+00,\n            2.3340e+00,  2.0105e-01],\n          [-4.2227e+00, -3.5039e+00, -1.8633e+00,  ...,  5.6094e+00,\n            2.4238e+00,  1.3643e+00],\n          [-4.9414e+00, -9.6094e-01, -2.8672e+00,  ...,  6.8633e+00,\n            9.5801e-01,  8.6865e-01]]],\n\n\n        [[[-2.1406e+00,  4.0977e+00,  1.1650e+00,  ...,  1.2959e+00,\n            9.8340e-01,  1.1904e+00],\n          [-2.2734e+00,  5.1250e+00,  3.5586e+00,  ...,  2.7988e+00,\n            1.0500e+01, -3.4219e+00],\n          [-6.1172e+00,  1.4000e+01,  5.3438e+00,  ...,  4.3711e+00,\n            1.5367e+01, -2.6152e+00],\n          ...,\n          [-5.3047e+00,  4.4414e+00,  9.8438e+00,  ...,  3.1445e+00,\n            1.0703e+00, -9.5156e+00],\n          [-1.0562e+01,  2.2637e+00,  3.5508e+00,  ..., -1.4219e+00,\n           -6.0664e+00, -8.0859e+00],\n          [-3.2891e+00, -2.1738e+00,  2.0664e+00,  ..., -7.3438e-01,\n           -2.9712e-01, -1.0539e+01]],\n\n         [[ 4.2896e-01, -5.9113e-02,  1.1504e+00,  ...,  3.6055e+00,\n           -4.2798e-01, -4.3398e+00],\n          [ 1.1487e-01, -9.9854e-02,  1.6943e+00,  ...,  7.4844e+00,\n            5.0039e+00,  7.8271e-01],\n          [-1.5215e+00, -9.5020e-01, -3.5508e+00,  ...,  9.8047e-01,\n            7.8633e+00,  5.8984e+00],\n          ...,\n          [-6.8828e+00,  2.3047e+00,  3.2109e+00,  ..., -5.7461e+00,\n           -6.1133e+00, -1.2930e+00],\n          [-2.4121e+00, -1.3096e+00, -1.6418e-01,  ..., -7.5625e+00,\n           -3.9648e+00, -7.1133e+00],\n          [-7.0039e+00,  1.0811e+00, -7.6484e+00,  ..., -3.8242e+00,\n           -2.1445e+00, -1.4697e+00]],\n\n         [[-1.3428e+00, -4.8877e-01, -2.3242e+00,  ...,  8.7549e-01,\n           -5.7983e-02, -8.4219e+00],\n          [-3.7061e-01,  3.9180e+00, -5.7539e+00,  ...,  7.0469e+00,\n           -4.9648e+00, -3.4590e+00],\n          [-8.3203e+00,  5.5312e+00, -2.5801e+00,  ...,  8.7031e+00,\n           -2.5410e+00, -6.5234e+00],\n          ...,\n          [ 6.0498e-01, -2.7305e+00,  4.2227e+00,  ...,  1.4111e+00,\n            4.0391e+00, -1.5186e+00],\n          [ 4.3125e+00, -8.7549e-01,  3.4219e+00,  ..., -2.2864e-01,\n           -1.6113e-01, -1.2979e+00],\n          [ 2.3164e+00, -2.6270e+00,  4.3281e+00,  ...,  4.7461e+00,\n            9.3506e-01,  1.2441e+00]],\n\n         ...,\n\n         [[-3.4043e+00,  3.0801e+00, -1.3023e+01,  ..., -7.8281e+00,\n           -5.7578e+00,  6.9922e+00],\n          [-2.8477e+00, -4.1133e+00, -1.1109e+01,  ..., -5.0977e+00,\n           -6.7656e+00,  3.7949e+00],\n          [-6.7539e+00, -3.5327e-01, -9.8438e+00,  ..., -4.6992e+00,\n           -4.8633e+00,  2.3516e+00],\n          ...,\n          [-3.4805e+00,  2.9062e+00,  2.5801e+00,  ..., -1.0008e+01,\n           -8.1299e-01, -1.0381e+00],\n          [ 5.8691e-01, -1.2773e+00,  8.8984e+00,  ..., -1.0734e+01,\n           -7.4141e+00, -1.6266e-02],\n          [ 9.4678e-01,  4.7852e+00,  4.0078e+00,  ..., -6.6328e+00,\n            2.8003e-01, -3.8887e+00]],\n\n         [[ 2.0156e+00, -9.8975e-01, -1.1895e+00,  ..., -3.9219e+00,\n           -2.2188e+00,  2.9043e+00],\n          [ 3.6152e+00, -2.3438e+00, -9.8450e-02,  ..., -3.9062e+00,\n           -2.8340e+00, -2.7422e+00],\n          [-9.8584e-01, -4.8555e+00, -1.9600e+00,  ...,  4.1162e-01,\n           -1.5002e-01,  6.4727e+00],\n          ...,\n          [ 2.3887e+00,  4.5078e+00, -4.0781e+00,  ...,  1.4316e+00,\n           -1.1148e+01, -1.6074e+00],\n          [-1.3789e+00,  9.8516e+00, -5.3008e+00,  ...,  2.4023e+00,\n           -4.7031e+00, -2.4453e+00],\n          [ 3.5278e-01,  3.5527e+00,  3.0762e-01,  ..., -4.5337e-01,\n           -7.5078e+00,  2.1387e+00]],\n\n         [[-2.9863e+00,  1.0205e+00, -3.5083e-01,  ...,  1.0305e+01,\n            1.0859e+00, -6.1084e-01],\n          [-2.2266e+00, -1.3926e+00, -4.0308e-01,  ...,  3.2168e+00,\n            6.8477e+00, -1.7719e+01],\n          [-4.1914e+00,  6.1279e-01, -2.9082e+00,  ..., -3.9316e+00,\n            3.0586e+00, -1.9406e+01],\n          ...,\n          [-3.9863e+00, -3.0234e+00, -2.1172e+00,  ...,  1.7500e+00,\n            5.8691e-01,  2.8594e+00],\n          [-4.7734e+00,  1.8916e+00, -3.5000e+00,  ...,  5.6562e+00,\n            3.4883e+00,  5.1680e+00],\n          [-4.8242e+00, -3.5039e+00, -3.0059e+00,  ...,  4.3320e+00,\n            2.1230e+00, -1.2128e-01]]],\n\n\n        ...,\n\n\n        [[[ 2.3398e+00,  6.2773e+00,  4.4023e+00,  ...,  4.8706e-01,\n            1.3008e+00, -1.7695e+00],\n          [-3.4746e+00,  1.2867e+01,  8.6172e+00,  ..., -3.5625e+00,\n            7.4766e+00, -9.6406e+00],\n          [-5.6592e-01,  4.2305e+00,  8.1484e+00,  ..., -3.4355e+00,\n           -3.5117e+00, -4.2148e+00],\n          ...,\n          [-3.9512e+00,  5.5634e-02,  5.5156e+00,  ..., -1.2969e+00,\n            7.3975e-01, -6.4609e+00],\n          [-1.2969e+00, -2.4375e+00,  3.2383e+00,  ...,  1.0869e+00,\n            5.7227e+00, -7.5898e+00],\n          [ 1.9248e+00, -4.8730e-01,  4.9902e-01,  ...,  1.3926e+00,\n           -9.6172e+00,  4.8281e+00]],\n\n         [[ 2.8867e+00, -2.0078e+00, -3.7915e-01,  ..., -3.9902e+00,\n           -1.4043e+00, -2.5352e+00],\n          [-5.3164e+00, -3.5078e+00, -1.7285e-01,  ..., -5.6055e+00,\n            4.7852e+00,  2.2559e+00],\n          [-2.2637e+00, -1.5615e+00,  5.0117e+00,  ...,  6.3232e-01,\n            4.8828e+00,  3.6699e+00],\n          ...,\n          [-1.8535e+00,  4.4312e-01, -3.5605e+00,  ..., -3.8984e+00,\n           -9.1875e+00, -4.5703e+00],\n          [-3.8848e+00,  3.0410e+00, -2.9077e-01,  ..., -1.4463e+00,\n           -7.7266e+00, -2.2773e+00],\n          [-2.3398e+00, -2.8672e+00,  1.0712e-01,  ...,  1.5283e+00,\n           -4.1797e+00, -1.8154e+00]],\n\n         [[-1.2354e+00,  4.3477e+00,  1.4424e+00,  ...,  4.6523e+00,\n           -9.9951e-01, -3.3574e+00],\n          [-2.6035e+00,  4.7583e-01, -1.0068e+00,  ...,  3.5508e+00,\n           -3.2598e+00, -3.9258e+00],\n          [ 4.2188e+00,  6.9688e+00, -2.7871e+00,  ..., -6.2744e-01,\n           -1.6807e+00, -2.8398e+00],\n          ...,\n          [ 3.2891e+00,  2.4683e-01,  8.3750e+00,  ...,  5.3203e+00,\n           -3.5625e+00, -6.0508e+00],\n          [ 9.7734e+00,  2.0859e+00,  1.8203e+00,  ...,  4.8047e+00,\n           -1.9072e+00, -3.2461e+00],\n          [-3.4785e+00, -5.5371e-01,  2.8589e-01,  ...,  1.4319e-01,\n           -7.4121e-01,  9.6328e+00]],\n\n         ...,\n\n         [[-5.1367e+00, -3.0938e+00, -6.2539e+00,  ..., -1.0928e+00,\n           -5.2891e+00, -3.2886e-01],\n          [ 9.4385e-01,  1.1055e+00,  8.4521e-01,  ..., -2.0098e+00,\n           -3.4805e+00, -5.3662e-01],\n          [ 3.0746e-02,  1.4648e+00, -2.6699e+00,  ...,  2.0352e+00,\n           -2.6895e+00,  1.0830e+00],\n          ...,\n          [ 7.5439e-01,  7.1133e+00,  5.6133e+00,  ..., -9.4297e+00,\n           -6.2461e+00, -3.9453e+00],\n          [ 5.5391e+00,  3.5918e+00,  6.2134e-02,  ..., -8.5703e+00,\n           -3.7539e+00, -4.2969e+00],\n          [-3.7866e-01, -1.3611e-01,  2.6348e+00,  ...,  3.8047e+00,\n            3.7231e-01,  3.4043e+00]],\n\n         [[-1.7510e+00, -1.5703e+00,  1.4219e+00,  ..., -2.5332e+00,\n           -5.3398e+00, -3.4180e+00],\n          [-2.6221e-01,  1.7129e+00, -1.5117e+00,  ..., -2.9648e+00,\n           -8.5449e-01,  2.5664e+00],\n          [-2.7100e-02,  2.0645e+00,  3.9316e+00,  ..., -4.8164e+00,\n           -1.9648e+00,  5.8906e+00],\n          ...,\n          [ 4.2578e+00,  2.1855e+00, -4.6602e+00,  ...,  1.2646e+00,\n           -8.8047e+00,  2.5840e+00],\n          [ 5.8750e+00, -1.3672e-01,  2.7441e+00,  ...,  5.6787e-01,\n           -2.6289e+00,  1.0166e+00],\n          [ 8.0713e-01,  2.6562e+00, -2.1230e+00,  ..., -7.8369e-01,\n            3.7441e+00,  8.8184e-01]],\n\n         [[-3.5859e+00, -1.1387e+00,  4.4287e-01,  ...,  1.0148e+01,\n           -6.1523e-01,  8.0391e+00],\n          [-5.2266e+00, -5.9453e+00,  3.1152e+00,  ..., -3.8594e+00,\n            3.7871e+00, -1.0031e+01],\n          [-7.9219e+00, -1.5576e+00,  1.4188e+01,  ..., -4.8242e+00,\n            3.6875e+00,  1.9199e+00],\n          ...,\n          [-9.6484e+00, -2.1765e-01, -2.1716e-01,  ...,  5.1836e+00,\n            5.3984e+00,  2.5762e+00],\n          [-4.1836e+00, -4.3516e+00, -1.7832e+00,  ...,  2.9180e+00,\n           -6.0352e-01,  2.6973e+00],\n          [-1.1005e-01,  1.2324e+00, -5.1484e+00,  ...,  4.1250e+00,\n           -1.7295e+00,  9.6953e+00]]],\n\n\n        [[[-9.9170e-01,  4.7383e+00, -9.5459e-01,  ...,  5.5430e+00,\n            6.7617e+00,  3.3633e+00],\n          [ 3.3447e-01,  1.2688e+01,  1.0125e+01,  ..., -3.1094e+00,\n            1.0930e+01,  2.2876e-01],\n          [ 4.9258e+00,  7.4648e+00,  9.1016e+00,  ...,  5.9277e-01,\n            3.0879e+00,  6.5430e+00],\n          ...,\n          [-1.2988e+00,  3.6035e+00,  5.1914e+00,  ..., -2.0918e+00,\n            1.8076e+00, -3.6328e+00],\n          [-2.0430e+00, -9.4043e-01,  4.0430e+00,  ...,  2.0996e+00,\n            3.6621e-01, -4.6328e+00],\n          [-1.4668e+00, -2.7979e-01,  3.9160e+00,  ...,  7.1777e-01,\n           -2.5723e+00, -3.3750e+00]],\n\n         [[-3.6523e+00, -4.9102e+00,  4.2266e+00,  ...,  1.3281e-01,\n            2.3770e+00, -7.0391e+00],\n          [-3.8516e+00, -2.3301e+00,  3.2617e+00,  ..., -4.5361e-01,\n            4.5469e+00,  4.0000e+00],\n          [ 4.2930e+00,  4.4297e+00,  1.5732e+00,  ...,  5.9766e-01,\n           -5.4248e-01,  8.6182e-02],\n          ...,\n          [-3.4766e+00,  1.7441e+00, -2.6709e-01,  ...,  1.8301e+00,\n           -3.6758e+00, -3.3105e+00],\n          [-3.1426e+00, -1.3281e-01,  1.5615e+00,  ..., -1.1163e-01,\n           -1.2822e+00,  2.0422e-01],\n          [-5.8906e+00,  1.4316e+00,  2.3516e+00,  ..., -2.0918e+00,\n           -9.2578e+00, -5.1758e+00]],\n\n         [[ 4.3750e+00,  4.7734e+00,  2.9736e-01,  ..., -4.7583e-01,\n            5.2100e-01, -3.0469e+00],\n          [-4.4961e+00, -3.8574e+00, -1.6768e+00,  ...,  2.9844e+00,\n            1.6260e+00, -9.3828e+00],\n          [-2.9824e+00,  3.4707e+00,  1.1475e+00,  ...,  1.5107e+00,\n            9.6875e-01, -5.0000e+00],\n          ...,\n          [ 6.1680e+00, -3.8281e+00,  2.2871e+00,  ...,  3.0352e+00,\n           -1.1025e+00,  7.3669e-02],\n          [ 1.6621e+00, -6.2773e+00,  3.6699e+00,  ...,  4.5156e+00,\n           -1.6670e+00, -1.3398e+00],\n          [-8.1482e-02,  1.6855e+00,  6.6953e+00,  ...,  3.4180e+00,\n           -1.6924e+00, -1.1484e+00]],\n\n         ...,\n\n         [[-3.2168e+00,  2.5156e+00, -8.1328e+00,  ..., -5.1328e+00,\n           -2.7305e+00,  7.4141e+00],\n          [ 3.4515e-02,  4.8906e+00, -5.3438e+00,  ..., -7.5430e+00,\n            4.5349e-02,  2.1250e+00],\n          [ 3.1953e+00,  5.5234e+00, -1.0844e+01,  ..., -7.2305e+00,\n           -1.8604e+00,  2.0430e+00],\n          ...,\n          [-8.0664e-01,  4.4678e-01,  3.9512e+00,  ..., -1.1289e+01,\n           -4.2328e-02, -4.5703e+00],\n          [-2.0117e+00,  1.0283e+00,  1.4482e+00,  ..., -1.0562e+01,\n            7.1240e-01, -2.5391e+00],\n          [ 2.9712e-01,  3.3750e+00,  5.1367e+00,  ..., -1.0242e+01,\n           -1.3049e-01, -7.4023e-01]],\n\n         [[ 1.9004e+00, -7.6123e-01, -1.5771e+00,  ...,  7.1045e-01,\n           -3.3281e+00,  2.3750e+00],\n          [ 6.3623e-01, -3.1719e+00, -3.0508e+00,  ...,  1.0723e+00,\n           -5.8633e+00,  4.2188e+00],\n          [ 3.3646e-03, -3.1191e+00, -2.6562e+00,  ..., -4.1406e+00,\n           -7.9648e+00,  1.1855e+00],\n          ...,\n          [ 3.3750e+00,  3.7246e+00, -1.4307e+00,  ..., -1.5405e-01,\n           -6.0273e+00, -8.6670e-01],\n          [ 4.8242e+00, -1.9250e-01,  1.3008e+00,  ...,  3.8306e-01,\n           -1.0781e+01,  4.6992e+00],\n          [ 9.6973e-01,  3.8613e+00,  3.1909e-01,  ..., -3.8361e-02,\n           -5.4531e+00,  1.9414e+00]],\n\n         [[ 1.8027e+00, -5.0352e+00, -4.6094e+00,  ...,  4.5430e+00,\n           -1.2500e+00, -7.0801e-01],\n          [-5.9961e-01, -3.8711e+00,  1.1621e+00,  ..., -8.1543e-01,\n            2.6191e+00, -8.3594e+00],\n          [-6.3818e-01,  5.0781e-01,  1.9556e-01,  ...,  3.5234e+00,\n           -1.2314e+00,  7.9346e-01],\n          ...,\n          [-4.7812e+00, -3.0234e+00, -3.0020e+00,  ...,  2.5059e+00,\n            1.8921e-01,  4.0698e-01],\n          [-5.7500e+00, -2.2754e+00,  1.7246e+00,  ...,  1.8418e+00,\n            3.6475e-01,  1.1816e+00],\n          [-5.0859e+00, -3.2305e+00,  2.1797e+00,  ...,  8.2109e+00,\n            1.4092e+00,  3.3281e+00]]],\n\n\n        [[[-1.1227e+01,  1.5023e+01,  3.5273e+00,  ...,  9.4434e-01,\n            8.9844e+00, -6.5430e+00],\n          [-4.5898e+00,  8.2578e+00, -1.5146e+00,  ...,  1.1621e+00,\n            1.3016e+01,  6.1484e+00],\n          [ 1.3623e+00,  1.2719e+01,  4.6680e+00,  ..., -1.8730e+00,\n            9.4141e+00,  6.9180e+00],\n          ...,\n          [-1.2705e+00,  2.6621e+00,  3.0195e+00,  ...,  2.6992e+00,\n            3.5801e+00, -7.0000e+00],\n          [-2.9883e+00,  1.1113e+00,  3.4492e+00,  ...,  3.3555e+00,\n            5.5938e+00, -4.1328e+00],\n          [-3.0625e+00, -1.5186e+00,  5.9766e+00,  ..., -4.4629e-01,\n           -3.3379e+00, -5.7109e+00]],\n\n         [[-3.8965e-01, -1.2031e+00,  3.3262e+00,  ..., -3.0469e+00,\n           -3.7476e-01, -6.0596e-01],\n          [-3.1973e+00, -1.3486e+00, -2.8359e+00,  ..., -9.3799e-01,\n            7.7188e+00, -5.7969e+00],\n          [-6.4492e+00, -4.0820e-01,  6.2930e+00,  ...,  6.0117e+00,\n            5.6602e+00,  2.7461e+00],\n          ...,\n          [-6.4258e+00,  1.0547e+00, -1.8848e+00,  ..., -3.7969e+00,\n           -9.8750e+00, -7.2461e-01],\n          [-4.3828e+00,  2.9121e+00, -1.9031e-01,  ..., -2.6733e-01,\n           -5.0156e+00, -3.7891e+00],\n          [-1.7998e+00,  9.0576e-01, -2.2305e+00,  ..., -3.8438e+00,\n           -1.1297e+01,  3.3203e-01]],\n\n         [[ 2.6953e-01,  3.6348e+00,  2.4629e+00,  ...,  1.8652e+00,\n            3.4785e+00, -4.5625e+00],\n          [ 2.8945e+00,  4.5430e+00,  6.7734e+00,  ...,  1.8545e+00,\n            6.6914e+00, -8.2969e+00],\n          [-1.4336e+00,  4.7461e+00,  1.8281e+00,  ...,  2.2598e+00,\n            8.7266e+00, -7.8203e+00],\n          ...,\n          [-5.8643e-01, -1.0645e+00,  3.4707e+00,  ...,  3.0938e+00,\n           -3.6387e+00, -6.1055e+00],\n          [ 2.2637e+00,  4.6631e-01,  1.3174e+00,  ...,  3.8027e+00,\n            3.7930e+00, -3.8633e+00],\n          [ 1.2021e+00,  1.1279e-01,  5.3906e+00,  ...,  7.4922e+00,\n            2.4512e-01, -5.0742e+00]],\n\n         ...,\n\n         [[ 3.3535e+00,  1.1407e-01, -5.9326e-01,  ..., -7.5938e+00,\n           -6.8477e+00,  4.6719e+00],\n          [ 4.1055e+00,  4.5000e+00, -1.2523e+01,  ..., -1.4141e+00,\n           -8.0938e+00,  7.1680e+00],\n          [ 3.4082e+00,  5.1523e+00, -8.5312e+00,  ...,  2.6035e+00,\n           -2.2388e-01,  1.7412e+00],\n          ...,\n          [ 1.1602e+00,  1.6312e-02,  6.5625e+00,  ..., -7.1758e+00,\n           -5.6602e+00, -3.0547e+00],\n          [ 1.5381e-01, -7.8809e-01,  2.7891e+00,  ..., -5.2305e+00,\n           -1.8965e+00,  4.8535e-01],\n          [ 2.0273e+00,  2.7734e+00,  5.8633e+00,  ..., -9.1953e+00,\n           -3.2539e+00, -5.7773e+00]],\n\n         [[ 4.9727e+00, -2.2090e+00, -8.4375e+00,  ...,  2.4590e+00,\n            2.3926e+00,  4.1953e+00],\n          [ 8.9453e+00, -5.3906e+00, -2.4941e+00,  ...,  7.2461e+00,\n           -3.8477e+00,  2.5508e+00],\n          [ 1.3340e+00, -1.5479e+00, -1.4270e-01,  ...,  6.6484e+00,\n           -1.9697e+00,  5.7344e+00],\n          ...,\n          [ 2.7168e+00,  5.3438e+00, -7.9961e+00,  ...,  3.3906e+00,\n           -6.4258e+00,  5.0537e-01],\n          [ 2.6797e+00,  3.2520e+00, -5.1367e+00,  ...,  6.6699e-01,\n           -2.6797e+00, -2.9238e+00],\n          [ 1.2734e+00,  4.0820e+00, -7.9609e+00,  ..., -1.3320e+00,\n           -7.3867e+00, -5.3906e+00]],\n\n         [[-3.7207e+00, -1.0049e+00, -7.6641e+00,  ..., -1.6367e+00,\n            3.3301e+00, -1.2273e+01],\n          [ 9.1748e-01,  1.6055e+00, -2.5273e+00,  ..., -3.0811e-01,\n            5.8594e+00, -1.1133e+01],\n          [-7.8320e-01,  4.2505e-01,  6.6680e+00,  ...,  3.0820e+00,\n           -1.6113e-01, -1.3047e+01],\n          ...,\n          [-6.4297e+00, -4.8125e+00, -4.9375e+00,  ...,  6.8594e+00,\n            8.2178e-01, -2.3086e+00],\n          [-6.4375e+00, -6.6953e+00, -3.2754e+00,  ...,  1.0234e+01,\n           -6.5469e+00, -3.2188e+00],\n          [-6.5625e+00, -1.5459e+00,  1.3562e-01,  ...,  5.0000e+00,\n            2.0527e+00,  5.7188e+00]]]], device='cuda:0', dtype=torch.float16,\n       grad_fn=<TransposeBackward0>), tensor([[[[ 6.5000e+00, -5.2656e+00,  1.2055e+01,  ..., -8.4688e+00,\n            8.0078e+00, -2.5391e+00],\n          [ 6.4392e-02,  4.4922e+00, -4.1680e+00,  ...,  4.9756e-01,\n            8.7031e+00,  1.3461e+01],\n          [ 7.8857e-01, -4.7383e+00,  1.2100e+00,  ...,  3.6484e+00,\n           -5.5156e+00,  1.4072e+00],\n          ...,\n          [ 6.3711e+00,  6.9766e+00, -2.1992e+00,  ..., -1.0596e+00,\n           -1.1543e+00, -2.2715e+00],\n          [ 1.1914e+00,  2.6855e+00, -5.8945e+00,  ...,  2.9238e+00,\n           -1.0977e+01, -3.8223e+00],\n          [ 1.6281e+01,  4.3398e+00,  4.2227e+00,  ..., -1.1133e+00,\n           -1.0400e+00,  1.1309e+00]],\n\n         [[-1.8330e+00,  3.4297e+00,  3.8926e+00,  ..., -3.4863e+00,\n           -4.7656e+00,  2.0812e+01],\n          [-1.4852e+01,  1.8594e+00, -7.8047e+00,  ...,  7.7656e+00,\n            4.5039e+00,  1.9844e+00],\n          [-2.8809e+00,  1.8320e+00, -1.2805e+01,  ...,  1.0047e+01,\n            6.9766e+00,  5.9453e+00],\n          ...,\n          [-7.8125e+00,  5.0117e+00, -5.4199e-01,  ...,  5.0469e+00,\n            6.3320e+00,  8.2656e+00],\n          [ 1.5430e+00,  2.3555e+00, -2.0410e+00,  ...,  1.1555e+01,\n            9.6172e+00, -3.3340e+00],\n          [-9.6562e+00, -2.4170e-01, -3.6172e+00,  ...,  1.3359e+01,\n           -1.5244e+00,  7.7656e+00]],\n\n         [[-8.5703e+00,  2.1504e+00, -1.3984e+00,  ...,  8.7031e+00,\n            1.6133e+00,  8.7656e+00],\n          [-3.6426e+00, -5.2578e+00,  1.2012e+00,  ...,  5.6406e+00,\n            1.8359e+00, -6.1680e+00],\n          [ 9.1875e+00, -3.5273e+00, -6.6367e+00,  ..., -1.3250e+01,\n           -3.6562e+00,  3.3887e-01],\n          ...,\n          [-6.4492e+00,  1.2393e+00,  7.4844e+00,  ..., -9.6172e+00,\n            1.0688e+01,  4.7266e+00],\n          [ 3.5137e+00,  1.4053e+00,  1.1031e+01,  ..., -1.4000e+01,\n           -1.1035e+00,  6.9102e+00],\n          [-1.1180e+01,  1.4094e+01,  7.5625e+00,  ..., -7.3008e+00,\n            3.2266e+00, -5.6016e+00]],\n\n         ...,\n\n         [[ 1.5227e+01, -1.2773e+00, -5.0586e-01,  ..., -9.6172e+00,\n            5.1016e+00, -2.3572e-01],\n          [ 2.5625e+00, -3.6875e+00,  1.0008e+01,  ...,  4.5508e+00,\n            8.0391e+00, -3.5859e+00],\n          [-9.8438e+00, -1.0578e+01, -1.1133e+01,  ...,  1.3742e+01,\n           -1.4453e+00, -2.5664e+00],\n          ...,\n          [ 5.3438e+00,  8.3750e+00,  1.3428e+00,  ..., -9.7812e+00,\n            5.3418e-01, -9.8672e+00],\n          [-5.2148e+00, -3.4985e-01,  1.1547e+01,  ..., -6.4062e+00,\n            8.1172e+00, -7.0039e+00],\n          [ 6.4922e+00,  7.3730e-01,  6.2305e+00,  ..., -6.4688e+00,\n           -9.0674e-01, -7.2070e-01]],\n\n         [[ 3.0703e+00,  9.9375e+00, -1.9014e+00,  ...,  1.0656e+01,\n            1.1816e+00,  1.6777e+00],\n          [ 3.0332e+00,  1.1539e+01, -9.0234e-01,  ...,  4.5781e+00,\n            5.9844e+00,  4.4375e+00],\n          [ 4.5996e-01, -1.8652e+00, -1.6260e+00,  ...,  3.5449e-01,\n            3.1895e+00, -2.1426e+00],\n          ...,\n          [-6.6328e+00,  9.8281e+00,  5.7578e+00,  ..., -5.9805e+00,\n           -6.6797e+00, -6.6040e-02],\n          [ 1.7422e+00,  8.6953e+00,  8.0234e+00,  ..., -6.5117e+00,\n           -1.2598e+00,  4.3242e+00],\n          [-3.3359e+00,  8.4453e+00,  5.7266e+00,  ..., -1.1891e+01,\n           -3.5840e+00,  9.1406e+00]],\n\n         [[ 1.8500e+01, -5.7422e+00,  4.4141e+00,  ..., -6.6602e+00,\n            8.0703e+00, -8.6406e+00],\n          [ 5.4922e+00,  9.0391e+00,  1.7438e+01,  ..., -5.2500e+00,\n           -8.3984e+00, -5.5117e+00],\n          [ 1.3664e+01,  2.0156e+00, -4.7266e+00,  ...,  5.1758e+00,\n            3.8672e+00, -1.0781e+00],\n          ...,\n          [ 5.9727e+00,  4.6973e-01,  1.2086e+01,  ..., -8.1484e+00,\n            4.4766e+00,  4.3398e+00],\n          [-2.5801e+00,  1.1969e+01,  1.2875e+01,  ..., -6.4883e+00,\n            8.6094e+00,  5.6396e-01],\n          [-8.8379e-01, -1.4038e-01,  1.0336e+01,  ..., -2.0215e+00,\n            1.1438e+01, -1.8359e+00]]],\n\n\n        [[[-3.3789e+00,  5.1992e+00,  5.8594e+00,  ..., -9.4844e+00,\n            1.9932e+00,  5.7861e-01],\n          [-7.7695e+00,  1.1414e+01, -2.4043e+00,  ..., -1.1631e+00,\n           -9.9297e+00, -1.0336e+01],\n          [-3.3457e+00, -4.5703e+00, -5.7031e+00,  ..., -5.3398e+00,\n           -1.3574e-01,  4.4727e+00],\n          ...,\n          [-4.6484e+00,  5.7695e+00,  1.0693e+00,  ..., -6.0117e+00,\n           -1.2492e+01, -2.4531e+00],\n          [-1.0727e+01,  9.8438e+00, -2.5864e-02,  ..., -1.0859e+00,\n           -5.8203e+00, -9.0391e+00],\n          [ 3.1152e+00,  5.6680e+00,  4.7891e+00,  ..., -8.5703e+00,\n           -2.9863e+00, -3.2734e+00]],\n\n         [[ 1.1078e+01, -5.7471e-01, -1.3445e+01,  ..., -9.4531e+00,\n           -8.8047e+00, -1.5320e-01],\n          [ 1.4053e+00, -9.6797e+00,  2.3926e+00,  ...,  1.8154e+00,\n           -1.1711e+01,  1.3184e+00],\n          [ 1.4570e+00,  6.0742e+00, -1.2090e+00,  ...,  6.3320e+00,\n           -9.6484e+00,  1.1492e+01],\n          ...,\n          [-5.4291e-02,  1.1219e+01, -1.2477e+01,  ..., -1.6589e-01,\n            8.2129e-01,  9.3438e+00],\n          [-5.8203e+00,  8.5156e+00, -1.2969e+01,  ...,  5.0234e+00,\n           -2.0137e+00, -1.1504e+00],\n          [-2.9844e+00,  2.0430e+00, -2.4004e+00,  ...,  2.5020e+00,\n            1.2725e+00,  8.7578e+00]],\n\n         [[-3.5859e+00,  9.4609e+00,  8.9609e+00,  ...,  5.5508e+00,\n            6.8203e+00,  3.5449e+00],\n          [ 4.3555e+00, -2.7031e+00, -8.6953e+00,  ..., -5.6406e+00,\n           -9.1562e+00,  1.8766e+01],\n          [-6.1797e+00,  2.0703e+00,  3.5879e+00,  ...,  5.4531e+00,\n           -1.3770e+00, -2.6211e+00],\n          ...,\n          [ 5.2295e-01, -4.0820e+00,  7.0820e+00,  ..., -1.2633e+01,\n           -4.9375e+00,  4.9688e+00],\n          [-3.7793e+00,  2.3254e-01,  1.5773e+01,  ..., -1.4551e+00,\n           -3.3613e+00,  1.3535e+00],\n          [-6.6953e+00,  5.7891e+00,  1.4203e+01,  ..., -7.9023e+00,\n            6.3789e+00,  8.7109e-01]],\n\n         ...,\n\n         [[ 6.6133e+00, -6.3359e+00,  1.7578e+01,  ..., -1.2133e+01,\n            9.9411e-03,  5.6133e+00],\n          [ 2.7988e+00,  7.4922e+00,  5.8438e+00,  ..., -1.8958e-01,\n           -2.4492e+00,  1.9062e+01],\n          [-9.6250e+00,  3.8281e+00,  3.6348e+00,  ...,  2.5156e+00,\n           -1.0719e+01,  4.0918e-01],\n          ...,\n          [ 6.4844e+00,  1.1016e+01,  7.3711e+00,  ..., -7.3828e+00,\n            3.2363e+00, -1.8535e+00],\n          [-5.3516e-01, -2.5742e+00,  5.4062e+00,  ..., -6.0000e+00,\n            3.5859e+00, -1.4820e+01],\n          [ 9.0312e+00, -8.6279e-01,  9.8984e+00,  ..., -2.5273e+00,\n            1.1416e+00, -1.3758e+01]],\n\n         [[ 4.1016e+00,  9.0234e-01,  4.6875e+00,  ...,  5.8242e+00,\n            9.4531e+00, -6.1055e+00],\n          [-1.0727e+01,  2.9980e+00,  1.9141e+00,  ...,  1.0523e+01,\n            5.7695e+00, -6.2695e-01],\n          [-1.0625e+00, -2.5098e+00, -1.9648e+00,  ...,  1.1055e+01,\n            8.3057e-01,  2.1562e+00],\n          ...,\n          [ 7.4805e-01,  1.4320e+01, -3.1787e-01,  ..., -3.1777e+00,\n           -4.2148e+00,  1.0156e+01],\n          [-4.8398e+00,  1.6734e+01,  3.2988e+00,  ..., -2.6406e+00,\n           -1.8506e+00,  2.1172e+00],\n          [-7.5742e+00,  6.3516e+00,  6.7070e+00,  ..., -6.3398e+00,\n            1.9287e+00,  1.3914e+01]],\n\n         [[ 8.4766e+00, -4.6172e+00, -7.2539e+00,  ...,  9.1641e+00,\n            1.0688e+01, -2.4961e+00],\n          [ 1.9482e+00, -3.3828e+00,  1.3195e+01,  ...,  8.3672e+00,\n            9.3125e+00, -6.7676e-01],\n          [ 6.1406e+00,  1.2295e+00, -3.1016e+00,  ..., -4.6562e+00,\n           -8.7734e+00, -4.8672e+00],\n          ...,\n          [ 7.7393e-01,  8.2617e-01,  1.7637e+00,  ..., -5.5078e+00,\n            8.7031e+00,  2.4980e+00],\n          [ 3.0703e+00,  7.5781e+00,  3.3711e+00,  ..., -6.9648e+00,\n           -1.0547e+00,  3.3105e+00],\n          [ 1.2051e+00, -7.0361e-01,  7.7227e+00,  ..., -5.8125e+00,\n            8.3125e+00,  3.7573e-01]]],\n\n\n        [[[-4.8711e+00, -3.9980e+00,  3.3809e+00,  ...,  5.5469e+00,\n            7.4062e+00, -3.2090e+00],\n          [-1.4473e+00, -1.8184e+00,  1.5557e+00,  ...,  9.0149e-02,\n            3.4375e+00, -7.7500e+00],\n          [ 1.4111e+00, -1.2734e+00, -9.1094e+00,  ..., -1.0469e+00,\n            9.8750e+00,  2.5781e+00],\n          ...,\n          [ 7.7031e+00,  1.8662e+00, -6.3594e+00,  ..., -4.1914e+00,\n           -2.2227e+00,  3.0508e+00],\n          [-4.5781e+00,  1.5508e+00, -4.7363e-01,  ..., -1.0605e+00,\n            6.8594e+00, -5.8047e+00],\n          [ 4.1758e+00, -8.5547e+00,  3.0645e+00,  ..., -4.5117e+00,\n           -1.4375e+00,  4.0703e+00]],\n\n         [[-7.8984e+00,  7.9688e+00,  9.5234e+00,  ...,  1.0449e+00,\n           -1.5227e+01, -6.9219e+00],\n          [ 2.7656e+00,  9.8438e+00, -5.9531e+00,  ..., -6.7344e+00,\n           -8.2031e+00,  2.8457e+00],\n          [-3.7363e+00,  9.8281e+00,  1.6641e+00,  ..., -9.5000e+00,\n           -1.1055e+01,  2.4531e+00],\n          ...,\n          [-3.2812e+00, -6.8594e+00, -5.8398e+00,  ...,  1.7219e+01,\n            8.7969e+00,  4.9961e+00],\n          [-9.9219e-01, -1.9414e+00,  3.8613e+00,  ...,  4.1602e+00,\n           -2.4688e+00, -1.2793e-01],\n          [-1.8613e+00, -7.5430e+00, -1.4814e+00,  ...,  2.8477e+00,\n            8.5703e+00, -2.3848e+00]],\n\n         [[-1.4547e+01,  1.5693e+00, -1.7203e+01,  ..., -7.3291e-01,\n           -1.7266e+01,  9.9688e+00],\n          [ 8.1738e-01,  1.0945e+01, -7.1172e+00,  ...,  1.5176e+00,\n           -4.9292e-01,  1.6885e+00],\n          [ 1.9365e+00,  2.7207e+00, -8.5625e+00,  ...,  9.7500e+00,\n           -1.7031e+01,  7.7695e+00],\n          ...,\n          [-5.3867e+00,  4.4258e+00,  1.3555e+01,  ..., -9.0078e+00,\n            8.6094e+00, -1.3389e+00],\n          [-1.9453e+00,  1.6484e+00,  1.1641e+01,  ..., -4.5547e+00,\n           -2.9712e-01, -4.7383e+00],\n          [-7.8594e+00, -2.0762e+00,  9.8203e+00,  ..., -1.9053e+00,\n            8.2734e+00, -4.7109e+00]],\n\n         ...,\n\n         [[ 4.3125e+00,  1.7750e+01, -9.5801e-01,  ..., -6.0312e+00,\n            8.3359e+00,  8.0312e+00],\n          [ 1.1047e+01,  1.6172e+01, -4.1172e+00,  ...,  1.3672e+00,\n            4.3086e+00,  8.8750e+00],\n          [ 4.1055e+00,  1.3445e+01, -9.6328e+00,  ..., -5.6738e-01,\n           -7.8662e-01,  1.1789e+01],\n          ...,\n          [ 6.0303e-01,  4.4961e+00,  8.6172e+00,  ..., -1.7432e+00,\n           -2.9590e+00,  2.7285e+00],\n          [ 5.5156e+00, -1.1086e+01,  7.4805e+00,  ...,  1.2047e+01,\n            4.8398e+00, -8.2969e+00],\n          [ 3.8477e+00,  1.5527e+00,  1.0555e+01,  ..., -1.1938e+01,\n            2.3535e+00, -3.1958e-01]],\n\n         [[-3.5977e+00,  4.6914e+00, -2.2559e+00,  ..., -2.5684e+00,\n            3.6387e+00,  1.0164e+01],\n          [-4.4727e+00,  6.0820e+00, -5.0732e-01,  ..., -2.6855e+00,\n            3.1128e-01, -3.6582e+00],\n          [ 4.1680e+00, -1.0840e+00, -4.6836e+00,  ..., -7.7656e+00,\n            3.5664e+00,  1.5955e-01],\n          ...,\n          [-3.6250e+00,  9.8750e+00,  6.9453e+00,  ..., -7.1328e+00,\n           -5.0342e-01,  2.2109e+00],\n          [-1.6738e+00,  1.1859e+01,  1.1805e+01,  ..., -7.6211e+00,\n            3.4375e+00,  3.0640e-01],\n          [-8.2397e-02,  6.6484e+00,  5.4102e+00,  ..., -9.5469e+00,\n           -6.6758e+00,  1.4648e+01]],\n\n         [[-1.7047e+01, -2.8730e+00,  8.8203e+00,  ...,  1.0273e+01,\n            2.6348e+00, -3.2852e+00],\n          [-5.2734e-01, -4.3359e+00, -1.0641e+01,  ..., -1.1309e+00,\n           -2.3848e+00, -6.7285e-01],\n          [ 3.5625e+00, -2.9858e-01, -3.9102e+00,  ..., -1.7236e+00,\n           -1.2703e+01,  2.9004e-01],\n          ...,\n          [ 5.4258e+00,  5.6875e+00,  9.2773e-02,  ..., -5.9258e+00,\n            5.0625e+00,  1.6436e+00],\n          [ 8.5938e+00,  9.0078e+00,  9.2578e-01,  ...,  2.4238e+00,\n            6.0273e+00,  5.9922e+00],\n          [-7.9414e+00,  1.3500e+01,  7.7188e+00,  ..., -7.0195e+00,\n            1.1469e+01,  3.4332e-02]]],\n\n\n        ...,\n\n\n        [[[ 6.9946e-02,  1.0875e+01,  4.9688e+00,  ..., -1.6699e+00,\n            2.4453e+00,  1.5859e+00],\n          [-4.0508e+00, -4.7266e+00, -8.8916e-01,  ..., -6.4531e+00,\n           -2.5215e+00,  4.8398e+00],\n          [ 1.2258e+01,  4.3320e+00,  1.1938e+01,  ..., -7.8594e+00,\n           -3.1191e+00,  6.8047e+00],\n          ...,\n          [ 3.1641e-01,  4.2227e+00, -3.8125e+00,  ..., -1.0234e+01,\n           -9.4043e-01,  5.8047e+00],\n          [ 9.3984e+00, -4.0312e+00,  7.7383e+00,  ..., -1.3525e+00,\n            1.7393e+00,  9.3359e+00],\n          [ 8.1592e-01,  6.0596e-01,  1.2197e+00,  ...,  6.4795e-01,\n            3.0786e-01,  2.4673e-02]],\n\n         [[ 1.2930e+00,  6.2383e+00,  2.0254e+00,  ...,  5.1758e+00,\n            5.6289e+00,  6.4531e+00],\n          [ 1.7051e+00,  5.0156e+00, -1.9072e+00,  ..., -3.0820e+00,\n           -6.4209e-01,  2.3320e+00],\n          [-1.4219e+01,  2.7070e+00, -4.9023e+00,  ..., -3.0840e+00,\n            7.4297e+00,  1.8484e+01],\n          ...,\n          [-1.3057e+00,  5.3086e+00, -2.3906e+00,  ...,  5.4980e-01,\n           -3.5254e+00,  6.4102e+00],\n          [ 9.3438e+00, -3.7715e+00, -6.5469e+00,  ...,  1.0391e+00,\n            1.6377e+00,  4.7109e+00],\n          [-3.9526e-01,  5.6122e-02, -1.4124e-01,  ...,  9.5850e-01,\n           -1.6003e-01,  8.7549e-01]],\n\n         [[ 1.2242e+01,  2.5781e+00,  7.6719e+00,  ...,  3.4121e+00,\n            6.2656e+00,  4.9805e+00],\n          [ 2.2305e+00, -1.1641e+00,  5.5811e-01,  ..., -6.3047e+00,\n           -2.0898e+00,  3.6445e+00],\n          [ 4.8477e+00,  1.3691e+00,  1.2500e+00,  ..., -1.2438e+01,\n            3.4258e+00,  5.7734e+00],\n          ...,\n          [-3.1445e+00,  1.0703e+01,  2.2078e+01,  ..., -2.9531e+00,\n            3.1094e+00, -7.1719e+00],\n          [-4.2617e+00, -7.8359e+00,  1.4820e+01,  ...,  2.6113e+00,\n            5.6484e+00, -3.9238e+00],\n          [ 1.8387e-02,  3.5156e-02,  2.0218e-02,  ..., -4.6509e-01,\n            2.3315e-01, -3.4082e-01]],\n\n         ...,\n\n         [[ 1.0922e+01,  1.0070e+01,  4.1484e+00,  ..., -8.9531e+00,\n           -1.0203e+01,  9.8926e-01],\n          [ 2.4023e+00,  5.7227e-01, -1.3297e+01,  ..., -6.5625e+00,\n           -7.1914e+00, -2.6602e+00],\n          [ 4.9922e+00, -3.4399e-01, -8.6484e+00,  ...,  1.1475e+00,\n           -2.8555e+00,  3.8672e+00],\n          ...,\n          [ 7.2734e+00, -1.8887e+00,  1.2920e+00,  ..., -3.7646e-01,\n            3.5742e+00, -1.9141e+01],\n          [ 5.0352e+00,  1.2219e+01,  8.6719e+00,  ...,  1.4033e+00,\n           -1.7510e+00, -7.5742e+00],\n          [ 3.1055e-01, -2.7417e-01,  5.8057e-01,  ...,  3.9062e-01,\n           -4.7803e-01, -1.5808e-01]],\n\n         [[-3.9277e+00,  5.4688e+00,  2.5801e+00,  ..., -2.4258e+00,\n            3.7207e+00,  6.3906e+00],\n          [ 2.6387e+00, -2.6094e+00,  6.3633e+00,  ..., -1.3369e+00,\n            6.9297e+00,  3.2520e+00],\n          [-1.7412e+00,  9.7812e+00,  2.9590e+00,  ..., -5.5078e+00,\n            3.5605e+00, -1.3018e+00],\n          ...,\n          [-6.3320e+00,  3.0879e+00,  1.0727e+01,  ..., -9.6008e-02,\n           -1.5967e+00,  1.1680e+01],\n          [-1.3379e+00,  9.8828e+00, -4.7891e+00,  ..., -2.5254e+00,\n            6.0938e+00,  6.7266e+00],\n          [ 8.9746e-01,  3.1452e-03,  8.0139e-02,  ..., -6.0010e-01,\n           -1.8154e+00,  1.3513e-01]],\n\n         [[-9.4844e+00,  8.4531e+00,  1.4795e+00,  ...,  1.6172e+01,\n            1.0180e+01, -3.2949e+00],\n          [ 5.7324e-01, -3.2852e+00, -7.5469e+00,  ..., -4.2070e+00,\n           -1.1455e+00, -2.2168e+00],\n          [-8.5391e+00, -5.1211e+00,  5.2695e+00,  ...,  4.3125e+00,\n            7.1211e+00, -6.5977e+00],\n          ...,\n          [ 6.5820e+00,  6.0010e-01,  2.3203e+00,  ..., -1.2898e+01,\n            6.5547e+00, -4.0552e-01],\n          [ 5.9922e+00,  4.0723e-01,  8.8428e-01,  ..., -1.8906e+01,\n            1.1852e+01, -3.3223e+00],\n          [ 2.1655e-01, -2.7832e-02,  1.4087e-01,  ..., -7.8857e-02,\n            2.5513e-01, -1.4038e-01]]],\n\n\n        [[[ 8.4766e+00,  2.1250e+00,  5.5352e+00,  ..., -1.1617e+01,\n            9.2676e-01, -2.7949e+00],\n          [ 9.8633e-01,  3.1680e+00,  1.0029e+00,  ..., -4.0039e+00,\n           -2.8340e+00,  5.8555e+00],\n          [-1.6469e+01, -4.4297e+00, -7.8516e+00,  ..., -3.8984e+00,\n           -4.3164e+00,  1.1039e+01],\n          ...,\n          [ 1.1084e+00,  6.7422e+00, -1.0688e+01,  ..., -4.7656e+00,\n            2.0605e+00,  1.3742e+01],\n          [ 3.3887e+00,  2.5488e+00, -1.1211e+00,  ..., -1.2570e+01,\n           -8.9219e+00,  6.9609e+00],\n          [ 5.1016e+00, -2.7661e-01, -6.2812e+00,  ..., -4.3320e+00,\n           -2.8535e+00,  3.7402e+00]],\n\n         [[ 7.9492e+00,  6.7676e-01,  1.0273e+01,  ...,  1.1312e+01,\n            4.4805e+00,  1.0617e+01],\n          [ 1.8516e+00,  1.8184e+00, -2.6758e+00,  ...,  6.0586e+00,\n            3.9629e+00,  7.9453e+00],\n          [-4.4219e+00, -1.1445e+00, -7.5234e+00,  ...,  1.1102e+01,\n            7.7930e+00, -1.0215e+00],\n          ...,\n          [-1.9004e+00,  1.7207e+00,  1.4619e+00,  ...,  6.2646e-01,\n           -6.2793e-01,  7.9297e+00],\n          [ 8.0811e-01, -7.1211e+00, -8.2812e-01,  ...,  3.3340e+00,\n            1.0674e+00,  8.3750e+00],\n          [-1.9834e+00, -5.2617e+00, -7.3584e-01,  ...,  2.9766e+00,\n           -1.5322e+00,  4.5859e+00]],\n\n         [[-7.3281e+00,  3.7461e+00,  7.9961e+00,  ..., -1.4092e+00,\n            8.3828e+00,  4.6562e+00],\n          [-3.1758e+00, -2.1426e+00, -5.6133e+00,  ...,  1.0930e+01,\n            9.0918e-01,  9.0859e+00],\n          [ 1.7529e+00, -6.2656e+00, -1.2125e+01,  ..., -3.3154e-01,\n           -5.8789e-01,  5.1172e+00],\n          ...,\n          [-1.0879e+00, -1.0049e+00,  1.0820e+00,  ..., -5.9141e+00,\n            7.7812e+00, -3.4922e+00],\n          [-1.0406e+01,  8.8135e-02,  5.8125e+00,  ..., -8.0391e+00,\n            8.9609e+00, -1.3262e+00],\n          [-6.3828e+00,  6.2451e-01,  9.6719e+00,  ..., -6.0977e+00,\n            8.9453e+00,  4.3828e+00]],\n\n         ...,\n\n         [[-1.2211e+01,  1.8457e+00,  2.8984e+00,  ..., -5.5352e+00,\n           -5.3223e-01,  6.3398e+00],\n          [ 5.1270e-01,  6.7930e+00,  4.2773e+00,  ..., -1.2383e+01,\n           -5.8057e-01, -2.3965e+00],\n          [ 2.6562e+00,  3.9629e+00, -4.5469e+00,  ..., -9.1016e+00,\n           -6.4688e+00,  4.1445e+00],\n          ...,\n          [-2.0703e+00,  1.4695e+01,  8.9375e+00,  ...,  1.4229e+00,\n            4.1055e+00, -6.9434e-01],\n          [ 3.4551e+00,  9.3359e+00, -4.2188e+00,  ..., -6.6289e+00,\n            5.6602e+00, -3.0820e+00],\n          [ 8.4922e+00,  1.7375e+01,  2.9023e+00,  ..., -1.5703e+01,\n           -2.5293e+00, -7.7656e+00]],\n\n         [[-1.1797e+01, -5.0469e+00, -7.6602e+00,  ...,  1.2242e+01,\n            3.9316e+00, -1.3695e+01],\n          [-4.3594e+00, -3.8027e+00,  2.5098e+00,  ...,  1.9570e+00,\n            4.6719e+00, -7.6875e+00],\n          [-6.9775e-01, -7.3906e+00, -1.7554e-01,  ..., -1.1117e+01,\n            2.2129e+00,  3.5137e+00],\n          ...,\n          [ 2.0215e+00,  1.4664e+01,  7.9346e-01,  ...,  2.4915e-01,\n           -8.0078e+00,  4.6924e-01],\n          [-4.9180e+00, -8.4106e-02,  1.0518e+00,  ..., -2.7559e+00,\n           -2.4297e+00,  1.1008e+01],\n          [-4.5547e+00,  3.2656e+00,  3.8262e+00,  ..., -1.4033e+00,\n           -6.7305e+00,  3.9082e+00]],\n\n         [[ 4.4961e+00, -2.3105e+00,  1.5967e+00,  ..., -8.1299e-02,\n            4.0586e+00, -8.1094e+00],\n          [ 2.1738e+00, -3.3496e-01, -3.7559e+00,  ...,  3.2832e+00,\n           -2.3496e+00, -1.0172e+01],\n          [-8.5781e+00, -6.9414e+00,  8.8562e-02,  ..., -1.4990e+00,\n            8.0781e+00, -2.0531e+01],\n          ...,\n          [ 9.5078e+00, -3.2207e+00,  8.3438e+00,  ..., -1.3156e+01,\n            6.6328e+00, -1.4033e+00],\n          [ 1.1812e+01, -1.5781e+00,  9.6826e-01,  ..., -1.0000e+01,\n            1.5609e+01, -6.7930e+00],\n          [ 1.0203e+01,  2.6719e+00,  8.8828e+00,  ..., -1.2484e+01,\n            1.0328e+01,  6.7930e+00]]],\n\n\n        [[[-9.0781e+00,  6.4531e+00,  2.3809e+00,  ...,  1.8176e-01,\n           -4.6680e+00, -1.0508e+01],\n          [ 4.7891e+00, -1.0742e+01, -5.1016e+00,  ...,  2.6270e+00,\n           -5.9180e-01,  4.5801e-01],\n          [-5.2539e+00,  1.7236e+00, -6.4023e+00,  ...,  1.0391e+01,\n            5.4453e+00,  9.5938e+00],\n          ...,\n          [-1.7695e+00,  2.3809e+00, -2.2031e+00,  ..., -3.3652e+00,\n            1.8154e+00,  2.8418e+00],\n          [ 1.3555e+01, -2.9512e+00, -1.1703e+01,  ..., -2.7129e+00,\n           -1.9121e+00,  5.1680e+00],\n          [ 4.9141e+00,  6.9883e+00,  1.0977e+00,  ..., -1.5039e+01,\n            9.4062e+00,  2.7168e+00]],\n\n         [[-1.2949e+00,  3.1211e+00,  4.4688e+00,  ...,  3.7168e+00,\n            4.9585e-01,  6.8398e+00],\n          [-1.4922e+01, -3.2324e+00,  1.0852e+01,  ...,  1.1570e+01,\n           -1.0547e+00, -1.4719e+01],\n          [-5.1484e+00, -3.1465e+00,  5.3438e+00,  ...,  1.9344e+01,\n           -4.6753e-01, -7.5352e+00],\n          ...,\n          [ 4.1953e+00, -9.8125e+00,  7.9590e-02,  ...,  7.6094e+00,\n            6.8477e+00, -1.5928e+00],\n          [-5.7666e-01, -4.4844e+00,  1.2529e+00,  ...,  8.6172e+00,\n            1.0664e+01,  2.2129e+00],\n          [ 1.6143e+00, -1.1273e+01, -3.5430e+00,  ...,  7.9258e+00,\n            1.8997e-02,  6.1797e+00]],\n\n         [[ 9.4062e+00,  6.4375e+00,  5.8750e+00,  ..., -5.4844e+00,\n           -2.2168e+00,  5.6602e+00],\n          [-8.7109e+00, -4.3828e+00, -4.9961e+00,  ..., -6.2656e+00,\n           -1.0727e+01,  2.1984e+01],\n          [ 5.7070e+00,  2.1359e+01,  1.1219e+01,  ..., -3.7383e+00,\n           -2.5234e+00,  9.6328e+00],\n          ...,\n          [-1.1055e+01,  1.7490e+00,  3.1230e+00,  ..., -1.4406e+01,\n           -4.1431e-01, -3.8848e+00],\n          [-1.1539e+01,  7.7773e+00, -3.9199e+00,  ..., -8.8672e+00,\n            1.7568e+00,  7.6465e-01],\n          [-1.2203e+01,  1.9189e+00,  1.0664e+01,  ..., -1.8094e+01,\n            1.5801e+00, -3.3516e+00]],\n\n         ...,\n\n         [[ 1.0914e+01, -1.5633e+01,  3.2363e+00,  ...,  9.2334e-01,\n           -1.7578e+01, -7.5664e+00],\n          [ 1.2646e+00,  1.3984e+00,  9.9297e+00,  ..., -8.8906e+00,\n            7.0234e+00, -5.3828e+00],\n          [ 1.0727e+01, -8.1172e+00,  1.6631e+00,  ..., -2.5020e+00,\n           -4.1328e+00, -1.0906e+01],\n          ...,\n          [ 1.1141e+01, -2.9316e+00,  5.9688e+00,  ..., -3.6738e+00,\n            3.6797e+00, -2.3672e+00],\n          [ 4.1914e+00, -1.5225e+00,  1.4658e+00,  ..., -4.4922e+00,\n            5.3662e-01, -3.1016e+00],\n          [-2.4961e+00,  2.9824e+00,  7.9961e+00,  ..., -6.3984e+00,\n           -2.6270e-01, -4.1406e+00]],\n\n         [[ 1.0773e+01,  5.1484e+00, -1.0699e-01,  ...,  6.0234e+00,\n            7.2227e+00, -6.1016e+00],\n          [-1.6230e+00,  8.9404e-01,  8.8916e-01,  ...,  1.5918e+00,\n           -2.9629e+00, -1.4727e+00],\n          [ 1.8335e-01, -4.3711e+00,  1.0820e+01,  ..., -2.6855e+00,\n            1.0391e+01,  3.5645e+00],\n          ...,\n          [ 2.5957e+00, -1.0977e+00,  5.7109e+00,  ..., -3.8594e+00,\n           -4.2617e+00,  4.7578e+00],\n          [ 1.5391e+00,  1.6885e+00,  1.5750e+01,  ..., -2.9004e+00,\n            5.6875e+00,  6.4805e+00],\n          [ 5.3711e+00,  4.4102e+00,  9.0000e+00,  ..., -3.1211e+00,\n           -2.5215e+00,  6.9727e+00]],\n\n         [[ 1.3891e+01,  1.7373e+00,  2.5430e+00,  ..., -8.8965e-01,\n            7.2188e+00, -1.0996e+00],\n          [ 5.8252e-01, -9.2812e+00,  4.5078e+00,  ..., -7.1992e+00,\n            1.0375e+01, -4.9492e+00],\n          [ 8.5547e+00,  8.4609e+00,  1.1750e+01,  ...,  3.5742e+00,\n            1.5420e+00,  5.3086e+00],\n          ...,\n          [-2.6211e+00,  5.3320e+00,  2.5254e+00,  ..., -7.0703e+00,\n            1.1008e+01, -1.6328e+00],\n          [-1.0967e+00,  1.0969e+01,  7.3242e-01,  ..., -1.3023e+01,\n            1.4055e+01, -4.3906e+00],\n          [ 1.1195e+01, -6.6016e+00,  5.0469e+00,  ..., -7.1729e-01,\n            1.2516e+01, -1.7510e+00]]]], device='cuda:0', dtype=torch.float16,\n       grad_fn=<TransposeBackward0>)), tensor([[[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        ...,\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]]], device='cuda:0',\n       grad_fn=<AddBackward0>), tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        ...,\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]]], device='cuda:0'))\u001b[0m\u001b[0;34m\n        \u001b[0mhidden_states \u001b[0;34m= tensor([[[-3.9580e+02,  2.4767e+02, -7.2931e+01,  ...,  3.8053e+02,\n          -6.4637e+02, -4.8657e+02],\n         [-1.2749e+02, -8.8088e+01,  2.4900e+01,  ..., -3.7472e+02,\n          -1.8831e+02, -8.1284e+02],\n         [ 1.0088e+01, -1.3811e+02, -3.5234e+02,  ..., -3.7094e+02,\n           5.3798e+01, -5.3244e+02],\n         ...,\n         [-6.0796e+01,  1.0092e+02, -2.5755e+01,  ...,  2.6453e+02,\n          -5.0002e+01, -3.0047e+02],\n         [-1.4893e+02,  5.1579e+01,  3.0008e+01,  ...,  2.4587e+02,\n          -5.1689e+02, -2.8850e+02],\n         [-1.2358e+02,  6.0058e+01,  2.5703e+01,  ...,  1.1384e+02,\n          -4.2387e+02, -2.3618e+02]],\n\n        [[ 4.1202e+00,  5.0162e+01, -4.1346e+01,  ...,  7.5681e+01,\n           2.3860e+02,  2.3550e+02],\n         [-2.0577e+02,  1.1997e+02,  2.5339e+02,  ..., -5.0824e+02,\n          -2.7210e+02,  4.6938e+01],\n         [-5.9783e+02,  1.1258e+00, -6.1282e+02,  ..., -3.6052e+02,\n          -1.2050e+00,  6.6538e+02],\n         ...,\n         [ 6.0902e+01,  3.5652e+01,  1.4630e+02,  ..., -1.2697e+02,\n           1.4870e+02,  7.8078e+01],\n         [ 1.0076e+02,  5.9857e+01,  1.1207e+02,  ...,  4.1839e+01,\n           2.5267e+02,  2.6653e+02],\n         [ 6.1666e+01,  1.8881e+01,  1.1667e+02,  ..., -1.4297e+02,\n           1.3583e+02,  2.1588e+02]],\n\n        [[ 1.4321e+02,  2.6521e+01, -1.9856e+02,  ...,  3.9943e+01,\n          -2.3000e+02,  2.9039e+00],\n         [ 5.1894e+02, -2.8780e+02, -6.5460e+02,  ...,  3.2682e+02,\n          -3.2717e+02, -5.6882e+02],\n         [-1.2565e+02, -6.4311e+02, -7.1021e+02,  ..., -2.0594e+02,\n           2.6969e+02,  3.0860e+02],\n         ...,\n         [ 1.1608e+02, -1.0791e+02,  1.7948e+01,  ..., -1.2139e+02,\n          -7.3849e+01, -1.9148e+02],\n         [ 1.3399e+02, -1.5552e+02,  7.6590e+01,  ...,  2.9785e+01,\n          -1.5029e+02, -1.1276e+02],\n         [ 1.2852e+02, -1.3101e+02,  4.2857e+01,  ..., -1.2577e+02,\n          -1.3094e+02, -1.1035e+02]],\n\n        ...,\n\n        [[-1.9247e+02,  2.0719e+02, -2.4518e+02,  ..., -2.2249e+02,\n          -1.6554e+02,  3.8623e+01],\n         [-3.0558e+02, -1.1055e+02, -2.2065e+02,  ...,  8.8692e+01,\n          -2.2254e+02, -6.0727e+01],\n         [-4.4662e+01,  5.9589e+02, -6.2137e+02,  ..., -1.3274e+02,\n          -6.1440e+02, -4.6574e+02],\n         ...,\n         [-4.7629e+00,  4.4491e+01, -8.1681e+01,  ..., -1.3646e+02,\n          -8.2333e+01,  3.0740e+01],\n         [-6.2734e+01,  2.8587e+01, -8.2016e+01,  ..., -2.3431e+02,\n          -1.7656e+02,  2.2520e+01],\n         [-7.0461e+01,  8.4793e+01, -4.4015e+01,  ..., -1.9447e+02,\n          -1.3073e+02,  4.1494e+01]],\n\n        [[ 2.4631e+00,  2.9960e+02, -1.4315e+01,  ..., -1.5654e+01,\n           2.9429e+02,  1.6030e+01],\n         [-1.7511e+02, -5.4824e+02, -2.2807e+02,  ...,  1.5704e+02,\n          -2.3582e+02, -2.0936e+02],\n         [-3.1996e+02,  1.5875e+02,  5.1828e+02,  ..., -2.4916e+02,\n          -4.6174e+01, -4.3444e+02],\n         ...,\n         [ 4.7171e+01,  9.3918e+01, -1.4850e+01,  ..., -1.3771e+02,\n           1.8458e+01,  8.3231e+01],\n         [ 5.3622e+01,  4.9918e+01, -5.2572e+01,  ..., -1.2729e+02,\n           1.3833e+01,  1.3946e+01],\n         [ 5.8013e+01,  8.5568e+01, -2.7452e+01,  ..., -9.1226e+01,\n           1.0243e+02, -1.7468e-01]],\n\n        [[ 7.4680e+01,  1.5303e+02, -5.8707e+01,  ..., -7.5620e+01,\n          -3.2263e+02,  3.5384e+00],\n         [-2.3415e+01,  9.2403e+01, -3.2122e+02,  ..., -1.4288e+02,\n          -4.9064e+02,  6.4593e+01],\n         [-2.3628e+02,  2.8563e+01,  2.7882e+02,  ..., -3.6215e+02,\n          -1.5245e+02, -6.6334e+01],\n         ...,\n         [ 1.7690e+02, -1.0148e+02,  3.3293e+01,  ..., -9.4638e+01,\n          -1.6065e+02, -1.0381e+02],\n         [ 1.6100e+02, -6.9087e+01,  6.1568e+01,  ..., -1.2287e+02,\n          -1.1618e+02, -5.3916e+01],\n         [ 1.1145e+02,  8.4470e+01,  5.4469e+01,  ..., -1.8014e+02,\n          -1.3798e+02, -2.0567e+01]]], device='cuda:0', grad_fn=<AddBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mextended_attention_mask \u001b[0;34m= tensor([[[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        ...,\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]]], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0mposition_bias \u001b[0;34m= tensor([[[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        ...,\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]]], device='cuda:0',\n       grad_fn=<AddBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mencoder_hidden_states \u001b[0;34m= tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,  ...,  7.3756e-01,\n          -6.4471e-01, -4.3072e-02],\n         [-0.0000e+00,  1.4130e-01, -2.8108e-01,  ...,  3.3907e-01,\n          -3.6488e-02, -3.6908e-01],\n         [ 9.3195e-02,  3.7150e-01,  9.4190e-02,  ..., -6.1819e-02,\n           3.5049e-03, -0.0000e+00],\n         ...,\n         [-2.3926e-02, -4.8247e-01,  2.3497e-01,  ...,  3.9880e-01,\n          -8.4972e-02, -9.8002e-02],\n         [ 3.5569e-02, -2.3768e-01,  2.7225e-01,  ...,  3.4104e-01,\n           3.3176e-01, -5.0014e-01],\n         [ 1.6381e-01,  5.2310e-02, -7.1547e-02,  ...,  3.7553e-01,\n           5.6716e-01, -0.0000e+00]],\n\n        [[ 6.2732e-01,  3.2355e-01,  0.0000e+00,  ...,  2.2136e-01,\n           0.0000e+00,  5.1227e-01],\n         [-7.4636e-02,  6.8215e-01,  1.2985e-01,  ..., -8.1269e-02,\n          -2.4353e-02, -1.7799e-01],\n         [-0.0000e+00,  3.3901e-01,  1.7466e-01,  ...,  0.0000e+00,\n           1.4631e-01,  1.6036e-02],\n         ...,\n         [ 7.9134e-02, -0.0000e+00, -9.6235e-02,  ...,  6.9691e-01,\n           3.6416e-01, -0.0000e+00],\n         [-1.3400e-01, -3.2879e-01,  2.6364e-01,  ...,  7.1581e-01,\n           2.3241e-01, -2.1390e-01],\n         [ 2.8726e-01, -1.8657e-01,  1.5836e-01,  ...,  0.0000e+00,\n           2.7848e-01, -0.0000e+00]],\n\n        [[ 8.7178e-02,  9.1433e-02,  2.7344e-01,  ...,  1.7490e-02,\n          -2.9121e-01, -1.4335e-01],\n         [ 1.5637e-01, -4.9306e-01,  2.6517e-01,  ...,  3.8128e-01,\n          -3.1244e-01, -1.5314e-01],\n         [ 1.7885e-01, -0.0000e+00, -1.0513e-03,  ...,  6.7242e-02,\n          -1.3838e-01, -3.2072e-01],\n         ...,\n         [ 2.0394e-01, -9.1664e-02,  3.0847e-03,  ...,  6.7082e-01,\n           5.4445e-01, -8.3819e-02],\n         [-2.3183e-01, -7.7485e-03, -5.5931e-01,  ...,  5.3906e-01,\n           1.6026e-01, -0.0000e+00],\n         [ 3.8757e-01, -6.0742e-02, -8.9720e-02,  ...,  3.7971e-01,\n           5.8543e-01,  6.9683e-02]],\n\n        ...,\n\n        [[ 2.5694e-01,  3.8836e-01,  3.6472e-01,  ...,  1.9078e-01,\n           1.1912e-01,  4.2067e-01],\n         [ 1.7438e-01,  1.1089e-01,  5.0688e-01,  ...,  1.4332e-01,\n          -1.7266e-01,  4.9860e-01],\n         [-5.2431e-01,  1.1103e+00,  5.6405e-02,  ..., -3.1441e-02,\n          -2.7494e-01,  2.1520e-01],\n         ...,\n         [-3.7092e-02, -2.6356e-01, -2.1277e-01,  ...,  4.0784e-01,\n           0.0000e+00,  2.2286e-01],\n         [ 2.4721e-01,  1.2555e-01, -2.7395e-01,  ..., -1.1099e-01,\n           7.4805e-02,  4.2801e-01],\n         [ 4.3440e-03, -0.0000e+00, -1.2907e-02,  ..., -7.4690e-03,\n           9.0333e-03,  2.5710e-03]],\n\n        [[ 0.0000e+00, -1.9124e-01,  0.0000e+00,  ...,  0.0000e+00,\n           3.4049e-01, -2.3081e-01],\n         [ 1.1040e-01, -1.6056e-01,  3.6804e-01,  ..., -4.7069e-01,\n          -0.0000e+00,  2.2921e-01],\n         [-2.3070e-01,  2.6098e-01,  2.6266e-01,  ..., -6.6238e-01,\n          -4.0395e-01, -2.9121e-01],\n         ...,\n         [-3.7796e-01, -7.3537e-02, -1.4169e-01,  ...,  3.3294e-01,\n           7.2322e-02, -2.4653e-01],\n         [-1.8759e-02, -1.0135e-01, -1.6968e-01,  ..., -4.5800e-03,\n           1.7808e-01, -3.0931e-01],\n         [-2.0037e-01, -3.4233e-01,  1.8222e-01,  ...,  2.3297e-02,\n           1.6224e-01, -0.0000e+00]],\n\n        [[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  7.6713e-02,\n          -2.7339e-01,  2.0604e-01],\n         [-3.8789e-02,  1.4853e-01,  2.4096e-01,  ...,  9.6284e-02,\n          -0.0000e+00,  3.6521e-01],\n         [ 0.0000e+00,  4.6811e-01, -2.0381e-01,  ..., -3.3839e-01,\n           2.3055e-01,  4.2210e-01],\n         ...,\n         [ 2.2385e-01, -7.3387e-02, -5.2777e-01,  ...,  1.3698e-01,\n           5.1136e-01,  1.2732e-01],\n         [ 2.8012e-01, -1.1424e-01, -5.9984e-01,  ...,  3.8852e-01,\n           6.4370e-01, -4.2405e-01],\n         [ 1.0257e-01, -6.2944e-01, -0.0000e+00,  ...,  3.9602e-01,\n           3.1288e-01, -4.1677e-01]]], device='cuda:0',\n       grad_fn=<NativeDropoutBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mencoder_extended_attention_mask \u001b[0;34m= tensor([[[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        ...,\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]]], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0mencoder_decoder_position_bias \u001b[0;34m= tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        ...,\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]]], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0mlayer_head_mask \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mcross_attn_layer_head_mask \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0mpast_key_value \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0muse_cache \u001b[0;34m= True\u001b[0m\u001b[0;34m\n        \u001b[0moutput_attentions \u001b[0;34m= False\u001b[0m\n\u001b[1;32m   1075\u001b[0m         hidden_states,\n\u001b[1;32m   1076\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1077\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1078\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1079\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1080\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1081\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1082\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1083\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1084\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1085\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     )\n\u001b[1;32m   1088\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self=T5Block(\n  (layer): ModuleList(\n    (0): T5Layer...opout): Dropout(p=0.1, inplace=False)\n    )\n  )\n), *args=(tensor([[[-3.9580e+02,  2.4767e+02, -7.2931e+01,...e+01]]], device='cuda:0', grad_fn=<AddBackward0>),), **kwargs={'attention_mask': tensor([[[[-0.0000e+00, -3.4028e+38, -3.4028e+38...   -0.0000e+00, -0.0000e+00]]]], device='cuda:0'), 'cross_attn_layer_head_mask': None, 'encoder_attention_mask': tensor([[[[-0.0000e+00, -0.0000e+00, -0.0000e+00...   -3.4028e+38, -3.4028e+38]]]], device='cuda:0'), 'encoder_decoder_position_bias': tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00...   -3.4028e+38, -3.4028e+38]]]], device='cuda:0'), 'encoder_hidden_states': tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,...cuda:0',\n       grad_fn=<NativeDropoutBackward0>), 'layer_head_mask': None, 'output_attentions': False, 'past_key_value': None, 'position_bias': tensor([[[[ 2.4052e+00, -3.4028e+38, -3.4028e+38..., device='cuda:0',\n       grad_fn=<AddBackward0>), 'use_cache': True})\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n        forward_call \u001b[0;34m= <bound method T5Block.forward of T5Block(\n  (layer): ModuleList(\n    (0): T5LayerSelfAttention(\n      (SelfAttention): T5Attention(\n        (q): Linear(in_features=768, out_features=768, bias=False)\n        (k): Linear(in_features=768, out_features=768, bias=False)\n        (v): Linear(in_features=768, out_features=768, bias=False)\n        (o): Linear(in_features=768, out_features=768, bias=False)\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (1): T5LayerCrossAttention(\n      (EncDecAttention): T5Attention(\n        (q): Linear(in_features=768, out_features=768, bias=False)\n        (k): Linear(in_features=768, out_features=768, bias=False)\n        (v): Linear(in_features=768, out_features=768, bias=False)\n        (o): Linear(in_features=768, out_features=768, bias=False)\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (2): T5LayerFF(\n      (DenseReluDense): T5DenseActDense(\n        (wi): Linear(in_features=768, out_features=3072, bias=False)\n        (wo): Linear(in_features=3072, out_features=768, bias=False)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (act): ReLU()\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n)>\u001b[0m\u001b[0;34m\n        \u001b[0margs \u001b[0;34m= (tensor([[[-3.9580e+02,  2.4767e+02, -7.2931e+01,  ...,  3.8053e+02,\n          -6.4637e+02, -4.8657e+02],\n         [-1.2749e+02, -8.8088e+01,  2.4900e+01,  ..., -3.7472e+02,\n          -1.8831e+02, -8.1284e+02],\n         [ 1.0088e+01, -1.3811e+02, -3.5234e+02,  ..., -3.7094e+02,\n           5.3798e+01, -5.3244e+02],\n         ...,\n         [-6.0796e+01,  1.0092e+02, -2.5755e+01,  ...,  2.6453e+02,\n          -5.0002e+01, -3.0047e+02],\n         [-1.4893e+02,  5.1579e+01,  3.0008e+01,  ...,  2.4587e+02,\n          -5.1689e+02, -2.8850e+02],\n         [-1.2358e+02,  6.0058e+01,  2.5703e+01,  ...,  1.1384e+02,\n          -4.2387e+02, -2.3618e+02]],\n\n        [[ 4.1202e+00,  5.0162e+01, -4.1346e+01,  ...,  7.5681e+01,\n           2.3860e+02,  2.3550e+02],\n         [-2.0577e+02,  1.1997e+02,  2.5339e+02,  ..., -5.0824e+02,\n          -2.7210e+02,  4.6938e+01],\n         [-5.9783e+02,  1.1258e+00, -6.1282e+02,  ..., -3.6052e+02,\n          -1.2050e+00,  6.6538e+02],\n         ...,\n         [ 6.0902e+01,  3.5652e+01,  1.4630e+02,  ..., -1.2697e+02,\n           1.4870e+02,  7.8078e+01],\n         [ 1.0076e+02,  5.9857e+01,  1.1207e+02,  ...,  4.1839e+01,\n           2.5267e+02,  2.6653e+02],\n         [ 6.1666e+01,  1.8881e+01,  1.1667e+02,  ..., -1.4297e+02,\n           1.3583e+02,  2.1588e+02]],\n\n        [[ 1.4321e+02,  2.6521e+01, -1.9856e+02,  ...,  3.9943e+01,\n          -2.3000e+02,  2.9039e+00],\n         [ 5.1894e+02, -2.8780e+02, -6.5460e+02,  ...,  3.2682e+02,\n          -3.2717e+02, -5.6882e+02],\n         [-1.2565e+02, -6.4311e+02, -7.1021e+02,  ..., -2.0594e+02,\n           2.6969e+02,  3.0860e+02],\n         ...,\n         [ 1.1608e+02, -1.0791e+02,  1.7948e+01,  ..., -1.2139e+02,\n          -7.3849e+01, -1.9148e+02],\n         [ 1.3399e+02, -1.5552e+02,  7.6590e+01,  ...,  2.9785e+01,\n          -1.5029e+02, -1.1276e+02],\n         [ 1.2852e+02, -1.3101e+02,  4.2857e+01,  ..., -1.2577e+02,\n          -1.3094e+02, -1.1035e+02]],\n\n        ...,\n\n        [[-1.9247e+02,  2.0719e+02, -2.4518e+02,  ..., -2.2249e+02,\n          -1.6554e+02,  3.8623e+01],\n         [-3.0558e+02, -1.1055e+02, -2.2065e+02,  ...,  8.8692e+01,\n          -2.2254e+02, -6.0727e+01],\n         [-4.4662e+01,  5.9589e+02, -6.2137e+02,  ..., -1.3274e+02,\n          -6.1440e+02, -4.6574e+02],\n         ...,\n         [-4.7629e+00,  4.4491e+01, -8.1681e+01,  ..., -1.3646e+02,\n          -8.2333e+01,  3.0740e+01],\n         [-6.2734e+01,  2.8587e+01, -8.2016e+01,  ..., -2.3431e+02,\n          -1.7656e+02,  2.2520e+01],\n         [-7.0461e+01,  8.4793e+01, -4.4015e+01,  ..., -1.9447e+02,\n          -1.3073e+02,  4.1494e+01]],\n\n        [[ 2.4631e+00,  2.9960e+02, -1.4315e+01,  ..., -1.5654e+01,\n           2.9429e+02,  1.6030e+01],\n         [-1.7511e+02, -5.4824e+02, -2.2807e+02,  ...,  1.5704e+02,\n          -2.3582e+02, -2.0936e+02],\n         [-3.1996e+02,  1.5875e+02,  5.1828e+02,  ..., -2.4916e+02,\n          -4.6174e+01, -4.3444e+02],\n         ...,\n         [ 4.7171e+01,  9.3918e+01, -1.4850e+01,  ..., -1.3771e+02,\n           1.8458e+01,  8.3231e+01],\n         [ 5.3622e+01,  4.9918e+01, -5.2572e+01,  ..., -1.2729e+02,\n           1.3833e+01,  1.3946e+01],\n         [ 5.8013e+01,  8.5568e+01, -2.7452e+01,  ..., -9.1226e+01,\n           1.0243e+02, -1.7468e-01]],\n\n        [[ 7.4680e+01,  1.5303e+02, -5.8707e+01,  ..., -7.5620e+01,\n          -3.2263e+02,  3.5384e+00],\n         [-2.3415e+01,  9.2403e+01, -3.2122e+02,  ..., -1.4288e+02,\n          -4.9064e+02,  6.4593e+01],\n         [-2.3628e+02,  2.8563e+01,  2.7882e+02,  ..., -3.6215e+02,\n          -1.5245e+02, -6.6334e+01],\n         ...,\n         [ 1.7690e+02, -1.0148e+02,  3.3293e+01,  ..., -9.4638e+01,\n          -1.6065e+02, -1.0381e+02],\n         [ 1.6100e+02, -6.9087e+01,  6.1568e+01,  ..., -1.2287e+02,\n          -1.1618e+02, -5.3916e+01],\n         [ 1.1145e+02,  8.4470e+01,  5.4469e+01,  ..., -1.8014e+02,\n          -1.3798e+02, -2.0567e+01]]], device='cuda:0', grad_fn=<AddBackward0>),)\u001b[0m\u001b[0;34m\n        \u001b[0mkwargs \u001b[0;34m= {'attention_mask': tensor([[[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        ...,\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -3.4028e+38],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]]], device='cuda:0'), 'position_bias': tensor([[[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        ...,\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]],\n\n\n        [[[ 2.4052e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.3132e+00,  2.4052e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7818e+00,  3.3132e+00,  2.4052e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.4052e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  3.3132e+00,\n            2.4052e+00, -3.4028e+38],\n          [ 7.6229e+00,  7.6229e+00,  7.6229e+00,  ...,  2.7818e+00,\n            3.3132e+00,  2.4052e+00]],\n\n         [[ 3.9374e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.4525e+00,  3.9374e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0295e+00,  3.4525e+00,  3.9374e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.9374e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  3.4525e+00,\n            3.9374e+00, -3.4028e+38],\n          [ 3.2750e+01,  3.2750e+01,  3.2750e+01,  ...,  2.0295e+00,\n            3.4525e+00,  3.9374e+00]],\n\n         [[ 2.0449e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 4.4082e+00,  2.0449e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.0471e+00,  4.4082e+00,  2.0449e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  2.0449e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  4.4082e+00,\n            2.0449e+00, -3.4028e+38],\n          [ 2.8750e+01,  2.8750e+01,  2.8750e+01,  ...,  3.0471e+00,\n            4.4082e+00,  2.0449e+00]],\n\n         ...,\n\n         [[ 2.4858e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.7374e+00,  2.4858e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0341e+00,  2.7374e+00,  2.4858e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.4858e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.7374e+00,\n            2.4858e+00, -3.4028e+38],\n          [-2.6375e+01, -2.6375e+01, -2.6375e+01,  ...,  2.0341e+00,\n            2.7374e+00,  2.4858e+00]],\n\n         [[ 3.4048e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.9394e+00,  3.4048e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 2.0337e+00,  2.9394e+00,  3.4048e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  3.4048e+00,\n           -3.4028e+38, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.9394e+00,\n            3.4048e+00, -3.4028e+38],\n          [-2.5000e+01, -2.5000e+01, -2.5000e+01,  ...,  2.0337e+00,\n            2.9394e+00,  3.4048e+00]],\n\n         [[ 3.7347e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.6884e+00,  3.7347e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 1.6786e+00,  3.6884e+00,  3.7347e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.7347e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  3.6884e+00,\n            3.7347e+00, -3.4028e+38],\n          [ 3.1375e+01,  3.1375e+01,  3.1375e+01,  ...,  1.6786e+00,\n            3.6884e+00,  3.7347e+00]]]], device='cuda:0',\n       grad_fn=<AddBackward0>), 'encoder_hidden_states': tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,  ...,  7.3756e-01,\n          -6.4471e-01, -4.3072e-02],\n         [-0.0000e+00,  1.4130e-01, -2.8108e-01,  ...,  3.3907e-01,\n          -3.6488e-02, -3.6908e-01],\n         [ 9.3195e-02,  3.7150e-01,  9.4190e-02,  ..., -6.1819e-02,\n           3.5049e-03, -0.0000e+00],\n         ...,\n         [-2.3926e-02, -4.8247e-01,  2.3497e-01,  ...,  3.9880e-01,\n          -8.4972e-02, -9.8002e-02],\n         [ 3.5569e-02, -2.3768e-01,  2.7225e-01,  ...,  3.4104e-01,\n           3.3176e-01, -5.0014e-01],\n         [ 1.6381e-01,  5.2310e-02, -7.1547e-02,  ...,  3.7553e-01,\n           5.6716e-01, -0.0000e+00]],\n\n        [[ 6.2732e-01,  3.2355e-01,  0.0000e+00,  ...,  2.2136e-01,\n           0.0000e+00,  5.1227e-01],\n         [-7.4636e-02,  6.8215e-01,  1.2985e-01,  ..., -8.1269e-02,\n          -2.4353e-02, -1.7799e-01],\n         [-0.0000e+00,  3.3901e-01,  1.7466e-01,  ...,  0.0000e+00,\n           1.4631e-01,  1.6036e-02],\n         ...,\n         [ 7.9134e-02, -0.0000e+00, -9.6235e-02,  ...,  6.9691e-01,\n           3.6416e-01, -0.0000e+00],\n         [-1.3400e-01, -3.2879e-01,  2.6364e-01,  ...,  7.1581e-01,\n           2.3241e-01, -2.1390e-01],\n         [ 2.8726e-01, -1.8657e-01,  1.5836e-01,  ...,  0.0000e+00,\n           2.7848e-01, -0.0000e+00]],\n\n        [[ 8.7178e-02,  9.1433e-02,  2.7344e-01,  ...,  1.7490e-02,\n          -2.9121e-01, -1.4335e-01],\n         [ 1.5637e-01, -4.9306e-01,  2.6517e-01,  ...,  3.8128e-01,\n          -3.1244e-01, -1.5314e-01],\n         [ 1.7885e-01, -0.0000e+00, -1.0513e-03,  ...,  6.7242e-02,\n          -1.3838e-01, -3.2072e-01],\n         ...,\n         [ 2.0394e-01, -9.1664e-02,  3.0847e-03,  ...,  6.7082e-01,\n           5.4445e-01, -8.3819e-02],\n         [-2.3183e-01, -7.7485e-03, -5.5931e-01,  ...,  5.3906e-01,\n           1.6026e-01, -0.0000e+00],\n         [ 3.8757e-01, -6.0742e-02, -8.9720e-02,  ...,  3.7971e-01,\n           5.8543e-01,  6.9683e-02]],\n\n        ...,\n\n        [[ 2.5694e-01,  3.8836e-01,  3.6472e-01,  ...,  1.9078e-01,\n           1.1912e-01,  4.2067e-01],\n         [ 1.7438e-01,  1.1089e-01,  5.0688e-01,  ...,  1.4332e-01,\n          -1.7266e-01,  4.9860e-01],\n         [-5.2431e-01,  1.1103e+00,  5.6405e-02,  ..., -3.1441e-02,\n          -2.7494e-01,  2.1520e-01],\n         ...,\n         [-3.7092e-02, -2.6356e-01, -2.1277e-01,  ...,  4.0784e-01,\n           0.0000e+00,  2.2286e-01],\n         [ 2.4721e-01,  1.2555e-01, -2.7395e-01,  ..., -1.1099e-01,\n           7.4805e-02,  4.2801e-01],\n         [ 4.3440e-03, -0.0000e+00, -1.2907e-02,  ..., -7.4690e-03,\n           9.0333e-03,  2.5710e-03]],\n\n        [[ 0.0000e+00, -1.9124e-01,  0.0000e+00,  ...,  0.0000e+00,\n           3.4049e-01, -2.3081e-01],\n         [ 1.1040e-01, -1.6056e-01,  3.6804e-01,  ..., -4.7069e-01,\n          -0.0000e+00,  2.2921e-01],\n         [-2.3070e-01,  2.6098e-01,  2.6266e-01,  ..., -6.6238e-01,\n          -4.0395e-01, -2.9121e-01],\n         ...,\n         [-3.7796e-01, -7.3537e-02, -1.4169e-01,  ...,  3.3294e-01,\n           7.2322e-02, -2.4653e-01],\n         [-1.8759e-02, -1.0135e-01, -1.6968e-01,  ..., -4.5800e-03,\n           1.7808e-01, -3.0931e-01],\n         [-2.0037e-01, -3.4233e-01,  1.8222e-01,  ...,  2.3297e-02,\n           1.6224e-01, -0.0000e+00]],\n\n        [[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  7.6713e-02,\n          -2.7339e-01,  2.0604e-01],\n         [-3.8789e-02,  1.4853e-01,  2.4096e-01,  ...,  9.6284e-02,\n          -0.0000e+00,  3.6521e-01],\n         [ 0.0000e+00,  4.6811e-01, -2.0381e-01,  ..., -3.3839e-01,\n           2.3055e-01,  4.2210e-01],\n         ...,\n         [ 2.2385e-01, -7.3387e-02, -5.2777e-01,  ...,  1.3698e-01,\n           5.1136e-01,  1.2732e-01],\n         [ 2.8012e-01, -1.1424e-01, -5.9984e-01,  ...,  3.8852e-01,\n           6.4370e-01, -4.2405e-01],\n         [ 1.0257e-01, -6.2944e-01, -0.0000e+00,  ...,  3.9602e-01,\n           3.1288e-01, -4.1677e-01]]], device='cuda:0',\n       grad_fn=<NativeDropoutBackward0>), 'encoder_attention_mask': tensor([[[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        ...,\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]]], device='cuda:0'), 'encoder_decoder_position_bias': tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        ...,\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         ...,\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38]]]], device='cuda:0'), 'layer_head_mask': None, 'cross_attn_layer_head_mask': None, 'past_key_value': None, 'use_cache': True, 'output_attentions': False}\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:745\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self=T5Block(\n  (layer): ModuleList(\n    (0): T5Layer...opout): Dropout(p=0.1, inplace=False)\n    )\n  )\n), hidden_states=tensor([[[-4.5808e+02,  3.2732e+02, -1.6407e+02,...e+01]]], device='cuda:0', grad_fn=<AddBackward0>), attention_mask=tensor([[[[-0.0000e+00, -3.4028e+38, -3.4028e+38...   -0.0000e+00, -0.0000e+00]]]], device='cuda:0'), position_bias=tensor([[[[ 2.4052e+00, -3.4028e+38, -3.4028e+38..., device='cuda:0',\n       grad_fn=<AddBackward0>), encoder_hidden_states=tensor([[[-3.0799e-01,  2.8784e-01,  4.5686e-01,...cuda:0',\n       grad_fn=<NativeDropoutBackward0>), encoder_attention_mask=tensor([[[[-0.0000e+00, -0.0000e+00, -0.0000e+00...   -3.4028e+38, -3.4028e+38]]]], device='cuda:0'), encoder_decoder_position_bias=tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00...   -3.4028e+38, -3.4028e+38]]]], device='cuda:0'), layer_head_mask=None, cross_attn_layer_head_mask=None, past_key_value=None, use_cache=True, output_attentions=False, return_dict=True)\u001b[0m\n\u001b[1;32m    742\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    744\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n        hidden_states \u001b[0;34m= tensor([[[-4.5808e+02,  3.2732e+02, -1.6407e+02,  ...,  3.1219e+02,\n          -5.1455e+02, -6.2220e+02],\n         [ 6.1342e+00, -7.0963e+01,  6.5572e+01,  ..., -5.9832e+02,\n          -1.1672e+02, -7.1259e+02],\n         [-7.6225e+01, -1.3903e+02, -4.3484e+02,  ..., -5.9708e+02,\n           5.3288e+00, -5.6920e+02],\n         ...,\n         [-1.6408e+02,  1.5863e+02, -1.2699e+02,  ...,  1.9416e+02,\n           2.1248e+01, -3.0047e+02],\n         [-1.5955e+02,  7.1735e+01, -5.7054e+01,  ...,  1.7898e+02,\n          -4.7328e+02, -3.3234e+02],\n         [-2.3054e+02,  9.3526e+01, -4.9391e+01,  ...,  4.1012e+01,\n          -3.4604e+02, -3.2630e+02]],\n\n        [[ 3.8198e+01,  1.1505e+02,  6.1853e+00,  ...,  1.6369e+01,\n           2.0698e+02,  2.1272e+02],\n         [-6.0571e+01,  6.5898e+01,  2.8919e+02,  ..., -5.8921e+02,\n          -1.5657e+02,  2.7985e+01],\n         [-5.6652e+02, -7.2726e+00, -4.8016e+02,  ..., -4.5730e+02,\n          -1.3149e+02,  6.7482e+02],\n         ...,\n         [ 1.5353e+02,  7.8715e+01,  6.9392e+01,  ..., -4.3843e+01,\n           1.1915e+02,  2.4657e+01],\n         [ 1.7362e+02,  1.1958e+02,  9.8289e+01,  ...,  4.2402e+01,\n           2.6104e+02,  2.5017e+02],\n         [ 1.3596e+02,  6.3303e+01,  3.5854e+01,  ..., -1.2650e+02,\n           1.2770e+02,  1.7447e+02]],\n\n        [[ 2.3934e+02, -2.9502e+01, -2.0509e+02,  ...,  5.7248e+01,\n          -2.2415e+02,  1.2888e+01],\n         [ 6.1794e+02, -2.8721e+02, -6.8688e+02,  ...,  2.4867e+02,\n          -4.1648e+02, -5.1873e+02],\n         [-1.1572e+02, -6.3932e+02, -7.1134e+02,  ..., -2.3844e+02,\n           2.5559e+02,  4.7764e+02],\n         ...,\n         [ 1.1005e+02, -1.4408e+02,  5.1128e+01,  ..., -1.5154e+02,\n          -2.1335e+02, -2.3279e+02],\n         [ 7.5258e+01, -1.8612e+02,  1.0054e+02,  ..., -1.5527e+01,\n          -2.9601e+02, -1.3669e+02],\n         [ 6.7095e+01, -1.5715e+02,  9.8779e+01,  ..., -1.1569e+02,\n          -2.4653e+02, -1.3091e+02]],\n\n        ...,\n\n        [[-2.2087e+02,  2.0763e+02, -3.6474e+02,  ..., -2.2249e+02,\n          -1.6606e+02, -5.1205e+01],\n         [-5.9239e+02, -1.0025e+02, -1.5515e+02,  ...,  4.9349e+01,\n          -3.7339e+02, -1.4879e+02],\n         [-2.4341e+02,  6.0751e+02, -5.7049e+02,  ...,  1.6773e+02,\n          -8.8015e+02, -7.2333e+02],\n         ...,\n         [-4.2716e+01,  3.3967e+01, -5.1713e+01,  ..., -2.0721e+02,\n          -9.7786e+01,  1.6162e-01],\n         [-1.1289e+02, -2.1159e+00, -6.1984e+01,  ..., -2.5941e+02,\n          -1.9961e+02, -1.3074e+01],\n         [-8.1257e+01,  6.5676e+01, -5.8484e+01,  ..., -2.6540e+02,\n          -1.6826e+02,  2.7531e+01]],\n\n        [[-4.0654e+01,  3.0270e+02, -1.0605e+02,  ...,  1.4455e+02,\n           3.5776e+02, -1.0747e+02],\n         [-2.7152e+02, -6.0196e+02, -2.0326e+02,  ...,  1.4765e+02,\n          -2.7204e+02, -2.7630e+02],\n         [-2.6490e+02,  5.4918e+01,  7.1700e+02,  ..., -3.3160e+02,\n          -2.0617e+02, -5.9447e+02],\n         ...,\n         [-3.3142e+01,  7.7949e+01, -9.3131e+01,  ..., -6.5361e+01,\n           7.3692e+01, -2.1064e+01],\n         [ 9.9726e-01,  3.9507e+01, -1.3076e+02,  ..., -4.3385e+01,\n           5.3365e+01, -9.7434e+01],\n         [-2.4557e+00,  1.0613e+02, -7.7452e+01,  ..., -2.7495e+00,\n           1.2537e+02, -6.9637e+01]],\n\n        [[ 6.3051e+00,  1.7268e+02, -6.9356e+01,  ...,  2.4473e+01,\n          -4.2627e+02,  4.5806e+01],\n         [ 1.1882e+02,  1.3553e+02, -3.2633e+02,  ..., -1.7919e+02,\n          -4.6435e+02,  1.8874e+01],\n         [-2.7603e+02,  3.8684e+01,  2.5643e+02,  ..., -5.2122e+02,\n           1.0112e+01,  1.5682e+02],\n         ...,\n         [ 2.1990e+02, -2.1949e+01, -6.0644e+01,  ..., -1.0697e+02,\n          -2.7722e+02, -1.0381e+02],\n         [ 1.3988e+02, -2.9837e+01,  3.7931e+01,  ..., -1.0546e+02,\n          -2.4771e+02, -5.1013e-01],\n         [ 1.4580e+02,  1.3956e+02, -2.0875e+01,  ..., -1.0689e+02,\n          -2.4926e+02,  2.3941e+01]]], device='cuda:0', grad_fn=<AddBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= T5Block(\n  (layer): ModuleList(\n    (0): T5LayerSelfAttention(\n      (SelfAttention): T5Attention(\n        (q): Linear(in_features=768, out_features=768, bias=False)\n        (k): Linear(in_features=768, out_features=768, bias=False)\n        (v): Linear(in_features=768, out_features=768, bias=False)\n        (o): Linear(in_features=768, out_features=768, bias=False)\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (1): T5LayerCrossAttention(\n      (EncDecAttention): T5Attention(\n        (q): Linear(in_features=768, out_features=768, bias=False)\n        (k): Linear(in_features=768, out_features=768, bias=False)\n        (v): Linear(in_features=768, out_features=768, bias=False)\n        (o): Linear(in_features=768, out_features=768, bias=False)\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (2): T5LayerFF(\n      (DenseReluDense): T5DenseActDense(\n        (wi): Linear(in_features=768, out_features=3072, bias=False)\n        (wo): Linear(in_features=3072, out_features=768, bias=False)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (act): ReLU()\n      )\n      (layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self=T5LayerFF(\n  (DenseReluDense): T5DenseActDense(\n...rm()\n  (dropout): Dropout(p=0.1, inplace=False)\n), *args=(tensor([[[-4.5808e+02,  3.2732e+02, -1.6407e+02,...e+01]]], device='cuda:0', grad_fn=<AddBackward0>),), **kwargs={})\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n        forward_call \u001b[0;34m= <bound method T5LayerFF.forward of T5LayerFF(\n  (DenseReluDense): T5DenseActDense(\n    (wi): Linear(in_features=768, out_features=3072, bias=False)\n    (wo): Linear(in_features=3072, out_features=768, bias=False)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (act): ReLU()\n  )\n  (layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)>\u001b[0m\u001b[0;34m\n        \u001b[0margs \u001b[0;34m= (tensor([[[-4.5808e+02,  3.2732e+02, -1.6407e+02,  ...,  3.1219e+02,\n          -5.1455e+02, -6.2220e+02],\n         [ 6.1342e+00, -7.0963e+01,  6.5572e+01,  ..., -5.9832e+02,\n          -1.1672e+02, -7.1259e+02],\n         [-7.6225e+01, -1.3903e+02, -4.3484e+02,  ..., -5.9708e+02,\n           5.3288e+00, -5.6920e+02],\n         ...,\n         [-1.6408e+02,  1.5863e+02, -1.2699e+02,  ...,  1.9416e+02,\n           2.1248e+01, -3.0047e+02],\n         [-1.5955e+02,  7.1735e+01, -5.7054e+01,  ...,  1.7898e+02,\n          -4.7328e+02, -3.3234e+02],\n         [-2.3054e+02,  9.3526e+01, -4.9391e+01,  ...,  4.1012e+01,\n          -3.4604e+02, -3.2630e+02]],\n\n        [[ 3.8198e+01,  1.1505e+02,  6.1853e+00,  ...,  1.6369e+01,\n           2.0698e+02,  2.1272e+02],\n         [-6.0571e+01,  6.5898e+01,  2.8919e+02,  ..., -5.8921e+02,\n          -1.5657e+02,  2.7985e+01],\n         [-5.6652e+02, -7.2726e+00, -4.8016e+02,  ..., -4.5730e+02,\n          -1.3149e+02,  6.7482e+02],\n         ...,\n         [ 1.5353e+02,  7.8715e+01,  6.9392e+01,  ..., -4.3843e+01,\n           1.1915e+02,  2.4657e+01],\n         [ 1.7362e+02,  1.1958e+02,  9.8289e+01,  ...,  4.2402e+01,\n           2.6104e+02,  2.5017e+02],\n         [ 1.3596e+02,  6.3303e+01,  3.5854e+01,  ..., -1.2650e+02,\n           1.2770e+02,  1.7447e+02]],\n\n        [[ 2.3934e+02, -2.9502e+01, -2.0509e+02,  ...,  5.7248e+01,\n          -2.2415e+02,  1.2888e+01],\n         [ 6.1794e+02, -2.8721e+02, -6.8688e+02,  ...,  2.4867e+02,\n          -4.1648e+02, -5.1873e+02],\n         [-1.1572e+02, -6.3932e+02, -7.1134e+02,  ..., -2.3844e+02,\n           2.5559e+02,  4.7764e+02],\n         ...,\n         [ 1.1005e+02, -1.4408e+02,  5.1128e+01,  ..., -1.5154e+02,\n          -2.1335e+02, -2.3279e+02],\n         [ 7.5258e+01, -1.8612e+02,  1.0054e+02,  ..., -1.5527e+01,\n          -2.9601e+02, -1.3669e+02],\n         [ 6.7095e+01, -1.5715e+02,  9.8779e+01,  ..., -1.1569e+02,\n          -2.4653e+02, -1.3091e+02]],\n\n        ...,\n\n        [[-2.2087e+02,  2.0763e+02, -3.6474e+02,  ..., -2.2249e+02,\n          -1.6606e+02, -5.1205e+01],\n         [-5.9239e+02, -1.0025e+02, -1.5515e+02,  ...,  4.9349e+01,\n          -3.7339e+02, -1.4879e+02],\n         [-2.4341e+02,  6.0751e+02, -5.7049e+02,  ...,  1.6773e+02,\n          -8.8015e+02, -7.2333e+02],\n         ...,\n         [-4.2716e+01,  3.3967e+01, -5.1713e+01,  ..., -2.0721e+02,\n          -9.7786e+01,  1.6162e-01],\n         [-1.1289e+02, -2.1159e+00, -6.1984e+01,  ..., -2.5941e+02,\n          -1.9961e+02, -1.3074e+01],\n         [-8.1257e+01,  6.5676e+01, -5.8484e+01,  ..., -2.6540e+02,\n          -1.6826e+02,  2.7531e+01]],\n\n        [[-4.0654e+01,  3.0270e+02, -1.0605e+02,  ...,  1.4455e+02,\n           3.5776e+02, -1.0747e+02],\n         [-2.7152e+02, -6.0196e+02, -2.0326e+02,  ...,  1.4765e+02,\n          -2.7204e+02, -2.7630e+02],\n         [-2.6490e+02,  5.4918e+01,  7.1700e+02,  ..., -3.3160e+02,\n          -2.0617e+02, -5.9447e+02],\n         ...,\n         [-3.3142e+01,  7.7949e+01, -9.3131e+01,  ..., -6.5361e+01,\n           7.3692e+01, -2.1064e+01],\n         [ 9.9726e-01,  3.9507e+01, -1.3076e+02,  ..., -4.3385e+01,\n           5.3365e+01, -9.7434e+01],\n         [-2.4557e+00,  1.0613e+02, -7.7452e+01,  ..., -2.7495e+00,\n           1.2537e+02, -6.9637e+01]],\n\n        [[ 6.3051e+00,  1.7268e+02, -6.9356e+01,  ...,  2.4473e+01,\n          -4.2627e+02,  4.5806e+01],\n         [ 1.1882e+02,  1.3553e+02, -3.2633e+02,  ..., -1.7919e+02,\n          -4.6435e+02,  1.8874e+01],\n         [-2.7603e+02,  3.8684e+01,  2.5643e+02,  ..., -5.2122e+02,\n           1.0112e+01,  1.5682e+02],\n         ...,\n         [ 2.1990e+02, -2.1949e+01, -6.0644e+01,  ..., -1.0697e+02,\n          -2.7722e+02, -1.0381e+02],\n         [ 1.3988e+02, -2.9837e+01,  3.7931e+01,  ..., -1.0546e+02,\n          -2.4771e+02, -5.1013e-01],\n         [ 1.4580e+02,  1.3956e+02, -2.0875e+01,  ..., -1.0689e+02,\n          -2.4926e+02,  2.3941e+01]]], device='cuda:0', grad_fn=<AddBackward0>),)\u001b[0m\u001b[0;34m\n        \u001b[0mkwargs \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:343\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self=T5LayerFF(\n  (DenseReluDense): T5DenseActDense(\n...rm()\n  (dropout): Dropout(p=0.1, inplace=False)\n), hidden_states=tensor([[[-4.5808e+02,  3.2732e+02, -1.6407e+02,...e+01]]], device='cuda:0', grad_fn=<AddBackward0>))\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    342\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 343\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n        forwarded_states \u001b[0;34m= tensor([[[-2.4402e+00,  2.0900e+00, -1.1311e+00,  ...,  1.7192e+00,\n          -2.5901e+00, -3.3132e+00],\n         [ 4.8126e-02, -6.6733e-01,  6.6576e-01,  ..., -4.8524e+00,\n          -8.6529e-01, -5.5884e+00],\n         [-6.4487e-01, -1.4099e+00, -4.7609e+00,  ..., -5.2217e+00,\n           4.2601e-02, -4.8136e+00],\n         ...,\n         [-1.0483e+00,  1.2148e+00, -1.0500e+00,  ...,  1.2823e+00,\n           1.2828e-01, -1.9189e+00],\n         [-1.0411e+00,  5.6106e-01, -4.8179e-01,  ...,  1.2072e+00,\n          -2.9182e+00, -2.1677e+00],\n         [-1.6431e+00,  7.9897e-01, -4.5554e-01,  ...,  3.0215e-01,\n          -2.3305e+00, -2.3246e+00]],\n\n        [[ 1.7208e-01,  6.2124e-01,  3.6059e-02,  ...,  7.6225e-02,\n           8.8105e-01,  9.5789e-01],\n         [-3.5024e-01,  4.5674e-01,  2.1640e+00,  ..., -3.5219e+00,\n          -8.5549e-01,  1.6175e-01],\n         [-3.5961e+00, -5.5334e-02, -3.9444e+00,  ..., -3.0007e+00,\n          -7.8868e-01,  4.2818e+00],\n         ...,\n         [ 1.2198e+00,  7.4965e-01,  7.1352e-01,  ..., -3.6010e-01,\n           8.9460e-01,  1.9583e-01],\n         [ 1.2706e+00,  1.0489e+00,  9.3088e-01,  ...,  3.2077e-01,\n           1.8052e+00,  1.8301e+00],\n         [ 1.0867e+00,  6.0648e-01,  3.7087e-01,  ..., -1.0452e+00,\n           9.6454e-01,  1.3940e+00]],\n\n        [[ 1.2804e+00, -1.8918e-01, -1.4199e+00,  ...,  3.1659e-01,\n          -1.1331e+00,  6.8921e-02],\n         [ 4.4527e+00, -2.4806e+00, -6.4053e+00,  ...,  1.8523e+00,\n          -2.8358e+00, -3.7363e+00],\n         [-9.1369e-01, -6.0504e+00, -7.2682e+00,  ..., -1.9461e+00,\n           1.9069e+00,  3.7696e+00],\n         ...,\n         [ 8.0063e-01, -1.2564e+00,  4.8139e-01,  ..., -1.1397e+00,\n          -1.4667e+00, -1.6929e+00],\n         [ 5.3848e-01, -1.5962e+00,  9.3099e-01,  ..., -1.1485e-01,\n          -2.0014e+00, -9.7767e-01],\n         [ 4.9715e-01, -1.3957e+00,  9.4720e-01,  ..., -8.8614e-01,\n          -1.7261e+00, -9.6960e-01]],\n\n        ...,\n\n        [[-9.9575e-01,  1.1220e+00, -2.1280e+00,  ..., -1.0369e+00,\n          -7.0742e-01, -2.3075e-01],\n         [-3.7874e+00, -7.6828e-01, -1.2837e+00,  ...,  3.2614e-01,\n          -2.2558e+00, -9.5088e-01],\n         [-1.2580e+00,  3.7636e+00, -3.8158e+00,  ...,  8.9610e-01,\n          -4.2985e+00, -3.7370e+00],\n         ...,\n         [-2.7555e-01,  2.6264e-01, -4.3171e-01,  ..., -1.3817e+00,\n          -5.9606e-01,  1.0422e-03],\n         [-7.2751e-01, -1.6345e-02, -5.1695e-01,  ..., -1.7281e+00,\n          -1.2156e+00, -8.4222e-02],\n         [-5.4700e-01,  5.2993e-01, -5.0949e-01,  ..., -1.8469e+00,\n          -1.0703e+00,  1.8526e-01]],\n\n        [[-2.0859e-01,  1.8617e+00, -7.0418e-01,  ...,  7.6668e-01,\n           1.7346e+00, -5.5120e-01],\n         [-1.9697e+00, -5.2342e+00, -1.9082e+00,  ...,  1.1072e+00,\n          -1.8648e+00, -2.0035e+00],\n         [-1.7188e+00,  4.2712e-01,  6.0206e+00,  ..., -2.2241e+00,\n          -1.2641e+00, -3.8557e+00],\n         ...,\n         [-2.0131e-01,  5.6752e-01, -7.3208e-01,  ..., -4.1040e-01,\n           4.2297e-01, -1.2789e-01],\n         [ 6.1457e-03,  2.9183e-01, -1.0428e+00,  ..., -2.7638e-01,\n           3.1076e-01, -6.0020e-01],\n         [-1.4121e-02,  7.3147e-01, -5.7634e-01,  ..., -1.6343e-02,\n           6.8120e-01, -4.0025e-01]],\n\n        [[ 3.1523e-02,  1.0348e+00, -4.4874e-01,  ...,  1.2648e-01,\n          -2.0138e+00,  2.2892e-01],\n         [ 9.9459e-01,  1.3598e+00, -3.5350e+00,  ..., -1.5505e+00,\n          -3.6729e+00,  1.5792e-01],\n         [-2.2491e+00,  3.7782e-01,  2.7041e+00,  ..., -4.3902e+00,\n           7.7859e-02,  1.2773e+00],\n         ...,\n         [ 1.6515e+00, -1.9758e-01, -5.8941e-01,  ..., -8.3042e-01,\n          -1.9673e+00, -7.7933e-01],\n         [ 1.0684e+00, -2.7316e-01,  3.7492e-01,  ..., -8.3268e-01,\n          -1.7878e+00, -3.8947e-03],\n         [ 1.0129e+00,  1.1621e+00, -1.8767e-01,  ..., -7.6757e-01,\n          -1.6362e+00,  1.6625e-01]]], device='cuda:0', grad_fn=<MulBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= T5LayerFF(\n  (DenseReluDense): T5DenseActDense(\n    (wi): Linear(in_features=768, out_features=3072, bias=False)\n    (wo): Linear(in_features=3072, out_features=768, bias=False)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (act): ReLU()\n  )\n  (layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)\u001b[0m\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self=T5DenseActDense(\n  (wi): Linear(in_features=768,...: Dropout(p=0.1, inplace=False)\n  (act): ReLU()\n), *args=(tensor([[[-2.4402e+00,  2.0900e+00, -1.1311e+00,...e-01]]], device='cuda:0', grad_fn=<MulBackward0>),), **kwargs={})\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n        forward_call \u001b[0;34m= <bound method T5DenseActDense.forward of T5DenseActDense(\n  (wi): Linear(in_features=768, out_features=3072, bias=False)\n  (wo): Linear(in_features=3072, out_features=768, bias=False)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (act): ReLU()\n)>\u001b[0m\u001b[0;34m\n        \u001b[0margs \u001b[0;34m= (tensor([[[-2.4402e+00,  2.0900e+00, -1.1311e+00,  ...,  1.7192e+00,\n          -2.5901e+00, -3.3132e+00],\n         [ 4.8126e-02, -6.6733e-01,  6.6576e-01,  ..., -4.8524e+00,\n          -8.6529e-01, -5.5884e+00],\n         [-6.4487e-01, -1.4099e+00, -4.7609e+00,  ..., -5.2217e+00,\n           4.2601e-02, -4.8136e+00],\n         ...,\n         [-1.0483e+00,  1.2148e+00, -1.0500e+00,  ...,  1.2823e+00,\n           1.2828e-01, -1.9189e+00],\n         [-1.0411e+00,  5.6106e-01, -4.8179e-01,  ...,  1.2072e+00,\n          -2.9182e+00, -2.1677e+00],\n         [-1.6431e+00,  7.9897e-01, -4.5554e-01,  ...,  3.0215e-01,\n          -2.3305e+00, -2.3246e+00]],\n\n        [[ 1.7208e-01,  6.2124e-01,  3.6059e-02,  ...,  7.6225e-02,\n           8.8105e-01,  9.5789e-01],\n         [-3.5024e-01,  4.5674e-01,  2.1640e+00,  ..., -3.5219e+00,\n          -8.5549e-01,  1.6175e-01],\n         [-3.5961e+00, -5.5334e-02, -3.9444e+00,  ..., -3.0007e+00,\n          -7.8868e-01,  4.2818e+00],\n         ...,\n         [ 1.2198e+00,  7.4965e-01,  7.1352e-01,  ..., -3.6010e-01,\n           8.9460e-01,  1.9583e-01],\n         [ 1.2706e+00,  1.0489e+00,  9.3088e-01,  ...,  3.2077e-01,\n           1.8052e+00,  1.8301e+00],\n         [ 1.0867e+00,  6.0648e-01,  3.7087e-01,  ..., -1.0452e+00,\n           9.6454e-01,  1.3940e+00]],\n\n        [[ 1.2804e+00, -1.8918e-01, -1.4199e+00,  ...,  3.1659e-01,\n          -1.1331e+00,  6.8921e-02],\n         [ 4.4527e+00, -2.4806e+00, -6.4053e+00,  ...,  1.8523e+00,\n          -2.8358e+00, -3.7363e+00],\n         [-9.1369e-01, -6.0504e+00, -7.2682e+00,  ..., -1.9461e+00,\n           1.9069e+00,  3.7696e+00],\n         ...,\n         [ 8.0063e-01, -1.2564e+00,  4.8139e-01,  ..., -1.1397e+00,\n          -1.4667e+00, -1.6929e+00],\n         [ 5.3848e-01, -1.5962e+00,  9.3099e-01,  ..., -1.1485e-01,\n          -2.0014e+00, -9.7767e-01],\n         [ 4.9715e-01, -1.3957e+00,  9.4720e-01,  ..., -8.8614e-01,\n          -1.7261e+00, -9.6960e-01]],\n\n        ...,\n\n        [[-9.9575e-01,  1.1220e+00, -2.1280e+00,  ..., -1.0369e+00,\n          -7.0742e-01, -2.3075e-01],\n         [-3.7874e+00, -7.6828e-01, -1.2837e+00,  ...,  3.2614e-01,\n          -2.2558e+00, -9.5088e-01],\n         [-1.2580e+00,  3.7636e+00, -3.8158e+00,  ...,  8.9610e-01,\n          -4.2985e+00, -3.7370e+00],\n         ...,\n         [-2.7555e-01,  2.6264e-01, -4.3171e-01,  ..., -1.3817e+00,\n          -5.9606e-01,  1.0422e-03],\n         [-7.2751e-01, -1.6345e-02, -5.1695e-01,  ..., -1.7281e+00,\n          -1.2156e+00, -8.4222e-02],\n         [-5.4700e-01,  5.2993e-01, -5.0949e-01,  ..., -1.8469e+00,\n          -1.0703e+00,  1.8526e-01]],\n\n        [[-2.0859e-01,  1.8617e+00, -7.0418e-01,  ...,  7.6668e-01,\n           1.7346e+00, -5.5120e-01],\n         [-1.9697e+00, -5.2342e+00, -1.9082e+00,  ...,  1.1072e+00,\n          -1.8648e+00, -2.0035e+00],\n         [-1.7188e+00,  4.2712e-01,  6.0206e+00,  ..., -2.2241e+00,\n          -1.2641e+00, -3.8557e+00],\n         ...,\n         [-2.0131e-01,  5.6752e-01, -7.3208e-01,  ..., -4.1040e-01,\n           4.2297e-01, -1.2789e-01],\n         [ 6.1457e-03,  2.9183e-01, -1.0428e+00,  ..., -2.7638e-01,\n           3.1076e-01, -6.0020e-01],\n         [-1.4121e-02,  7.3147e-01, -5.7634e-01,  ..., -1.6343e-02,\n           6.8120e-01, -4.0025e-01]],\n\n        [[ 3.1523e-02,  1.0348e+00, -4.4874e-01,  ...,  1.2648e-01,\n          -2.0138e+00,  2.2892e-01],\n         [ 9.9459e-01,  1.3598e+00, -3.5350e+00,  ..., -1.5505e+00,\n          -3.6729e+00,  1.5792e-01],\n         [-2.2491e+00,  3.7782e-01,  2.7041e+00,  ..., -4.3902e+00,\n           7.7859e-02,  1.2773e+00],\n         ...,\n         [ 1.6515e+00, -1.9758e-01, -5.8941e-01,  ..., -8.3042e-01,\n          -1.9673e+00, -7.7933e-01],\n         [ 1.0684e+00, -2.7316e-01,  3.7492e-01,  ..., -8.3268e-01,\n          -1.7878e+00, -3.8947e-03],\n         [ 1.0129e+00,  1.1621e+00, -1.8767e-01,  ..., -7.6757e-01,\n          -1.6362e+00,  1.6625e-01]]], device='cuda:0', grad_fn=<MulBackward0>),)\u001b[0m\u001b[0;34m\n        \u001b[0mkwargs \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:296\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[0;34m(self=T5DenseActDense(\n  (wi): Linear(in_features=768,...: Dropout(p=0.1, inplace=False)\n  (act): ReLU()\n), hidden_states=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n       ...=torch.float16, grad_fn=<NativeDropoutBackward0>))\u001b[0m\n\u001b[1;32m    290\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    292\u001b[0m     \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo\u001b[39m.\u001b[39mweight, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m    293\u001b[0m     \u001b[39mand\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    294\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mint8\n\u001b[1;32m    295\u001b[0m ):\n\u001b[0;32m--> 296\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwo\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdtype)\n        hidden_states \u001b[0;34m= tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n       dtype=torch.float16, grad_fn=<NativeDropoutBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= T5DenseActDense(\n  (wi): Linear(in_features=768, out_features=3072, bias=False)\n  (wo): Linear(in_features=3072, out_features=768, bias=False)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (act): ReLU()\n)\u001b[0m\n\u001b[1;32m    297\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo(hidden_states)\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.70 GiB total capacity; 20.39 GiB already allocated; 50.25 MiB free; 22.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = train(model, 32, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treinapython39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
