{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula6 - Doc2Query\n",
    "\n",
    "[Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
    "\n",
    "Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmRLgbyi_Dvg"
   },
   "source": [
    "# Organizando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRETORIO_TRABALHO = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local/doc2query'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DIRETORIO_TRABALHO), f\"Path para {DIRETORIO_TRABALHO} não existe!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DKAZ8CWCAM3-"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9XgIWvkkH-kn"
   },
   "outputs": [],
   "source": [
    "def mostra_memoria(lista_mem=['cpu']):\n",
    "  \"\"\"\n",
    "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
    "\n",
    "  Parâmetros:\n",
    "  -----------\n",
    "  lista_mem : list, opcional\n",
    "      Lista com strings 'cpu' e/ou 'gpu'. \n",
    "      'cpu' - exibe informações de memória da CPU.\n",
    "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
    "      O valor padrão é ['cpu'].\n",
    "\n",
    "  Saída:\n",
    "  -------\n",
    "  A função não retorna nada, apenas exibe as informações na tela.\n",
    "\n",
    "  Exemplo de uso:\n",
    "  ---------------\n",
    "  Para exibir informações de memória da CPU:\n",
    "      mostra_memoria(['cpu'])\n",
    "\n",
    "  Para exibir informações de memória da CPU e GPU:\n",
    "      mostra_memoria(['cpu', 'gpu'])\n",
    "  \n",
    "  Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "  \"\"\"  \n",
    "  if 'cpu' in lista_mem:\n",
    "    vm = virtual_memory()\n",
    "    ram={}\n",
    "    ram['total']=round(vm.total / 1e9,2)\n",
    "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
    "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
    "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
    "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
    "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
    "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
    "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
    "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
    "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
    "    print('/nGPU')\n",
    "    gpu_info = !nvidia-smi\n",
    "  if 'gpu' in lista_mem:\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "      print('Not connected to a GPU')\n",
    "    else:\n",
    "      print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dri9iiMAvCT",
    "outputId": "53aebd5a-e29f-4c8e-d233-5221aae9f9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 56.5\n",
      " used 9.57\n",
      " free 5.8\n",
      " cached 50.07\n",
      " buffers 1.92\n",
      "/nGPU\n",
      "Sun Apr  9 23:23:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 70%   54C    P8    27W / 370W |     61MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1241      G   /usr/lib/xorg/Xorg                 45MiB |\n",
      "|    0   N/A  N/A      1381      G   /usr/bin/gnome-shell               14MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achvQ78sa3p3"
   },
   "source": [
    "## Fixando as seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AG9RjMb8Qlot"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bkETIyWGkbOf"
   },
   "outputs": [],
   "source": [
    "def inicializa_seed(num_semente:int=123):\n",
    "  \"\"\"\n",
    "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
    "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
    "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
    "  \n",
    "  Args:\n",
    "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
    "  \n",
    "  References:\n",
    "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
    "  \"\"\"\n",
    "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
    "  random.seed(num_semente)\n",
    "  np.random.seed(num_semente)\n",
    "  torch.manual_seed(num_semente)\n",
    "  \n",
    "  # Define as sementes da GPU\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  #torch.cuda.manual_seed(num_semente)\n",
    "  #Cuda algorithms\n",
    "  #torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ViMcw_kVkbOf"
   },
   "outputs": [],
   "source": [
    "num_semente=123\n",
    "inicializa_seed(num_semente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v2gtkEPhA0t"
   },
   "source": [
    "## Preparando para debug e display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BJ6S4P5Hw4iG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kebsl1uQDFUf",
    "outputId": "4b1ed269-722b-4a2a-f1a0-908d1683cc62"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rnR2kDS_2FgZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZEqQ7mKg5fs"
   },
   "source": [
    "https://zohaib.me/debugging-in-google-collab-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjrlXHq1hC8n",
    "outputId": "18d94254-4bf8-4632-ef84-012727944746"
   },
   "outputs": [],
   "source": [
    "# !pip install -Uqq ipdb\n",
    "import ipdb\n",
    "# %pdb off # desativa debug em exceção\n",
    "# %pdb on  # ativa debug em exceção\n",
    "# ipdb.set_trace(context=8)  para execução nesse ponto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wQ5pmlOHxHhk"
   },
   "outputs": [],
   "source": [
    "def config_display():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de display do Pandas.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configurando formato saída Pandas\n",
    "  # define o número máximo de colunas que serão exibidas\n",
    "  pd.options.display.max_columns = None\n",
    "\n",
    "  # define a largura máxima de uma linha\n",
    "  pd.options.display.width = 1000\n",
    "\n",
    "  # define o número máximo de linhas que serão exibidas\n",
    "  pd.options.display.max_rows = 100\n",
    "\n",
    "  # define o número máximo de caracteres por coluna\n",
    "  pd.options.display.max_colwidth = 50\n",
    "\n",
    "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
    "  pd.options.display.show_dimensions = True\n",
    "\n",
    "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
    "  pd.options.display.precision = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b2tDy72ATNHs"
   },
   "outputs": [],
   "source": [
    "def config_debug():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
    "  transformers e datasets.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define opções de impressão de tensores para o modo científico\n",
    "  torch.set_printoptions(sci_mode=True) \n",
    "  \"\"\"\n",
    "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
    "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
    "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
    "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
    "    uma melhor compreensão dos números envolvidos.  \n",
    "  \"\"\"\n",
    "\n",
    "  # Habilita detecção de anomalias no autograd do PyTorch\n",
    "  torch.autograd.set_detect_anomaly(True)\n",
    "  \"\"\"\n",
    "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
    "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
    "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
    "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
    "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
    "    antes que se torne um problema maior.\n",
    "\n",
    "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
    "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
    "    ela deve ser usada com cautela e apenas para depuração.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
    "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "  \"\"\"\n",
    "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
    "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
    "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
    "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
    "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
    "    e removê-lo após a solução do problema.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define o nível de verbosity do pacote transformers para info\n",
    "  # transformers.utils.logging.set_verbosity_info() \n",
    "  \n",
    "  \n",
    "  \"\"\"\n",
    "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
    "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
    "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
    "\n",
    "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
    "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
    "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
    "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
    "    necessidade de cada tarefa.\n",
    "  \n",
    "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
    "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
    "  \n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Define o modo verbose do xmode, que é utilizado no debug\n",
    "  # %xmode Verbose \n",
    "\n",
    "  \"\"\"\n",
    "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
    "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
    "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
    "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
    "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
    "\n",
    "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
    "    comentar a linha acima e descomentar a linha abaixo:\n",
    "    %xmode Plain\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "    Dica:\n",
    "    1.  pdb (Python Debugger)\n",
    "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
    "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
    "\n",
    "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
    "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
    "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
    "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
    "\n",
    "\n",
    "    2. ipdb\n",
    "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
    "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
    "      \n",
    "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
    "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
    "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
    "      comandos para investigar o comportamento.\n",
    "\n",
    "      Durante a depuração, você pode usar comandos:\n",
    "        next (para executar a próxima linha de código), \n",
    "        step (para entrar em uma função chamada na próxima linha de código) \n",
    "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
    "\n",
    "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
    "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
    "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Tb4aqtcExR84"
   },
   "outputs": [],
   "source": [
    "config_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5Bq4043fkfh",
    "outputId": "fa8e5db1-1feb-4393-fb66-d394d1ad693c"
   },
   "outputs": [],
   "source": [
    "config_debug()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga dos dados msmarco_triples.train.tiny.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(f\"{DIRETORIO_TRABALHO}/msmarco_triples.train.tiny.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DIRETORIO_TRABALHO}/msmarco_triples.train.tiny.tsv\"):\n",
    "    !wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
    "    !mv msmarco_triples.train.tiny.tsv {DIRETORIO_TRABALHO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{DIRETORIO_TRABALHO}/msmarco_triples.train.tiny.tsv\", delimiter=\"\\t\", \n",
    "                 header=None, names=[\"query\", \"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "igtPldcHPwCS",
    "outputId": "594683c5-90e1-43d7-c00e-8d066b3fe161"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is a little caffeine ok during pregnancy</td>\n",
       "      <td>We donât know a lot about the effects of caf...</td>\n",
       "      <td>It is generally safe for pregnant women to eat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what fruit is native to australia</td>\n",
       "      <td>Passiflora herbertiana. A rare passion fruit n...</td>\n",
       "      <td>The kola nut is the fruit of the kola tree, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how large is the canadian military</td>\n",
       "      <td>The Canadian Armed Forces. 1  The first large-...</td>\n",
       "      <td>The Canadian Physician Health Institute (CPHI)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>types of fruit trees</td>\n",
       "      <td>Cherry. Cherry trees are found throughout the ...</td>\n",
       "      <td>The kola nut is the fruit of the kola tree, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how many calories a day are lost breastfeeding</td>\n",
       "      <td>Not only is breastfeeding better for the baby,...</td>\n",
       "      <td>However, you still need some niacin each day; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query                                           positive                                           negative\n",
       "0        is a little caffeine ok during pregnancy  We donât know a lot about the effects of caf...  It is generally safe for pregnant women to eat...\n",
       "1               what fruit is native to australia  Passiflora herbertiana. A rare passion fruit n...  The kola nut is the fruit of the kola tree, a ...\n",
       "2              how large is the canadian military  The Canadian Armed Forces. 1  The first large-...  The Canadian Physician Health Institute (CPHI)...\n",
       "3                            types of fruit trees  Cherry. Cherry trees are found throughout the ...  The kola nut is the fruit of the kola tree, a ...\n",
       "4  how many calories a day are lost breastfeeding  Not only is breastfeeding better for the baby,...  However, you still need some niacin each day; ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ua7i1BCTRTGx",
    "outputId": "7e8162d8-a630-4014-9e2c-057d66a94768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query       object\n",
       "positive    object\n",
       "negative    object\n",
       "Length: 3, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ3S90BBTrKJ"
   },
   "source": [
    "Verificando correção do arquivo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qm7ss2BfTlOu",
    "outputId": "a7140122-cf21-46cd-cb04-decdcf3cb959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query       0\n",
      "positive    0\n",
      "negative    0\n",
      "Length: 3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q ftfy\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'We donât know a lot about the effects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We don't know a lot about the effects\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftfy.fix_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query        34.2256364\n",
       "positive    353.7535455\n",
       "negative    340.4646364\n",
       "Length: 3, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.applymap(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pois treinaremos doc2query apenas para geração de queries relevantes\n",
    "del df['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Lg0b1wj0SEsY"
   },
   "outputs": [],
   "source": [
    "df['query'] = df['query'].apply(ftfy.fix_text)\n",
    "df['positive'] = df['positive'].apply(ftfy.fix_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "igtPldcHPwCS",
    "outputId": "594683c5-90e1-43d7-c00e-8d066b3fe161"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is a little caffeine ok during pregnancy</td>\n",
       "      <td>We don't know a lot about the effects of caffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what fruit is native to australia</td>\n",
       "      <td>Passiflora herbertiana. A rare passion fruit n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how large is the canadian military</td>\n",
       "      <td>The Canadian Armed Forces. 1  The first large-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>types of fruit trees</td>\n",
       "      <td>Cherry. Cherry trees are found throughout the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how many calories a day are lost breastfeeding</td>\n",
       "      <td>Not only is breastfeeding better for the baby,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query                                           positive\n",
       "0        is a little caffeine ok during pregnancy  We don't know a lot about the effects of caffe...\n",
       "1               what fruit is native to australia  Passiflora herbertiana. A rare passion fruit n...\n",
       "2              how large is the canadian military  The Canadian Armed Forces. 1  The first large-...\n",
       "3                            types of fruit trees  Cherry. Cherry trees are found throughout the ...\n",
       "4  how many calories a day are lost breastfeeding  Not only is breastfeeding better for the baby,...\n",
       "\n",
       "[5 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisão em treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(df['positive'].values, \n",
    "                                                      df['query'].values,\n",
    "                                                      test_size=1000, \n",
    "                                                      random_state=num_semente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (1000,), 'how many fitbit steps equal a mile')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_valid), Y_valid.shape, Y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (10000,),\n",
       " 'There are 40 weeks in a school year - and 12 weeks of holidays. Unless you live somewhere snowy and you have to take snow days. These can extend the length by another week or so. There are 40 weeks in a school year - and 12 weeks of holidays. Unless you live somewhere snowy and you have to take snow days.')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), X_train.shape, X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando o tokenizador do t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, None, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id,tokenizer.cls_token_id,tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='it is just a test!'\n",
    "y='it is just a continuation!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 19, 131, 3, 9, 794, 55, 1, 34, 19, 131, 3, 9, 25192, 55, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode_plus(x,y)['input_ids']\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'just', '', 'a', 'test', '!', '</s>', 'it', 'is', 'just', '', 'a', 'continuation', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34, 19, 131, 3, 9, 794, 55, 1], [34, 19, 131, 3, 9, 25192, 55, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([x,y])['input_ids']\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '<unk>', '<pad>', '<extra_id_0>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511, 512)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence, tokenizer.model_max_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "      Classe para criar um dataset de texto e query.\n",
    "    \"\"\"  \n",
    "    def __init__(self, texts: np.ndarray, queries:np.ndarray, tokenizer):\n",
    "      \"\"\"\n",
    "      Inicializa um novo objeto MyDataset.\n",
    "\n",
    "      Args:\n",
    "          texts (np.ndarray): um array com as strings de texto. Cada linha deve ter 2 strings.\n",
    "          tokenizer: um objeto tokenizer do Hugging Face Transformers.\n",
    "          max_seq_length (int): o tamanho máximo da sequência a ser considerado.\n",
    "      Raises:\n",
    "          AssertionError: se os parâmetros não estiverem no formato esperado.\n",
    "      \"\"\"\n",
    "      # Verifica se os parâmetros são do tipo esperado\n",
    "      assert isinstance(texts, np.ndarray), f\"Parâmetro texts deve ser do tipo np.ndarray e não {type(texts)}\"\n",
    "      assert isinstance(queries, np.ndarray), f\"Parâmetro queries deve ser do tipo np.ndarray e não {type(queries)}\"\n",
    "      for row in texts:\n",
    "          assert isinstance(row, str), f\"Each element in texts.row must be a string e não {type(row)}\"\n",
    "          break\n",
    "\n",
    "      self.max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "      # Salvar os dados dos tensores para adiantar o tempo de processamento\n",
    "      self.tokenized_texts = tokenizer.batch_encode_plus(texts, return_length=True)\n",
    "      self.tokenized_queries = tokenizer.batch_encode_plus(queries, return_attention_mask=False, return_length=True)\n",
    "      \n",
    "      print(\"tokenized_texts size stats:\\n{}\\n\".format(stats.describe(self.tokenized_texts['length'])))\n",
    "      print(\"tokenized_queries size stats:\\n{}\\n\".format(stats.describe(self.tokenized_queries['length']))) \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "          Retorna o tamanho do dataset (= tamanho do array texts)\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "          Retorna um dicionário com os dados do texto e sua classe correspondente, em um formato que pode \n",
    "          ser usado pelo dataloader do PyTorch para alimentar um modelo de aprendizado de máquina.\n",
    "        \"\"\"\n",
    "        # print(f\"getitem index={index} self.tokenized_texts['input_ids'][index] {self.tokenized_texts['input_ids'][index]}\")\n",
    "        saida = {\"input_ids\": self.tokenized_texts[\"input_ids\"][index], \n",
    "                \"attention_mask\": self.tokenized_texts[\"attention_mask\"][index], \n",
    "                \"labels\": self.tokenized_texts[\"input_ids\"][index]} \n",
    "        # print(f\"saida {saida}\")\n",
    "        return saida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17AcNhe2C4tb"
   },
   "source": [
    "#### Testando o MyDataset e o Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_tNkc8kmC8ie"
   },
   "outputs": [],
   "source": [
    "# Cria dados fictícios\n",
    "texts = np.array(['This is the first text',\n",
    "                  'This is text 2.1',\n",
    "                  'This is text 3.1',\n",
    "                  'This is text 4.1',\n",
    "                  'This is text 5.1',\n",
    "                  'This is text 6.1',\n",
    "                  'This is text 7.1'])\n",
    "queries = np.array(['This is the first query',\n",
    "                  'This is query 2.1',\n",
    "                  'This is query 3.1',\n",
    "                  'This is query 4.1',\n",
    "                  'This is query 5.1',\n",
    "                  'This is query 6.1',\n",
    "                  'This is query 7.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 19, 8, 166, 1499, 1], [100, 19, 1499, 3, 14489, 1], [100, 19, 1499, 3, 18495, 1], [100, 19, 1499, 3, 19708, 1], [100, 19, 1499, 3, 20519, 1], [100, 19, 1499, 3, 23769, 1], [100, 19, 1499, 3, 25059, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbaCMoKRDy20",
    "outputId": "1853f452-f5a4-4803-d33b-b0dd58c8ba65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_texts size stats:\n",
      "DescribeResult(nobs=7, minmax=(6, 6), mean=6.0, variance=0.0, skewness=nan, kurtosis=nan)\n",
      "\n",
      "tokenized_queries size stats:\n",
      "DescribeResult(nobs=7, minmax=(6, 6), mean=6.0, variance=0.0, skewness=nan, kurtosis=nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139730/1543394617.py:29: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  print(\"tokenized_texts size stats:\\n{}\\n\".format(stats.describe(self.tokenized_texts['length'])))\n",
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/scipy/stats/_stats_py.py:1522: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  sk = skew(a, axis, bias=bias)\n",
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/scipy/stats/_stats_py.py:1523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurt = kurtosis(a, axis, bias=bias)\n",
      "/tmp/ipykernel_139730/1543394617.py:30: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  print(\"tokenized_queries size stats:\\n{}\\n\".format(stats.describe(self.tokenized_queries['length'])))\n"
     ]
    }
   ],
   "source": [
    "# Cria um objeto da classe MyDataset\n",
    "dummy_dataset = MyDataset(texts=texts, queries=queries, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFNuWyMYDz7v",
    "outputId": "f74ff3bd-84d6-42a8-a7de-88501b882de9"
   },
   "outputs": [],
   "source": [
    "# Testa o método __len__()\n",
    "assert len(dummy_dataset) == 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testa o método __getitem__()\n",
    "sample = dummy_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert set(sample.keys()) == {'input_ids', 'attention_mask', 'labels'} # \n",
    "assert isinstance(sample['input_ids'], list)\n",
    "assert isinstance(sample['attention_mask'], list)\n",
    "assert isinstance(sample['labels'], list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9ng05CvFu1t",
    "outputId": "476a8515-b00e-4df7-b7f4-be4382ae8917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [100, 19, 8, 166, 1499, 1], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [100, 19, 8, 166, 1499, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando dataset e dataloaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5INVpV1RGuau"
   },
   "outputs": [],
   "source": [
    "dummy_loader = DataLoader(dummy_dataset, batch_size=3, shuffle=False, num_workers=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(dummy_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([100, 100, 100]),\n",
       "  tensor([19, 19, 19]),\n",
       "  tensor([   8, 1499, 1499]),\n",
       "  tensor([166,   3,   3]),\n",
       "  tensor([ 1499, 14489, 18495]),\n",
       "  tensor([1, 1, 1])],\n",
       " 'attention_mask': [tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([1, 1, 1])],\n",
       " 'labels': [tensor([100, 100, 100]),\n",
       "  tensor([19, 19, 19]),\n",
       "  tensor([   8, 1499, 1499]),\n",
       "  tensor([166,   3,   3]),\n",
       "  tensor([ 1499, 14489, 18495]),\n",
       "  tensor([1, 1, 1])]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_texts size stats:\n",
      "DescribeResult(nobs=1000, minmax=(20, 299), mean=86.623, variance=1287.2781491491492, skewness=1.203754702156062, kurtosis=1.888567910351541)\n",
      "\n",
      "tokenized_queries size stats:\n",
      "DescribeResult(nobs=1000, minmax=(3, 25), mean=9.517, variance=10.392103103103105, skewness=0.8108317830729576, kurtosis=1.3237936614062669)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = MyDataset(X_train, Y_train, tokenizer)\n",
    "val_dataset = MyDataset(X_valid, Y_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_texts size stats:\n",
      "DescribeResult(nobs=10000, minmax=(14, 326), mean=86.9556, variance=1262.5688855285528, skewness=1.138437592548724, kurtosis=1.5776621817771685)\n",
      "\n",
      "tokenized_queries size stats:\n",
      "DescribeResult(nobs=10000, minmax=(3, 57), mean=9.5016, variance=12.030400480048003, skewness=1.7924405763231719, kurtosis=10.845883054521773)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(X_train, Y_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEPTUNE_ALLOW_SELF_SIGNED_CERTIFICATE'] = 'TRUE'\n",
    "os.environ['NEPTUNE_PROJECT'] = 'marcusborela/IA386DD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEPTUNE_API_TOKEN'] = getpass.getpass('Informe NEPTUNE_API_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Precisa informar NEPTUNE_API_TOKEN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código fonte em https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    print(f\"type(decoded_preds) {type(decoded_preds)} len(decoded_preds) {len(decoded_preds)} len(decoded_labels) {len(decoded_labels)}  decoded_preds[0] {decoded_preds[0]}, decoded_labels[0] {decoded_labels[0]}\")\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # print(f\"Em compute_metrics: result={result}\")\n",
    "\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    print(f\"Em compute_metrics: result[bleu]={result['bleu']}\")\n",
    "\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainerCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Fonte inicial: Eduardo Seiti \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, best_validation_yet=99999, model=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.best_validation_metric = best_validation_yet\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
    "        print(f'CustomTrainerCallback.on_evaluate - Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} metrics={metrics}' )\n",
    "        print(f\"metrics['eval_loss']={metrics['eval_loss']} metrics['eval_bleu']={metrics['eval_bleu']} self.best_validation_metric={self.best_validation_metric}\")\n",
    "\n",
    "        if metrics['eval_bleu'] > self.best_validation_metric:\n",
    "            self.best_validation_metric = metrics['eval_bleu']\n",
    "        # caso queira salvar por aqui:\n",
    "        #    nome_arquivo = f\"{DIRETORIO_TRABALHO}/model-checkpoint-{state.global_step}-{metrics['eval_bleu']:.4f}\"\n",
    "        #    print(f\"vou salvar {nome_arquivo}\")\n",
    "        #    self.model.save_pretrained(nome_arquivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parm_model, # parm_optimizer, parm_lr_scheduler,\n",
    "          num_batch_size:int=24, num_epochs:int=3, num_acum_steps:int=8):   # , num_steps_eval:int=100):\n",
    "    \"\"\"\n",
    "    Função auxiliar de treinamento. \n",
    "    parm_model: o modelo Seq2Seq que será treinado.\n",
    "    num_batch_size: o tamanho do lote (batch size) para treinamento e avaliação. Padrão é 24, o que significa que o valor será determinado pelo parâmetro per_device_train_batch_size em training_args.\n",
    "    num_epochs: o número de épocas de treinamento. Padrão é 3.\n",
    "    num_acum_steps: o número de passos de acumulação de gradiente. Padrão é 8.\n",
    "    num_steps_eval: o número de passos para avaliação durante o treinamento. Padrão é 100.\n",
    "\n",
    "    \"\"\"\n",
    "    global steps, diretorio, tokenizer, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "    num_training_steps = num_epochs * int(len(train_dataset) // (num_batch_size * num_acum_steps))\n",
    "    # será avaliado a cada époica\n",
    "    if num_epochs > 1:\n",
    "        num_steps_eval = math.ceil(num_training_steps / num_epochs)  \n",
    "    else:\n",
    "        num_steps_eval = math.ceil(num_training_steps*0.1)\n",
    "    print(f\"num_training_steps = {num_training_steps} batch size = {num_batch_size} num_steps_eval={num_steps_eval}\")\n",
    "\n",
    "    trainer_callback = CustomTrainerCallback(best_validation_yet=-1, model=parm_model)\n",
    "\n",
    "\n",
    "    # dicas em https://huggingface.co/transformers/v4.3.3/main_classes/trainer.html#seq2seqtrainingarguments \n",
    "    # Argumentos de treinamento do modelo Seq2Seq\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=DIRETORIO_TRABALHO, # Onde os modelos são salvos\n",
    "        logging_dir = DIRETORIO_TRABALHO+\"/logs\",\n",
    "        # logging_strategy=\"steps\",  # Especifique a estratégia de registro (por passos)\n",
    "        logging_strategy=\"steps\",  # Especifique a estratégia de registro (por passos)\n",
    "        logging_steps=num_steps_eval, # Número de etapas para registrar os logs\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=num_batch_size, # Tamanho do batch por dispositivo durante o treinamento\n",
    "        per_device_eval_batch_size=num_batch_size, # Tamanho do batch por dispositivo durante a avaliação\n",
    "        gradient_accumulation_steps=num_acum_steps, # Número de etapas de acumulação de gradiente\n",
    "        evaluation_strategy='steps', # Estratégia de avaliação durante o treinamento\n",
    "        eval_steps=num_steps_eval, # Número de etapas para realizar a avaliação\n",
    "        save_steps=num_steps_eval, # Em cada avaliação\n",
    "        # lr_scheduler_type=None,  # Set to None to disable the default scheduler\n",
    "        # fp16=True, # Usar precisão mista (half-precision) para acelerar o treinamento   \n",
    "        bf16=True, # opção de configuração que permite o uso de aritmética de ponto flutuante de 16 bits (half-precision) \n",
    "        num_train_epochs=num_epochs, # Número de épocas de treinamento\n",
    "        report_to=\"neptune\",\n",
    "        dataloader_pin_memory = True, # os dados carregados em memória pelo DataLoader são fixados (pinned) na memória do sistema \n",
    "                                      # Pode acelerar a transferência dos dados para a GPU,\n",
    "                                      #  uma vez que a GPU pode ler os dados diretamente da memória fixada sem precisar fazer uma cópia adicional dos dados\n",
    "        load_best_model_at_end=True, # Carregar o melhor modelo ao final do treinamento\n",
    "        metric_for_best_model='bleu', # Métrica usada para selecionar o melhor modelo\n",
    "        greater_is_better = True,\n",
    "        # predict_with_generate é usado quando você deseja gerar sequências completas como saída do modelo,\n",
    "        # enquanto do_predict é usado quando você deseja fazer predições em formato de passo a passo durante a inferência.\n",
    "        predict_with_generate=True, # Permitir geração de predições com o modelo\n",
    "        # do_predict=True, \n",
    "        warmup_steps = num_training_steps * 0.05, #  5% do total de passos \n",
    "        learning_rate=5e-3,\n",
    "        #Implementa um aumento linear na taxa de aprendizado durante a fase de aquecimento (warmup) e, em seguida,\n",
    "        # diminui linearmente a taxa de aprendizado após a fase de aquecimento. É uma escolha comum para tarefas de sequência para sequência.\n",
    "        lr_scheduler_type= 'linear',\n",
    "        weight_decay=1e-4,\n",
    "        disable_tqdm=False, # not to disable the tqdm progress bars and table of metrics produced by NotebookTrainingTracker\n",
    "        dataloader_drop_last = True, #r to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
    "        save_total_limit=2 # Número máximo de checkpoints a serem salvos\n",
    "    )\n",
    "    \n",
    "    # Objeto de collator de dados para ajustar os dados de treinamento\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=parm_model,\n",
    "        label_pad_token_id=-100, # Token de padding para os rótulos\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None, # Valor de padding múltiplo de 8 para aproveitar otimizações com FP16\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Inicialização do treinador Seq2Seq\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=parm_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset, # Conjunto de dados de treinamento\n",
    "        eval_dataset=val_dataset, # Conjunto de dados de avaliação\n",
    "        data_collator=data_collator, # Collator de dados\n",
    "        tokenizer=tokenizer, # Tokenizer\n",
    "        callbacks=[trainer_callback], \n",
    "        # default to an instance of AdamW on your model and a scheduler given by get_linear_schedule_with_warmup() controlled by args\n",
    "        # optimizers=(parm_optimizer, parm_lr_scheduler),\n",
    "        compute_metrics=compute_metrics_bleu, # Função para calcular as métricas de avaliação\n",
    "    )\n",
    "\n",
    "    # Treinamento do modelo\n",
    "    train_results = trainer.train()\n",
    "    \n",
    "    return trainer, train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch_size=8\n",
    "num_epochs=80\n",
    "num_acum_steps=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps 12480 num_steps_eval 156 \n"
     ]
    }
   ],
   "source": [
    "num_training_steps = num_epochs * int(len(train_dataset) // (num_batch_size * num_acum_steps))\n",
    "if num_epochs > 1:\n",
    "    num_steps_eval = math.ceil(num_training_steps / num_epochs)  \n",
    "else:\n",
    "    num_steps_eval = math.ceil(num_training_steps*0.1)\n",
    "print(f\"num_training_steps {num_training_steps} num_steps_eval {num_steps_eval} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = Adafactor(model.parameters(), relative_step=False,lr=2e-4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)\n",
    "lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, \n",
    "                                                              num_warmup_steps=10,\n",
    "                                                              num_training_steps=num_training_steps, \n",
    "                                                              num_cycles=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#optimizer = Adafactor(model.parameters(), lr=2e-4, scale_parameter=True, relative_step=True, warmup_init=True)\n",
    "optimizer = Adafactor(model.parameters(), relative_step=False,lr=2e-4)\n",
    "lr_scheduler = AdafactorSchedule(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps = 12480 batch size = 8 num_steps_eval=156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-70\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0%|          | 0/12480 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer, train_results = train(parm_model=model, num_batch_size=num_batch_size, num_epochs=num_epochs, num_acum_steps=num_acum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino anterior em que o bleu ficou constante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps = 3120 batch size = 8 num_steps_eval=156\n",
      "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-63\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 4 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 4 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3120 [02:24<20:49:45, 24.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1096, 'learning_rate': 0.00017402086941225244, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.000625331187620759, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 114.1313, 'eval_samples_per_second': 8.762, 'eval_steps_per_second': 1.095, 'epoch': 1.0}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 18:20:34] metrics.keys()\n",
      "metrics['eval_loss']=0.000625331187620759 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0025, 'learning_rate': 0.00010454414749853126, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00039186549838632345, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 113.8188, 'eval_samples_per_second': 8.786, 'eval_steps_per_second': 1.098, 'epoch': 2.0}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 18:38:31] metrics.keys()\n",
      "metrics['eval_loss']=0.00039186549838632345 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 3.238927594185127e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00022956101747695357, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 118.4152, 'eval_samples_per_second': 8.445, 'eval_steps_per_second': 1.056, 'epoch': 3.0}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 18:56:46] metrics.keys()\n",
      "metrics['eval_loss']=0.00022956101747695357 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 8.162249484809925e-08, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00021174507855903357, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 113.5693, 'eval_samples_per_second': 8.805, 'eval_steps_per_second': 1.101, 'epoch': 3.99}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 19:14:24] metrics.keys()\n",
      "metrics['eval_loss']=0.00021174507855903357 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 0.00017333789690128252, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00023728149244561791, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 113.1692, 'eval_samples_per_second': 8.836, 'eval_steps_per_second': 1.105, 'epoch': 4.99}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 19:31:56] metrics.keys()\n",
      "metrics['eval_loss']=0.00023728149244561791 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 0.00010353481789694257, 'epoch': 5.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6702794452834624}\n",
      "{'eval_loss': 0.00033597485162317753, 'eval_bleu': 2.6702794452834624, 'eval_runtime': 114.6051, 'eval_samples_per_second': 8.726, 'eval_steps_per_second': 1.091, 'epoch': 5.99}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 19:49:34] metrics.keys()\n",
      "metrics['eval_loss']=0.00033597485162317753 metrics['eval_bleu']=2.6702794452834624 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 3.1648450158757403e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00017881274106912315, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 110.026, 'eval_samples_per_second': 9.089, 'eval_steps_per_second': 1.136, 'epoch': 6.99}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 20:07:07] metrics.keys()\n",
      "metrics['eval_loss']=0.00017881274106912315 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 4.5915386419270736e-08, 'epoch': 7.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00016723510634619743, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 112.2274, 'eval_samples_per_second': 8.91, 'eval_steps_per_second': 1.114, 'epoch': 7.99}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 20:24:37] metrics.keys()\n",
      "metrics['eval_loss']=0.00016723510634619743 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 0.00017264744090818282, 'epoch': 8.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00014261491014622152, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 112.7022, 'eval_samples_per_second': 8.873, 'eval_steps_per_second': 1.109, 'epoch': 8.99}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 20:42:08] metrics.keys()\n",
      "metrics['eval_loss']=0.00014261491014622152 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 0.00010252512759852884, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em compute_metrics: result={'bleu': 2.6708990050044994}\n",
      "{'eval_loss': 0.00013065339589957148, 'eval_bleu': 2.6708990050044994, 'eval_runtime': 118.8047, 'eval_samples_per_second': 8.417, 'eval_steps_per_second': 1.052, 'epoch': 9.98}\n",
      "CustomTrainerCallback.on_evaluate - Momento: [2023-Apr-09 21:00:02] metrics.keys()\n",
      "metrics['eval_loss']=0.00013065339589957148 metrics['eval_bleu']=2.6708990050044994 self.best_validation_metric=2.6708990050044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_122422/288774366.py\", line 1, in <module>\n",
      "    trainer, train_results = train(parm_model=model, parm_optimizer = optimizer, parm_lr_scheduler = lr_scheduler, num_batch_size=num_batch_size, num_epochs=num_epochs, num_acum_steps=num_acum_steps)\n",
      "  File \"/tmp/ipykernel_122422/1918548302.py\", line 75, in train\n",
      "    train_results = trainer.train()\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 1737, in forward\n",
      "    loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer, train_results \u001b[39m=\u001b[39m train(parm_model\u001b[39m=\u001b[39;49mmodel, parm_optimizer \u001b[39m=\u001b[39;49m optimizer, parm_lr_scheduler \u001b[39m=\u001b[39;49m lr_scheduler, num_batch_size\u001b[39m=\u001b[39;49mnum_batch_size, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, num_acum_steps\u001b[39m=\u001b[39;49mnum_acum_steps)\n",
      "Cell \u001b[0;32mIn[79], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(parm_model, parm_optimizer, parm_lr_scheduler, num_batch_size, num_epochs, num_acum_steps)\u001b[0m\n\u001b[1;32m     62\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     63\u001b[0m     model\u001b[39m=\u001b[39mparm_model,\n\u001b[1;32m     64\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics_bleu, \u001b[39m# Função para calcular as métricas de avaliação\u001b[39;00m\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[39m# Treinamento do modelo\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m trainer, train_results\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/trainer.py:2655\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2656\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[1;32m   2657\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/treinapython39/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "trainer, train_results = train(parm_model=model, parm_optimizer = optimizer, parm_lr_scheduler = lr_scheduler, num_batch_size=num_batch_size, num_epochs=num_epochs, num_acum_steps=num_acum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Para aqui a execução",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPara aqui a execução\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Para aqui a execução"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Para aqui a execução\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treinapython39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
