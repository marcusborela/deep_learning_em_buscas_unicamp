{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 7 - DPR\n",
    "\n",
    "[Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
    "\n",
    "Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado do Exercício"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer o finetuning de um buscador denso\n",
    "\n",
    "Usar como treino o dataset \"tiny\" do MS MARCO\n",
    "https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
    "\n",
    "Avaliar o modelo no TREC-COVID, e comparar os resultados com o BM25 e doc2query\n",
    "\n",
    "Comparar busca \"exaustiva\" (semelhança do vetor query com todos os vetores do corpus) com a busca aproximada (Approximate Nearest Neighbor - ANN)\n",
    "\n",
    "Para a busca aproximada, usar os algoritmos existentes na biblioteca sentence-transformers (ex: hnswlib) OU implemente um você mesmo (Bonus!)\n",
    "\n",
    "Dicas:\n",
    "\n",
    "    Usar a média dos vetores da última camada (conhecido como mean pooling) do transformer para representar queries e passagens; Alternativamente, usar apenas o vetor do [CLS] da última cada.\n",
    "    Tente inicialmente uma loss fácil de implementar, como a entropia-cruzada\n",
    "    Começar o treino a partir do microsoft/MiniLM-L12-H384-uncased\n",
    "    Avaliar o pipeline usando um modelo já bem treinado: sentence-transformers/all-mpnet-base-v2\n",
    "    Comparar resultados usando semelhança de cosseno e produto escalar como funções de similaridade\n",
    "    Para checar se seu codigo de avaliação está correto, comparar o seu desempenho com o do modelo já treinado no MS MARCO:   https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2; O nDCG@10 no TREC-COVID deve ser ~0.47\n",
    "    Usar a biblioteca do sentence-transformers para avaliar o modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase\n",
    "\n",
    "Utilizando DPR treinado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4AJiH6lQQHc5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import  BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRETORIO_TRABALHO = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local/dpr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta já existia!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DIRETORIO_TRABALHO):\n",
    "    print('pasta já existia!')\n",
    "else:\n",
    "    os.makedirs(DIRETORIO_TRABALHO)\n",
    "    print('pasta criada!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "zwyzd_W8gOAL"
   },
   "outputs": [],
   "source": [
    "DIRETORIO_RUN = f\"{DIRETORIO_TRABALHO}/runs\"\n",
    "CAMINHO_RUN = f\"{DIRETORIO_RUN}/run-trec-covid-bm25.txt\"\n",
    "CAMINHO_RUN_BUSCA_APROXIMADA = f\"{DIRETORIO_RUN}/run-trec-covid-bm25-aproximada.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta já existia!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DIRETORIO_RUN):\n",
    "    print('pasta já existia!')\n",
    "else:\n",
    "    os.makedirs(DIRETORIO_RUN)\n",
    "    print('pasta criada!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JVM_PATH'] = '/usr/lib/jvm/java-11-openjdk-amd64/lib/server/libjvm.so'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DKAZ8CWCAM3-"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9XgIWvkkH-kn"
   },
   "outputs": [],
   "source": [
    "def mostra_memoria(lista_mem=['cpu']):\n",
    "  \"\"\"\n",
    "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
    "\n",
    "  Parâmetros:\n",
    "  -----------\n",
    "  lista_mem : list, opcional\n",
    "      Lista com strings 'cpu' e/ou 'gpu'. \n",
    "      'cpu' - exibe informações de memória da CPU.\n",
    "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
    "      O valor padrão é ['cpu'].\n",
    "\n",
    "  Saída:\n",
    "  -------\n",
    "  A função não retorna nada, apenas exibe as informações na tela.\n",
    "\n",
    "  Exemplo de uso:\n",
    "  ---------------\n",
    "  Para exibir informações de memória da CPU:\n",
    "      mostra_memoria(['cpu'])\n",
    "\n",
    "  Para exibir informações de memória da CPU e GPU:\n",
    "      mostra_memoria(['cpu', 'gpu'])\n",
    "  \n",
    "  Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "  \"\"\"  \n",
    "  if 'cpu' in lista_mem:\n",
    "    vm = virtual_memory()\n",
    "    ram={}\n",
    "    ram['total']=round(vm.total / 1e9,2)\n",
    "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
    "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
    "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
    "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
    "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
    "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
    "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
    "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
    "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
    "    print('/nGPU')\n",
    "    gpu_info = !nvidia-smi\n",
    "  if 'gpu' in lista_mem:\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "      print('Not connected to a GPU')\n",
    "    else:\n",
    "      print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dri9iiMAvCT",
    "outputId": "53aebd5a-e29f-4c8e-d233-5221aae9f9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 61.7\n",
      " used 4.69\n",
      " free 59.78\n",
      " cached 2.61\n",
      " buffers 0.27\n",
      "/nGPU\n",
      "Wed Apr 19 11:51:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   45C    P8    33W / 370W |     60MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achvQ78sa3p3"
   },
   "source": [
    "## Fixando as seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AG9RjMb8Qlot"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bkETIyWGkbOf"
   },
   "outputs": [],
   "source": [
    "def inicializa_seed(num_semente:int=123):\n",
    "  \"\"\"\n",
    "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
    "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
    "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
    "  \n",
    "  Args:\n",
    "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
    "  \n",
    "  References:\n",
    "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
    "  \"\"\"\n",
    "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
    "  random.seed(num_semente)\n",
    "  np.random.seed(num_semente)\n",
    "  torch.manual_seed(num_semente)\n",
    "  \n",
    "  # Define as sementes da GPU\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  #torch.cuda.manual_seed(num_semente)\n",
    "  #Cuda algorithms\n",
    "  #torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ViMcw_kVkbOf"
   },
   "outputs": [],
   "source": [
    "num_semente=123\n",
    "inicializa_seed(num_semente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v2gtkEPhA0t"
   },
   "source": [
    "## Preparando para debug e display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BJ6S4P5Hw4iG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kebsl1uQDFUf",
    "outputId": "4b1ed269-722b-4a2a-f1a0-908d1683cc62"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rnR2kDS_2FgZ"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjrlXHq1hC8n",
    "outputId": "18d94254-4bf8-4632-ef84-012727944746"
   },
   "outputs": [],
   "source": [
    "# https://zohaib.me/debugging-in-google-collab-notebook/\n",
    "# !pip install -Uqq ipdb\n",
    "import ipdb\n",
    "# %pdb off # desativa debug em exceção\n",
    "# %pdb on  # ativa debug em exceção\n",
    "# ipdb.set_trace(context=8)  para execução nesse ponto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wQ5pmlOHxHhk"
   },
   "outputs": [],
   "source": [
    "def config_display():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de display do Pandas.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configurando formato saída Pandas\n",
    "  # define o número máximo de colunas que serão exibidas\n",
    "  pd.options.display.max_columns = None\n",
    "\n",
    "  # define a largura máxima de uma linha\n",
    "  pd.options.display.width = 1000\n",
    "\n",
    "  # define o número máximo de linhas que serão exibidas\n",
    "  pd.options.display.max_rows = 100\n",
    "\n",
    "  # define o número máximo de caracteres por coluna\n",
    "  pd.options.display.max_colwidth = 50\n",
    "\n",
    "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
    "  pd.options.display.show_dimensions = True\n",
    "\n",
    "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
    "  pd.options.display.precision = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "b2tDy72ATNHs"
   },
   "outputs": [],
   "source": [
    "def config_debug():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
    "  transformers e datasets.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define opções de impressão de tensores para o modo científico\n",
    "  torch.set_printoptions(sci_mode=True) \n",
    "  \"\"\"\n",
    "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
    "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
    "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
    "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
    "    uma melhor compreensão dos números envolvidos.  \n",
    "  \"\"\"\n",
    "\n",
    "  # Habilita detecção de anomalias no autograd do PyTorch\n",
    "  torch.autograd.set_detect_anomaly(True)\n",
    "  \"\"\"\n",
    "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
    "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
    "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
    "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
    "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
    "    antes que se torne um problema maior.\n",
    "\n",
    "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
    "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
    "    ela deve ser usada com cautela e apenas para depuração.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
    "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "  \"\"\"\n",
    "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
    "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
    "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
    "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
    "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
    "    e removê-lo após a solução do problema.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define o nível de verbosity do pacote transformers para info\n",
    "  # transformers.utils.logging.set_verbosity_info() \n",
    "  \n",
    "  \n",
    "  \"\"\"\n",
    "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
    "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
    "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
    "\n",
    "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
    "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
    "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
    "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
    "    necessidade de cada tarefa.\n",
    "  \n",
    "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
    "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
    "  \n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Define o modo verbose do xmode, que é utilizado no debug\n",
    "  # %xmode Verbose \n",
    "\n",
    "  \"\"\"\n",
    "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
    "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
    "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
    "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
    "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
    "\n",
    "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
    "    comentar a linha acima e descomentar a linha abaixo:\n",
    "    %xmode Plain\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "    Dica:\n",
    "    1.  pdb (Python Debugger)\n",
    "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
    "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
    "\n",
    "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
    "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
    "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
    "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
    "\n",
    "\n",
    "    2. ipdb\n",
    "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
    "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
    "      \n",
    "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
    "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
    "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
    "      comandos para investigar o comportamento.\n",
    "\n",
    "      Durante a depuração, você pode usar comandos:\n",
    "        next (para executar a próxima linha de código), \n",
    "        step (para entrar em uma função chamada na próxima linha de código) \n",
    "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
    "\n",
    "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
    "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
    "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Tb4aqtcExR84"
   },
   "outputs": [],
   "source": [
    "config_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5Bq4043fkfh",
    "outputId": "fa8e5db1-1feb-4393-fb66-d394d1ad693c"
   },
   "outputs": [],
   "source": [
    "config_debug()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixando o dataset para avaliação (trec-covid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import get_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 queries total\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics('covid-round5')\n",
    "print(f'{len(topics)} queries total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is known about an mRNA vaccine for the SARS-CoV-2 virus?',\n",
       " 'query': 'mRNA vaccine coronavirus',\n",
       " 'narrative': 'Looking for studies specifically focusing on mRNA vaccines for COVID-19, including how mRNA vaccines work, why they are promising, and any results from actual clinical studies.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevância (qrel) de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{DIRETORIO_TRABALHO}/test.tsv'):\n",
    "    !wget https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv\n",
    "    !mv test.tsv {DIRETORIO_TRABALHO}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel = pd.read_csv(f\"{DIRETORIO_TRABALHO}/test.tsv\", sep=\"\\t\", header=None, \n",
    "                   skiprows=1, names=[\"query\", \"docid\", \"rel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>g7dhmyyo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query     docid  rel\n",
       "0      1  005b2j4b    2\n",
       "1      1  00fmeepz    1\n",
       "2      1  g7dhmyyo    2\n",
       "3      1  0194oljo    1\n",
       "4      1  021q9884    1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>g7dhmyyo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query     docid  rel\n",
       "0      1  005b2j4b    2\n",
       "1      1  00fmeepz    1\n",
       "2      1  g7dhmyyo    2\n",
       "3      1  0194oljo    1\n",
       "4      1  021q9884    1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel[\"q0\"] = \"q0\"\n",
    "qrel_dict = qrel.to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '005b2j4b', 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel_dict['query'][0], qrel_dict['docid'][0], qrel_dict['rel'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos a serem indexados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Já existia a pasta\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f\"{DIRETORIO_TRABALHO}/corpus.jsonl.gz\"):\n",
    "    !wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz\n",
    "    !mv corpus.jsonl.gz {DIRETORIO_TRABALHO}\n",
    "    print('Baixado')\n",
    "else:\n",
    "    print('Já existia a pasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descompacte o arquivo para a memória\n",
    "with gzip.open(f'{DIRETORIO_TRABALHO}/corpus.jsonl.gz', 'rt') as f:\n",
    "    # Leia o conteúdo do arquivo descompactado\n",
    "    corpus = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> len(corpus): 171332 corpus[0] {'_id': 'ug7v899j', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia', 'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'metadata': {'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35282/', 'pubmed_id': '11472636'}}\n"
     ]
    }
   ],
   "source": [
    "# Exiba os dados carregados\n",
    "print(f\"{type(corpus)} len(corpus): {len(corpus)} corpus[0] {corpus[0]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'title', 'text', 'metadata'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lista_doc_id_passage) 171332\n"
     ]
    }
   ],
   "source": [
    "lista_doc_id_passage = [doc['_id'] for doc in corpus]\n",
    "print(f\"len(lista_doc_id_passage) {len(lista_doc_id_passage)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_embeddings(parm_model:AutoModel , parm_tokenizer:AutoTokenizer, parm_sentences:list, parm_tipo_resumo:str='cls'):\n",
    "    \"\"\"\n",
    "    Função para gerar embeddings de sentenças usando um modelo pré-treinado.\n",
    "\n",
    "    Args:\n",
    "        parm_model (AutoModel): Modelo pré-treinado para geração de embeddings.\n",
    "        parm_tokenizer (AutoTokenizer): Tokenizer associado ao modelo pré-treinado.\n",
    "        parm_sentences (list): Lista de sentenças para as quais os embeddings serão gerados.\n",
    "        parm_tipo_resumo (str, opcional): Tipo de resumo a ser aplicado nas sentenças. Pode ser 'cls' para usar o token [CLS]\n",
    "            ou 'mean' para usar a média das embeddings dos tokens. O padrão é 'cls'.\n",
    "\n",
    "    Returns:\n",
    "        embeddings (torch.Tensor): Embeddings gerados para as sentenças, como um tensor do PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = parm_tokenizer(parm_sentences, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    # print('após tokenizer',encoded_input)\n",
    "\n",
    "    # Executa o tokenizador nas sentenças fornecidas, com opções de padding, truncation e retorno como tensores do PyTorch\n",
    "\n",
    "    # print(f\"padded_batch {padded_batch}\")\n",
    "    encoded_input = BatchEncoding(encoded_input)\n",
    "    # print('após BatchEncoding',encoded_input)\n",
    "    # Move os dados para o dispositivo especificado (CPU ou GPU)\n",
    "    encoded_input = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Desativa o cálculo de gradientes para economizar memória e acelerar a inferência\n",
    "        model_output = parm_model(**encoded_input)\n",
    "        # Passa os inputs tokenizados para o modelo e obtém a saída do modelo\n",
    "    if parm_tipo_resumo == 'cls':\n",
    "        # Se o tipo de resumo for 'cls', retorna o embedding do token [CLS]\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "        # Seleciona o embedding do token [CLS], que está na primeira posição do output do modelo\n",
    "    elif parm_tipo_resumo == 'pooler':\n",
    "        embeddings = model_output.pooler_output\n",
    "    else:\n",
    "        # Se o tipo de resumo não for válido, levanta uma exceção\n",
    "        raise Exception(f\"parm_tipo_resumo deve ser cls ou mean, não  {parm_tipo_resumo}\")\n",
    "\n",
    "    # normalizando\n",
    "    embeddings    = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    return embeddings.to('cpu').squeeze()\n",
    "    # Retorna as embeddings geradas como um tensor do PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_embedding_corpus(model, parm_tokenizer, corpus, parm_batch_size:int=64, parm_type:str='cls'):\n",
    "    embeddings = None\n",
    "    qtd_bloco = math.ceil(len(corpus)/parm_batch_size) \n",
    "    for ndx in tqdm(range(qtd_bloco)):\n",
    "        # print(ndx, f\"ndx*parm_batch_size:ndx*parm_batch_size+parm_batch_size {ndx}*{parm_batch_size}:{ndx}*{parm_batch_size}+{parm_batch_size} \")\n",
    "        lista_doctos = [docto['title'] + ' ' + docto['text'] for docto in corpus[ndx*parm_batch_size:ndx*parm_batch_size+parm_batch_size]]\n",
    "        embeddings_batch = gera_embeddings(parm_model=model, parm_tokenizer=parm_tokenizer, parm_sentences=lista_doctos, parm_tipo_resumo=parm_type)\n",
    "        if embeddings is None:\n",
    "            embeddings = embeddings_batch\n",
    "        else:\n",
    "            embeddings = torch.cat( (embeddings, embeddings_batch), dim=0)\n",
    "\n",
    "        # embeddings.extend(embeddings_batch)\n",
    "    print(\"embeddings gerados: shape {embeddings.shape}\")\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_embedding_queries(model, parm_queries, parm_type:str='cls'):\n",
    "    dict_queries_encoded = {}\n",
    "    for id, value in parm_queries.items():\n",
    "        # print(id, value)\n",
    "        dict_queries_encoded[id]= gera_embeddings(model, tokenizer, [value['question']], 'cls')\n",
    "    return dict_queries_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_queries_busca_exaustiva(parm_dict_queries_encoded:{}, parm_corpus_encoded, parm_lista_doc_id_passage:list, parm_num_max_hits:int=1000):\n",
    "    # versão baseada na do colega Carísio, com adaptaçõess\n",
    "    with open(CAMINHO_RUN, 'w') as runfile:\n",
    "        for cnt, (query_id, query_embedding) in enumerate(parm_dict_queries_encoded.items()):\n",
    "            if cnt % 5 == 0:\n",
    "                print(f'{cnt} queries completadas')\n",
    "\n",
    "            # Pega os primeiros 1000 resultados\n",
    "\n",
    "            score = torch.matmul(parm_corpus_encoded, query_embedding)\n",
    "            # Ordena\n",
    "            sorted_score, indices_score = torch.sort(score, descending=True)\n",
    "\n",
    "            # parm_num_max_hits primeiros\n",
    "            sorted_score = sorted_score[0:parm_num_max_hits]\n",
    "            indices_score = indices_score[0:parm_num_max_hits]\n",
    "\n",
    "            # ids dos documentos\n",
    "            ids_docs = [parm_lista_doc_id_passage[i] for i in indices_score]\n",
    "\n",
    "            #  query  q0     docid  rank     score    descr    \n",
    "            for i, (id_doc, score) in enumerate(zip(ids_docs, sorted_score)):\n",
    "                texto_docto = f'{query_id} Q0 {id_doc} {i+1} {float(score):.6f} Pesquisa\\n'\n",
    "                _ = runfile.write(texto_docto)\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classe_mais_proxima(vetor_query): \n",
    "  # fonte: colega Carísio\n",
    "  classe_mais_proxima = -1\n",
    "  dist_mais_proxima = 1e10\n",
    "  for classe, vetor_classe in enumerate(kmeans.cluster_centers_):\n",
    "    dist = np.linalg.norm(vetor_query - vetor_classe)\n",
    "    if dist < dist_mais_proxima:\n",
    "      dist_mais_proxima = dist\n",
    "      classe_mais_proxima = classe\n",
    "  \n",
    "  return classe_mais_proxima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_queries_busca_aproximada(parm_dict_queries_encoded:{}, \n",
    "            parm_corpus_encoded, \n",
    "            parm_lista_doc_id_passage:list,\n",
    "            parm_num_max_hits:int=1000):\n",
    "    # versão baseada na do colega Carísio, com adaptaçõess\n",
    "    with open(CAMINHO_RUN_BUSCA_APROXIMADA, 'w') as runfile:\n",
    "        for cnt, (query_id, query_embedding) in enumerate(parm_dict_queries_encoded.items()):\n",
    "            if cnt % 5 == 0:\n",
    "                print(f'{cnt} queries completadas')\n",
    "\n",
    "            classe_mais_proxima = get_classe_mais_proxima(query_embedding)\n",
    "            indices_na_classe = indices_por_cluster[classe_mais_proxima]['indice']\n",
    "\n",
    "            matriz_filtrada = parm_corpus_encoded[torch.LongTensor(indices_na_classe).to(torch.int)]\n",
    "            ids_docs_filtrados = indices_por_cluster[classe_mais_proxima]['ids_doc']\n",
    "            \n",
    "            # pesquisa_query_e_retorna_n_primeiros_docs(matriz_filtrada, ids_docs_filtrados, query, n)\n",
    "            score = torch.matmul(matriz_filtrada, query_embedding)\n",
    "            # Ordena\n",
    "            sorted_score, indices_score = torch.sort(score, descending=True)\n",
    "            # parm_num_max_hits primeiros\n",
    "            sorted_score = sorted_score[0:parm_num_max_hits]\n",
    "            indices_score = indices_score[0:parm_num_max_hits]\n",
    "            # Extrai os ids dos documentos\n",
    "            ids_docs = [ids_docs_filtrados[i] for i in indices_score]\n",
    "\n",
    "            #  query  q0     docid  rank     score    descr    \n",
    "            for i, (id_doc, score) in enumerate(zip(ids_docs, sorted_score)):\n",
    "                texto_docto = f'{query_id} Q0 {id_doc} {i+1} {float(score):.6f} Pesquisa\\n'\n",
    "                _ = runfile.write(texto_docto)\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline com modelo que fiz fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODELO_FINAL = \"/home/borela/fontes/deep_learning_em_buscas_unicamp/local/dpr/dpr_best_model_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 30522\n"
     ]
    }
   ],
   "source": [
    "# Se tiver que treinar os modelos, abre\n",
    "models = {'query': AutoModel.from_pretrained(f\"{PATH_MODELO_FINAL}_query\").to(device),\n",
    "'passage' : AutoModel.from_pretrained(f\"{PATH_MODELO_FINAL}_passage\").to(device)}\n",
    "print(models['query'].config.max_position_embeddings, models['passage'].config.vocab_size)\n",
    "for model in models:\n",
    "    models[model].eval()\n",
    "# 512 e 30522\n",
    "#models['query'].config.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos de teste de encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = [\"This is an example sentence\", \"Each sentence is converted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences_test, padding=True, truncation=True, return_tensors='pt')\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "após padding {'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.pad(encoded_input, return_tensors='pt')\n",
    "print('após padding',encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "após BatchEncoding {'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# print(f\"padded_batch {padded_batch}\")\n",
    "encoded_input = BatchEncoding(encoded_input)\n",
    "print('após BatchEncoding',encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Move os dados para o dispositivo especificado (CPU ou GPU)\n",
    "encoded_input = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['passage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = models['passage'](**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 384])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "tensor([3.1420e-01, -1.6939e-01, -1.3007e-01, -2.6283e-01, 3.6589e-01, -1.5062e-01, -2.1353e-01, 1.5691e-02,\n",
      "        4.9411e-01, 1.8600e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "print(embeddings.shape)\n",
    "print(embeddings[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "tensor([3.5197e-02, -1.8975e-02, -1.4571e-02, -2.9443e-02, 4.0988e-02, -1.6872e-02, -2.3920e-02, 1.7577e-03,\n",
      "        5.5351e-02, 2.0836e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings    = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "print(embeddings.shape)\n",
    "print(embeddings[0,:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração dos embeddings "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando enconding para queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_queries_encoded = gera_embedding_queries(models['query'], topics, parm_type='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " tensor([-6.1646e-02, 4.6534e-02, -2.1565e-02, -2.2928e-02, 3.1714e-02, -4.7085e-02, -4.7286e-02, 1.2534e-02,\n",
       "         -2.9679e-02, 2.3128e-02]))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_queries_encoded), dict_queries_encoded[1][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando enconding para docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sem batch: 25min\n",
    "batch size 256: 10min,26s\n",
    "batch size 512: 10min,15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 335/335 [10:16<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings gerados: shape {embeddings.shape}\n",
      "CPU times: user 21min 23s, sys: 27.5 s, total: 21min 50s\n",
      "Wall time: 10min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_encoded = gera_embedding_corpus(models['passage'], parm_tokenizer=tokenizer, corpus=corpus, parm_batch_size=512, parm_type='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0054e-02, -2.2153e-02, 2.0548e-02, -5.3252e-03, 9.7119e-02, -2.8494e-02, 6.3460e-03, 5.7801e-02,\n",
       "        -7.7143e-03, 4.6578e-03])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_encoded[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171332"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 55.52\n",
      " used 10.62\n",
      " free 51.8\n",
      " cached 4.55\n",
      " buffers 0.38\n",
      "/nGPU\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Apr 19 15:04:35 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 67%   50C    P8    27W / 370W |  18687MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A      3312      C   ...treinapython39/bin/python    18627MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 55.49\n",
      " used 10.64\n",
      " free 51.77\n",
      " cached 4.56\n",
      " buffers 0.38\n",
      "/nGPU\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Apr 19 15:04:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 66%   54C    P2    79W / 370W |   1811MiB / 24576MiB |     78%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A      3312      C   ...treinapython39/bin/python     1751MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os dados dos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMINHO_ARQUIVO_EMBEDDINGS = f\"{DIRETORIO_TRABALHO}/data_index_meu_modelo_norm_cls.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAMINHO_ARQUIVO_EMBEDDINGS, 'wb') as outputFile:\n",
    "    pickle.dump({'dict_queries_encoded': dict_queries_encoded,\n",
    "                 'corpus_encoded': corpus_encoded}, outputFile, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os doctos para as queries - busca exaustiva\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os dados salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAMINHO_ARQUIVO_EMBEDDINGS, \"rb\") as f:\n",
    "  teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_queries_encoded = teste['dict_queries_encoded']\n",
    "corpus_encoded = teste['corpus_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus_encoded)==171332, f\"Tamanho len(corpus_encoded) deveria ser 171332 e foi {len(corpus_encoded)}\"\n",
    "assert len(dict_queries_encoded)==50, f\"Tamanho len(dict_queries_encoded) deveria ser 171332 e foi {len(dict_queries_encoded)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([171332, 384])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 queries completadas\n",
      "5 queries completadas\n",
      "10 queries completadas\n",
      "15 queries completadas\n",
      "20 queries completadas\n",
      "25 queries completadas\n",
      "30 queries completadas\n",
      "35 queries completadas\n",
      "40 queries completadas\n",
      "45 queries completadas\n"
     ]
    }
   ],
   "source": [
    "run_all_queries_busca_exaustiva(parm_dict_queries_encoded=dict_queries_encoded,\n",
    " parm_corpus_encoded=corpus_encoded, \n",
    " parm_lista_doc_id_passage=lista_doc_id_passage, parm_num_max_hits=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query  q0     docid  rank     score    system\n",
      "0     44  Q0  8t8psk46     1  0.883771  Pesquisa\n",
      "1     44  Q0  6wxjm7m0     2  0.874808  Pesquisa\n",
      "2     44  Q0  r1oqwdkz     3  0.874691  Pesquisa\n",
      "3     44  Q0  2tmu1wzk     4  0.873972  Pesquisa\n",
      "4     44  Q0  wjtc808r     5  0.870026  Pesquisa\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "NDCG@10: 0.36520473545274407\n",
      "Resultados: {'runid': 'Pesquisa', 'num_ret': 50000, 'num_rel': 24673, 'num_rel_ret': 4407, 'num_q': 50, 'map': 0.06862109415448163, 'gm_map': 0.019367495131862766, 'bpref': 0.17813768898671128, 'Rprec': 0.13289035688618756, 'recip_rank': 0.5784036042177612, 'P@5': 0.41999999999999993, 'P@10': 0.38199999999999995, 'P@15': 0.3693333333333333, 'P@20': 0.3670000000000001, 'P@30': 0.3413333333333333, 'P@100': 0.24660000000000004, 'P@200': 0.19270000000000004, 'P@500': 0.12732000000000002, 'P@1000': 0.08814, 'NDCG@5': 0.39433032879905805, 'NDCG@10': 0.36520473545274407, 'NDCG@15': 0.3524808170252889, 'NDCG@20': 0.348396848882813, 'NDCG@30': 0.33074421218976374, 'NDCG@100': 0.24937727886736843, 'NDCG@200': 0.20492116400399893, 'NDCG@500': 0.17991460232157291, 'NDCG@1000': 0.20528498511034107}\n"
     ]
    }
   ],
   "source": [
    "### Calculando métricas\n",
    "run = pd.read_csv(f\"{CAMINHO_RUN}\", sep=\"\\s+\", \n",
    "                names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "print(run.head())\n",
    "run = run.to_dict(orient=\"list\")\n",
    "results = trec_eval.compute(predictions=[run], references=[qrel_dict])\n",
    "\n",
    "# salvando métricas    \n",
    "print(f\"NDCG@10: {results['NDCG@10']}\")\n",
    "print(f\"Resultados: {results}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os doctos para as queries - busca por clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Cálculo dos centróides:\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "classe_doc = kmeans.fit_predict(corpus_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs na classe 0 = 19370\n",
      "Total docs na classe 1 = 23619\n",
      "Total docs na classe 2 = 15997\n",
      "Total docs na classe 3 = 12729\n",
      "Total docs na classe 4 = 10077\n",
      "Total docs na classe 5 = 27049\n",
      "Total docs na classe 6 = 19107\n",
      "Total docs na classe 7 = 12179\n",
      "Total docs na classe 8 = 9085\n",
      "Total docs na classe 9 = 22120\n",
      "[128, 193, 197]\n",
      "['mzn448zk', 'sswimukk', 'g370ygbu']\n"
     ]
    }
   ],
   "source": [
    "# separação dos índices dos documentos nesses 10 clusters e deixar em um dict só\n",
    "# fonte: Colega Carísio\n",
    "indices_por_cluster = {}\n",
    "\n",
    "for classe in range(0, n_clusters):\n",
    "  indices_por_cluster[classe] = {\n",
    "      'indice': [indice for indice, classe_doc in enumerate(classe_doc) if classe == classe_doc],\n",
    "      'ids_doc': [lista_doc_id_passage[indice] for indice, classe_doc in enumerate(classe_doc) if classe == classe_doc]\n",
    "  }\n",
    "  print(f'Total docs na classe {classe} = {len(indices_por_cluster[classe][\"indice\"])}')\n",
    "\n",
    "print(indices_por_cluster[0]['indice'][0:3])\n",
    "print(indices_por_cluster[0]['ids_doc'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_classe_mais_proxima(dict_queries_encoded[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 queries completadas\n",
      "5 queries completadas\n",
      "10 queries completadas\n",
      "15 queries completadas\n",
      "20 queries completadas\n",
      "25 queries completadas\n",
      "30 queries completadas\n",
      "35 queries completadas\n",
      "40 queries completadas\n",
      "45 queries completadas\n"
     ]
    }
   ],
   "source": [
    "run_all_queries_busca_aproximada(parm_dict_queries_encoded=dict_queries_encoded,\n",
    " parm_corpus_encoded=corpus_encoded, \n",
    " parm_lista_doc_id_passage=lista_doc_id_passage, parm_num_max_hits=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query  q0     docid  rank     score    system\n",
      "0     44  Q0  8t8psk46     1  0.883771  Pesquisa\n",
      "1     44  Q0  wo9awq4g     2  0.868284  Pesquisa\n",
      "2     44  Q0  a0mdq6xr     3  0.866033  Pesquisa\n",
      "3     44  Q0  eimqd5b9     4  0.865012  Pesquisa\n",
      "4     44  Q0  dc8j05vf     5  0.864485  Pesquisa\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "NDCG@10: 0.31237215342977803\n",
      "Resultados: {'runid': 'Pesquisa', 'num_ret': 50000, 'num_rel': 24673, 'num_rel_ret': 2721, 'num_q': 50, 'map': 0.034313994677037385, 'gm_map': 0.009811242772995174, 'bpref': 0.1104174652221865, 'Rprec': 0.08189459861574137, 'recip_rank': 0.5336316894693491, 'P@5': 0.36, 'P@10': 0.33, 'P@15': 0.3213333333333333, 'P@20': 0.30400000000000005, 'P@30': 0.27799999999999997, 'P@100': 0.18000000000000005, 'P@200': 0.1295, 'P@500': 0.08043999999999998, 'P@1000': 0.05442, 'NDCG@5': 0.3353950015646631, 'NDCG@10': 0.31237215342977803, 'NDCG@15': 0.3054478843497728, 'NDCG@20': 0.29059976725743564, 'NDCG@30': 0.2705672040529948, 'NDCG@100': 0.18758453135006886, 'NDCG@200': 0.14260874851560804, 'NDCG@500': 0.11758491765286058, 'NDCG@1000': 0.13101147941445374}\n"
     ]
    }
   ],
   "source": [
    "### Calculando métricas\n",
    "run = pd.read_csv(CAMINHO_RUN_BUSCA_APROXIMADA, sep=\"\\s+\", \n",
    "                names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "print(run.head())\n",
    "run = run.to_dict(orient=\"list\")\n",
    "results = trec_eval.compute(predictions=[run], references=[qrel_dict])\n",
    "\n",
    "# salvando métricas    \n",
    "print(f\"NDCG@10: {results['NDCG@10']}\")\n",
    "print(f\"Resultados: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline com modelo já treinado all-MiniLM-L12-v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para checar se seu codigo de avaliação está correto, comparar o seu desempenho com o do modelo já treinado no MS MARCO:   https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2; O nDCG@10 no TREC-COVID deve ser ~0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L12-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "#models['query'] = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "models = {'passage': model,\n",
    "    'query': model}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos de teste de encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = [\"This is an example sentence\", \"Each sentence is converted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences_test, padding=True, truncation=True, return_tensors='pt')\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "após padding {'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.pad(encoded_input, return_tensors='pt')\n",
    "print('após padding',encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "após BatchEncoding {'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# print(f\"padded_batch {padded_batch}\")\n",
    "encoded_input = BatchEncoding(encoded_input)\n",
    "print('após BatchEncoding',encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Move os dados para o dispositivo especificado (CPU ou GPU)\n",
    "encoded_input = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models['passage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = models['passage'](**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 384])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "tensor([5.1886e-02, -3.0087e-02, 2.1376e-01, 2.5196e-02, 9.8660e-02, -9.7260e-02, 8.8231e-02, 1.2464e-01,\n",
      "        9.9461e-02, -4.3355e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "print(embeddings.shape)\n",
    "print(embeddings[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "tensor([1.1514e-02, -6.6767e-03, 4.7438e-02, 5.5914e-03, 2.1894e-02, -2.1584e-02, 1.9580e-02, 2.7659e-02,\n",
      "        2.2072e-02, -9.6212e-03], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings    = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "print(embeddings.shape)\n",
    "print(embeddings[0,:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração dos embeddings "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando enconding para queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_queries_encoded = gera_embedding_queries(models['query'], topics, parm_type='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " tensor([-6.1646e-02, 4.6534e-02, -2.1565e-02, -2.2928e-02, 3.1714e-02, -4.7085e-02, -4.7286e-02, 1.2534e-02,\n",
       "         -2.9679e-02, 2.3128e-02]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(dict_queries_encoded), dict_queries_encoded[1][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando enconding para docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pré-treinado \n",
    "\n",
    "        sem batch: 25min\n",
    "        batch size 256: 10min,26s\n",
    "        batch size 512: 10min,15s\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_encoded = gera_embedding_corpus(models['passage'], parm_tokenizer=tokenizer, corpus=corpus, parm_batch_size=512, parm_type='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4235e-05, -2.4650e-02, -5.4210e-02, -3.9027e-02,  1.7240e-02,\n",
       "        -5.6386e-03, -4.2778e-02,  2.0824e-02, -7.0401e-03,  1.7235e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_encoded[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 58.68\n",
      " used 7.72\n",
      " free 56.04\n",
      " cached 3.32\n",
      " buffers 0.28\n",
      "/nGPU\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Apr 19 12:14:15 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 59%   47C    P8    28W / 370W |  16211MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A      3312      C   ...treinapython39/bin/python    16151MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 58.67\n",
      " used 7.73\n",
      " free 56.03\n",
      " cached 3.31\n",
      " buffers 0.28\n",
      "/nGPU\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Apr 19 12:14:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 59%   47C    P8    27W / 370W |   1231MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A      3312      C   ...treinapython39/bin/python     1171MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os dados dos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMINHO_ARQUIVO_EMBEDDINGS = f\"{DIRETORIO_TRABALHO}/data_index_all_minilm_norm_cls.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAMINHO_ARQUIVO_EMBEDDINGS, 'wb') as outputFile:\n",
    "    pickle.dump({'dict_queries_encoded': dict_queries_encoded,\n",
    "                 'corpus_encoded': corpus_encoded}, outputFile, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os doctos para as queries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os dados salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAMINHO_ARQUIVO_EMBEDDINGS, \"rb\") as f:\n",
    "  teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_queries_encoded = teste['dict_queries_encoded']\n",
    "corpus_encoded = teste['corpus_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus_encoded)==171332, f\"Tamanho len(corpus_encoded) deveria ser 171332 e foi {len(corpus_encoded)}\"\n",
    "assert len(dict_queries_encoded)==50, f\"Tamanho len(dict_queries_encoded) deveria ser 171332 e foi {len(dict_queries_encoded)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([171332, 384])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 queries completadas\n",
      "5 queries completadas\n",
      "10 queries completadas\n",
      "15 queries completadas\n",
      "20 queries completadas\n",
      "25 queries completadas\n",
      "30 queries completadas\n",
      "35 queries completadas\n",
      "40 queries completadas\n",
      "45 queries completadas\n"
     ]
    }
   ],
   "source": [
    "run_all_queries_busca_exaustiva(parm_dict_queries_encoded=dict_queries_encoded,\n",
    " parm_corpus_encoded=corpus_encoded, \n",
    " parm_lista_doc_id_passage=lista_doc_id_passage, parm_num_max_hits=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query  q0     docid  rank     score    system\n",
      "0     44  Q0  a6gaoeie     1  0.950502  Pesquisa\n",
      "1     44  Q0  appqx6sc     2  0.950065  Pesquisa\n",
      "2     44  Q0  e3icgsl9     3  0.944932  Pesquisa\n",
      "3     44  Q0  vtxu50wz     4  0.941929  Pesquisa\n",
      "4     44  Q0  0zrlkbkx     5  0.940018  Pesquisa\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "NDCG@10: 0.3697687091654517\n",
      "Resultados: {'runid': 'Pesquisa', 'num_ret': 50000, 'num_rel': 24673, 'num_rel_ret': 4087, 'num_q': 50, 'map': 0.060542553645249964, 'gm_map': 0.013651706105066396, 'bpref': 0.1650159727700088, 'Rprec': 0.11628386262893219, 'recip_rank': 0.5874959521077168, 'P@5': 0.444, 'P@10': 0.38800000000000007, 'P@15': 0.3373333333333333, 'P@20': 0.314, 'P@30': 0.288, 'P@100': 0.19939999999999997, 'P@200': 0.15780000000000002, 'P@500': 0.11043999999999998, 'P@1000': 0.08174000000000001, 'NDCG@5': 0.40845529112965623, 'NDCG@10': 0.3697687091654517, 'NDCG@15': 0.3319511085204645, 'NDCG@20': 0.3135865419513995, 'NDCG@30': 0.2903264436958502, 'NDCG@100': 0.2103160267681228, 'NDCG@200': 0.17232373727993774, 'NDCG@500': 0.15675354897600166, 'NDCG@1000': 0.18774341214197524}\n"
     ]
    }
   ],
   "source": [
    "### Calculando métricas\n",
    "run = pd.read_csv(f\"{CAMINHO_RUN}\", sep=\"\\s+\", \n",
    "                names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "print(run.head())\n",
    "run = run.to_dict(orient=\"list\")\n",
    "results = trec_eval.compute(predictions=[run], references=[qrel_dict])\n",
    "\n",
    "# salvando métricas    \n",
    "print(f\"NDCG@10: {results['NDCG@10']}\")\n",
    "print(f\"Resultados: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste com índice Faiss que não deu certo (deixado para histórico e futuro ajuste)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para gerar índice usando faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_indice(parm_corpus_encoded:{}, parm_tipo_similaridade_indice:str='cosseno'):\n",
    "    \"\"\"\n",
    "        Ainda não implementados outros tipos de similaridade.\n",
    "        Outras métricas de similaridade suportadas pelo FAISS incluem a distância euclidiana (faiss.IndexFlatL2) e \n",
    "        a distância Manhattan (faiss.IndexFlatL1).\n",
    "        Você pode consultar a documentação oficial do FAISS para obter mais informações sobre as métricas disponíveis e como usá-las: https://github.com/facebookresearch/faiss/blob/master/docs/indexes.md#available-indexes\n",
    "    \"\"\"\n",
    "    embed_dim = parm_corpus_encoded.shape[1]  # dimensão dos embeddings\n",
    "    print(f\"embed_dim: {embed_dim}\")    \n",
    "    if parm_tipo_similaridade_indice == 'cosseno':\n",
    "        #  a métrica cosseno é usada com o índice FlatIP, que é uma versão otimizada para a métrica cosseno do índice Flat, que é o índice padrão do FAISS.\n",
    "        index = faiss.IndexFlatL2(embed_dim)  # índice com métrica L2 (euclidiana)\n",
    "    else:\n",
    "        raise Exception(f\"parm_tipo_similaridade_indice deve ser cosseno, não  {parm_tipo_similaridade_indice}\")\n",
    "    index.add(parm_corpus_encoded.numpy())  # adiciona os embeddings do corpus ao índice\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_dim: 384\n",
      "CPU times: user 54.5 ms, sys: 56.1 ms, total: 111 ms\n",
      "Wall time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "indice  = gera_indice(corpus_encoded, 'cosseno')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faiss.swigfaiss_avx2.IndexFlatL2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ug7v899j', '02tnwd4m', 'ejv2xln0', '2b73a28n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lista_doc_id_passage[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id =  10\n",
    "query_value = dict_queries_encoded[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_value.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_result_search, index_result_search = indice.search(query_value.unsqueeze(dim=0), 1000)  # realiza a pesquisa no índice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1000), (1, 1000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_result_search.shape, index_result_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_result_search.squeeze().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando ordenação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.11012619, 0.08140977, 0.08162256, 0.08971895,  0.12045887, 0.12120931, 0.13146943, 0.13230261, 0.13461232, 0.13461915]], dtype=float)\n",
    "b = np.array([[58763, 113883, 128603, 127203, 58215, 37539, 97725, 167823, 169760, 39305]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 7 6 5 4 0 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "# indices = a.argsort()[::-1]  # obtém os índices em ordem decrescente\n",
    "indices = np.argsort(-a)[0]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13461915 0.13461232 0.13230261 0.13146943 0.12120931 0.12045887\n",
      " 0.11012619 0.08971895 0.08162256 0.08140977]\n",
      "[ 39305 169760 167823  97725  37539  58215  58763 127203 128603 113883]\n"
     ]
    }
   ],
   "source": [
    "a = a[0][indices]\n",
    "b = b[0][indices]\n",
    "\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.08140977, 0.08162256, 0.08971895, 0.11012619, 0.12045887,\n",
       "        0.12120931, 0.13146943, 0.13230261, 0.13461232, 0.13461915],\n",
       "       dtype=float32),\n",
       " array([ 58763, 113883, 128603, 127203,  58215,  37539,  97725, 167823,\n",
       "        169760,  39305]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_result_search[0,:10], index_result_search[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ug7v899j'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lista_doc_id_passage[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(scores_result_search[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar buscas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all queries in topics, retrive top 1k for each query\n",
    "def run_all_queries_embed_index_faiss(parm_dict_queries_encoded:{}, parm_indice_com_embeddings:faiss.swigfaiss_avx2.IndexFlatL2, parm_lista_doc_id_passage:list, parm_num_max_hits:int=1000):\n",
    "    \"\"\"\n",
    "    A função run_all_queries é responsável por realizar todas as consultas armazenadas no dicionário topics utilizando o objeto searcher fornecido e salvar os resultados em um arquivo de texto.\n",
    "    Usada no notebook da aula 2\n",
    "\n",
    "    Parâmetros:\n",
    "\n",
    "    file: caminho do arquivo de saída onde serão salvos os resultados das consultas.\n",
    "    topics: dicionário contendo as consultas a serem executadas. Cada consulta é representada por uma chave única no dicionário. O valor correspondente a cada chave é um outro dicionário contendo as informações da consulta, como seu título e outras informações relevantes.\n",
    "    searcher: objeto do tipo Searcher que será utilizado para realizar as consultas.\n",
    "    num_max_hits: número máximo de documentos relevantes que serão retornados para cada consulta.\n",
    "    Retorno:\n",
    "\n",
    "    A função não retorna nenhum valor, mas salva os resultados das consultas no arquivo especificado em file.\n",
    "    Comentário:\n",
    "\n",
    "    A função usa a biblioteca tqdm para exibir uma barra de progresso enquanto executa as consultas.\n",
    "    O número de consultas concluídas é impresso a cada 100 consultas.\n",
    "    \"\"\"\n",
    "    global CAMINHO_RUN\n",
    "    queries_encoded_np = np.array(list(parm_dict_queries_encoded.values()))\n",
    "    scores_doctos_result_search = {}\n",
    "    index_doctos_result_search = {}\n",
    "    for query_id, query_value in parm_dict_queries_encoded.items():\n",
    "        scores_doctos, index_doctos = indice.search(query_value.unsqueeze(dim=0), parm_num_max_hits)  # realiza a pesquisa no índice\n",
    "        # print(f\"query_id: {query_id}\")\n",
    "        # print(f\"scores_doctos: {scores_doctos}\")\n",
    "        # print(f\"index_doctos: {index_doctos}\")\n",
    "        # reordena por ordem descrescente de score\n",
    "        indices = np.argsort(-scores_doctos)[0]\n",
    "        scores_doctos_result_search[query_id] = scores_doctos[0][indices]\n",
    "        index_doctos_result_search[query_id] = index_doctos[0][indices] \n",
    "    \n",
    "    print(\"para query_id = 1\")\n",
    "    print(f\"Após pesquisa, scores_doctos_result_search[1].shape: {scores_doctos_result_search[1].shape}, scores_doctos_result_search[1][:10]:{scores_doctos_result_search[1][:10]} \")\n",
    "    print(f\"Após pesquisa, index_doctos_result_search[1].shape: {index_doctos_result_search[1].shape} , index_doctos_result_search[1][:10]:{index_doctos_result_search[1][:10]} \")\n",
    "\n",
    "\n",
    "    #  query  q0     docid  rank     score    descr\n",
    "    with open(CAMINHO_RUN, 'w') as runfile:\n",
    "        for query_id in parm_dict_queries_encoded.keys():\n",
    "            # print(query_id)\n",
    "            for i in range(0, parm_num_max_hits):\n",
    "                ndx_docto = index_doctos_result_search[query_id][i]\n",
    "                texto_docto = f'{query_id} Q0 {parm_lista_doc_id_passage[ndx_docto]} {i+1} {scores_doctos_result_search[query_id][i]:.6f} Pesquisa\\n'\n",
    "                _ = runfile.write(texto_docto)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3312/3025828365.py:22: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  queries_encoded_np = np.array(list(parm_dict_queries_encoded.values()))\n",
      "/tmp/ipykernel_3312/3025828365.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  queries_encoded_np = np.array(list(parm_dict_queries_encoded.values()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para query_id = 1\n",
      "Após pesquisa, scores_doctos_result_search[1].shape: (1000,), scores_doctos_result_search[1][:10]:[0.2639407  0.26384005 0.26374525 0.26370186 0.26363018 0.26363018\n",
      " 0.2635759  0.2635759  0.26354355 0.2634717 ] \n",
      "Após pesquisa, index_doctos_result_search[1].shape: (1000,) , index_doctos_result_search[1][:10]:[ 88278  46571 110917 115432  99316 168440 112248 149914  60352 106858] \n"
     ]
    }
   ],
   "source": [
    "run_all_queries_embed_index_faiss(parm_dict_queries_encoded=dict_queries_encoded,\n",
    " parm_indice_com_embeddings=indice, \n",
    " parm_lista_doc_id_passage=lista_doc_id_passage, parm_num_max_hits=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação dos resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval = load(\"trec_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query  q0     docid  rank     score    system\n",
      "0     44  Q0  i00q5hfu     1  0.442576  Pesquisa\n",
      "1     44  Q0  s8tg24te     2  0.442530  Pesquisa\n",
      "2     44  Q0  4l446lug     3  0.442494  Pesquisa\n",
      "3     44  Q0  fis7kq3b     4  0.442468  Pesquisa\n",
      "4     44  Q0  62f8a96g     5  0.442273  Pesquisa\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "NDCG@10: 0.040467891064239\n",
      "Resultados: {'runid': 'Pesquisa', 'num_ret': 50000, 'num_rel': 24673, 'num_rel_ret': 4087, 'num_q': 50, 'map': 0.021051398306511784, 'gm_map': 0.00295207512878423, 'bpref': 0.16256979096245014, 'Rprec': 0.0564165619124504, 'recip_rank': 0.10332331368454076, 'P@5': 0.044000000000000004, 'P@10': 0.05, 'P@15': 0.04800000000000001, 'P@20': 0.049, 'P@30': 0.04666666666666666, 'P@100': 0.043, 'P@200': 0.04699999999999999, 'P@500': 0.05304, 'P@1000': 0.08174000000000001, 'NDCG@5': 0.03598953016807816, 'NDCG@10': 0.040467891064239, 'NDCG@15': 0.03994276405299504, 'NDCG@20': 0.040997985599810924, 'NDCG@30': 0.03936349909230019, 'NDCG@100': 0.03688294763402532, 'NDCG@200': 0.03949009550358964, 'NDCG@500': 0.05461625925169705, 'NDCG@1000': 0.1388218002362557}\n"
     ]
    }
   ],
   "source": [
    "### Calculando métricas\n",
    "run = pd.read_csv(f\"{CAMINHO_RUN}\", sep=\"\\s+\", \n",
    "                names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "print(run.head())\n",
    "run = run.to_dict(orient=\"list\")\n",
    "results = trec_eval.compute(predictions=[run], references=[qrel_dict])\n",
    "\n",
    "# salvando métricas    \n",
    "print(f\"NDCG@10: {results['NDCG@10']}\")\n",
    "print(f\"Resultados: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treinapython39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
