{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Treino Modelo de Linguagem\n",
        "\n",
        "Aula 4 - [Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)"
      ],
      "metadata": {
        "id": "8hmoQRiMWOU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab latest github version](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcusborela/deep_learning_em_buscas_unicamp/blob/main/code/aula4_treino_modelo_de_linguagem.ipynb) [Open In Colab latest github version]"
      ],
      "metadata": {
        "id": "pTLk-sf8Wid7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "# Enunciado exercício\n",
        "\n",
        "Treinar um modelo de linguagem em dados em portugues\n",
        "\n",
        "Avaliar o modelo usando a perplexidade, que é simplesmente a exponencial de todas as losses do dataset de validação\n",
        "\n",
        "Iremos treinar o modelo para prever o próximo token dado os anteriores (também conhecido como Causal Language Modeling). Não confundir com o Masked Language Modeling (MLM), que consiste em prever tokens mascarados em uma dada sequência (ex: BERT's MLM)\n",
        "\n",
        "\n",
        "Dicas:\n",
        "\n",
        "Usar como ponto de partida o modelo OPT-125M, que já foi treinado em 300B de tokens (maioria em Inglês)\n",
        "\n",
        "Usar este dataset reduzido do mc4 portugues, com ~300M de tokens: gs://unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonte de apoio: caderno usado na aula 10 do curso IA025 do próprio autor\n",
        "\n",
        "(Modelo de Linguagem com auto-atenção)\n",
        "[Exercício Aula 10: Modelo de Linguagem com auto-atenção](https://colab.research.google.com/drive/1a-L79jgyLkQITFE0EPVGBF_kpAaeYx25#scrollTo=Pwep987wSLIx)"
      ],
      "metadata": {
        "id": "6jW-hliBXGwH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmRLgbyi_Dvg"
      },
      "source": [
        "# Organizando o ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação dos pacotes"
      ],
      "metadata": {
        "id": "0Upk7A-8Zdnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "39667ff3-9860-40bf-f95a-9aac6f71a49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MO5ssyR9qeP4"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lUx84Olw88cZ"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm_notebook\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LTF06xq49F24"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "d49e688a-44a4-4792-aadc-ad9e1ad32dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 28 03:44:59 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DKAZ8CWCAM3-"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria(lista_mem=['cpu']):\n",
        "  \"\"\"\n",
        "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
        "\n",
        "  Parâmetros:\n",
        "  -----------\n",
        "  lista_mem : list, opcional\n",
        "      Lista com strings 'cpu' e/ou 'gpu'. \n",
        "      'cpu' - exibe informações de memória da CPU.\n",
        "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
        "      O valor padrão é ['cpu'].\n",
        "\n",
        "  Saída:\n",
        "  -------\n",
        "  A função não retorna nada, apenas exibe as informações na tela.\n",
        "\n",
        "  Exemplo de uso:\n",
        "  ---------------\n",
        "  Para exibir informações de memória da CPU:\n",
        "      mostra_memoria(['cpu'])\n",
        "\n",
        "  Para exibir informações de memória da CPU e GPU:\n",
        "      mostra_memoria(['cpu', 'gpu'])\n",
        "  \n",
        "  Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "  \"\"\"  \n",
        "  if 'cpu' in lista_mem:\n",
        "    vm = virtual_memory()\n",
        "    ram={}\n",
        "    ram['total']=round(vm.total / 1e9,2)\n",
        "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
        "    print('/nGPU')\n",
        "    gpu_info = !nvidia-smi\n",
        "  if 'gpu' in lista_mem:\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "2ddd75fa-3123-4027-8d69-8bc94fc85615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 25.95\n",
            " used 0.94\n",
            " free 15.46\n",
            " cached 10.5\n",
            " buffers 0.42\n",
            "/nGPU\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xRdgUGMPgh"
      },
      "source": [
        "### Vinculando pasta do google drive para salvar dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IsJiN6H8K6pe"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae-Iy2oz_9os",
        "outputId": "db156e3e-b231-47d9-8f49-1d6040feb7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYGL4MV_yhQ",
        "outputId": "5ad18083-48f1-4577-b0c5-863e1c45c1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content\n"
          ]
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "print(\"Current directory:\", current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3qjXs6X2ME"
      },
      "source": [
        "## Fixando as seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AG9RjMb8Qlot"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ySVSWRCXX2ME"
      },
      "outputs": [],
      "source": [
        "def inicializa_seed(num_semente:int=123):\n",
        "  \"\"\"\n",
        "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
        "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
        "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
        "  \n",
        "  Args:\n",
        "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
        "  \n",
        "  References:\n",
        "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
        "  \"\"\"\n",
        "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
        "  random.seed(num_semente)\n",
        "  np.random.seed(num_semente)\n",
        "  torch.manual_seed(num_semente)\n",
        "  \n",
        "  # Define as sementes da GPU\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  #torch.cuda.manual_seed(num_semente)\n",
        "  #Cuda algorithms\n",
        "  #torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j9dUkABwX2ME"
      },
      "outputs": [],
      "source": [
        "num_semente=123\n",
        "inicializa_seed(num_semente)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3W-iu3UX2MF"
      },
      "source": [
        "## Definindo Hiperparâmetros iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_FxOjwQcX2MF"
      },
      "outputs": [],
      "source": [
        "def inicia_hparam()->dict:\n",
        "  # Inicialização dos parâmetros\n",
        "  hparam = {}\n",
        "  hparam[\"num_workers_dataloader\"] = 0\n",
        "  hparam[\"device\"] = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  if torch.cuda.is_available(): print(torch. cuda. get_device_name(hparam[\"device\"]))    \n",
        "  return hparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90846d9-b669-4df5-a60a-4ab4d9ed986b",
        "id": "N2HKHbrYX2MF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "hparam=inicia_hparam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMz76cmAX2MF"
      },
      "source": [
        "## Preparando para debug e display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BJ6S4P5Hw4iG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKNAMVTnX2MG"
      },
      "source": [
        "https://zohaib.me/debugging-in-google-collab-notebook/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Tb1KfD9-X2MG"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "# %pdb off # desativa debug em exceção\n",
        "# %pdb on  # ativa debug em exceção\n",
        "# ipdb.set_trace(context=8)  para execução nesse ponto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wQ5pmlOHxHhk"
      },
      "outputs": [],
      "source": [
        "def config_display():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de display do Pandas.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurando formato saída Pandas\n",
        "  # define o número máximo de colunas que serão exibidas\n",
        "  pd.options.display.max_columns = None\n",
        "\n",
        "  # define a largura máxima de uma linha\n",
        "  pd.options.display.width = 1000\n",
        "\n",
        "  # define o número máximo de linhas que serão exibidas\n",
        "  pd.options.display.max_rows = 100\n",
        "\n",
        "  # define o número máximo de caracteres por coluna\n",
        "  pd.options.display.max_colwidth = 50\n",
        "\n",
        "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
        "  pd.options.display.show_dimensions = True\n",
        "\n",
        "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
        "  pd.options.display.precision = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "b2tDy72ATNHs"
      },
      "outputs": [],
      "source": [
        "def config_debug():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
        "  transformers e datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define opções de impressão de tensores para o modo científico\n",
        "  torch.set_printoptions(sci_mode=True) \n",
        "  \"\"\"\n",
        "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
        "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
        "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
        "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
        "    uma melhor compreensão dos números envolvidos.  \n",
        "  \"\"\"\n",
        "\n",
        "  # Habilita detecção de anomalias no autograd do PyTorch\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  \"\"\"\n",
        "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
        "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
        "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
        "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
        "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
        "    antes que se torne um problema maior.\n",
        "\n",
        "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
        "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
        "    ela deve ser usada com cautela e apenas para depuração.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "  \"\"\"\n",
        "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
        "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
        "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
        "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
        "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
        "    e removê-lo após a solução do problema.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define o nível de verbosity do pacote transformers para info\n",
        "  transformers.utils.logging.set_verbosity_info() \n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
        "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
        "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
        "\n",
        "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
        "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
        "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
        "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
        "    necessidade de cada tarefa.\n",
        "  \n",
        "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
        "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
        "  \n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define o modo verbose do xmode, que é utilizado no debug\n",
        "  %xmode Verbose \n",
        "\n",
        "  \"\"\"\n",
        "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
        "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
        "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
        "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
        "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
        "\n",
        "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
        "    comentar a linha acima e descomentar a linha abaixo:\n",
        "    %xmode Plain\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "    Dica:\n",
        "    1.  pdb (Python Debugger)\n",
        "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
        "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
        "\n",
        "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
        "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
        "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
        "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
        "\n",
        "\n",
        "    2. ipdb\n",
        "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
        "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
        "      \n",
        "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
        "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
        "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
        "      comandos para investigar o comportamento.\n",
        "\n",
        "      Durante a depuração, você pode usar comandos:\n",
        "        next (para executar a próxima linha de código), \n",
        "        step (para entrar em uma função chamada na próxima linha de código) \n",
        "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
        "\n",
        "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
        "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
        "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tb4aqtcExR84"
      },
      "outputs": [],
      "source": [
        "config_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f0af4e-e2e2-44d7-9946-f46152af906d",
        "id": "6bGQAf_YX2MJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception reporting mode: Verbose\n"
          ]
        }
      ],
      "source": [
        "config_debug()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VEyI8MpX2MJ"
      },
      "source": [
        "## Rastro (neptune.ai)\n",
        "\n",
        "Gerado rastro da execução no Neptune (detalhes no artigo [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik2HW7K-X2MJ"
      },
      "source": [
        "### Importação de libraries para Rastro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alyRE86Id40N",
        "outputId": "26483b0f-7e13-47b7-f0e2-737cd7640d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.9/dist-packages (1.1.1)\n",
            "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (1.26.100)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (1.26.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from neptune-client) (5.9.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (8.1.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (1.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (3.2.2)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.9/dist-packages (from neptune-client) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from neptune-client) (23.0)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (0.18.3)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (3.0.3)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (1.5.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (2.27.1)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (8.4.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.9/dist-packages (from neptune-client) (3.1.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neptune-client) (1.4.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3>=1.16.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3>=1.16.0->neptune-client) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.100 in /usr/local/lib/python3.9/dist-packages (from boto3>=1.16.0->neptune-client) (1.29.100)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (5.17.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune-client) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune-client) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune-client) (3.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas->neptune-client) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neptune-client) (2022.7.1)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.9/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (22.2.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.3)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.12)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.0)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install neptune-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M63geOdvd2kt",
        "outputId": "699aca9b-7523-48be-ab72-1c8617398015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/neptune/internal/backends/hosted_client.py:50: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
            "  from neptune.version import version as neptune_client_version\n"
          ]
        }
      ],
      "source": [
        "import neptune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ncf5Ncupk4f",
        "outputId": "aa03ff3c-1a04-43b6-b472-c88951f1f060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.9/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchviz) (1.13.1+cu116)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->torchviz) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7cQvuqrppnIZ"
      },
      "outputs": [],
      "source": [
        "from torchviz import make_dot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0XcZKZJ_X2MK"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import copy\n",
        "import time\n",
        "import re\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95u6fg6QX2MK"
      },
      "source": [
        "### Código Rastro\n",
        "\n",
        "Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664), autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "03PO_YjJX2MK"
      },
      "outputs": [],
      "source": [
        "def converte_optimizer_state_dict(parm_optimizer)-> dict:\n",
        "  \"\"\"\n",
        "    Recebe um objeto \"parm_optimizer\" que é do tipo \"torch.optim.Optimizer\" e retorna um dicionário \n",
        "    com informações sobre o otimizador.\n",
        "\n",
        "    O dicionário de retorno é gerado a partir do estado do otimizador que é extraído da propriedade\n",
        "    \"state_dict()\" do objeto \"parm_optimizer\", seu primeiro grupo de parâmetros do otimizador.\n",
        "  \"\"\"\n",
        "  # return str(hparam['optimizer'])\n",
        "  return parm_optimizer.state_dict()['param_groups'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TOrNttzncJ7f"
      },
      "outputs": [],
      "source": [
        "def gera_tag_rastro_experiencia_treino(parm_aula: str, hparam: dict) -> str:\n",
        "    \"\"\"\n",
        "    Gera uma string formatada com informações de hiperparâmetros para ser usada como tag de rastro de experiência de treino.\n",
        "\n",
        "    Args:\n",
        "        parm_aula (str): Nome da aula que está sendo treinada.\n",
        "        hparam (dict): Dicionário contendo os hiperparâmetros utilizados no treinamento.\n",
        "\n",
        "    Returns:\n",
        "        str: String formatada com as informações de hiperparâmetros.\n",
        "\n",
        "    Uso: \n",
        "\n",
        "    hparam['lista_tag_rastro_experiencia_treino'] =        gera_tag_rastro_experiencia_treino(parm_aula='aula7', hparam=hparam)\n",
        "    \"\"\"\n",
        "    # Inicializa uma lista vazia para armazenar as tags\n",
        "    lista_tag = []\n",
        "    \n",
        "    # Lista com as chaves dos hiperparâmetros que serão utilizados\n",
        "    lista_chaves = ['embed_dim', 'leiaute_input', 'dim_feedforward', 'max_seq_length', 'ind_activation_function', 'batch_size', 'learning_rate', 'weight_decay', 'amsgrad', 'decrease_factor_lr', 'max_examples', 'eval_every_steps']\n",
        "    \n",
        "    # Itera pelas chaves da lista e cria uma string com a chave e o valor correspondente em hparam,\n",
        "    # adicionando essa string à lista_tag\n",
        "    for chave in lista_chaves:\n",
        "        if chave in hparam:\n",
        "          tag = f\"{chave} {hparam[chave]}\"\n",
        "          lista_tag.append(tag)\n",
        "    \n",
        "    # Concatena a lista de tags em uma única string, separando cada tag por '|',\n",
        "    # e adicionando o nome da aula como prefixo\n",
        "    tag_formatada = f\"{parm_aula}|\" + \"|\".join(lista_tag)\n",
        "    \n",
        "    return tag_formatada\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "x2IIVG8EX2ML"
      },
      "outputs": [],
      "source": [
        "class NeptuneRastroRun():\n",
        "    \"\"\"\n",
        "      Classe para geração de rastro de experimento utilizando a ferramenta Neptune.\n",
        "\n",
        "      Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664),\n",
        "      autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n",
        "      Attributes:\n",
        "      -----------\n",
        "      se_geracao_rastro : bool\n",
        "          Indica se deve ser gerado rastro de experimento. \n",
        "      neptune_project : str\n",
        "          Nome do projeto criado no Neptune. \n",
        "      tag_contexto_rastro : str\n",
        "          Nome da tag utilizada para identificar o experimento.\n",
        "      neptune_api_token : str\n",
        "          Token utilizado para autenticação na API do Neptune. \n",
        "      run_neptune : object\n",
        "          Objeto que representa o experimento no Neptune.\n",
        "      device : str\n",
        "          Dispositivo utilizado para o treinamento do modelo.\n",
        "      tmpDir : str\n",
        "        Diretório temporário utilizado para salvar gráfico do modelo.          \n",
        "    \"\"\"\n",
        "    se_geracao_rastro = True \n",
        "    neptune_project = \"\"\n",
        "    tag_contexto_rastro = \"\"\n",
        "    neptune_api_token = \"\"\n",
        "\n",
        "    def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "      \"\"\"\n",
        "        Método construtor da classe NeptuneRastroRun.\n",
        "        \n",
        "        Args:\n",
        "        - parm_params: dicionário contendo os parâmetros do modelo.\n",
        "        - parm_lista_tag: lista contendo tags adicionais para o experimento.\n",
        "      \"\"\"      \n",
        "      # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "      if self.__class__.se_geracao_rastro:      \n",
        "        self.run_neptune = neptune.init_run(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "        self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "        vparams = copy.deepcopy(parm_params)\n",
        "        if \"optimizer\" in vparams:\n",
        "          vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "        if 'criterion'  in vparams:\n",
        "          vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "        if 'scheduler'  in vparams:\n",
        "          vparams[\"scheduler\"] = str(type(vparams[\"scheduler\"]))\n",
        "        if 'device' in vparams:\n",
        "          vparams['device'] = str(vparams[\"device\"])\n",
        "        self.device = vparams[\"device\"]\n",
        "        for tag in parm_lista_tag:\n",
        "          self.run_neptune['sys/tags'].add(tag)\n",
        "        self.run_neptune['parameters'] = vparams\n",
        "        # self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "    @property\n",
        "    def run():\n",
        "      \"\"\"\n",
        "      Retorna a instância do objeto run_neptune.\n",
        "      \"\"\"      \n",
        "      return self.run_neptune\n",
        "\n",
        "    @classmethod\n",
        "    def ativa_geracao_rastro(cls):\n",
        "      \"\"\"\n",
        "      Ativa a geração de rastro.\n",
        "      \"\"\"      \n",
        "      cls.se_geracao_rastro = True      \n",
        "\n",
        "    @classmethod\n",
        "    def def_contexto(cls):\n",
        "      \"\"\"\n",
        "      Define o contexto para a geração de rastro.\n",
        "      \"\"\"      \n",
        "      cls.se_geracao_rastro = True      \n",
        "\n",
        "    @classmethod\n",
        "    def desativa_geracao_rastro(cls):\n",
        "      \"\"\"\n",
        "      Desativa a geração de rastro.\n",
        "      \"\"\"      \n",
        "      cls.se_geracao_rastro = False      \n",
        "\n",
        "    @classmethod\n",
        "    def retorna_status_geracao_rastro(cls):\n",
        "      \"\"\"\n",
        "        Retorna o status da geração de rastro.\n",
        "        \n",
        "        Returns:\n",
        "        - True se a geração de rastro está ativada, False caso contrário.\n",
        "      \"\"\"      \n",
        "      return cls.se_geracao_rastro      \n",
        "\n",
        "    @classmethod\n",
        "    def retorna_tag_contexto_rastro(cls):\n",
        "      \"\"\"\n",
        "        Retorna a tag do contexto de rastro.\n",
        "      \"\"\"      \n",
        "      return cls.tag_contexto_rastro \n",
        "\n",
        "    @classmethod\n",
        "    def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "      \"\"\"\n",
        "      Inicia o contexto de execução no Neptune.\n",
        "\n",
        "      Args:\n",
        "          neptune_project (str): Nome do projeto no Neptune.\n",
        "          tag_contexto_rastro (str): Tag que identifica o contexto de execução no Neptune.\n",
        "          neptune_api_token (str): Token de acesso à API do Neptune.\n",
        "\n",
        "      Raises:\n",
        "          AssertionError: Caso a tag_contexto_rastro possua um ponto (.), \n",
        "            o que pode gerar erros na gravação de arquivo.\n",
        "      \"\"\"      \n",
        "      assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "      cls.neptune_api_token = neptune_api_token\n",
        "      cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "      cls.neptune_project = neptune_project\n",
        "\n",
        "    def salva_metrica(self, parm_metricas={}):\n",
        "      \"\"\"\n",
        "        Salva as métricas no Neptune Run caso a geração de rastro esteja ativa.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        parm_metricas: dict\n",
        "            Dicionário contendo as métricas a serem salvas. As chaves devem ser os nomes das métricas e os valores devem ser\n",
        "            os valores das métricas.\n",
        "      \"\"\"\n",
        "      #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "      if self.__class__.se_geracao_rastro:\n",
        "        for metrica, valor in parm_metricas.items(): \n",
        "          self.run_neptune[metrica].append(valor)\n",
        " \n",
        "    def gera_grafico_modelo(self, loader_train, model):\n",
        "      \"\"\"\n",
        "        Gera um gráfico do modelo e o envia para o Neptune. \n",
        "        Para gerar o gráfico, um forward pass é realizado em um batch de exemplos \n",
        "        de treino e o resultado é renderizado como um gráfico de nós conectados. \n",
        "        O gráfico é salvo em um arquivo .png e enviado para o Neptune como um arquivo anexo.\n",
        "\n",
        "        Args:\n",
        "            loader_train (torch.utils.data.DataLoader): DataLoader do conjunto de treinamento.\n",
        "            model (torch.nn.Module): Modelo a ser visualizado.\n",
        "        \n",
        "        Pendente:\n",
        "          Evolui para usar from io import StringIO (buffer = io.StringIO()) ao invés de tempdir \n",
        "      \"\"\"    \n",
        "      return\n",
        "\n",
        "      \"\"\"\n",
        "      falta ajustar make_dot\n",
        "      if self.__class__.se_geracao_rastro: \n",
        "        # efetuar um forward \n",
        "        batch = next(iter(loader_train))\n",
        "        # falta generalizar linha abaixo. Criar função que recebe modelo e batch como parâmetro?\n",
        "        outputs = model(input_ids=batch['input_ids'].to(hparam['device']), attention_mask=batch['attention_mask'].to(hparam['device']), token_type_ids=batch['token_type_ids'].to(hparam['device']), labels=batch['labels'].to(hparam['device']))\n",
        "        nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "        make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "        self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "        self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "      \"\"\"\n",
        "        Para a execução do objeto Neptune. Todos os experimentos do Neptune são sincronizados com o servidor, e nenhum outro \n",
        "        experimento poderá ser adicionado a este objeto após a chamada a este método.\n",
        "      \"\"\"\n",
        "      if self.__class__.se_geracao_rastro:         \n",
        "        self.run_neptune.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYAbHMteX2MM"
      },
      "source": [
        "### Definindo parâmetros para o rastro\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a98945-28fa-4a9e-b50e-7253ad27184a",
        "id": "vz6I6FITX2MM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Informe NEPTUNE_API_TOKEN··········\n"
          ]
        }
      ],
      "source": [
        "NeptuneRastroRun.inicia_contexto('marcusborela/IA386DD', 'Aula 2 - classificador de texto como reranqueador',   getpass.getpass('Informe NEPTUNE_API_TOKEN'))\n",
        "#NeptuneRastroRun.desativa_geracao_rastro()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "# Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "outputs": [],
      "source": [
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK6yCyrpp4Tl",
        "outputId": "35ab5bc5-9256-43e8-b831-5537ccda60b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c']\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "a = [['a','b'], ['c']]\n",
        "print(list(itertools.chain.from_iterable(a)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DMePGM1MUqOK"
      },
      "outputs": [],
      "source": [
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "      \"\"\"\n",
        "      gera sentencas x e y com max_seq_length\n",
        "      como para a última posição de x não haveria label\n",
        "        salva-se sentence_data com max_length + 1\n",
        "        na hora de salvar\n",
        "        x:sentence_data[0:max_length]\n",
        "        y:sentence_data[1:max_length+1]\n",
        "      \"\"\"\n",
        "      assert isinstance(texts,list), 'texts deve ser do tipo list'\n",
        "      assert isinstance(texts[0],str), 'texts deve ser do tipo iterator of iterator of strings'\n",
        "      assert isinstance(max_seq_length,int), 'max_seq_length deve ser do tipo int'\n",
        "      assert max_seq_length > 3, 'max_seq_length deve ser maior do que 3'\n",
        "\n",
        "      sentence_data = []\n",
        "      sentence_length = max_seq_length + 1\n",
        "      self.qtd_sequencia = 0\n",
        "      tamanho_batch = 50\n",
        "      num_batch_entrada = math.ceil(len(texts)/tamanho_batch) # inteiro acima para pegar último batch parcial de sentenças\n",
        "      print(f\" len(texts) {len(texts)}; tamanho_batch {tamanho_batch};  num_batch_entrada {num_batch_entrada} \")\n",
        "      print(f\" max_seq_length {max_seq_length}; Mas salvando sentence_length {sentence_length}\")\n",
        "      for ndx_batch in range(num_batch_entrada): \n",
        "        # ipdb.set_trace(context=6)\n",
        "        #if ndx_batch % 100 == 0:\n",
        "        #    print(F'\\tInicio Mydataset ndx_batch+1: {ndx_batch+1}')\n",
        "        batch_texto_numericalizado = tokenizer.batch_encode_plus(texts[ndx_batch*tamanho_batch:ndx_batch*tamanho_batch+tamanho_batch],\n",
        "                                                                 return_attention_mask=False, return_token_type_ids = False, add_special_tokens=True).input_ids ## já retorna sos\n",
        "\n",
        "        # concatenando conforme sugerido em https://huggingface.co/course/chapter7/6?fw=pt \n",
        "        #   \"A more efficient way to prepare the data is to join all the tokenized samples \n",
        "        #    in a batch with an eos_token_id token in between, and then perform the chunking \n",
        "        #    on the concatenated sequences. \"\n",
        "        lista_tokens_concatenadas = list(itertools.chain.from_iterable(batch_texto_numericalizado))\n",
        "        num_sentencas = math.floor(len(lista_tokens_concatenadas)/sentence_length)  # eliminando a última com pads\n",
        "        # if ndx_batch % 100 == 0:\n",
        "        #    print(F'\\t === num_sentencas:{num_sentencas} ')\n",
        "        for cnt_sentenca_batch in range(num_sentencas): \n",
        "          texto_numericalizado = lista_tokens_concatenadas[cnt_sentenca_batch*sentence_length:cnt_sentenca_batch*sentence_length + sentence_length]\n",
        "          sentence_data.append(texto_numericalizado)\n",
        "          self.qtd_sequencia += 1\n",
        "        if ndx_batch % 10 == 0:\n",
        "            print(F'\\t === ndx_batch+1: {ndx_batch+1}  self.qtd_sequencia: {self.qtd_sequencia}; Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "      print(F'\\tVou converter lista para tensor;  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "      self.data_tensor = torch.tensor(sentence_data).long()\n",
        "      print(F'\\tConvertido: lista para tensor;  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "      print(f\"Carregado dataset com {self.qtd_sequencia} sentenças\")\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.qtd_sequencia\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if idx > self.qtd_sequencia:\n",
        "        raise Exception(\"Tentativa de ler além do limite\")\n",
        "      return self.data_tensor[idx][:-1], self.data_tensor[idx][1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqF-LbXgrpFP"
      },
      "source": [
        "## Carregando modelo e tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,  AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "1InUqeoncLP0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'facebook/opt-125m'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(hparam['device'])\n",
        "print(f'Por quê model.config.vocab_size ({model.config.vocab_size}) != tokenizer.vocab_size ({tokenizer.vocab_size}). Não deveriam ser iguais?')\n",
        "# tokenizer.vocab_size == 50265\n",
        "# model.config.vocab_size == 50272"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfn2hCybb9FA",
        "outputId": "daed14b1-c622-4786-c179-625c1b4d1ce8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/merges.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por quê model.config.vocab_size (50272) != tokenizer.vocab_size (50265). Não deveriam ser iguais?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hparam['vocab_size']= model.config.vocab_size # tokenizer.vocab_size\n",
        "# tokenizer.vocab_size == 50265\n",
        "# model.config.vocab_size == 50272\n",
        "# Por quê???"
      ],
      "metadata": {
        "id": "X238N2gSOcUW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wew-gFbWeBTq"
      },
      "source": [
        "## Teste da implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR6DBS6gIOUX",
        "outputId": "e8386155-c12d-4ecc-cbfe-078a278dd84c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "torch.tensor(tokenizer.pad_token_id, dtype=torch.long).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SKwP0mWxA_c8"
      },
      "outputs": [],
      "source": [
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza pelas manhãs', 'Meu Pai, que mora em Brasília, gosta muito de correr no parque e está de dieta agora.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRyWTs9oyX8",
        "outputId": "8f2a3de9-2f87-4999-ed0c-886adc69c3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 717, 257, 821, 2603, 139, 263, 9240, 8564], [2, 717, 2560, 821, 13334, 475, 6439, 139, 263, 3137, 254, 9366, 11188, 281, 313, 298, 17682, 29], [2, 5096, 257, 27039, 6, 1192, 14628, 102, 2841, 17128, 1977, 14190, 6, 821, 13334, 475, 6439, 139, 263, 9240, 8564, 117, 2242, 3407, 364, 3304, 1526, 263, 5626, 102, 5951, 4330, 4]]\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.batch_encode_plus(dummy_texts, return_attention_mask=False, return_token_type_ids = False, add_special_tokens=True).input_ids ## já retorna cls e sep\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZKGtVJHJaiz",
        "outputId": "aaf071ce-7358-44fb-ff42-675d8ca876ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "len(list(itertools.chain.from_iterable(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(itertools.chain.from_iterable(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pWLNl32csty",
        "outputId": "38847b2b-bf74-4676-a9bf-d077190a6341"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 717, 257, 821, 2603, 139, 263, 9240, 8564, 2, 717, 2560, 821, 13334, 475, 6439, 139, 263, 3137, 254, 9366, 11188, 281, 313, 298, 17682, 29, 2, 5096, 257, 27039, 6, 1192, 14628, 102, 2841, 17128, 1977, 14190, 6, 821, 13334, 475, 6439, 139, 263, 9240, 8564, 117, 2242, 3407, 364, 3304, 1526, 263, 5626, 102, 5951, 4330, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdjhH-mlo43Z",
        "outputId": "5a6be239-e176-45c2-91dd-397b6f639cd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('</s>', '<pad>')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "tokenizer.decode(2), tokenizer.decode(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC6ziNOp7Juy",
        "outputId": "60cfc9d7-cbfb-4c3e-c32a-8f959d1ce35d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[101,\n",
              "  1660,\n",
              "  5971,\n",
              "  785,\n",
              "  125,\n",
              "  1847,\n",
              "  13779,\n",
              "  15616,\n",
              "  449,\n",
              "  698,\n",
              "  125,\n",
              "  14559,\n",
              "  2535,\n",
              "  102]]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "tokenizer.batch_encode_plus([\"Ela gosta muito de comer pizza mas está de dieta agora\"]).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN52aFtTl219",
        "outputId": "143dd3c2-7a91-48cc-ae79-05fc3c0e6dc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>', '<pad>']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP4hcZfRl9Je",
        "outputId": "bdf98441-86c5-4a65-cf00-ddefb8b4c837"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None,\n",
              " None,\n",
              " '</s>',\n",
              " 1,\n",
              " {'bos_token': '</s>',\n",
              "  'eos_token': '</s>',\n",
              "  'unk_token': '</s>',\n",
              "  'pad_token': '<pad>'})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "tokenizer.cls_token_id, tokenizer.sep_token_id ,tokenizer.eos_token, tokenizer.pad_token_id , tokenizer.special_tokens_map "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh1jI4HVmivD",
        "outputId": "abdc20d6-217b-4def-db81-ec710e0cd745"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[2, 2], [2, 1]], 'attention_mask': [[1, 1], [1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "tokenizer.batch_encode_plus(tokenizer.all_special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se1_OZ3Jnjto",
        "outputId": "73e291f9-0382-40a2-975b-49cebad345c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " len(texts) 3; tamanho_batch 50;  num_batch_entrada 1 \n",
            " max_seq_length 9; Mas salvando sentence_length 10\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 6; Momento: [2023-Mar-27 23:47:09]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-27 23:47:09]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-27 23:47:09]\n",
            "Carregado dataset com 6 sentenças\n"
          ]
        }
      ],
      "source": [
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dummy_texts), [len(txt) for txt in tokenizer(dummy_texts)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvBD9vuGe6Zz",
        "outputId": "d4932aa8-3b03-40c5-eaa3-b3ebbcd3cb1f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, [18, 43, 85])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dummy_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AER4E0JCexae",
        "outputId": "fc77c3ba-eb15-4a74-d3b1-33bdc435399a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDHzYsn0exCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2BuTp4Dv8D8",
        "outputId": "616199b7-2b65-4e12-c61a-4fa5f69a989f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n"
          ]
        }
      ],
      "source": [
        "assert len(dummy_dataset) == 6 \n",
        "print('passou no assert de tamanho do dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NKR7NIdVnjtp"
      },
      "outputs": [],
      "source": [
        "first_batch_input, first_batch_target = next(iter(dummy_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiPSWmndJcuj",
        "outputId": "cbe2d54f-3037-43fc-afc2-3aacdc5be24f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    2,   717,   257,   821,  2603,   139,   263,  9240,  8564],\n",
              "         [  717,  2560,   821, 13334,   475,  6439,   139,   263,  3137],\n",
              "         [ 9366, 11188,   281,   313,   298, 17682,    29,     2,  5096],\n",
              "         [27039,     6,  1192, 14628,   102,  2841, 17128,  1977, 14190],\n",
              "         [  821, 13334,   475,  6439,   139,   263,  9240,  8564,   117],\n",
              "         [ 3407,   364,  3304,  1526,   263,  5626,   102,  5951,  4330]]),\n",
              " tensor([[  717,   257,   821,  2603,   139,   263,  9240,  8564,     2],\n",
              "         [ 2560,   821, 13334,   475,  6439,   139,   263,  3137,   254],\n",
              "         [11188,   281,   313,   298, 17682,    29,     2,  5096,   257],\n",
              "         [    6,  1192, 14628,   102,  2841, 17128,  1977, 14190,     6],\n",
              "         [13334,   475,  6439,   139,   263,  9240,  8564,   117,  2242],\n",
              "         [  364,  3304,  1526,   263,  5626,   102,  5951,  4330,     4]]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "first_batch_input, first_batch_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "N0xhZcFhVJ01"
      },
      "outputs": [],
      "source": [
        "correct_first_batch_input = torch.LongTensor(\n",
        "        [[    2,   717,   257,   821,  2603,   139,   263,  9240,  8564],\n",
        "         [  717,  2560,   821, 13334,   475,  6439,   139,   263,  3137],\n",
        "         [ 9366, 11188,   281,   313,   298, 17682,    29,     2,  5096],\n",
        "         [27039,     6,  1192, 14628,   102,  2841, 17128,  1977, 14190],\n",
        "         [  821, 13334,   475,  6439,   139,   263,  9240,  8564,   117],\n",
        "         [ 3407,   364,  3304,  1526,   263,  5626,   102,  5951,  4330]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([[  717,   257,   821,  2603,   139,   263,  9240,  8564,     2],\n",
        "         [ 2560,   821, 13334,   475,  6439,   139,   263,  3137,   254],\n",
        "         [11188,   281,   313,   298, 17682,    29,     2,  5096,   257],\n",
        "         [    6,  1192, 14628,   102,  2841, 17128,  1977, 14190,     6],\n",
        "         [13334,   475,  6439,   139,   263,  9240,  8564,   117,  2242],\n",
        "         [  364,  3304,  1526,   263,  5626,   102,  5951,  4330,     4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhfLZAyMVUki",
        "outputId": "4a906e75-b6b5-49ec-c989-ee85d2e49229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert ajustado de dataset.\n"
          ]
        }
      ],
      "source": [
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert ajustado de dataset.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4Cw7LANFEQ",
        "outputId": "424328ae-2eb5-40ae-c83d-9c39571d53f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_sentencas: 250000\n",
            "hparam['max_seq_length']: 100\n"
          ]
        }
      ],
      "source": [
        "#@title Seleção datasets\n",
        "hparam['num_sentenca_train'] = 249800 #@param [800, 249800] {type:'raw'}\n",
        "hparam['num_sentenca_valid'] = 100 # 100\n",
        "hparam['num_sentenca_test'] = 100 # 100\n",
        "hparam['max_seq_length'] = 100 #@param [9, 50, 100, 250] {type:'raw'}\n",
        "total_sentencas = hparam['num_sentenca_train']+hparam['num_sentenca_valid']+hparam['num_sentenca_test']\n",
        "print(f\"total_sentencas: {total_sentencas}\")\n",
        "print(f\"hparam['max_seq_length']: {hparam['max_seq_length']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyq5iuj_NFEQ",
        "outputId": "80d50cc5-a8a5-49b8-fd37-2355d7a59d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/treinamento/202301_IA368DD/aula4/dataset_max_seq_100_text_\n"
          ]
        }
      ],
      "source": [
        "prefixo_nome_diretorio = '/content/drive/My Drive/treinamento/202301_IA368DD/aula4/'\n",
        "\n",
        "infixo_nome= 'dataset_max_seq_'+ str(hparam['max_seq_length'])+'_text_'\n",
        "print(prefixo_nome_diretorio + infixo_nome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVTsirIiqJUB",
        "outputId": "082d20fa-1596-48fc-a5ab-85ae097adb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "#@title Boolean fields\n",
        "datasets_carregados_previamente = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(datasets_carregados_previamente)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io"
      ],
      "metadata": {
        "id": "FUqNOdVlGEYd"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "manqs7PdtOjQ"
      },
      "outputs": [],
      "source": [
        "if datasets_carregados_previamente:\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_test'])+'_test.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  test_dataset = torch.load(buffer)\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_valid'])+'_valid.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  valid_dataset = torch.load(buffer)\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_train'])+'_train.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  train_dataset = torch.load(buffer)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "uqEpL-vBtOjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e311b73-5ded-4d48-ee40-8a729f281b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample-1gb.txt’ already there; not retrieving.\n",
            "\n",
            "250000\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  !wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
        "  texts = open('sample-1gb.txt').readlines() \n",
        "  assert total_sentencas <= len(texts), f\"total sentencas deve ser <= len(texts)\"\n",
        "  # texts = texts[:total]  \n",
        "  print(len(texts)) # 250000 \n",
        "\n",
        "  # carga total para treino:\n",
        "  # hparam['num_sentenca_train'] = total_sentencas - (hparam['num_sentenca_valid'] + hparam['num_sentenca_test'])\n",
        "  # train_texts = texts[:hparam['num_sentenca_train'] ]\n",
        "  # hparam['num_sentenca_train'] = len(train_texts)\n",
        "  # carga parcial para treino:\n",
        "  \n",
        "  #ipdb.set_trace(context=6)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "A5uOibekNNNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6d8a6a-afe2-4ca4-838d-83408cfa615d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carregando valid_dataset\n",
            " len(texts) 100; tamanho_batch 50;  num_batch_entrada 2 \n",
            " max_seq_length 100; Mas salvando sentence_length 101\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 1155; Momento: [2023-Mar-28 04:10:36]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 04:10:36]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 04:10:36]\n",
            "Carregado dataset com 2036 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  valid_texts = texts[-(hparam['num_sentenca_valid'] + hparam['num_sentenca_test']):-hparam['num_sentenca_test']]\n",
        "  print(\"carregando valid_dataset\")\n",
        "  valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(valid_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_valid'])+'_valid.pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "0xkTjjo1NNNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3a486b-76c4-4949-9ae4-649a904b11f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carregando test_dataset\n",
            " len(texts) 100; tamanho_batch 50;  num_batch_entrada 2 \n",
            " max_seq_length 100; Mas salvando sentence_length 101\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 612; Momento: [2023-Mar-28 04:10:42]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 04:10:42]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 04:10:42]\n",
            "Carregado dataset com 1143 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  print(\"carregando test_dataset\")\n",
        "  test_texts = texts[-hparam['num_sentenca_test']:]  \n",
        "  test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(test_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_test'])+'_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "zhKHOXM5NNNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6ee036-0ce2-4740-a92f-53aa16e77777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carregando train_dataset\n",
            " len(texts) 249800; tamanho_batch 50;  num_batch_entrada 4996 \n",
            " max_seq_length 100; Mas salvando sentence_length 101\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 983; Momento: [2023-Mar-28 04:10:48]\n",
            "\t === ndx_batch+1: 11  self.qtd_sequencia: 11200; Momento: [2023-Mar-28 04:10:49]\n",
            "\t === ndx_batch+1: 21  self.qtd_sequencia: 18582; Momento: [2023-Mar-28 04:10:50]\n",
            "\t === ndx_batch+1: 31  self.qtd_sequencia: 27553; Momento: [2023-Mar-28 04:10:51]\n",
            "\t === ndx_batch+1: 41  self.qtd_sequencia: 36533; Momento: [2023-Mar-28 04:10:52]\n",
            "\t === ndx_batch+1: 51  self.qtd_sequencia: 45133; Momento: [2023-Mar-28 04:10:53]\n",
            "\t === ndx_batch+1: 61  self.qtd_sequencia: 56403; Momento: [2023-Mar-28 04:10:54]\n",
            "\t === ndx_batch+1: 71  self.qtd_sequencia: 67434; Momento: [2023-Mar-28 04:10:56]\n",
            "\t === ndx_batch+1: 81  self.qtd_sequencia: 75804; Momento: [2023-Mar-28 04:10:57]\n",
            "\t === ndx_batch+1: 91  self.qtd_sequencia: 84314; Momento: [2023-Mar-28 04:10:58]\n",
            "\t === ndx_batch+1: 101  self.qtd_sequencia: 91674; Momento: [2023-Mar-28 04:10:58]\n",
            "\t === ndx_batch+1: 111  self.qtd_sequencia: 100422; Momento: [2023-Mar-28 04:10:59]\n",
            "\t === ndx_batch+1: 121  self.qtd_sequencia: 110686; Momento: [2023-Mar-28 04:11:01]\n",
            "\t === ndx_batch+1: 131  self.qtd_sequencia: 119048; Momento: [2023-Mar-28 04:11:02]\n",
            "\t === ndx_batch+1: 141  self.qtd_sequencia: 128969; Momento: [2023-Mar-28 04:11:03]\n",
            "\t === ndx_batch+1: 151  self.qtd_sequencia: 136550; Momento: [2023-Mar-28 04:11:04]\n",
            "\t === ndx_batch+1: 161  self.qtd_sequencia: 144763; Momento: [2023-Mar-28 04:11:04]\n",
            "\t === ndx_batch+1: 171  self.qtd_sequencia: 156009; Momento: [2023-Mar-28 04:11:06]\n",
            "\t === ndx_batch+1: 181  self.qtd_sequencia: 164864; Momento: [2023-Mar-28 04:11:07]\n",
            "\t === ndx_batch+1: 191  self.qtd_sequencia: 175120; Momento: [2023-Mar-28 04:11:08]\n",
            "\t === ndx_batch+1: 201  self.qtd_sequencia: 184472; Momento: [2023-Mar-28 04:11:09]\n",
            "\t === ndx_batch+1: 211  self.qtd_sequencia: 192821; Momento: [2023-Mar-28 04:11:10]\n",
            "\t === ndx_batch+1: 221  self.qtd_sequencia: 202103; Momento: [2023-Mar-28 04:11:11]\n",
            "\t === ndx_batch+1: 231  self.qtd_sequencia: 210264; Momento: [2023-Mar-28 04:11:12]\n",
            "\t === ndx_batch+1: 241  self.qtd_sequencia: 218502; Momento: [2023-Mar-28 04:11:12]\n",
            "\t === ndx_batch+1: 251  self.qtd_sequencia: 226462; Momento: [2023-Mar-28 04:11:13]\n",
            "\t === ndx_batch+1: 261  self.qtd_sequencia: 235788; Momento: [2023-Mar-28 04:11:14]\n",
            "\t === ndx_batch+1: 271  self.qtd_sequencia: 243988; Momento: [2023-Mar-28 04:11:15]\n",
            "\t === ndx_batch+1: 281  self.qtd_sequencia: 251118; Momento: [2023-Mar-28 04:11:15]\n",
            "\t === ndx_batch+1: 291  self.qtd_sequencia: 260965; Momento: [2023-Mar-28 04:11:17]\n",
            "\t === ndx_batch+1: 301  self.qtd_sequencia: 269829; Momento: [2023-Mar-28 04:11:18]\n",
            "\t === ndx_batch+1: 311  self.qtd_sequencia: 278550; Momento: [2023-Mar-28 04:11:19]\n",
            "\t === ndx_batch+1: 321  self.qtd_sequencia: 286014; Momento: [2023-Mar-28 04:11:19]\n",
            "\t === ndx_batch+1: 331  self.qtd_sequencia: 293659; Momento: [2023-Mar-28 04:11:20]\n",
            "\t === ndx_batch+1: 341  self.qtd_sequencia: 301289; Momento: [2023-Mar-28 04:11:21]\n",
            "\t === ndx_batch+1: 351  self.qtd_sequencia: 309798; Momento: [2023-Mar-28 04:11:22]\n",
            "\t === ndx_batch+1: 361  self.qtd_sequencia: 317906; Momento: [2023-Mar-28 04:11:23]\n",
            "\t === ndx_batch+1: 371  self.qtd_sequencia: 325450; Momento: [2023-Mar-28 04:11:24]\n",
            "\t === ndx_batch+1: 381  self.qtd_sequencia: 333532; Momento: [2023-Mar-28 04:11:25]\n",
            "\t === ndx_batch+1: 391  self.qtd_sequencia: 341929; Momento: [2023-Mar-28 04:11:25]\n",
            "\t === ndx_batch+1: 401  self.qtd_sequencia: 349774; Momento: [2023-Mar-28 04:11:26]\n",
            "\t === ndx_batch+1: 411  self.qtd_sequencia: 358009; Momento: [2023-Mar-28 04:11:27]\n",
            "\t === ndx_batch+1: 421  self.qtd_sequencia: 367370; Momento: [2023-Mar-28 04:11:28]\n",
            "\t === ndx_batch+1: 431  self.qtd_sequencia: 374805; Momento: [2023-Mar-28 04:11:29]\n",
            "\t === ndx_batch+1: 441  self.qtd_sequencia: 382742; Momento: [2023-Mar-28 04:11:29]\n",
            "\t === ndx_batch+1: 451  self.qtd_sequencia: 391826; Momento: [2023-Mar-28 04:11:30]\n",
            "\t === ndx_batch+1: 461  self.qtd_sequencia: 402322; Momento: [2023-Mar-28 04:11:32]\n",
            "\t === ndx_batch+1: 471  self.qtd_sequencia: 411670; Momento: [2023-Mar-28 04:11:32]\n",
            "\t === ndx_batch+1: 481  self.qtd_sequencia: 420235; Momento: [2023-Mar-28 04:11:33]\n",
            "\t === ndx_batch+1: 491  self.qtd_sequencia: 428970; Momento: [2023-Mar-28 04:11:34]\n",
            "\t === ndx_batch+1: 501  self.qtd_sequencia: 438233; Momento: [2023-Mar-28 04:11:35]\n",
            "\t === ndx_batch+1: 511  self.qtd_sequencia: 446532; Momento: [2023-Mar-28 04:11:36]\n",
            "\t === ndx_batch+1: 521  self.qtd_sequencia: 454340; Momento: [2023-Mar-28 04:11:37]\n",
            "\t === ndx_batch+1: 531  self.qtd_sequencia: 461889; Momento: [2023-Mar-28 04:11:38]\n",
            "\t === ndx_batch+1: 541  self.qtd_sequencia: 470677; Momento: [2023-Mar-28 04:11:39]\n",
            "\t === ndx_batch+1: 551  self.qtd_sequencia: 477870; Momento: [2023-Mar-28 04:11:40]\n",
            "\t === ndx_batch+1: 561  self.qtd_sequencia: 485708; Momento: [2023-Mar-28 04:11:41]\n",
            "\t === ndx_batch+1: 571  self.qtd_sequencia: 494053; Momento: [2023-Mar-28 04:11:42]\n",
            "\t === ndx_batch+1: 581  self.qtd_sequencia: 502445; Momento: [2023-Mar-28 04:11:43]\n",
            "\t === ndx_batch+1: 591  self.qtd_sequencia: 511135; Momento: [2023-Mar-28 04:11:44]\n",
            "\t === ndx_batch+1: 601  self.qtd_sequencia: 520048; Momento: [2023-Mar-28 04:11:45]\n",
            "\t === ndx_batch+1: 611  self.qtd_sequencia: 528624; Momento: [2023-Mar-28 04:11:46]\n",
            "\t === ndx_batch+1: 621  self.qtd_sequencia: 537774; Momento: [2023-Mar-28 04:11:46]\n",
            "\t === ndx_batch+1: 631  self.qtd_sequencia: 545814; Momento: [2023-Mar-28 04:11:47]\n",
            "\t === ndx_batch+1: 641  self.qtd_sequencia: 556084; Momento: [2023-Mar-28 04:11:48]\n",
            "\t === ndx_batch+1: 651  self.qtd_sequencia: 564634; Momento: [2023-Mar-28 04:11:49]\n",
            "\t === ndx_batch+1: 661  self.qtd_sequencia: 573473; Momento: [2023-Mar-28 04:11:50]\n",
            "\t === ndx_batch+1: 671  self.qtd_sequencia: 582965; Momento: [2023-Mar-28 04:11:51]\n",
            "\t === ndx_batch+1: 681  self.qtd_sequencia: 592355; Momento: [2023-Mar-28 04:11:52]\n",
            "\t === ndx_batch+1: 691  self.qtd_sequencia: 599758; Momento: [2023-Mar-28 04:11:53]\n",
            "\t === ndx_batch+1: 701  self.qtd_sequencia: 607997; Momento: [2023-Mar-28 04:11:54]\n",
            "\t === ndx_batch+1: 711  self.qtd_sequencia: 615934; Momento: [2023-Mar-28 04:11:55]\n",
            "\t === ndx_batch+1: 721  self.qtd_sequencia: 624098; Momento: [2023-Mar-28 04:11:56]\n",
            "\t === ndx_batch+1: 731  self.qtd_sequencia: 632150; Momento: [2023-Mar-28 04:11:56]\n",
            "\t === ndx_batch+1: 741  self.qtd_sequencia: 640008; Momento: [2023-Mar-28 04:11:57]\n",
            "\t === ndx_batch+1: 751  self.qtd_sequencia: 648742; Momento: [2023-Mar-28 04:11:58]\n",
            "\t === ndx_batch+1: 761  self.qtd_sequencia: 656562; Momento: [2023-Mar-28 04:11:58]\n",
            "\t === ndx_batch+1: 771  self.qtd_sequencia: 665276; Momento: [2023-Mar-28 04:11:59]\n",
            "\t === ndx_batch+1: 781  self.qtd_sequencia: 673454; Momento: [2023-Mar-28 04:12:00]\n",
            "\t === ndx_batch+1: 791  self.qtd_sequencia: 681085; Momento: [2023-Mar-28 04:12:01]\n",
            "\t === ndx_batch+1: 801  self.qtd_sequencia: 690246; Momento: [2023-Mar-28 04:12:02]\n",
            "\t === ndx_batch+1: 811  self.qtd_sequencia: 698137; Momento: [2023-Mar-28 04:12:02]\n",
            "\t === ndx_batch+1: 821  self.qtd_sequencia: 707549; Momento: [2023-Mar-28 04:12:03]\n",
            "\t === ndx_batch+1: 831  self.qtd_sequencia: 715173; Momento: [2023-Mar-28 04:12:04]\n",
            "\t === ndx_batch+1: 841  self.qtd_sequencia: 723134; Momento: [2023-Mar-28 04:12:05]\n",
            "\t === ndx_batch+1: 851  self.qtd_sequencia: 730918; Momento: [2023-Mar-28 04:12:05]\n",
            "\t === ndx_batch+1: 861  self.qtd_sequencia: 739672; Momento: [2023-Mar-28 04:12:07]\n",
            "\t === ndx_batch+1: 871  self.qtd_sequencia: 747631; Momento: [2023-Mar-28 04:12:07]\n",
            "\t === ndx_batch+1: 881  self.qtd_sequencia: 755491; Momento: [2023-Mar-28 04:12:08]\n",
            "\t === ndx_batch+1: 891  self.qtd_sequencia: 764776; Momento: [2023-Mar-28 04:12:11]\n",
            "\t === ndx_batch+1: 901  self.qtd_sequencia: 773065; Momento: [2023-Mar-28 04:12:11]\n",
            "\t === ndx_batch+1: 911  self.qtd_sequencia: 780908; Momento: [2023-Mar-28 04:12:12]\n",
            "\t === ndx_batch+1: 921  self.qtd_sequencia: 788977; Momento: [2023-Mar-28 04:12:13]\n",
            "\t === ndx_batch+1: 931  self.qtd_sequencia: 797471; Momento: [2023-Mar-28 04:12:14]\n",
            "\t === ndx_batch+1: 941  self.qtd_sequencia: 804994; Momento: [2023-Mar-28 04:12:15]\n",
            "\t === ndx_batch+1: 951  self.qtd_sequencia: 811567; Momento: [2023-Mar-28 04:12:15]\n",
            "\t === ndx_batch+1: 961  self.qtd_sequencia: 824537; Momento: [2023-Mar-28 04:12:17]\n",
            "\t === ndx_batch+1: 971  self.qtd_sequencia: 833971; Momento: [2023-Mar-28 04:12:18]\n",
            "\t === ndx_batch+1: 981  self.qtd_sequencia: 842499; Momento: [2023-Mar-28 04:12:18]\n",
            "\t === ndx_batch+1: 991  self.qtd_sequencia: 850151; Momento: [2023-Mar-28 04:12:19]\n",
            "\t === ndx_batch+1: 1001  self.qtd_sequencia: 861577; Momento: [2023-Mar-28 04:12:20]\n",
            "\t === ndx_batch+1: 1011  self.qtd_sequencia: 870920; Momento: [2023-Mar-28 04:12:22]\n",
            "\t === ndx_batch+1: 1021  self.qtd_sequencia: 880229; Momento: [2023-Mar-28 04:12:23]\n",
            "\t === ndx_batch+1: 1031  self.qtd_sequencia: 889794; Momento: [2023-Mar-28 04:12:24]\n",
            "\t === ndx_batch+1: 1041  self.qtd_sequencia: 897410; Momento: [2023-Mar-28 04:12:25]\n",
            "\t === ndx_batch+1: 1051  self.qtd_sequencia: 906995; Momento: [2023-Mar-28 04:12:26]\n",
            "\t === ndx_batch+1: 1061  self.qtd_sequencia: 914533; Momento: [2023-Mar-28 04:12:26]\n",
            "\t === ndx_batch+1: 1071  self.qtd_sequencia: 923176; Momento: [2023-Mar-28 04:12:27]\n",
            "\t === ndx_batch+1: 1081  self.qtd_sequencia: 930521; Momento: [2023-Mar-28 04:12:28]\n",
            "\t === ndx_batch+1: 1091  self.qtd_sequencia: 938487; Momento: [2023-Mar-28 04:12:29]\n",
            "\t === ndx_batch+1: 1101  self.qtd_sequencia: 946372; Momento: [2023-Mar-28 04:12:29]\n",
            "\t === ndx_batch+1: 1111  self.qtd_sequencia: 956152; Momento: [2023-Mar-28 04:12:30]\n",
            "\t === ndx_batch+1: 1121  self.qtd_sequencia: 963986; Momento: [2023-Mar-28 04:12:31]\n",
            "\t === ndx_batch+1: 1131  self.qtd_sequencia: 974618; Momento: [2023-Mar-28 04:12:32]\n",
            "\t === ndx_batch+1: 1141  self.qtd_sequencia: 983448; Momento: [2023-Mar-28 04:12:33]\n",
            "\t === ndx_batch+1: 1151  self.qtd_sequencia: 993099; Momento: [2023-Mar-28 04:12:34]\n",
            "\t === ndx_batch+1: 1161  self.qtd_sequencia: 1002578; Momento: [2023-Mar-28 04:12:35]\n",
            "\t === ndx_batch+1: 1171  self.qtd_sequencia: 1010262; Momento: [2023-Mar-28 04:12:36]\n",
            "\t === ndx_batch+1: 1181  self.qtd_sequencia: 1016674; Momento: [2023-Mar-28 04:12:37]\n",
            "\t === ndx_batch+1: 1191  self.qtd_sequencia: 1026557; Momento: [2023-Mar-28 04:12:38]\n",
            "\t === ndx_batch+1: 1201  self.qtd_sequencia: 1036756; Momento: [2023-Mar-28 04:12:39]\n",
            "\t === ndx_batch+1: 1211  self.qtd_sequencia: 1045664; Momento: [2023-Mar-28 04:12:40]\n",
            "\t === ndx_batch+1: 1221  self.qtd_sequencia: 1055454; Momento: [2023-Mar-28 04:12:42]\n",
            "\t === ndx_batch+1: 1231  self.qtd_sequencia: 1063742; Momento: [2023-Mar-28 04:12:42]\n",
            "\t === ndx_batch+1: 1241  self.qtd_sequencia: 1073399; Momento: [2023-Mar-28 04:12:43]\n",
            "\t === ndx_batch+1: 1251  self.qtd_sequencia: 1080760; Momento: [2023-Mar-28 04:12:44]\n",
            "\t === ndx_batch+1: 1261  self.qtd_sequencia: 1090513; Momento: [2023-Mar-28 04:12:45]\n",
            "\t === ndx_batch+1: 1271  self.qtd_sequencia: 1099761; Momento: [2023-Mar-28 04:12:46]\n",
            "\t === ndx_batch+1: 1281  self.qtd_sequencia: 1107893; Momento: [2023-Mar-28 04:12:47]\n",
            "\t === ndx_batch+1: 1291  self.qtd_sequencia: 1116931; Momento: [2023-Mar-28 04:12:49]\n",
            "\t === ndx_batch+1: 1301  self.qtd_sequencia: 1125859; Momento: [2023-Mar-28 04:12:50]\n",
            "\t === ndx_batch+1: 1311  self.qtd_sequencia: 1133291; Momento: [2023-Mar-28 04:12:51]\n",
            "\t === ndx_batch+1: 1321  self.qtd_sequencia: 1141411; Momento: [2023-Mar-28 04:12:52]\n",
            "\t === ndx_batch+1: 1331  self.qtd_sequencia: 1148864; Momento: [2023-Mar-28 04:12:53]\n",
            "\t === ndx_batch+1: 1341  self.qtd_sequencia: 1158429; Momento: [2023-Mar-28 04:12:54]\n",
            "\t === ndx_batch+1: 1351  self.qtd_sequencia: 1167288; Momento: [2023-Mar-28 04:12:55]\n",
            "\t === ndx_batch+1: 1361  self.qtd_sequencia: 1174999; Momento: [2023-Mar-28 04:12:56]\n",
            "\t === ndx_batch+1: 1371  self.qtd_sequencia: 1185334; Momento: [2023-Mar-28 04:12:57]\n",
            "\t === ndx_batch+1: 1381  self.qtd_sequencia: 1194803; Momento: [2023-Mar-28 04:12:58]\n",
            "\t === ndx_batch+1: 1391  self.qtd_sequencia: 1203521; Momento: [2023-Mar-28 04:12:59]\n",
            "\t === ndx_batch+1: 1401  self.qtd_sequencia: 1211218; Momento: [2023-Mar-28 04:13:00]\n",
            "\t === ndx_batch+1: 1411  self.qtd_sequencia: 1219879; Momento: [2023-Mar-28 04:13:00]\n",
            "\t === ndx_batch+1: 1421  self.qtd_sequencia: 1228184; Momento: [2023-Mar-28 04:13:01]\n",
            "\t === ndx_batch+1: 1431  self.qtd_sequencia: 1236570; Momento: [2023-Mar-28 04:13:02]\n",
            "\t === ndx_batch+1: 1441  self.qtd_sequencia: 1245686; Momento: [2023-Mar-28 04:13:03]\n",
            "\t === ndx_batch+1: 1451  self.qtd_sequencia: 1254008; Momento: [2023-Mar-28 04:13:04]\n",
            "\t === ndx_batch+1: 1461  self.qtd_sequencia: 1261224; Momento: [2023-Mar-28 04:13:04]\n",
            "\t === ndx_batch+1: 1471  self.qtd_sequencia: 1269464; Momento: [2023-Mar-28 04:13:05]\n",
            "\t === ndx_batch+1: 1481  self.qtd_sequencia: 1277021; Momento: [2023-Mar-28 04:13:06]\n",
            "\t === ndx_batch+1: 1491  self.qtd_sequencia: 1286038; Momento: [2023-Mar-28 04:13:07]\n",
            "\t === ndx_batch+1: 1501  self.qtd_sequencia: 1293932; Momento: [2023-Mar-28 04:13:08]\n",
            "\t === ndx_batch+1: 1511  self.qtd_sequencia: 1303280; Momento: [2023-Mar-28 04:13:09]\n",
            "\t === ndx_batch+1: 1521  self.qtd_sequencia: 1312223; Momento: [2023-Mar-28 04:13:10]\n",
            "\t === ndx_batch+1: 1531  self.qtd_sequencia: 1320514; Momento: [2023-Mar-28 04:13:11]\n",
            "\t === ndx_batch+1: 1541  self.qtd_sequencia: 1328546; Momento: [2023-Mar-28 04:13:12]\n",
            "\t === ndx_batch+1: 1551  self.qtd_sequencia: 1336631; Momento: [2023-Mar-28 04:13:13]\n",
            "\t === ndx_batch+1: 1561  self.qtd_sequencia: 1346941; Momento: [2023-Mar-28 04:13:14]\n",
            "\t === ndx_batch+1: 1571  self.qtd_sequencia: 1355803; Momento: [2023-Mar-28 04:13:15]\n",
            "\t === ndx_batch+1: 1581  self.qtd_sequencia: 1365086; Momento: [2023-Mar-28 04:13:16]\n",
            "\t === ndx_batch+1: 1591  self.qtd_sequencia: 1373687; Momento: [2023-Mar-28 04:13:17]\n",
            "\t === ndx_batch+1: 1601  self.qtd_sequencia: 1382274; Momento: [2023-Mar-28 04:13:18]\n",
            "\t === ndx_batch+1: 1611  self.qtd_sequencia: 1390596; Momento: [2023-Mar-28 04:13:19]\n",
            "\t === ndx_batch+1: 1621  self.qtd_sequencia: 1399526; Momento: [2023-Mar-28 04:13:19]\n",
            "\t === ndx_batch+1: 1631  self.qtd_sequencia: 1408844; Momento: [2023-Mar-28 04:13:21]\n",
            "\t === ndx_batch+1: 1641  self.qtd_sequencia: 1418108; Momento: [2023-Mar-28 04:13:21]\n",
            "\t === ndx_batch+1: 1651  self.qtd_sequencia: 1426968; Momento: [2023-Mar-28 04:13:23]\n",
            "\t === ndx_batch+1: 1661  self.qtd_sequencia: 1435461; Momento: [2023-Mar-28 04:13:24]\n",
            "\t === ndx_batch+1: 1671  self.qtd_sequencia: 1444637; Momento: [2023-Mar-28 04:13:25]\n",
            "\t === ndx_batch+1: 1681  self.qtd_sequencia: 1453739; Momento: [2023-Mar-28 04:13:26]\n",
            "\t === ndx_batch+1: 1691  self.qtd_sequencia: 1462110; Momento: [2023-Mar-28 04:13:27]\n",
            "\t === ndx_batch+1: 1701  self.qtd_sequencia: 1470410; Momento: [2023-Mar-28 04:13:28]\n",
            "\t === ndx_batch+1: 1711  self.qtd_sequencia: 1480572; Momento: [2023-Mar-28 04:13:29]\n",
            "\t === ndx_batch+1: 1721  self.qtd_sequencia: 1489384; Momento: [2023-Mar-28 04:13:29]\n",
            "\t === ndx_batch+1: 1731  self.qtd_sequencia: 1497586; Momento: [2023-Mar-28 04:13:30]\n",
            "\t === ndx_batch+1: 1741  self.qtd_sequencia: 1505674; Momento: [2023-Mar-28 04:13:31]\n",
            "\t === ndx_batch+1: 1751  self.qtd_sequencia: 1515536; Momento: [2023-Mar-28 04:13:32]\n",
            "\t === ndx_batch+1: 1761  self.qtd_sequencia: 1524393; Momento: [2023-Mar-28 04:13:33]\n",
            "\t === ndx_batch+1: 1771  self.qtd_sequencia: 1531720; Momento: [2023-Mar-28 04:13:34]\n",
            "\t === ndx_batch+1: 1781  self.qtd_sequencia: 1540938; Momento: [2023-Mar-28 04:13:35]\n",
            "\t === ndx_batch+1: 1791  self.qtd_sequencia: 1549676; Momento: [2023-Mar-28 04:13:35]\n",
            "\t === ndx_batch+1: 1801  self.qtd_sequencia: 1558553; Momento: [2023-Mar-28 04:13:39]\n",
            "\t === ndx_batch+1: 1811  self.qtd_sequencia: 1567009; Momento: [2023-Mar-28 04:13:40]\n",
            "\t === ndx_batch+1: 1821  self.qtd_sequencia: 1575108; Momento: [2023-Mar-28 04:13:41]\n",
            "\t === ndx_batch+1: 1831  self.qtd_sequencia: 1584646; Momento: [2023-Mar-28 04:13:42]\n",
            "\t === ndx_batch+1: 1841  self.qtd_sequencia: 1593759; Momento: [2023-Mar-28 04:13:43]\n",
            "\t === ndx_batch+1: 1851  self.qtd_sequencia: 1602535; Momento: [2023-Mar-28 04:13:44]\n",
            "\t === ndx_batch+1: 1861  self.qtd_sequencia: 1614453; Momento: [2023-Mar-28 04:13:45]\n",
            "\t === ndx_batch+1: 1871  self.qtd_sequencia: 1622289; Momento: [2023-Mar-28 04:13:46]\n",
            "\t === ndx_batch+1: 1881  self.qtd_sequencia: 1630870; Momento: [2023-Mar-28 04:13:47]\n",
            "\t === ndx_batch+1: 1891  self.qtd_sequencia: 1638369; Momento: [2023-Mar-28 04:13:48]\n",
            "\t === ndx_batch+1: 1901  self.qtd_sequencia: 1648029; Momento: [2023-Mar-28 04:13:49]\n",
            "\t === ndx_batch+1: 1911  self.qtd_sequencia: 1655586; Momento: [2023-Mar-28 04:13:50]\n",
            "\t === ndx_batch+1: 1921  self.qtd_sequencia: 1665032; Momento: [2023-Mar-28 04:13:51]\n",
            "\t === ndx_batch+1: 1931  self.qtd_sequencia: 1672110; Momento: [2023-Mar-28 04:13:52]\n",
            "\t === ndx_batch+1: 1941  self.qtd_sequencia: 1680917; Momento: [2023-Mar-28 04:13:54]\n",
            "\t === ndx_batch+1: 1951  self.qtd_sequencia: 1688581; Momento: [2023-Mar-28 04:13:55]\n",
            "\t === ndx_batch+1: 1961  self.qtd_sequencia: 1698333; Momento: [2023-Mar-28 04:13:56]\n",
            "\t === ndx_batch+1: 1971  self.qtd_sequencia: 1707054; Momento: [2023-Mar-28 04:13:58]\n",
            "\t === ndx_batch+1: 1981  self.qtd_sequencia: 1716388; Momento: [2023-Mar-28 04:13:59]\n",
            "\t === ndx_batch+1: 1991  self.qtd_sequencia: 1726202; Momento: [2023-Mar-28 04:14:00]\n",
            "\t === ndx_batch+1: 2001  self.qtd_sequencia: 1736045; Momento: [2023-Mar-28 04:14:01]\n",
            "\t === ndx_batch+1: 2011  self.qtd_sequencia: 1744310; Momento: [2023-Mar-28 04:14:01]\n",
            "\t === ndx_batch+1: 2021  self.qtd_sequencia: 1753240; Momento: [2023-Mar-28 04:14:02]\n",
            "\t === ndx_batch+1: 2031  self.qtd_sequencia: 1761414; Momento: [2023-Mar-28 04:14:03]\n",
            "\t === ndx_batch+1: 2041  self.qtd_sequencia: 1770762; Momento: [2023-Mar-28 04:14:04]\n",
            "\t === ndx_batch+1: 2051  self.qtd_sequencia: 1781161; Momento: [2023-Mar-28 04:14:05]\n",
            "\t === ndx_batch+1: 2061  self.qtd_sequencia: 1789418; Momento: [2023-Mar-28 04:14:06]\n",
            "\t === ndx_batch+1: 2071  self.qtd_sequencia: 1799203; Momento: [2023-Mar-28 04:14:07]\n",
            "\t === ndx_batch+1: 2081  self.qtd_sequencia: 1809220; Momento: [2023-Mar-28 04:14:08]\n",
            "\t === ndx_batch+1: 2091  self.qtd_sequencia: 1815917; Momento: [2023-Mar-28 04:14:08]\n",
            "\t === ndx_batch+1: 2101  self.qtd_sequencia: 1825312; Momento: [2023-Mar-28 04:14:10]\n",
            "\t === ndx_batch+1: 2111  self.qtd_sequencia: 1834297; Momento: [2023-Mar-28 04:14:11]\n",
            "\t === ndx_batch+1: 2121  self.qtd_sequencia: 1844196; Momento: [2023-Mar-28 04:14:12]\n",
            "\t === ndx_batch+1: 2131  self.qtd_sequencia: 1852535; Momento: [2023-Mar-28 04:14:13]\n",
            "\t === ndx_batch+1: 2141  self.qtd_sequencia: 1861123; Momento: [2023-Mar-28 04:14:14]\n",
            "\t === ndx_batch+1: 2151  self.qtd_sequencia: 1869749; Momento: [2023-Mar-28 04:14:15]\n",
            "\t === ndx_batch+1: 2161  self.qtd_sequencia: 1879138; Momento: [2023-Mar-28 04:14:16]\n",
            "\t === ndx_batch+1: 2171  self.qtd_sequencia: 1887005; Momento: [2023-Mar-28 04:14:17]\n",
            "\t === ndx_batch+1: 2181  self.qtd_sequencia: 1894796; Momento: [2023-Mar-28 04:14:17]\n",
            "\t === ndx_batch+1: 2191  self.qtd_sequencia: 1904606; Momento: [2023-Mar-28 04:14:18]\n",
            "\t === ndx_batch+1: 2201  self.qtd_sequencia: 1913278; Momento: [2023-Mar-28 04:14:19]\n",
            "\t === ndx_batch+1: 2211  self.qtd_sequencia: 1919669; Momento: [2023-Mar-28 04:14:20]\n",
            "\t === ndx_batch+1: 2221  self.qtd_sequencia: 1929204; Momento: [2023-Mar-28 04:14:21]\n",
            "\t === ndx_batch+1: 2231  self.qtd_sequencia: 1939713; Momento: [2023-Mar-28 04:14:22]\n",
            "\t === ndx_batch+1: 2241  self.qtd_sequencia: 1948309; Momento: [2023-Mar-28 04:14:23]\n",
            "\t === ndx_batch+1: 2251  self.qtd_sequencia: 1956725; Momento: [2023-Mar-28 04:14:23]\n",
            "\t === ndx_batch+1: 2261  self.qtd_sequencia: 1965023; Momento: [2023-Mar-28 04:14:24]\n",
            "\t === ndx_batch+1: 2271  self.qtd_sequencia: 1975558; Momento: [2023-Mar-28 04:14:26]\n",
            "\t === ndx_batch+1: 2281  self.qtd_sequencia: 1984373; Momento: [2023-Mar-28 04:14:27]\n",
            "\t === ndx_batch+1: 2291  self.qtd_sequencia: 1993339; Momento: [2023-Mar-28 04:14:28]\n",
            "\t === ndx_batch+1: 2301  self.qtd_sequencia: 2001890; Momento: [2023-Mar-28 04:14:29]\n",
            "\t === ndx_batch+1: 2311  self.qtd_sequencia: 2010531; Momento: [2023-Mar-28 04:14:29]\n",
            "\t === ndx_batch+1: 2321  self.qtd_sequencia: 2019007; Momento: [2023-Mar-28 04:14:30]\n",
            "\t === ndx_batch+1: 2331  self.qtd_sequencia: 2026388; Momento: [2023-Mar-28 04:14:31]\n",
            "\t === ndx_batch+1: 2341  self.qtd_sequencia: 2035011; Momento: [2023-Mar-28 04:14:32]\n",
            "\t === ndx_batch+1: 2351  self.qtd_sequencia: 2044111; Momento: [2023-Mar-28 04:14:33]\n",
            "\t === ndx_batch+1: 2361  self.qtd_sequencia: 2052697; Momento: [2023-Mar-28 04:14:33]\n",
            "\t === ndx_batch+1: 2371  self.qtd_sequencia: 2060180; Momento: [2023-Mar-28 04:14:34]\n",
            "\t === ndx_batch+1: 2381  self.qtd_sequencia: 2068637; Momento: [2023-Mar-28 04:14:35]\n",
            "\t === ndx_batch+1: 2391  self.qtd_sequencia: 2075726; Momento: [2023-Mar-28 04:14:36]\n",
            "\t === ndx_batch+1: 2401  self.qtd_sequencia: 2083366; Momento: [2023-Mar-28 04:14:36]\n",
            "\t === ndx_batch+1: 2411  self.qtd_sequencia: 2091461; Momento: [2023-Mar-28 04:14:37]\n",
            "\t === ndx_batch+1: 2421  self.qtd_sequencia: 2101645; Momento: [2023-Mar-28 04:14:42]\n",
            "\t === ndx_batch+1: 2431  self.qtd_sequencia: 2109936; Momento: [2023-Mar-28 04:14:43]\n",
            "\t === ndx_batch+1: 2441  self.qtd_sequencia: 2118230; Momento: [2023-Mar-28 04:14:43]\n",
            "\t === ndx_batch+1: 2451  self.qtd_sequencia: 2126692; Momento: [2023-Mar-28 04:14:44]\n",
            "\t === ndx_batch+1: 2461  self.qtd_sequencia: 2134297; Momento: [2023-Mar-28 04:14:45]\n",
            "\t === ndx_batch+1: 2471  self.qtd_sequencia: 2142234; Momento: [2023-Mar-28 04:14:46]\n",
            "\t === ndx_batch+1: 2481  self.qtd_sequencia: 2149865; Momento: [2023-Mar-28 04:14:47]\n",
            "\t === ndx_batch+1: 2491  self.qtd_sequencia: 2157734; Momento: [2023-Mar-28 04:14:47]\n",
            "\t === ndx_batch+1: 2501  self.qtd_sequencia: 2166289; Momento: [2023-Mar-28 04:14:48]\n",
            "\t === ndx_batch+1: 2511  self.qtd_sequencia: 2176236; Momento: [2023-Mar-28 04:14:49]\n",
            "\t === ndx_batch+1: 2521  self.qtd_sequencia: 2184509; Momento: [2023-Mar-28 04:14:50]\n",
            "\t === ndx_batch+1: 2531  self.qtd_sequencia: 2193342; Momento: [2023-Mar-28 04:14:51]\n",
            "\t === ndx_batch+1: 2541  self.qtd_sequencia: 2202793; Momento: [2023-Mar-28 04:14:52]\n",
            "\t === ndx_batch+1: 2551  self.qtd_sequencia: 2212358; Momento: [2023-Mar-28 04:14:54]\n",
            "\t === ndx_batch+1: 2561  self.qtd_sequencia: 2220111; Momento: [2023-Mar-28 04:14:55]\n",
            "\t === ndx_batch+1: 2571  self.qtd_sequencia: 2227345; Momento: [2023-Mar-28 04:14:56]\n",
            "\t === ndx_batch+1: 2581  self.qtd_sequencia: 2235887; Momento: [2023-Mar-28 04:14:57]\n",
            "\t === ndx_batch+1: 2591  self.qtd_sequencia: 2244115; Momento: [2023-Mar-28 04:14:57]\n",
            "\t === ndx_batch+1: 2601  self.qtd_sequencia: 2253462; Momento: [2023-Mar-28 04:14:59]\n",
            "\t === ndx_batch+1: 2611  self.qtd_sequencia: 2262425; Momento: [2023-Mar-28 04:15:00]\n",
            "\t === ndx_batch+1: 2621  self.qtd_sequencia: 2270803; Momento: [2023-Mar-28 04:15:00]\n",
            "\t === ndx_batch+1: 2631  self.qtd_sequencia: 2278800; Momento: [2023-Mar-28 04:15:01]\n",
            "\t === ndx_batch+1: 2641  self.qtd_sequencia: 2288882; Momento: [2023-Mar-28 04:15:02]\n",
            "\t === ndx_batch+1: 2651  self.qtd_sequencia: 2297181; Momento: [2023-Mar-28 04:15:03]\n",
            "\t === ndx_batch+1: 2661  self.qtd_sequencia: 2304429; Momento: [2023-Mar-28 04:15:04]\n",
            "\t === ndx_batch+1: 2671  self.qtd_sequencia: 2313011; Momento: [2023-Mar-28 04:15:05]\n",
            "\t === ndx_batch+1: 2681  self.qtd_sequencia: 2321574; Momento: [2023-Mar-28 04:15:05]\n",
            "\t === ndx_batch+1: 2691  self.qtd_sequencia: 2329715; Momento: [2023-Mar-28 04:15:06]\n",
            "\t === ndx_batch+1: 2701  self.qtd_sequencia: 2339392; Momento: [2023-Mar-28 04:15:07]\n",
            "\t === ndx_batch+1: 2711  self.qtd_sequencia: 2345705; Momento: [2023-Mar-28 04:15:08]\n",
            "\t === ndx_batch+1: 2721  self.qtd_sequencia: 2352305; Momento: [2023-Mar-28 04:15:08]\n",
            "\t === ndx_batch+1: 2731  self.qtd_sequencia: 2359196; Momento: [2023-Mar-28 04:15:09]\n",
            "\t === ndx_batch+1: 2741  self.qtd_sequencia: 2367904; Momento: [2023-Mar-28 04:15:10]\n",
            "\t === ndx_batch+1: 2751  self.qtd_sequencia: 2377413; Momento: [2023-Mar-28 04:15:11]\n",
            "\t === ndx_batch+1: 2761  self.qtd_sequencia: 2387301; Momento: [2023-Mar-28 04:15:13]\n",
            "\t === ndx_batch+1: 2771  self.qtd_sequencia: 2396233; Momento: [2023-Mar-28 04:15:14]\n",
            "\t === ndx_batch+1: 2781  self.qtd_sequencia: 2406044; Momento: [2023-Mar-28 04:15:15]\n",
            "\t === ndx_batch+1: 2791  self.qtd_sequencia: 2414804; Momento: [2023-Mar-28 04:15:16]\n",
            "\t === ndx_batch+1: 2801  self.qtd_sequencia: 2422973; Momento: [2023-Mar-28 04:15:16]\n",
            "\t === ndx_batch+1: 2811  self.qtd_sequencia: 2431009; Momento: [2023-Mar-28 04:15:17]\n",
            "\t === ndx_batch+1: 2821  self.qtd_sequencia: 2439251; Momento: [2023-Mar-28 04:15:18]\n",
            "\t === ndx_batch+1: 2831  self.qtd_sequencia: 2446338; Momento: [2023-Mar-28 04:15:19]\n",
            "\t === ndx_batch+1: 2841  self.qtd_sequencia: 2455188; Momento: [2023-Mar-28 04:15:19]\n",
            "\t === ndx_batch+1: 2851  self.qtd_sequencia: 2463289; Momento: [2023-Mar-28 04:15:20]\n",
            "\t === ndx_batch+1: 2861  self.qtd_sequencia: 2470427; Momento: [2023-Mar-28 04:15:21]\n",
            "\t === ndx_batch+1: 2871  self.qtd_sequencia: 2479492; Momento: [2023-Mar-28 04:15:22]\n",
            "\t === ndx_batch+1: 2881  self.qtd_sequencia: 2487984; Momento: [2023-Mar-28 04:15:23]\n",
            "\t === ndx_batch+1: 2891  self.qtd_sequencia: 2497760; Momento: [2023-Mar-28 04:15:24]\n",
            "\t === ndx_batch+1: 2901  self.qtd_sequencia: 2504709; Momento: [2023-Mar-28 04:15:24]\n",
            "\t === ndx_batch+1: 2911  self.qtd_sequencia: 2513453; Momento: [2023-Mar-28 04:15:25]\n",
            "\t === ndx_batch+1: 2921  self.qtd_sequencia: 2521328; Momento: [2023-Mar-28 04:15:26]\n",
            "\t === ndx_batch+1: 2931  self.qtd_sequencia: 2529987; Momento: [2023-Mar-28 04:15:27]\n",
            "\t === ndx_batch+1: 2941  self.qtd_sequencia: 2537080; Momento: [2023-Mar-28 04:15:28]\n",
            "\t === ndx_batch+1: 2951  self.qtd_sequencia: 2546875; Momento: [2023-Mar-28 04:15:29]\n",
            "\t === ndx_batch+1: 2961  self.qtd_sequencia: 2554349; Momento: [2023-Mar-28 04:15:30]\n",
            "\t === ndx_batch+1: 2971  self.qtd_sequencia: 2560952; Momento: [2023-Mar-28 04:15:31]\n",
            "\t === ndx_batch+1: 2981  self.qtd_sequencia: 2569536; Momento: [2023-Mar-28 04:15:31]\n",
            "\t === ndx_batch+1: 2991  self.qtd_sequencia: 2579182; Momento: [2023-Mar-28 04:15:32]\n",
            "\t === ndx_batch+1: 3001  self.qtd_sequencia: 2587017; Momento: [2023-Mar-28 04:15:33]\n",
            "\t === ndx_batch+1: 3011  self.qtd_sequencia: 2594884; Momento: [2023-Mar-28 04:15:34]\n",
            "\t === ndx_batch+1: 3021  self.qtd_sequencia: 2602925; Momento: [2023-Mar-28 04:15:35]\n",
            "\t === ndx_batch+1: 3031  self.qtd_sequencia: 2611353; Momento: [2023-Mar-28 04:15:35]\n",
            "\t === ndx_batch+1: 3041  self.qtd_sequencia: 2621455; Momento: [2023-Mar-28 04:15:36]\n",
            "\t === ndx_batch+1: 3051  self.qtd_sequencia: 2630303; Momento: [2023-Mar-28 04:15:37]\n",
            "\t === ndx_batch+1: 3061  self.qtd_sequencia: 2641215; Momento: [2023-Mar-28 04:15:38]\n",
            "\t === ndx_batch+1: 3071  self.qtd_sequencia: 2649319; Momento: [2023-Mar-28 04:15:39]\n",
            "\t === ndx_batch+1: 3081  self.qtd_sequencia: 2657967; Momento: [2023-Mar-28 04:15:40]\n",
            "\t === ndx_batch+1: 3091  self.qtd_sequencia: 2666309; Momento: [2023-Mar-28 04:15:41]\n",
            "\t === ndx_batch+1: 3101  self.qtd_sequencia: 2674605; Momento: [2023-Mar-28 04:15:42]\n",
            "\t === ndx_batch+1: 3111  self.qtd_sequencia: 2683651; Momento: [2023-Mar-28 04:15:43]\n",
            "\t === ndx_batch+1: 3121  self.qtd_sequencia: 2693649; Momento: [2023-Mar-28 04:15:44]\n",
            "\t === ndx_batch+1: 3131  self.qtd_sequencia: 2701981; Momento: [2023-Mar-28 04:15:45]\n",
            "\t === ndx_batch+1: 3141  self.qtd_sequencia: 2711121; Momento: [2023-Mar-28 04:15:46]\n",
            "\t === ndx_batch+1: 3151  self.qtd_sequencia: 2721407; Momento: [2023-Mar-28 04:15:47]\n",
            "\t === ndx_batch+1: 3161  self.qtd_sequencia: 2729123; Momento: [2023-Mar-28 04:15:48]\n",
            "\t === ndx_batch+1: 3171  self.qtd_sequencia: 2739081; Momento: [2023-Mar-28 04:15:49]\n",
            "\t === ndx_batch+1: 3181  self.qtd_sequencia: 2746827; Momento: [2023-Mar-28 04:15:50]\n",
            "\t === ndx_batch+1: 3191  self.qtd_sequencia: 2756303; Momento: [2023-Mar-28 04:15:51]\n",
            "\t === ndx_batch+1: 3201  self.qtd_sequencia: 2763928; Momento: [2023-Mar-28 04:15:51]\n",
            "\t === ndx_batch+1: 3211  self.qtd_sequencia: 2773722; Momento: [2023-Mar-28 04:15:52]\n",
            "\t === ndx_batch+1: 3221  self.qtd_sequencia: 2782217; Momento: [2023-Mar-28 04:15:57]\n",
            "\t === ndx_batch+1: 3231  self.qtd_sequencia: 2791100; Momento: [2023-Mar-28 04:15:58]\n",
            "\t === ndx_batch+1: 3241  self.qtd_sequencia: 2800770; Momento: [2023-Mar-28 04:15:59]\n",
            "\t === ndx_batch+1: 3251  self.qtd_sequencia: 2809797; Momento: [2023-Mar-28 04:16:00]\n",
            "\t === ndx_batch+1: 3261  self.qtd_sequencia: 2819218; Momento: [2023-Mar-28 04:16:01]\n",
            "\t === ndx_batch+1: 3271  self.qtd_sequencia: 2827618; Momento: [2023-Mar-28 04:16:02]\n",
            "\t === ndx_batch+1: 3281  self.qtd_sequencia: 2835849; Momento: [2023-Mar-28 04:16:03]\n",
            "\t === ndx_batch+1: 3291  self.qtd_sequencia: 2847062; Momento: [2023-Mar-28 04:16:04]\n",
            "\t === ndx_batch+1: 3301  self.qtd_sequencia: 2855708; Momento: [2023-Mar-28 04:16:05]\n",
            "\t === ndx_batch+1: 3311  self.qtd_sequencia: 2864354; Momento: [2023-Mar-28 04:16:06]\n",
            "\t === ndx_batch+1: 3321  self.qtd_sequencia: 2872724; Momento: [2023-Mar-28 04:16:06]\n",
            "\t === ndx_batch+1: 3331  self.qtd_sequencia: 2882027; Momento: [2023-Mar-28 04:16:07]\n",
            "\t === ndx_batch+1: 3341  self.qtd_sequencia: 2888508; Momento: [2023-Mar-28 04:16:08]\n",
            "\t === ndx_batch+1: 3351  self.qtd_sequencia: 2896762; Momento: [2023-Mar-28 04:16:09]\n",
            "\t === ndx_batch+1: 3361  self.qtd_sequencia: 2905628; Momento: [2023-Mar-28 04:16:09]\n",
            "\t === ndx_batch+1: 3371  self.qtd_sequencia: 2914310; Momento: [2023-Mar-28 04:16:11]\n",
            "\t === ndx_batch+1: 3381  self.qtd_sequencia: 2921592; Momento: [2023-Mar-28 04:16:11]\n",
            "\t === ndx_batch+1: 3391  self.qtd_sequencia: 2929559; Momento: [2023-Mar-28 04:16:12]\n",
            "\t === ndx_batch+1: 3401  self.qtd_sequencia: 2937757; Momento: [2023-Mar-28 04:16:13]\n",
            "\t === ndx_batch+1: 3411  self.qtd_sequencia: 2946725; Momento: [2023-Mar-28 04:16:14]\n",
            "\t === ndx_batch+1: 3421  self.qtd_sequencia: 2954525; Momento: [2023-Mar-28 04:16:15]\n",
            "\t === ndx_batch+1: 3431  self.qtd_sequencia: 2963740; Momento: [2023-Mar-28 04:16:16]\n",
            "\t === ndx_batch+1: 3441  self.qtd_sequencia: 2973866; Momento: [2023-Mar-28 04:16:17]\n",
            "\t === ndx_batch+1: 3451  self.qtd_sequencia: 2980965; Momento: [2023-Mar-28 04:16:18]\n",
            "\t === ndx_batch+1: 3461  self.qtd_sequencia: 2990800; Momento: [2023-Mar-28 04:16:19]\n",
            "\t === ndx_batch+1: 3471  self.qtd_sequencia: 2999861; Momento: [2023-Mar-28 04:16:20]\n",
            "\t === ndx_batch+1: 3481  self.qtd_sequencia: 3008026; Momento: [2023-Mar-28 04:16:21]\n",
            "\t === ndx_batch+1: 3491  self.qtd_sequencia: 3015302; Momento: [2023-Mar-28 04:16:21]\n",
            "\t === ndx_batch+1: 3501  self.qtd_sequencia: 3024346; Momento: [2023-Mar-28 04:16:22]\n",
            "\t === ndx_batch+1: 3511  self.qtd_sequencia: 3033408; Momento: [2023-Mar-28 04:16:23]\n",
            "\t === ndx_batch+1: 3521  self.qtd_sequencia: 3042213; Momento: [2023-Mar-28 04:16:24]\n",
            "\t === ndx_batch+1: 3531  self.qtd_sequencia: 3052499; Momento: [2023-Mar-28 04:16:25]\n",
            "\t === ndx_batch+1: 3541  self.qtd_sequencia: 3060897; Momento: [2023-Mar-28 04:16:26]\n",
            "\t === ndx_batch+1: 3551  self.qtd_sequencia: 3070524; Momento: [2023-Mar-28 04:16:27]\n",
            "\t === ndx_batch+1: 3561  self.qtd_sequencia: 3078681; Momento: [2023-Mar-28 04:16:28]\n",
            "\t === ndx_batch+1: 3571  self.qtd_sequencia: 3090549; Momento: [2023-Mar-28 04:16:30]\n",
            "\t === ndx_batch+1: 3581  self.qtd_sequencia: 3100854; Momento: [2023-Mar-28 04:16:31]\n",
            "\t === ndx_batch+1: 3591  self.qtd_sequencia: 3107961; Momento: [2023-Mar-28 04:16:32]\n",
            "\t === ndx_batch+1: 3601  self.qtd_sequencia: 3117154; Momento: [2023-Mar-28 04:16:33]\n",
            "\t === ndx_batch+1: 3611  self.qtd_sequencia: 3124610; Momento: [2023-Mar-28 04:16:33]\n",
            "\t === ndx_batch+1: 3621  self.qtd_sequencia: 3133153; Momento: [2023-Mar-28 04:16:34]\n",
            "\t === ndx_batch+1: 3631  self.qtd_sequencia: 3144286; Momento: [2023-Mar-28 04:16:36]\n",
            "\t === ndx_batch+1: 3641  self.qtd_sequencia: 3153530; Momento: [2023-Mar-28 04:16:37]\n",
            "\t === ndx_batch+1: 3651  self.qtd_sequencia: 3161576; Momento: [2023-Mar-28 04:16:38]\n",
            "\t === ndx_batch+1: 3661  self.qtd_sequencia: 3169590; Momento: [2023-Mar-28 04:16:39]\n",
            "\t === ndx_batch+1: 3671  self.qtd_sequencia: 3177985; Momento: [2023-Mar-28 04:16:40]\n",
            "\t === ndx_batch+1: 3681  self.qtd_sequencia: 3184867; Momento: [2023-Mar-28 04:16:41]\n",
            "\t === ndx_batch+1: 3691  self.qtd_sequencia: 3195165; Momento: [2023-Mar-28 04:16:42]\n",
            "\t === ndx_batch+1: 3701  self.qtd_sequencia: 3201939; Momento: [2023-Mar-28 04:16:43]\n",
            "\t === ndx_batch+1: 3711  self.qtd_sequencia: 3211415; Momento: [2023-Mar-28 04:16:44]\n",
            "\t === ndx_batch+1: 3721  self.qtd_sequencia: 3220512; Momento: [2023-Mar-28 04:16:45]\n",
            "\t === ndx_batch+1: 3731  self.qtd_sequencia: 3228543; Momento: [2023-Mar-28 04:16:46]\n",
            "\t === ndx_batch+1: 3741  self.qtd_sequencia: 3238694; Momento: [2023-Mar-28 04:16:47]\n",
            "\t === ndx_batch+1: 3751  self.qtd_sequencia: 3246757; Momento: [2023-Mar-28 04:16:48]\n",
            "\t === ndx_batch+1: 3761  self.qtd_sequencia: 3256426; Momento: [2023-Mar-28 04:16:49]\n",
            "\t === ndx_batch+1: 3771  self.qtd_sequencia: 3264793; Momento: [2023-Mar-28 04:16:49]\n",
            "\t === ndx_batch+1: 3781  self.qtd_sequencia: 3273595; Momento: [2023-Mar-28 04:16:50]\n",
            "\t === ndx_batch+1: 3791  self.qtd_sequencia: 3283548; Momento: [2023-Mar-28 04:16:51]\n",
            "\t === ndx_batch+1: 3801  self.qtd_sequencia: 3293849; Momento: [2023-Mar-28 04:16:52]\n",
            "\t === ndx_batch+1: 3811  self.qtd_sequencia: 3301890; Momento: [2023-Mar-28 04:16:53]\n",
            "\t === ndx_batch+1: 3821  self.qtd_sequencia: 3311149; Momento: [2023-Mar-28 04:16:54]\n",
            "\t === ndx_batch+1: 3831  self.qtd_sequencia: 3318698; Momento: [2023-Mar-28 04:16:55]\n",
            "\t === ndx_batch+1: 3841  self.qtd_sequencia: 3326134; Momento: [2023-Mar-28 04:16:56]\n",
            "\t === ndx_batch+1: 3851  self.qtd_sequencia: 3335827; Momento: [2023-Mar-28 04:16:57]\n",
            "\t === ndx_batch+1: 3861  self.qtd_sequencia: 3344088; Momento: [2023-Mar-28 04:16:58]\n",
            "\t === ndx_batch+1: 3871  self.qtd_sequencia: 3352722; Momento: [2023-Mar-28 04:16:59]\n",
            "\t === ndx_batch+1: 3881  self.qtd_sequencia: 3360332; Momento: [2023-Mar-28 04:17:00]\n",
            "\t === ndx_batch+1: 3891  self.qtd_sequencia: 3368856; Momento: [2023-Mar-28 04:17:01]\n",
            "\t === ndx_batch+1: 3901  self.qtd_sequencia: 3376576; Momento: [2023-Mar-28 04:17:01]\n",
            "\t === ndx_batch+1: 3911  self.qtd_sequencia: 3385302; Momento: [2023-Mar-28 04:17:02]\n",
            "\t === ndx_batch+1: 3921  self.qtd_sequencia: 3394477; Momento: [2023-Mar-28 04:17:03]\n",
            "\t === ndx_batch+1: 3931  self.qtd_sequencia: 3402577; Momento: [2023-Mar-28 04:17:04]\n",
            "\t === ndx_batch+1: 3941  self.qtd_sequencia: 3412048; Momento: [2023-Mar-28 04:17:05]\n",
            "\t === ndx_batch+1: 3951  self.qtd_sequencia: 3421274; Momento: [2023-Mar-28 04:17:06]\n",
            "\t === ndx_batch+1: 3961  self.qtd_sequencia: 3429765; Momento: [2023-Mar-28 04:17:07]\n",
            "\t === ndx_batch+1: 3971  self.qtd_sequencia: 3438533; Momento: [2023-Mar-28 04:17:08]\n",
            "\t === ndx_batch+1: 3981  self.qtd_sequencia: 3447397; Momento: [2023-Mar-28 04:17:09]\n",
            "\t === ndx_batch+1: 3991  self.qtd_sequencia: 3456065; Momento: [2023-Mar-28 04:17:09]\n",
            "\t === ndx_batch+1: 4001  self.qtd_sequencia: 3466226; Momento: [2023-Mar-28 04:17:11]\n",
            "\t === ndx_batch+1: 4011  self.qtd_sequencia: 3473395; Momento: [2023-Mar-28 04:17:12]\n",
            "\t === ndx_batch+1: 4021  self.qtd_sequencia: 3481170; Momento: [2023-Mar-28 04:17:13]\n",
            "\t === ndx_batch+1: 4031  self.qtd_sequencia: 3489798; Momento: [2023-Mar-28 04:17:14]\n",
            "\t === ndx_batch+1: 4041  self.qtd_sequencia: 3497243; Momento: [2023-Mar-28 04:17:14]\n",
            "\t === ndx_batch+1: 4051  self.qtd_sequencia: 3506585; Momento: [2023-Mar-28 04:17:16]\n",
            "\t === ndx_batch+1: 4061  self.qtd_sequencia: 3515530; Momento: [2023-Mar-28 04:17:17]\n",
            "\t === ndx_batch+1: 4071  self.qtd_sequencia: 3524081; Momento: [2023-Mar-28 04:17:17]\n",
            "\t === ndx_batch+1: 4081  self.qtd_sequencia: 3532694; Momento: [2023-Mar-28 04:17:18]\n",
            "\t === ndx_batch+1: 4091  self.qtd_sequencia: 3540823; Momento: [2023-Mar-28 04:17:19]\n",
            "\t === ndx_batch+1: 4101  self.qtd_sequencia: 3547979; Momento: [2023-Mar-28 04:17:20]\n",
            "\t === ndx_batch+1: 4111  self.qtd_sequencia: 3555729; Momento: [2023-Mar-28 04:17:21]\n",
            "\t === ndx_batch+1: 4121  self.qtd_sequencia: 3565713; Momento: [2023-Mar-28 04:17:22]\n",
            "\t === ndx_batch+1: 4131  self.qtd_sequencia: 3574173; Momento: [2023-Mar-28 04:17:23]\n",
            "\t === ndx_batch+1: 4141  self.qtd_sequencia: 3582230; Momento: [2023-Mar-28 04:17:23]\n",
            "\t === ndx_batch+1: 4151  self.qtd_sequencia: 3590911; Momento: [2023-Mar-28 04:17:24]\n",
            "\t === ndx_batch+1: 4161  self.qtd_sequencia: 3598141; Momento: [2023-Mar-28 04:17:25]\n",
            "\t === ndx_batch+1: 4171  self.qtd_sequencia: 3607979; Momento: [2023-Mar-28 04:17:26]\n",
            "\t === ndx_batch+1: 4181  self.qtd_sequencia: 3617438; Momento: [2023-Mar-28 04:17:27]\n",
            "\t === ndx_batch+1: 4191  self.qtd_sequencia: 3626884; Momento: [2023-Mar-28 04:17:33]\n",
            "\t === ndx_batch+1: 4201  self.qtd_sequencia: 3634378; Momento: [2023-Mar-28 04:17:33]\n",
            "\t === ndx_batch+1: 4211  self.qtd_sequencia: 3643404; Momento: [2023-Mar-28 04:17:34]\n",
            "\t === ndx_batch+1: 4221  self.qtd_sequencia: 3653002; Momento: [2023-Mar-28 04:17:35]\n",
            "\t === ndx_batch+1: 4231  self.qtd_sequencia: 3661162; Momento: [2023-Mar-28 04:17:36]\n",
            "\t === ndx_batch+1: 4241  self.qtd_sequencia: 3670206; Momento: [2023-Mar-28 04:17:37]\n",
            "\t === ndx_batch+1: 4251  self.qtd_sequencia: 3679360; Momento: [2023-Mar-28 04:17:38]\n",
            "\t === ndx_batch+1: 4261  self.qtd_sequencia: 3687669; Momento: [2023-Mar-28 04:17:39]\n",
            "\t === ndx_batch+1: 4271  self.qtd_sequencia: 3697839; Momento: [2023-Mar-28 04:17:40]\n",
            "\t === ndx_batch+1: 4281  self.qtd_sequencia: 3706923; Momento: [2023-Mar-28 04:17:41]\n",
            "\t === ndx_batch+1: 4291  self.qtd_sequencia: 3716857; Momento: [2023-Mar-28 04:17:42]\n",
            "\t === ndx_batch+1: 4301  self.qtd_sequencia: 3726063; Momento: [2023-Mar-28 04:17:43]\n",
            "\t === ndx_batch+1: 4311  self.qtd_sequencia: 3733985; Momento: [2023-Mar-28 04:17:44]\n",
            "\t === ndx_batch+1: 4321  self.qtd_sequencia: 3743292; Momento: [2023-Mar-28 04:17:45]\n",
            "\t === ndx_batch+1: 4331  self.qtd_sequencia: 3751303; Momento: [2023-Mar-28 04:17:46]\n",
            "\t === ndx_batch+1: 4341  self.qtd_sequencia: 3759078; Momento: [2023-Mar-28 04:17:47]\n",
            "\t === ndx_batch+1: 4351  self.qtd_sequencia: 3767230; Momento: [2023-Mar-28 04:17:47]\n",
            "\t === ndx_batch+1: 4361  self.qtd_sequencia: 3775270; Momento: [2023-Mar-28 04:17:48]\n",
            "\t === ndx_batch+1: 4371  self.qtd_sequencia: 3783476; Momento: [2023-Mar-28 04:17:49]\n",
            "\t === ndx_batch+1: 4381  self.qtd_sequencia: 3790859; Momento: [2023-Mar-28 04:17:50]\n",
            "\t === ndx_batch+1: 4391  self.qtd_sequencia: 3798540; Momento: [2023-Mar-28 04:17:50]\n",
            "\t === ndx_batch+1: 4401  self.qtd_sequencia: 3806184; Momento: [2023-Mar-28 04:17:51]\n",
            "\t === ndx_batch+1: 4411  self.qtd_sequencia: 3816239; Momento: [2023-Mar-28 04:17:52]\n",
            "\t === ndx_batch+1: 4421  self.qtd_sequencia: 3824060; Momento: [2023-Mar-28 04:17:53]\n",
            "\t === ndx_batch+1: 4431  self.qtd_sequencia: 3832964; Momento: [2023-Mar-28 04:17:54]\n",
            "\t === ndx_batch+1: 4441  self.qtd_sequencia: 3841877; Momento: [2023-Mar-28 04:17:55]\n",
            "\t === ndx_batch+1: 4451  self.qtd_sequencia: 3849766; Momento: [2023-Mar-28 04:17:56]\n",
            "\t === ndx_batch+1: 4461  self.qtd_sequencia: 3858209; Momento: [2023-Mar-28 04:17:57]\n",
            "\t === ndx_batch+1: 4471  self.qtd_sequencia: 3867584; Momento: [2023-Mar-28 04:17:58]\n",
            "\t === ndx_batch+1: 4481  self.qtd_sequencia: 3874943; Momento: [2023-Mar-28 04:17:59]\n",
            "\t === ndx_batch+1: 4491  self.qtd_sequencia: 3884539; Momento: [2023-Mar-28 04:18:00]\n",
            "\t === ndx_batch+1: 4501  self.qtd_sequencia: 3893959; Momento: [2023-Mar-28 04:18:01]\n",
            "\t === ndx_batch+1: 4511  self.qtd_sequencia: 3904189; Momento: [2023-Mar-28 04:18:02]\n",
            "\t === ndx_batch+1: 4521  self.qtd_sequencia: 3911861; Momento: [2023-Mar-28 04:18:03]\n",
            "\t === ndx_batch+1: 4531  self.qtd_sequencia: 3919824; Momento: [2023-Mar-28 04:18:03]\n",
            "\t === ndx_batch+1: 4541  self.qtd_sequencia: 3927426; Momento: [2023-Mar-28 04:18:04]\n",
            "\t === ndx_batch+1: 4551  self.qtd_sequencia: 3939559; Momento: [2023-Mar-28 04:18:05]\n",
            "\t === ndx_batch+1: 4561  self.qtd_sequencia: 3950098; Momento: [2023-Mar-28 04:18:07]\n",
            "\t === ndx_batch+1: 4571  self.qtd_sequencia: 3959339; Momento: [2023-Mar-28 04:18:08]\n",
            "\t === ndx_batch+1: 4581  self.qtd_sequencia: 3969007; Momento: [2023-Mar-28 04:18:09]\n",
            "\t === ndx_batch+1: 4591  self.qtd_sequencia: 3975948; Momento: [2023-Mar-28 04:18:09]\n",
            "\t === ndx_batch+1: 4601  self.qtd_sequencia: 3985865; Momento: [2023-Mar-28 04:18:10]\n",
            "\t === ndx_batch+1: 4611  self.qtd_sequencia: 3994047; Momento: [2023-Mar-28 04:18:11]\n",
            "\t === ndx_batch+1: 4621  self.qtd_sequencia: 4003874; Momento: [2023-Mar-28 04:18:12]\n",
            "\t === ndx_batch+1: 4631  self.qtd_sequencia: 4011525; Momento: [2023-Mar-28 04:18:13]\n",
            "\t === ndx_batch+1: 4641  self.qtd_sequencia: 4020144; Momento: [2023-Mar-28 04:18:14]\n",
            "\t === ndx_batch+1: 4651  self.qtd_sequencia: 4030461; Momento: [2023-Mar-28 04:18:16]\n",
            "\t === ndx_batch+1: 4661  self.qtd_sequencia: 4038410; Momento: [2023-Mar-28 04:18:17]\n",
            "\t === ndx_batch+1: 4671  self.qtd_sequencia: 4049416; Momento: [2023-Mar-28 04:18:18]\n",
            "\t === ndx_batch+1: 4681  self.qtd_sequencia: 4058524; Momento: [2023-Mar-28 04:18:19]\n",
            "\t === ndx_batch+1: 4691  self.qtd_sequencia: 4068133; Momento: [2023-Mar-28 04:18:20]\n",
            "\t === ndx_batch+1: 4701  self.qtd_sequencia: 4076239; Momento: [2023-Mar-28 04:18:21]\n",
            "\t === ndx_batch+1: 4711  self.qtd_sequencia: 4084479; Momento: [2023-Mar-28 04:18:22]\n",
            "\t === ndx_batch+1: 4721  self.qtd_sequencia: 4093981; Momento: [2023-Mar-28 04:18:23]\n",
            "\t === ndx_batch+1: 4731  self.qtd_sequencia: 4102723; Momento: [2023-Mar-28 04:18:24]\n",
            "\t === ndx_batch+1: 4741  self.qtd_sequencia: 4110899; Momento: [2023-Mar-28 04:18:24]\n",
            "\t === ndx_batch+1: 4751  self.qtd_sequencia: 4119624; Momento: [2023-Mar-28 04:18:25]\n",
            "\t === ndx_batch+1: 4761  self.qtd_sequencia: 4129278; Momento: [2023-Mar-28 04:18:26]\n",
            "\t === ndx_batch+1: 4771  self.qtd_sequencia: 4138335; Momento: [2023-Mar-28 04:18:27]\n",
            "\t === ndx_batch+1: 4781  self.qtd_sequencia: 4147994; Momento: [2023-Mar-28 04:18:29]\n",
            "\t === ndx_batch+1: 4791  self.qtd_sequencia: 4156485; Momento: [2023-Mar-28 04:18:30]\n",
            "\t === ndx_batch+1: 4801  self.qtd_sequencia: 4164857; Momento: [2023-Mar-28 04:18:30]\n",
            "\t === ndx_batch+1: 4811  self.qtd_sequencia: 4172726; Momento: [2023-Mar-28 04:18:31]\n",
            "\t === ndx_batch+1: 4821  self.qtd_sequencia: 4182188; Momento: [2023-Mar-28 04:18:32]\n",
            "\t === ndx_batch+1: 4831  self.qtd_sequencia: 4191997; Momento: [2023-Mar-28 04:18:33]\n",
            "\t === ndx_batch+1: 4841  self.qtd_sequencia: 4200613; Momento: [2023-Mar-28 04:18:34]\n",
            "\t === ndx_batch+1: 4851  self.qtd_sequencia: 4209530; Momento: [2023-Mar-28 04:18:35]\n",
            "\t === ndx_batch+1: 4861  self.qtd_sequencia: 4218440; Momento: [2023-Mar-28 04:18:36]\n",
            "\t === ndx_batch+1: 4871  self.qtd_sequencia: 4227236; Momento: [2023-Mar-28 04:18:37]\n",
            "\t === ndx_batch+1: 4881  self.qtd_sequencia: 4236766; Momento: [2023-Mar-28 04:18:38]\n",
            "\t === ndx_batch+1: 4891  self.qtd_sequencia: 4245511; Momento: [2023-Mar-28 04:18:39]\n",
            "\t === ndx_batch+1: 4901  self.qtd_sequencia: 4254642; Momento: [2023-Mar-28 04:18:40]\n",
            "\t === ndx_batch+1: 4911  self.qtd_sequencia: 4262259; Momento: [2023-Mar-28 04:18:41]\n",
            "\t === ndx_batch+1: 4921  self.qtd_sequencia: 4270395; Momento: [2023-Mar-28 04:18:42]\n",
            "\t === ndx_batch+1: 4931  self.qtd_sequencia: 4279044; Momento: [2023-Mar-28 04:18:43]\n",
            "\t === ndx_batch+1: 4941  self.qtd_sequencia: 4285353; Momento: [2023-Mar-28 04:18:44]\n",
            "\t === ndx_batch+1: 4951  self.qtd_sequencia: 4293807; Momento: [2023-Mar-28 04:18:45]\n",
            "\t === ndx_batch+1: 4961  self.qtd_sequencia: 4302585; Momento: [2023-Mar-28 04:18:46]\n",
            "\t === ndx_batch+1: 4971  self.qtd_sequencia: 4310579; Momento: [2023-Mar-28 04:18:47]\n",
            "\t === ndx_batch+1: 4981  self.qtd_sequencia: 4318215; Momento: [2023-Mar-28 04:18:48]\n",
            "\t === ndx_batch+1: 4991  self.qtd_sequencia: 4325533; Momento: [2023-Mar-28 04:18:48]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 04:18:49]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 04:19:12]\n",
            "Carregado dataset com 4328981 sentenças\n",
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  assert hparam['num_sentenca_train'] <=  total_sentencas - (hparam['num_sentenca_valid'] + hparam['num_sentenca_test']), f\"Dados de treino não podem conter dados de validação/teste\"\n",
        "  train_texts = texts[:hparam['num_sentenca_train'] ]\n",
        "\n",
        "  print(\"carregando train_dataset\")\n",
        "  train_dataset = MyDataset(texts=train_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(train_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_train'])+'_train.pt')\n",
        "\n",
        "  drive.flush_and_unmount()\n",
        "  print('All changes made in this colab session should now be visible in Drive.')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqQfYzEBtOjS",
        "outputId": "4ed7c200-cb75-4b86-a6fb-c6765548654b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 4328981\n",
            "valid examples: 2036\n",
            "test examples: 1143\n"
          ]
        }
      ],
      "source": [
        "hparam['train_size'] = len(train_dataset) \n",
        "hparam['valid_size'] = len(valid_dataset) \n",
        "hparam['test_size'] = len(test_dataset) \n",
        "\n",
        "\n",
        "print(f\"training examples: {hparam['train_size']}\")\n",
        "print(f\"valid examples: {hparam['valid_size']}\")\n",
        "print(f\"test examples: {hparam['test_size']}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYIgLS4tOjT"
      },
      "source": [
        "max_seq_length=100, train_senteces = 800\n",
        "\n",
        "training examples: 14573\n",
        "valid examples: 2036\n",
        "test examples: 1143\n",
        "\n",
        "\n",
        "max_seq_length=100, train_senteces = 249800\n",
        "\n",
        "training examples: 4328981\n",
        "valid examples: 2036\n",
        "test examples: 1143\n",
        "\n",
        "\n",
        "max_seq_length=50, train_senteces = 249800\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm6_PTH2i98e"
      },
      "source": [
        "# Teste do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-QD1mZkMM9b",
        "outputId": "8df3c78f-a2cc-44ff-815a-17d59ebbdfc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 24.01\n",
            " used 2.87\n",
            " free 13.52\n",
            " cached 10.51\n",
            " buffers 0.42\n",
            "/nGPU\n",
            "Tue Mar 28 03:46:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    26W /  70W |   1159MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44wOmt6BqC52",
        "outputId": "a650ca6f-6639-4d54-9ef3-db18c3ac2a4f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bos_token',\n",
              " 'eos_token',\n",
              " 'unk_token',\n",
              " 'sep_token',\n",
              " 'pad_token',\n",
              " 'cls_token',\n",
              " 'mask_token',\n",
              " 'additional_special_tokens']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McvufVnXGlBt",
        "outputId": "56d3cc04-9ed2-4f35-afe5-2932720d5d8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_workers_dataloader': 0,\n",
              " 'device': device(type='cuda', index=0),\n",
              " 'vocab_size': 50272,\n",
              " 'num_sentenca_train': 800,\n",
              " 'num_sentenca_valid': 100,\n",
              " 'num_sentenca_test': 100,\n",
              " 'max_seq_length': 100,\n",
              " 'train_size': 14573,\n",
              " 'valid_size': 2036,\n",
              " 'test_size': 1143,\n",
              " 'batch_size': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "hparam['batch_size'] = 2\n",
        "hparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgeqpYyoHV6-",
        "outputId": "256625f3-50dd-49a5-d644-83f1de5e029a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_y.shape: torch.Size([2, 100]), sample_y: tensor([[35351,   366,  3840, 13151, 11474, 17833,  2072, 23473,   629,   281,\n",
            "           263,  8446,  1975,   493,  3381,  4214,  3137,  1021,   563,   139,\n",
            "          3840, 15551,  5873,     4,  3232,  5502,   379,  1021,   257,   475,\n",
            "          5655, 26437,  1499,  1526,  1069,   366, 40274,   424,    12,  1090,\n",
            "            10,   139,  2784,   100,   364,    10,     6,   117,   475, 17010,\n",
            "         11332,     6,  7252,  5133,  1977,    90, 25588,     6, 28312,  5563,\n",
            "           181,  4636, 37895,  1210,   271,   364, 29063,   257,   853,    10,\n",
            "         10306,  1001,  8344, 21647,    23,  6472,  5739,   109, 13736,  2527,\n",
            "           563,   139,  3840, 15551, 11474,   126,  7252,   586,   102,  3840,\n",
            "         15551, 11474,  3840, 26437,  1499,  1526,  1069,   366,   263,  1717],\n",
            "        [10969,  1916,  6757, 10071,  3381,  4214,     4,  4556,   139, 14039,\n",
            "         18416,    12,  1090,   116,  2884,   102,   283,  3381,   271,     6,\n",
            "          1521,   242,  1021,  5402,  8541,  3938,   263,  2628,   102,  6757,\n",
            "         10071,  3381,  4214,     6,  1192, 22936,  1943,  1526,    25,   856,\n",
            "          8367,   281,   263, 14237,  3900,  3381,  4214,   109, 15551,  5873,\n",
            "           364,  6821,  1526,  1021,   842,   257,  8541,  3938,  3137,  1021,\n",
            "          2784,   100,     4,   221, 22500,  7794,    10, 22706,   102,  3381,\n",
            "          4214,  3840,   740,  2095, 26012,  7450,   364,  1236, 29998,  1021,\n",
            "         18215,  5511,   139,    36, 32503,   808,  8367,   281,   295,  4214,\n",
            "           579,  4214,  1931,  1023, 19926,  3840, 26012,   428,  3985,  2841]])\n",
            "sample_x.shape: torch.Size([2, 100]), sample_x: tensor([[    2, 35351,   366,  3840, 13151, 11474, 17833,  2072, 23473,   629,\n",
            "           281,   263,  8446,  1975,   493,  3381,  4214,  3137,  1021,   563,\n",
            "           139,  3840, 15551,  5873,     4,  3232,  5502,   379,  1021,   257,\n",
            "           475,  5655, 26437,  1499,  1526,  1069,   366, 40274,   424,    12,\n",
            "          1090,    10,   139,  2784,   100,   364,    10,     6,   117,   475,\n",
            "         17010, 11332,     6,  7252,  5133,  1977,    90, 25588,     6, 28312,\n",
            "          5563,   181,  4636, 37895,  1210,   271,   364, 29063,   257,   853,\n",
            "            10, 10306,  1001,  8344, 21647,    23,  6472,  5739,   109, 13736,\n",
            "          2527,   563,   139,  3840, 15551, 11474,   126,  7252,   586,   102,\n",
            "          3840, 15551, 11474,  3840, 26437,  1499,  1526,  1069,   366,   263],\n",
            "        [ 1916, 10969,  1916,  6757, 10071,  3381,  4214,     4,  4556,   139,\n",
            "         14039, 18416,    12,  1090,   116,  2884,   102,   283,  3381,   271,\n",
            "             6,  1521,   242,  1021,  5402,  8541,  3938,   263,  2628,   102,\n",
            "          6757, 10071,  3381,  4214,     6,  1192, 22936,  1943,  1526,    25,\n",
            "           856,  8367,   281,   263, 14237,  3900,  3381,  4214,   109, 15551,\n",
            "          5873,   364,  6821,  1526,  1021,   842,   257,  8541,  3938,  3137,\n",
            "          1021,  2784,   100,     4,   221, 22500,  7794,    10, 22706,   102,\n",
            "          3381,  4214,  3840,   740,  2095, 26012,  7450,   364,  1236, 29998,\n",
            "          1021, 18215,  5511,   139,    36, 32503,   808,  8367,   281,   295,\n",
            "          4214,   579,  4214,  1931,  1023, 19926,  3840, 26012,   428,  3985]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "sample_x, sample_y = next(iter(DataLoader(valid_dataset,batch_size=hparam['batch_size'])))\n",
        "sample_x = sample_x.to(hparam['device'])\n",
        "print(f\"sample_y.shape: {sample_y.shape}, sample_y: {sample_y}\")\n",
        "print(f\"sample_x.shape: {sample_x.shape}, sample_x: {sample_x}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0KzkMqPk8aU0"
      },
      "outputs": [],
      "source": [
        "saida = model(sample_x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saida['logits'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7ISrblKorF4",
        "outputId": "e3fbae92-10bd-4ded-c203-2f8629202503"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 50272])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "J0Ct-TaTIICk"
      },
      "outputs": [],
      "source": [
        "assert saida['logits'].shape[2] == hparam['vocab_size'], \"Saída[2] deveria ser do tamanho do vocabulário do tokenizador\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert saida['logits'].shape[1] == hparam['max_seq_length'], \"Saída[1] deveria ser do tamanho de max_seq_length\""
      ],
      "metadata": {
        "id": "mfP4ntZRqia4"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id, model.config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ60n9LEr5jp",
        "outputId": "c98bfef7-7d90-4c73-84ec-96ab71da4099"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 50272)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_rYB_7aI2_x"
      },
      "source": [
        "## Perplexidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "f2jgDEsvBBsB"
      },
      "outputs": [],
      "source": [
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    # muda de torch.Size([2, 100, vocabsize])\n",
        "    # para torch.Size([200, vocabsize])  \n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    # e target de torch.Size([2, seq_length])\n",
        "    # para torch.Size([2*seq_length])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "\n",
        "    # calculando perplexidade \n",
        "    return torch.exp(loss), loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl8FwXjt_esf",
        "outputId": "8a15012e-a41a-40f3-dc66-b0a8565d8fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se não tivesse pré-treino perplexity: 50272\n",
            "my perplexity:              30\n",
            "my loss:              3\n"
          ]
        }
      ],
      "source": [
        "my_perplexity, my_loss = perplexity(logits=saida['logits'], target=sample_y.to(hparam['device']), ignore_token_id=model.config.pad_token_id)\n",
        "\n",
        "\n",
        "print(f'Se não tivesse pré-treino perplexity: {model.config.vocab_size}')\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'my loss:              {int(my_loss)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "-SjDJnoOQiqD"
      },
      "outputs": [],
      "source": [
        "del saida, sample_x, sample_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiJtrsqPnE_l"
      },
      "source": [
        "# Treinamento e Validação \n",
        "\n",
        "  Masked Language Modelling (MLM trainning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12HRQXz_NHIA"
      },
      "source": [
        "## Funções auxiliares "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNBajDdAaIDh"
      },
      "source": [
        "### De geração de texto"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "sTGkH-VHV3uv"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "DHbd9sLSURwj"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonte da função abaixo: colega Leandro Carísio\n",
        "def continuar_frase_pipeline(frase, max_length=100, amostragem_estocastica=True):\n",
        "  device_generator = 0 if hparam['device'].type == 'cuda' else None\n",
        "  generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device_generator)\n",
        "  output = generator(frase, max_length=max_length, do_sample=amostragem_estocastica)\n",
        "  return output[0]['generated_text']"
      ],
      "metadata": {
        "id": "Sw9V0JU9URwk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U5snaxJMGeO"
      },
      "source": [
        "### De treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "M3XfDRtAMUFw"
      },
      "outputs": [],
      "source": [
        "def validation_step(input, target, parm_model):\n",
        "    saida = parm_model(input)\n",
        "    # muda de torch.Size([2, 100, vocabsize])\n",
        "    # para torch.Size([200, vocabsize])    \n",
        "    logits = saida['logits'].reshape(-1, saida['logits'].shape[-1])\n",
        "    # e target de torch.Size([2, seq_length])\n",
        "    # para torch.Size([2*seq_length])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, ignore_index=parm_model.config.pad_token_id)\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "uwMnuznWtiAd"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "os0oYoIScCN0"
      },
      "outputs": [],
      "source": [
        "def add_param_weight_decay(net, l2_value, skip_list=()):\n",
        "  \"\"\"\n",
        "    A weight decay penalty of 10−4 was used in the Brown experiments \n",
        "    and a weight decay of 10−5 was used in the APNews experiments  (not applied to bias)\n",
        "    fonte: https://raberrytv.wordpress.com/2017/10/29/pytorch-weight-decay-made-easy/\n",
        "  \"\"\"\n",
        "  decay, no_decay = [], []\n",
        "  for name, param in net.named_parameters():\n",
        "    # if not param.requires_grad: continue # frozen weights\t\t            \n",
        "    if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list: no_decay.append(param)\n",
        "    else: decay.append(param)\n",
        "  return [{'params': no_decay, 'weight_decay': 0.}, {'params': decay, 'weight_decay': l2_value}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "UkNIPj_LuMJO"
      },
      "outputs": [],
      "source": [
        "def treina_modelo (parm_model, parm_loader_train, parm_loader_valid, parm_loader_test, hparam:dict, parm_se_apenas_uma_validacao:bool=False, parm_se_gera_rastro:bool=True, parm_verbose:bool = True, parm_intervalo_print = 10):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  global prefixo_nome_diretorio\n",
        "  if parm_se_gera_rastro:\n",
        "    rastro_neptune = NeptuneRastroRun(hparam, parm_lista_tag= gera_tag_rastro_experiencia_treino(parm_aula='aula4', hparam=hparam) )\n",
        "  try:\n",
        "    path_modelo = f'{prefixo_nome_diretorio}best_model_len_{hparam[\"max_seq_length\"]}_inicio_treino_{time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}.pt'\n",
        "    train_losses = []\n",
        "    n_examples = 0\n",
        "    step_avaliacao = 0\n",
        "    best_perplexidade_validacao = float('inf')\n",
        "    best_step = 0\n",
        "    history = []\n",
        "    parm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_ppl = np.exp(np.average([\n",
        "            validation_step(input_tmp.to(hparam['device']), target_tmp.to(hparam['device']), parm_model)\n",
        "            for input_tmp, target_tmp in parm_loader_valid]))\n",
        "    metrica_rastro = {\"valid/perplexidade\": valid_ppl}  \n",
        "    history.append(metrica_rastro)\n",
        "    if parm_se_gera_rastro:\n",
        "      rastro_neptune.salva_metrica(metrica_rastro)\n",
        "    print(f'hparam: {hparam}')\n",
        "    print(f'Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} Métricas iniciais em validação: {metrica_rastro} Serão treinadas {hparam[\"max_examples\"]} amostras')\n",
        "\n",
        "    time_inicio_treino = time.time()\n",
        "    ultimo_step_treinado = 0\n",
        "    se_continua_execucao = True\n",
        "    parm_model.train()\n",
        "    qtd_step_sem_melhor_metrica = 0\n",
        "    while n_examples < hparam['max_examples'] and se_continua_execucao:\n",
        "        # for qtd_param_update, (input, target) in enumerate(tqdm(parm_loader_train, desc=f\"Training {100*n_examples/hparam['max_examples']:.3f}%\", leave=False)):  \n",
        "        for qtd_param_update, (input, target) in enumerate(parm_loader_train):  \n",
        "        # for qtd_param_update in range(hparam['max_examples']):  \n",
        "            # input, target = next(iter(parm_loader_train))\n",
        "            ultimo_step_treinado += 1 \n",
        "            saida = parm_model(input.to(hparam['device']))\n",
        "            # muda de torch.Size([2, 100, vocabsize])\n",
        "            # para torch.Size([200, vocabsize])    \n",
        "            logits = saida['logits'].reshape(-1, saida['logits'].shape[-1])\n",
        "            # e target de torch.Size([2, seq_length])\n",
        "            # para torch.Size([2*seq_length])\n",
        "            target = target.reshape(-1).to(hparam['device'])\n",
        "            loss = nn.functional.cross_entropy(logits, target, ignore_index=parm_model.config.pad_token_id)            \n",
        "            # ipdb.set_trace(context=4)\n",
        "            hparam['optimizer'].zero_grad()            \n",
        "            fator_corte_loss = max(hparam['fator_corte_loss_maximo'], n_examples/hparam['max_examples'])\n",
        "            loss = hparam['criterion'](logits, target) * fator_corte_loss   # ajustando para diminuir a redução na loss quando perto do fim do treino\n",
        "            loss.backward()\n",
        "            hparam['optimizer'].step()\n",
        "            hparam['scheduler'].step()  # DÚVIDA: melhor fazer por step treino ou por step validação?  Esse último não impactou mudança.              \n",
        "            loss_batch = loss.item()/fator_corte_loss # desfazendo fator_corte_loss para não refletir na perplexidade\n",
        "            train_losses.append(loss_batch) \n",
        "            n_examples += len(input)  # Increment of batch size\n",
        "\n",
        "            if ultimo_step_treinado % hparam['eval_every_steps'] == 0:\n",
        "                train_ppl = np.exp(np.average(train_losses))\n",
        "                parm_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    valid_ppl = np.exp(np.average([\n",
        "                        validation_step(input_tmp.to(hparam['device']), target_tmp.to(hparam['device']), parm_model)\n",
        "                        for input_tmp, target_tmp in parm_loader_valid]))\n",
        "\n",
        "                train_losses = []\n",
        "\n",
        "                metrica_rastro = {\"train/perplexidade\": train_ppl,\n",
        "                                  \"train/loss\": loss_batch, \n",
        "                                  \"train/n_examples\": n_examples, \n",
        "                                  \"train/learning_rate\": hparam[\"optimizer\"].param_groups[1][\"lr\"],\n",
        "                                  \"valid/perplexidade\": valid_ppl}  \n",
        "                history.append(metrica_rastro)\n",
        "                if parm_se_gera_rastro:\n",
        "                  rastro_neptune.salva_metrica(metrica_rastro)\n",
        "\n",
        "                sufixo_msg = \"\"\n",
        "                \n",
        "                # Salvando o melhor modelo de acordo com a loss de validação\n",
        "                if valid_ppl < best_perplexidade_validacao:\n",
        "                    best_model_dict = parm_model.state_dict()\n",
        "                    best_perplexidade_validacao = valid_ppl\n",
        "                    best_step = ultimo_step_treinado\n",
        "                    sufixo_msg += f\" novo best valid {valid_ppl}\"\n",
        "\n",
        "                    # salva a cada vez que não houver redução seguida da métrica\n",
        "                    if qtd_step_sem_melhor_metrica > 1:                    \n",
        "                      torch.save(parm_model, path_modelo)    \n",
        "                      sufixo_msg += f\"; modelo salvo em {path_modelo}\"\n",
        "                      print(sufixo_msg)\n",
        "\n",
        "                    qtd_step_sem_melhor_metrica = 0\n",
        "                   \n",
        "                    # print('best model')\n",
        "                elif hparam['early_stop'] <= (ultimo_step_treinado - best_step):\n",
        "                    print(f\"Parando por critério de early_stop no step {ultimo_step_treinado} sendo best_step {best_step} e ealy_stop {hparam['early_stop']}\")\n",
        "                    se_continua_execucao = False\n",
        "                    break\n",
        "                else:\n",
        "                    qtd_step_sem_melhor_metrica +=1\n",
        "                if parm_se_apenas_uma_validacao:\n",
        "                    se_continua_execucao = False\n",
        "                    break\n",
        "                if parm_intervalo_print > 0:\n",
        "                    if (ultimo_step_treinado)%(parm_intervalo_print*hparam['eval_every_steps']) == 0: \n",
        "                        print(f'Step: {ultimo_step_treinado} Amostras:{n_examples:d}  {100*n_examples/hparam[\"max_examples\"]:.3f}%  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} lr: {hparam[\"optimizer\"].param_groups[1][\"lr\"]:.5e} Train loss: {loss_batch:.4f} perplexidade: {train_ppl:.4f} Validação perplexidade: {valid_ppl:.4f} {sufixo_msg}')\n",
        "\n",
        "                parm_model.train()\n",
        "\n",
        "\n",
        "            if n_examples >= hparam['max_examples']:              \n",
        "                break    \n",
        "            \n",
        "            # apenas para teste da lógica, tratar um batch por época\n",
        "\n",
        "\n",
        "    # calculando tempo gasto e médio por step\n",
        "    tempo_treino = time.time() - time_inicio_treino   \n",
        "    print(f\"Tempo gasto total {tempo_treino:9.5f}, steps: {ultimo_step_treinado}, tempo por step {tempo_treino/ultimo_step_treinado:9.5f}\")\n",
        "    \n",
        "    print(f'Final: Step: {ultimo_step_treinado} Amostras:{n_examples:d}  {100*n_examples/hparam[\"max_examples\"]:.3f}%  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} lr:{hparam[\"optimizer\"].param_groups[1][\"lr\"]:.5e} Train loss: {loss_batch:.4f} Train perplexidade: {train_ppl:.4f} Validação perplexidade: {valid_ppl:.4f} ')\n",
        "\n",
        "\n",
        "    parm_model.load_state_dict(best_model_dict)\n",
        "    parm_model.to(hparam['device'])\n",
        "    torch.save(parm_model, path_modelo)    \n",
        "    print(f\"Modelo com melhor resultado em validação (step {best_step}) salvo após treino em {path_modelo}\")\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_ppl = np.exp(np.average([\n",
        "            validation_step(input_tmp.to(hparam['device']), target_tmp.to(hparam['device']), parm_model)\n",
        "            for input_tmp, target_tmp in parm_loader_test\n",
        "        ]))\n",
        "\n",
        "\n",
        "    metrica_rastro = {\"test/perplexidade\": test_ppl}      \n",
        "    print(f\" Resultado com dados de teste para modelo treinado: {metrica_rastro}\")\n",
        "    if parm_se_gera_rastro:\n",
        "      rastro_neptune.run_neptune[\"context/tempo_treino\"] = tempo_treino\n",
        "      rastro_neptune.run_neptune[\"context/tempo_treino_por_step\"] = tempo_treino/ultimo_step_treinado\n",
        "      rastro_neptune.run_neptune[\"valid/best_step\"] = best_step\n",
        "      rastro_neptune.salva_metrica(metrica_rastro)\n",
        "      #rastro_neptune.gera_grafico_modelo(parm_loader_train, parm_model)    \n",
        "\n",
        "\n",
        "    frase_inicio = \"Praticar esportes é \"\n",
        "    frase_final = continuar_frase_pipeline(frase_inicio)\n",
        "    print(f\"Frase inicio: {frase_inicio}\")\n",
        "    print(f\"Frase final gerada: {frase_final}\")\n",
        "\n",
        "  finally:\n",
        "    if parm_se_gera_rastro:\n",
        "      rastro_neptune.stop()\n",
        "\n",
        "\n",
        "  return {\"perplexidade_test\":test_ppl, \"perplexidade_treino\":train_ppl, \"best_perplexidade_validacao\":best_perplexidade_validacao,  \"best_step\": best_step} #, \"best_model_dict\": best_model_dict}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMxjFSmAOH4H",
        "outputId": "612dc693-c89a-44d8-bb32-eec97ce259a5"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 11.66\n",
            " used 15.18\n",
            " free 3.27\n",
            " cached 8.81\n",
            " buffers 0.07\n",
            "/nGPU\n",
            "Tue Mar 28 04:35:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    26W /  70W |   5571MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpa o cache da memória da GPU\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "IpynnWi-SoTi"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_memoria(['gpu','cpu'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZu7FEvOSpni",
        "outputId": "2f1c826b-7283-4859-98ab-8cb3b6405398"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 23.05\n",
            " used 3.83\n",
            " free 12.56\n",
            " cached 10.52\n",
            " buffers 0.42\n",
            "/nGPU\n",
            "Tue Mar 28 03:48:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    26W /  70W |   1579MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "PZhfJhoC0Xmk"
      },
      "outputs": [],
      "source": [
        "def ajusta_parametro_grid(hparam, combinacao_parametro, model, se_treina_poucos_dados:bool=False):\n",
        "  parametro_esperado_grid = ( \"batch_size\", \n",
        "                             \"num_epochs\", \"learning_rate\", \"fator_corte_loss_maximo\", \n",
        "                             'decrease_factor_lr', 'weight_decay') #  'percent_unfreeze_embeddings') #, 'percent_unfreeze_embeddings')\n",
        "  if not model:\n",
        "    raise Exception(\"Necessário informar model!\")                            \n",
        "  for nome_parametro in parametro_esperado_grid:\n",
        "      if nome_parametro not in combinacao_parametro:\n",
        "          raise NotImplementedError(f'Gride de parâmetros está incompleto, não contem {nome_parametro}')\n",
        "      hparam[nome_parametro] = combinacao_parametro[nome_parametro]\n",
        "  for nome_parametro in combinacao_parametro:\n",
        "      if nome_parametro not in parametro_esperado_grid:\n",
        "          raise NotImplementedError(f'Gride de parâmetros está com parâmetro adicional não tratado: {nome_parametro}')\n",
        "      hparam[nome_parametro] = combinacao_parametro[nome_parametro]\n",
        "  hparam['num_workers_dataloader'] = 0\n",
        "  lambdalr = lambda qtd_param_update: 1/(1 + qtd_param_update * hparam['decrease_factor_lr'] )\n",
        "  hparam['drop_last'] = True\n",
        "  train_loader = DataLoader(train_dataset, batch_size=hparam['batch_size'], shuffle=True, drop_last=hparam['drop_last'], num_workers=hparam['num_workers_dataloader'])\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=hparam['batch_size'], shuffle=False, drop_last=hparam['drop_last'], num_workers=hparam[\"num_workers_dataloader\"])\n",
        "  test_loader = DataLoader(test_dataset, batch_size=hparam['batch_size'], shuffle=False, drop_last=hparam['drop_last'], num_workers=hparam['num_workers_dataloader'])\n",
        "  if se_treina_poucos_dados:\n",
        "    train_loader = [next(iter(train_loader))] # para overfit com poucos dados (1 batch)\n",
        "    hparam['max_examples'] = hparam['num_epochs'] * hparam['batch_size']  \n",
        "    hparam['percentual_eval_every_steps'] = 0.0025\n",
        "    # a cada percentual do total\n",
        "    hparam['eval_every_steps'] = math.ceil(hparam['percentual_eval_every_steps'] * (hparam['max_examples'] / hparam['batch_size']))  \n",
        "  else:\n",
        "    hparam['max_examples'] = hparam['num_epochs'] * hparam['train_size'] \n",
        "    hparam['percentual_eval_every_steps'] = 0.0025\n",
        "    # a cada percentual do total    \n",
        "    hparam['eval_every_steps'] = int(hparam['percentual_eval_every_steps'] * (hparam['max_examples'] / hparam['batch_size']))  \n",
        "  hparam['train_size'] = len(train_dataset) \n",
        "  hparam['valid_size'] = len(valid_dataset) \n",
        "  hparam['test_size'] = len(test_dataset) \n",
        "  hparam['early_stop'] = 7 * hparam['eval_every_steps']\n",
        "  hparam['criterion'] = torch.nn.CrossEntropyLoss()\n",
        "  inicializa_seed(123)\n",
        "  # model = model.float()\n",
        "  # print(f\"model.embedding_layer.weight.requires_grad: {model.embedding_layer.weight.requires_grad}\")\n",
        "\n",
        "  hparam['num_params'] = count_parameters(model)\n",
        "  print(f\"Number of model parameters: {hparam['num_params']}\")\n",
        "  # hparam['learning_rate'] =  3e-5 # 1e-3\n",
        "  # hparam['weight_decay'] = 1e-4\n",
        "  params = add_param_weight_decay(model, hparam['weight_decay'])\n",
        "  hparam['amsgrad']=False\n",
        "  hparam['optimizer'] = torch.optim.Adam(params, lr=hparam['learning_rate'], weight_decay= hparam['weight_decay'], amsgrad=hparam['amsgrad'])\n",
        "  hparam['scheduler'] = torch.optim.lr_scheduler.LambdaLR(hparam['optimizer'], lr_lambda=lambdalr, verbose=False)\n",
        "  # hparam['lista_tag_rastro_experiencia_treino'] =  gera_tag_rastro_experiencia_treino(parm_aula='aula7', hparam=hparam) \n",
        "  return hparam, model,train_loader,valid_loader,test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "-OKGe-cu9GkN"
      },
      "outputs": [],
      "source": [
        "def treina_grid(hparam, gridparam, model, parm_se_apenas_uma_validacao:bool=False, parm_se_gera_rastro:bool=True, se_treina_poucos_dados:bool=False): \n",
        "  if not model:\n",
        "    raise Exception(\"Necessário informar ou model ou classe_modelo, não os dois!\")                            \n",
        "\n",
        "  keys, values = zip(*gridparam.items())\n",
        "  lista_combinacao_grid = [dict(zip(keys, v)) for v in itertools.product(*values)]  \n",
        "  total_combinacao = len(lista_combinacao_grid)\n",
        "  print(f\"Serão {total_combinacao} experimentações\")\n",
        "  qtd_experimento = 1\n",
        "  # for cnt_combinacao, combinacao in enumerate(tqdm(lista_combinacao_grid, desc=f\"Experimento {qtd_experimento}/{total_combinacao}\")):\n",
        "  for cnt_combinacao, combinacao in enumerate(lista_combinacao_grid):\n",
        "    print(f\"\\n\\nNUM: {qtd_experimento}/{total_combinacao} : {combinacao} \")\n",
        "    hparam, model, train_loader, valid_loader, test_loader = ajusta_parametro_grid(hparam, combinacao, model, se_treina_poucos_dados=se_treina_poucos_dados)\n",
        "    #ipdb.set_trace(context=4)\n",
        "    resultado = treina_modelo(model, parm_loader_train=train_loader, parm_loader_valid=valid_loader,\n",
        "                          parm_loader_test=test_loader, hparam=hparam,\n",
        "                          parm_se_apenas_uma_validacao=parm_se_apenas_uma_validacao,\n",
        "                          parm_se_gera_rastro=parm_se_gera_rastro, parm_verbose=True, parm_intervalo_print=1)\n",
        "    qtd_experimento += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1RESg_Jb8vs"
      },
      "source": [
        "## Experimentos de treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxs2QdSY3da"
      },
      "source": [
        "### Testando em poucos dados (Overfit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Pwep987wSLIx"
      },
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[10],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[8],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }                           "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treina_grid(hparam, gridparam, model, parm_se_gera_rastro = False, se_treina_poucos_dados=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5tYXy4Y8Spu",
        "outputId": "5ff661ce-ed88-4844-a351-a79a66035b7f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 10, 'fator_corte_loss_maximo': 1, 'batch_size': 8, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'train_size': 14573, 'valid_size': 2036, 'test_size': 1143, 'batch_size': 8, 'num_epochs': 10, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'max_examples': 80, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 1, 'early_stop': 7, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f3051da7c70>}\n",
            "Momento: [2023-Mar-28 03:49:46] Métricas iniciais em validação: {'valid/perplexidade': 38.5976437671499} Serão treinadas 80 amostras\n",
            "Step: 1 Amostras:8  10.000%  Momento: [2023-Mar-28 03:50:05] lr: 9.99999e-05 Train loss: 3.5271 perplexidade: 34.0236 Validação perplexidade: 43.4348  novo best valid 43.43478546803648\n",
            "Step: 2 Amostras:16  20.000%  Momento: [2023-Mar-28 03:50:23] lr: 9.99998e-05 Train loss: 2.7963 perplexidade: 16.3835 Validação perplexidade: 45.6706 \n",
            "Step: 3 Amostras:24  30.000%  Momento: [2023-Mar-28 03:50:42] lr: 9.99997e-05 Train loss: 2.1973 perplexidade: 9.0005 Validação perplexidade: 52.5099 \n",
            "Step: 4 Amostras:32  40.000%  Momento: [2023-Mar-28 03:51:01] lr: 9.99996e-05 Train loss: 1.7663 perplexidade: 5.8490 Validação perplexidade: 56.6818 \n",
            "Step: 5 Amostras:40  50.000%  Momento: [2023-Mar-28 03:51:21] lr: 9.99995e-05 Train loss: 1.4200 perplexidade: 4.1373 Validação perplexidade: 62.5680 \n",
            "Step: 6 Amostras:48  60.000%  Momento: [2023-Mar-28 03:51:41] lr: 9.99994e-05 Train loss: 1.1248 perplexidade: 3.0795 Validação perplexidade: 69.6467 \n",
            "Step: 7 Amostras:56  70.000%  Momento: [2023-Mar-28 03:52:00] lr: 9.99993e-05 Train loss: 0.8776 perplexidade: 2.4051 Validação perplexidade: 75.7386 \n",
            "Parando por critério de early_stop no step 8 sendo best_step 1 e ealy_stop 7\n",
            "Tempo gasto total 153.14141, steps: 8, tempo por step  19.14268\n",
            "Final: Step: 8 Amostras:64  80.000%  Momento: [2023-Mar-28 03:52:19] lr:9.99992e-05 Train loss: 0.6845 Train perplexidade: 1.9828 Validação perplexidade: 80.5822 \n",
            "Modelo com melhor resultado em validação (step 1) salvo após treino em /content/drive/My Drive/treinamento/202301_IA368DD/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 03:49:29].pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Resultado com dados de teste para modelo treinado: {'test/perplexidade': 71.7844570425778}\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ills, mas não há esporte nenhum. Por conta-se, pois não sabem por eisso. Quatro seguinte, por ei, passou por esporte nenhum. Conta-se que estava entendendo o que temos na manhã de offinga-se. Conta-se que estava, cuspindo o cor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCGzCsRe8Sf9",
        "outputId": "0a3a88a8-99b0-4ccb-a9a1-9cbeb73f42c8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 22.53\n",
            " used 4.35\n",
            " free 10.5\n",
            " cached 12.06\n",
            " buffers 0.42\n",
            "/nGPU\n",
            "Tue Mar 28 03:52:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0    30W /  70W |   4431MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PVZTTkn7it"
      },
      "source": [
        "Treinado por 1 época em poucos dados (mas sem overfitar)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(hparam['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZjdE8fHXwql",
        "outputId": "4db3fe11-6b50-43c8-d31b-bd495708de36"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPEp1SNUX1Ft",
        "outputId": "37ad97cb-73f3-4756-8e0a-af628d5bab36"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 21.8\n",
            " used 5.08\n",
            " free 9.23\n",
            " cached 12.6\n",
            " buffers 0.43\n",
            "/nGPU\n",
            "Tue Mar 28 04:00:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    27W /  70W |   5263MiB / 15360MiB |     13%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-f1Rc2e4dvG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEPl_wNodulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "7_WOyjb6VhVl"
      },
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[1],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[8],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyS9KQylVhVm",
        "outputId": "347dce6d-6211-4515-e149-d6bf66f1dc6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 1, 'fator_corte_loss_maximo': 1, 'batch_size': 8, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'train_size': 14573, 'valid_size': 2036, 'test_size': 1143, 'batch_size': 8, 'num_epochs': 1, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'max_examples': 14573, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 4, 'early_stop': 28, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f305138d610>}\n",
            "Momento: [2023-Mar-28 04:00:51] Métricas iniciais em validação: {'valid/perplexidade': 38.5976437671499} Serão treinadas 14573 amostras\n",
            "Step: 4 Amostras:32  0.220%  Momento: [2023-Mar-28 04:01:11] lr: 9.99996e-05 Train loss: 3.5273 perplexidade: 36.9993 Validação perplexidade: 44.8040  novo best valid 44.803996601487306\n",
            "Step: 8 Amostras:64  0.439%  Momento: [2023-Mar-28 04:01:31] lr: 9.99992e-05 Train loss: 3.8986 perplexidade: 47.2555 Validação perplexidade: 44.6511  novo best valid 44.65110929599969\n",
            "Step: 12 Amostras:96  0.659%  Momento: [2023-Mar-28 04:01:52] lr: 9.99988e-05 Train loss: 3.7859 perplexidade: 45.4978 Validação perplexidade: 45.0201 \n",
            "Step: 16 Amostras:128  0.878%  Momento: [2023-Mar-28 04:02:13] lr: 9.99984e-05 Train loss: 3.9367 perplexidade: 48.7746 Validação perplexidade: 43.9386  novo best valid 43.938635583954735\n",
            "Step: 20 Amostras:160  1.098%  Momento: [2023-Mar-28 04:02:33] lr: 9.99980e-05 Train loss: 3.8781 perplexidade: 41.8311 Validação perplexidade: 44.1581 \n",
            "Step: 24 Amostras:192  1.318%  Momento: [2023-Mar-28 04:02:54] lr: 9.99976e-05 Train loss: 3.4981 perplexidade: 38.6188 Validação perplexidade: 43.8683  novo best valid 43.86832834208163\n",
            "Step: 28 Amostras:224  1.537%  Momento: [2023-Mar-28 04:03:15] lr: 9.99972e-05 Train loss: 3.5338 perplexidade: 37.4950 Validação perplexidade: 43.4054  novo best valid 43.40541896152308\n",
            "Step: 32 Amostras:256  1.757%  Momento: [2023-Mar-28 04:03:36] lr: 9.99968e-05 Train loss: 3.8748 perplexidade: 43.9163 Validação perplexidade: 43.1822  novo best valid 43.18216793355435\n",
            "Step: 36 Amostras:288  1.976%  Momento: [2023-Mar-28 04:03:57] lr: 9.99964e-05 Train loss: 3.7804 perplexidade: 43.6686 Validação perplexidade: 43.1936 \n",
            "Step: 40 Amostras:320  2.196%  Momento: [2023-Mar-28 04:04:18] lr: 9.99960e-05 Train loss: 3.6092 perplexidade: 37.3296 Validação perplexidade: 43.5605 \n",
            " novo best valid 42.56814429812349; modelo salvo em /content/drive/My Drive/treinamento/202301_IA368DD/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 04:00:33].pt\n",
            "Step: 44 Amostras:352  2.415%  Momento: [2023-Mar-28 04:04:39] lr: 9.99956e-05 Train loss: 4.1718 perplexidade: 48.6297 Validação perplexidade: 42.5681  novo best valid 42.56814429812349; modelo salvo em /content/drive/My Drive/treinamento/202301_IA368DD/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 04:00:33].pt\n",
            "Step: 48 Amostras:384  2.635%  Momento: [2023-Mar-28 04:05:00] lr: 9.99952e-05 Train loss: 4.0155 perplexidade: 34.6361 Validação perplexidade: 42.9141 \n",
            "Step: 52 Amostras:416  2.855%  Momento: [2023-Mar-28 04:05:21] lr: 9.99948e-05 Train loss: 3.6199 perplexidade: 41.5406 Validação perplexidade: 42.4119  novo best valid 42.411859512126064\n",
            "Step: 56 Amostras:448  3.074%  Momento: [2023-Mar-28 04:05:42] lr: 9.99944e-05 Train loss: 3.5671 perplexidade: 36.8647 Validação perplexidade: 42.9443 \n",
            "Step: 60 Amostras:480  3.294%  Momento: [2023-Mar-28 04:06:02] lr: 9.99940e-05 Train loss: 3.2903 perplexidade: 33.2560 Validação perplexidade: 43.1269 \n",
            "Step: 64 Amostras:512  3.513%  Momento: [2023-Mar-28 04:06:23] lr: 9.99936e-05 Train loss: 3.5758 perplexidade: 33.2286 Validação perplexidade: 42.5069 \n",
            "Step: 68 Amostras:544  3.733%  Momento: [2023-Mar-28 04:06:43] lr: 9.99932e-05 Train loss: 3.4987 perplexidade: 37.5386 Validação perplexidade: 42.6534 \n",
            "Step: 72 Amostras:576  3.953%  Momento: [2023-Mar-28 04:07:04] lr: 9.99928e-05 Train loss: 3.6420 perplexidade: 41.9807 Validação perplexidade: 42.6278 \n",
            "Step: 76 Amostras:608  4.172%  Momento: [2023-Mar-28 04:07:25] lr: 9.99924e-05 Train loss: 3.4792 perplexidade: 37.3779 Validação perplexidade: 42.7394 \n",
            "Parando por critério de early_stop no step 80 sendo best_step 52 e ealy_stop 28\n",
            "Tempo gasto total 415.19238, steps: 80, tempo por step   5.18990\n",
            "Final: Step: 80 Amostras:640  4.392%  Momento: [2023-Mar-28 04:07:46] lr:9.99920e-05 Train loss: 3.5705 Train perplexidade: 37.3992 Validação perplexidade: 44.0490 \n",
            "Modelo com melhor resultado em validação (step 52) salvo após treino em /content/drive/My Drive/treinamento/202301_IA368DD/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 04:00:33].pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Resultado com dados de teste para modelo treinado: {'test/perplexidade': 38.348272162320406}\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegalmente mais importante. Poréis, a região do Brasil é uma pessoa na que gosta de que, então, a sugestão que construir a região da empresa é a possível área não ou sua direção não efetuar na empresa é a responsável pelos coelhas da em\n"
          ]
        }
      ],
      "source": [
        "treina_grid(hparam, gridparam, model, parm_se_gera_rastro = False, se_treina_poucos_dados=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRcUZlvvdw2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrZaURBpdwsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "astYLFr-dwpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBIuzRKbdxNq"
      },
      "source": [
        "Treinado por 1 época em todos os dados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(hparam['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72383039-f7f0-4059-9cb4-484444f42e94",
        "id": "hAto1eZydxNq"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e5de56-90e2-4ae2-93e9-505e2f3b9834",
        "id": "BPj2HsMsdxNr"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 11.66\n",
            " used 15.19\n",
            " free 3.43\n",
            " cached 8.65\n",
            " buffers 0.07\n",
            "/nGPU\n",
            "Tue Mar 28 04:23:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    26W /  70W |   5571MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6ZvsB8UdwmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "bWyPky-Qd887"
      },
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[2],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[8],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f510b233-cdf8-4ae3-f0e9-4b0649f9fa20",
        "id": "rayM8tB9d888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 2, 'fator_corte_loss_maximo': 1, 'batch_size': 8, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'batch_size': 8, 'num_epochs': 2, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'max_examples': 8657962, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 2705, 'early_stop': 18935, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f2e96b502e0>}\n",
            "Momento: [2023-Mar-28 04:37:40] Métricas iniciais em validação: {'valid/perplexidade': 41.44951838526234} Serão treinadas 8657962 amostras\n"
          ]
        }
      ],
      "source": [
        "treina_grid(hparam, gridparam, model, parm_se_gera_rastro = False, se_treina_poucos_dados=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiH41qVuhG2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BmRLgbyi_Dvg",
        "0Upk7A-8Zdnd",
        "wew-gFbWeBTq"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}