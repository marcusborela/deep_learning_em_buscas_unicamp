{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hmoQRiMWOU7"
      },
      "source": [
        "# Treino Modelo de Linguagem\n",
        "\n",
        "Aula 4 - [Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTLk-sf8Wid7"
      },
      "source": [
        "[![Open In Colab latest github version](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcusborela/deep_learning_em_buscas_unicamp/blob/main/code/aula4_treino_modelo_de_linguagem.ipynb) [Open In Colab latest github version]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "# Enunciado exercício\n",
        "\n",
        "Treinar um modelo de linguagem em dados em portugues\n",
        "\n",
        "Avaliar o modelo usando a perplexidade, que é simplesmente a exponencial de todas as losses do dataset de validação\n",
        "\n",
        "Iremos treinar o modelo para prever o próximo token dado os anteriores (também conhecido como Causal Language Modeling). Não confundir com o Masked Language Modeling (MLM), que consiste em prever tokens mascarados em uma dada sequência (ex: BERT's MLM)\n",
        "\n",
        "\n",
        "Dicas:\n",
        "\n",
        "Usar como ponto de partida o modelo OPT-125M, que já foi treinado em 300B de tokens (maioria em Inglês)\n",
        "\n",
        "Usar este dataset reduzido do mc4 portugues, com ~300M de tokens: gs://unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jW-hliBXGwH"
      },
      "source": [
        "Fonte de apoio: caderno usado na aula 10 do curso IA025 do próprio autor\n",
        "\n",
        "(Modelo de Linguagem com auto-atenção)\n",
        "[Exercício Aula 10: Modelo de Linguagem com auto-atenção](https://colab.research.google.com/drive/1a-L79jgyLkQITFE0EPVGBF_kpAaeYx25#scrollTo=Pwep987wSLIx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmRLgbyi_Dvg"
      },
      "source": [
        "# Organizando o ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Upk7A-8Zdnd"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "39667ff3-9860-40bf-f95a-9aac6f71a49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MO5ssyR9qeP4"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lUx84Olw88cZ"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm_notebook\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LTF06xq49F24"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "d49e688a-44a4-4792-aadc-ad9e1ad32dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 28 17:13:41 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 58%   46C    P8    26W / 370W |     62MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DKAZ8CWCAM3-"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria(lista_mem=['cpu']):\n",
        "  \"\"\"\n",
        "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
        "\n",
        "  Parâmetros:\n",
        "  -----------\n",
        "  lista_mem : list, opcional\n",
        "      Lista com strings 'cpu' e/ou 'gpu'. \n",
        "      'cpu' - exibe informações de memória da CPU.\n",
        "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
        "      O valor padrão é ['cpu'].\n",
        "\n",
        "  Saída:\n",
        "  -------\n",
        "  A função não retorna nada, apenas exibe as informações na tela.\n",
        "\n",
        "  Exemplo de uso:\n",
        "  ---------------\n",
        "  Para exibir informações de memória da CPU:\n",
        "      mostra_memoria(['cpu'])\n",
        "\n",
        "  Para exibir informações de memória da CPU e GPU:\n",
        "      mostra_memoria(['cpu', 'gpu'])\n",
        "  \n",
        "  Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "  \"\"\"  \n",
        "  if 'cpu' in lista_mem:\n",
        "    vm = virtual_memory()\n",
        "    ram={}\n",
        "    ram['total']=round(vm.total / 1e9,2)\n",
        "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
        "    print('/nGPU')\n",
        "    gpu_info = !nvidia-smi\n",
        "  if 'gpu' in lista_mem:\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "2ddd75fa-3123-4027-8d69-8bc94fc85615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 59.3\n",
            " used 6.84\n",
            " free 14.37\n",
            " cached 45.59\n",
            " buffers 0.55\n",
            "/nGPU\n",
            "Tue Mar 28 17:13:42 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 58%   46C    P8    26W / 370W |     62MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu', 'gpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xRdgUGMPgh"
      },
      "source": [
        "### Vinculando pasta do google drive para salvar dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae-Iy2oz_9os",
        "outputId": "db156e3e-b231-47d9-8f49-1d6040feb7ac"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYGL4MV_yhQ",
        "outputId": "5ad18083-48f1-4577-b0c5-863e1c45c1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /home/borela/fontes/deep_learning_em_buscas_unicamp/code\n"
          ]
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "print(\"Current directory:\", current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3qjXs6X2ME"
      },
      "source": [
        "## Fixando as seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AG9RjMb8Qlot"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ySVSWRCXX2ME"
      },
      "outputs": [],
      "source": [
        "def inicializa_seed(num_semente:int=123):\n",
        "  \"\"\"\n",
        "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
        "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
        "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
        "  \n",
        "  Args:\n",
        "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
        "  \n",
        "  References:\n",
        "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
        "  \"\"\"\n",
        "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
        "  random.seed(num_semente)\n",
        "  np.random.seed(num_semente)\n",
        "  torch.manual_seed(num_semente)\n",
        "  \n",
        "  # Define as sementes da GPU\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  #torch.cuda.manual_seed(num_semente)\n",
        "  #Cuda algorithms\n",
        "  #torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j9dUkABwX2ME"
      },
      "outputs": [],
      "source": [
        "num_semente=123\n",
        "inicializa_seed(num_semente)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3W-iu3UX2MF"
      },
      "source": [
        "## Definindo Hiperparâmetros iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_FxOjwQcX2MF"
      },
      "outputs": [],
      "source": [
        "def inicia_hparam()->dict:\n",
        "  # Inicialização dos parâmetros\n",
        "  hparam = {}\n",
        "  hparam[\"num_workers_dataloader\"] = 0\n",
        "  hparam[\"device\"] = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  if torch.cuda.is_available(): print(torch. cuda. get_device_name(hparam[\"device\"]))    \n",
        "  return hparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2HKHbrYX2MF",
        "outputId": "f90846d9-b669-4df5-a60a-4ab4d9ed986b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce RTX 3090\n"
          ]
        }
      ],
      "source": [
        "hparam=inicia_hparam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMz76cmAX2MF"
      },
      "source": [
        "## Preparando para debug e display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BJ6S4P5Hw4iG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKNAMVTnX2MG"
      },
      "source": [
        "https://zohaib.me/debugging-in-google-collab-notebook/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tb1KfD9-X2MG"
      },
      "outputs": [],
      "source": [
        "# !pip install -Uqq ipdb\n",
        "import ipdb\n",
        "# %pdb off # desativa debug em exceção\n",
        "# %pdb on  # ativa debug em exceção\n",
        "# ipdb.set_trace(context=8)  para execução nesse ponto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wQ5pmlOHxHhk"
      },
      "outputs": [],
      "source": [
        "def config_display():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de display do Pandas.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurando formato saída Pandas\n",
        "  # define o número máximo de colunas que serão exibidas\n",
        "  pd.options.display.max_columns = None\n",
        "\n",
        "  # define a largura máxima de uma linha\n",
        "  pd.options.display.width = 1000\n",
        "\n",
        "  # define o número máximo de linhas que serão exibidas\n",
        "  pd.options.display.max_rows = 100\n",
        "\n",
        "  # define o número máximo de caracteres por coluna\n",
        "  pd.options.display.max_colwidth = 50\n",
        "\n",
        "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
        "  pd.options.display.show_dimensions = True\n",
        "\n",
        "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
        "  pd.options.display.precision = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b2tDy72ATNHs"
      },
      "outputs": [],
      "source": [
        "def config_debug():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
        "  transformers e datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define opções de impressão de tensores para o modo científico\n",
        "  torch.set_printoptions(sci_mode=True) \n",
        "  \"\"\"\n",
        "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
        "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
        "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
        "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
        "    uma melhor compreensão dos números envolvidos.  \n",
        "  \"\"\"\n",
        "\n",
        "  # Habilita detecção de anomalias no autograd do PyTorch\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  \"\"\"\n",
        "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
        "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
        "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
        "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
        "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
        "    antes que se torne um problema maior.\n",
        "\n",
        "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
        "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
        "    ela deve ser usada com cautela e apenas para depuração.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "  \"\"\"\n",
        "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
        "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
        "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
        "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
        "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
        "    e removê-lo após a solução do problema.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define o nível de verbosity do pacote transformers para info\n",
        "  transformers.utils.logging.set_verbosity_info() \n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
        "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
        "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
        "\n",
        "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
        "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
        "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
        "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
        "    necessidade de cada tarefa.\n",
        "  \n",
        "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
        "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
        "  \n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define o modo verbose do xmode, que é utilizado no debug\n",
        "  %xmode Verbose \n",
        "\n",
        "  \"\"\"\n",
        "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
        "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
        "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
        "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
        "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
        "\n",
        "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
        "    comentar a linha acima e descomentar a linha abaixo:\n",
        "    %xmode Plain\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "    Dica:\n",
        "    1.  pdb (Python Debugger)\n",
        "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
        "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
        "\n",
        "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
        "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
        "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
        "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
        "\n",
        "\n",
        "    2. ipdb\n",
        "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
        "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
        "      \n",
        "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
        "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
        "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
        "      comandos para investigar o comportamento.\n",
        "\n",
        "      Durante a depuração, você pode usar comandos:\n",
        "        next (para executar a próxima linha de código), \n",
        "        step (para entrar em uma função chamada na próxima linha de código) \n",
        "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
        "\n",
        "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
        "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
        "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Tb4aqtcExR84"
      },
      "outputs": [],
      "source": [
        "config_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bGQAf_YX2MJ",
        "outputId": "13f0af4e-e2e2-44d7-9946-f46152af906d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Verbose\n"
          ]
        }
      ],
      "source": [
        "config_debug()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VEyI8MpX2MJ"
      },
      "source": [
        "## Rastro (neptune.ai)\n",
        "\n",
        "Gerado rastro da execução no Neptune (detalhes no artigo [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik2HW7K-X2MJ"
      },
      "source": [
        "### Importação de libraries para Rastro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['NEPTUNE_ALLOW_SELF_SIGNED_CERTIFICATE'] = 'TRUE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alyRE86Id40N",
        "outputId": "26483b0f-7e13-47b7-f0e2-737cd7640d1d"
      },
      "outputs": [],
      "source": [
        "# !pip install neptune-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M63geOdvd2kt",
        "outputId": "699aca9b-7523-48be-ab72-1c8617398015"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/treinamento/lib/python3.7/site-packages/neptune/internal/backends/hosted_client.py:50: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
            "  from neptune.version import version as neptune_client_version\n"
          ]
        }
      ],
      "source": [
        "import neptune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7cQvuqrppnIZ"
      },
      "outputs": [],
      "source": [
        "# | pip install torchviz\n",
        "# from torchviz import make_dot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0XcZKZJ_X2MK"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import copy\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95u6fg6QX2MK"
      },
      "source": [
        "### Código Rastro\n",
        "\n",
        "Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664), autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "03PO_YjJX2MK"
      },
      "outputs": [],
      "source": [
        "def converte_optimizer_state_dict(parm_optimizer)-> dict:\n",
        "  \"\"\"\n",
        "    Recebe um objeto \"parm_optimizer\" que é do tipo \"torch.optim.Optimizer\" e retorna um dicionário \n",
        "    com informações sobre o otimizador.\n",
        "\n",
        "    O dicionário de retorno é gerado a partir do estado do otimizador que é extraído da propriedade\n",
        "    \"state_dict()\" do objeto \"parm_optimizer\", seu primeiro grupo de parâmetros do otimizador.\n",
        "  \"\"\"\n",
        "  # return str(hparam['optimizer'])\n",
        "  return parm_optimizer.state_dict()['param_groups'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TOrNttzncJ7f"
      },
      "outputs": [],
      "source": [
        "def gera_tag_rastro_experiencia_treino(parm_aula: str, hparam: dict) -> str:\n",
        "    \"\"\"\n",
        "    Gera uma string formatada com informações de hiperparâmetros para ser usada como tag de rastro de experiência de treino.\n",
        "\n",
        "    Args:\n",
        "        parm_aula (str): Nome da aula que está sendo treinada.\n",
        "        hparam (dict): Dicionário contendo os hiperparâmetros utilizados no treinamento.\n",
        "\n",
        "    Returns:\n",
        "        str: String formatada com as informações de hiperparâmetros.\n",
        "\n",
        "    Uso: \n",
        "\n",
        "    hparam['lista_tag_rastro_experiencia_treino'] =        gera_tag_rastro_experiencia_treino(parm_aula='aula7', hparam=hparam)\n",
        "    \"\"\"\n",
        "    # Inicializa uma lista vazia para armazenar as tags\n",
        "    lista_tag = []\n",
        "    \n",
        "    # Lista com as chaves dos hiperparâmetros que serão utilizados\n",
        "    lista_chaves = ['embed_dim', 'leiaute_input', 'dim_feedforward', 'max_seq_length', 'ind_activation_function', 'batch_size', 'learning_rate', 'weight_decay', 'amsgrad', 'decrease_factor_lr', 'max_examples', 'eval_every_steps']\n",
        "    \n",
        "    # Itera pelas chaves da lista e cria uma string com a chave e o valor correspondente em hparam,\n",
        "    # adicionando essa string à lista_tag\n",
        "    for chave in lista_chaves:\n",
        "        if chave in hparam:\n",
        "          tag = f\"{chave} {hparam[chave]}\"\n",
        "          lista_tag.append(tag)\n",
        "    \n",
        "    # Concatena a lista de tags em uma única string, separando cada tag por '|',\n",
        "    # e adicionando o nome da aula como prefixo\n",
        "    tag_formatada = f\"{parm_aula}|\" + \"|\".join(lista_tag)\n",
        "    \n",
        "    return tag_formatada\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "x2IIVG8EX2ML"
      },
      "outputs": [],
      "source": [
        "class NeptuneRastroRun():\n",
        "    \"\"\"\n",
        "      Classe para geração de rastro de experimento utilizando a ferramenta Neptune.\n",
        "\n",
        "      Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664),\n",
        "      autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n",
        "      Attributes:\n",
        "      -----------\n",
        "      se_geracao_rastro : bool\n",
        "          Indica se deve ser gerado rastro de experimento. \n",
        "      neptune_project : str\n",
        "          Nome do projeto criado no Neptune. \n",
        "      tag_contexto_rastro : str\n",
        "          Nome da tag utilizada para identificar o experimento.\n",
        "      neptune_api_token : str\n",
        "          Token utilizado para autenticação na API do Neptune. \n",
        "      run_neptune : object\n",
        "          Objeto que representa o experimento no Neptune.\n",
        "      device : str\n",
        "          Dispositivo utilizado para o treinamento do modelo.\n",
        "      tmpDir : str\n",
        "        Diretório temporário utilizado para salvar gráfico do modelo.          \n",
        "    \"\"\"\n",
        "    se_geracao_rastro = True \n",
        "    neptune_project = \"\"\n",
        "    tag_contexto_rastro = \"\"\n",
        "    neptune_api_token = \"\"\n",
        "\n",
        "    def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "      \"\"\"\n",
        "        Método construtor da classe NeptuneRastroRun.\n",
        "        \n",
        "        Args:\n",
        "        - parm_params: dicionário contendo os parâmetros do modelo.\n",
        "        - parm_lista_tag: lista contendo tags adicionais para o experimento.\n",
        "      \"\"\"      \n",
        "      # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "      if self.__class__.se_geracao_rastro:      \n",
        "        self.run_neptune = neptune.init_run(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "        self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "        vparams = copy.deepcopy(parm_params)\n",
        "        if \"optimizer\" in vparams:\n",
        "          vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "        if 'criterion'  in vparams:\n",
        "          vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "        if 'scheduler'  in vparams:\n",
        "          vparams[\"scheduler\"] = str(type(vparams[\"scheduler\"]))\n",
        "        if 'device' in vparams:\n",
        "          vparams['device'] = str(vparams[\"device\"])\n",
        "        self.device = vparams[\"device\"]\n",
        "        for tag in parm_lista_tag:\n",
        "          self.run_neptune['sys/tags'].add(tag)\n",
        "        self.run_neptune['parameters'] = vparams\n",
        "        # self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "    @property\n",
        "    def run():\n",
        "      \"\"\"\n",
        "      Retorna a instância do objeto run_neptune.\n",
        "      \"\"\"      \n",
        "      return self.run_neptune\n",
        "\n",
        "    @classmethod\n",
        "    def ativa_geracao_rastro(cls):\n",
        "      \"\"\"\n",
        "      Ativa a geração de rastro.\n",
        "      \"\"\"      \n",
        "      cls.se_geracao_rastro = True      \n",
        "\n",
        "    @classmethod\n",
        "    def def_contexto(cls):\n",
        "      \"\"\"\n",
        "      Define o contexto para a geração de rastro.\n",
        "      \"\"\"      \n",
        "      cls.se_geracao_rastro = True      \n",
        "\n",
        "    @classmethod\n",
        "    def desativa_geracao_rastro(cls):\n",
        "      \"\"\"\n",
        "      Desativa a geração de rastro.\n",
        "      \"\"\"      \n",
        "      cls.se_geracao_rastro = False      \n",
        "\n",
        "    @classmethod\n",
        "    def retorna_status_geracao_rastro(cls):\n",
        "      \"\"\"\n",
        "        Retorna o status da geração de rastro.\n",
        "        \n",
        "        Returns:\n",
        "        - True se a geração de rastro está ativada, False caso contrário.\n",
        "      \"\"\"      \n",
        "      return cls.se_geracao_rastro      \n",
        "\n",
        "    @classmethod\n",
        "    def retorna_tag_contexto_rastro(cls):\n",
        "      \"\"\"\n",
        "        Retorna a tag do contexto de rastro.\n",
        "      \"\"\"      \n",
        "      return cls.tag_contexto_rastro \n",
        "\n",
        "    @classmethod\n",
        "    def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "      \"\"\"\n",
        "      Inicia o contexto de execução no Neptune.\n",
        "\n",
        "      Args:\n",
        "          neptune_project (str): Nome do projeto no Neptune.\n",
        "          tag_contexto_rastro (str): Tag que identifica o contexto de execução no Neptune.\n",
        "          neptune_api_token (str): Token de acesso à API do Neptune.\n",
        "\n",
        "      Raises:\n",
        "          AssertionError: Caso a tag_contexto_rastro possua um ponto (.), \n",
        "            o que pode gerar erros na gravação de arquivo.\n",
        "      \"\"\"      \n",
        "      assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "      cls.neptune_api_token = neptune_api_token\n",
        "      cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "      cls.neptune_project = neptune_project\n",
        "\n",
        "    def salva_metrica(self, parm_metricas={}):\n",
        "      \"\"\"\n",
        "        Salva as métricas no Neptune Run caso a geração de rastro esteja ativa.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        parm_metricas: dict\n",
        "            Dicionário contendo as métricas a serem salvas. As chaves devem ser os nomes das métricas e os valores devem ser\n",
        "            os valores das métricas.\n",
        "      \"\"\"\n",
        "      #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "      if self.__class__.se_geracao_rastro:\n",
        "        for metrica, valor in parm_metricas.items(): \n",
        "          self.run_neptune[metrica].append(valor)\n",
        " \n",
        "    def gera_grafico_modelo(self, loader_train, model):\n",
        "      \"\"\"\n",
        "        Gera um gráfico do modelo e o envia para o Neptune. \n",
        "        Para gerar o gráfico, um forward pass é realizado em um batch de exemplos \n",
        "        de treino e o resultado é renderizado como um gráfico de nós conectados. \n",
        "        O gráfico é salvo em um arquivo .png e enviado para o Neptune como um arquivo anexo.\n",
        "\n",
        "        Args:\n",
        "            loader_train (torch.utils.data.DataLoader): DataLoader do conjunto de treinamento.\n",
        "            model (torch.nn.Module): Modelo a ser visualizado.\n",
        "        \n",
        "        Pendente:\n",
        "          Evolui para usar from io import StringIO (buffer = io.StringIO()) ao invés de tempdir \n",
        "      \"\"\"    \n",
        "      return\n",
        "\n",
        "      \"\"\"\n",
        "      falta ajustar make_dot\n",
        "      if self.__class__.se_geracao_rastro: \n",
        "        # efetuar um forward \n",
        "        batch = next(iter(loader_train))\n",
        "        # falta generalizar linha abaixo. Criar função que recebe modelo e batch como parâmetro?\n",
        "        outputs = model(input_ids=batch['input_ids'].to(hparam['device']), attention_mask=batch['attention_mask'].to(hparam['device']), token_type_ids=batch['token_type_ids'].to(hparam['device']), labels=batch['labels'].to(hparam['device']))\n",
        "        nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "        make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "        self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "        self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "      \"\"\"\n",
        "        Para a execução do objeto Neptune. Todos os experimentos do Neptune são sincronizados com o servidor, e nenhum outro \n",
        "        experimento poderá ser adicionado a este objeto após a chamada a este método.\n",
        "      \"\"\"\n",
        "      if self.__class__.se_geracao_rastro:         \n",
        "        self.run_neptune.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYAbHMteX2MM"
      },
      "source": [
        "### Definindo parâmetros para o rastro\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz6I6FITX2MM",
        "outputId": "f9a98945-28fa-4a9e-b50e-7253ad27184a"
      },
      "outputs": [],
      "source": [
        "NeptuneRastroRun.inicia_contexto('marcusborela/IA386DD', 'Aula 2 - classificador de texto como reranqueador',   getpass.getpass('Informe NEPTUNE_API_TOKEN'))\n",
        "#NeptuneRastroRun.desativa_geracao_rastro()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "# Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "outputs": [],
      "source": [
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK6yCyrpp4Tl",
        "outputId": "35ab5bc5-9256-43e8-b831-5537ccda60b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'b', 'c']\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "a = [['a','b'], ['c']]\n",
        "print(list(itertools.chain.from_iterable(a)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DMePGM1MUqOK"
      },
      "outputs": [],
      "source": [
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "      \"\"\"\n",
        "      gera sentencas x e y com max_seq_length\n",
        "      como para a última posição de x não haveria label\n",
        "        salva-se sentence_data com max_length + 1\n",
        "        na hora de salvar\n",
        "        x:sentence_data[0:max_length]\n",
        "        y:sentence_data[1:max_length+1]\n",
        "      \"\"\"\n",
        "      assert isinstance(texts,list), 'texts deve ser do tipo list'\n",
        "      assert isinstance(texts[0],str), 'texts deve ser do tipo iterator of iterator of strings'\n",
        "      assert isinstance(max_seq_length,int), 'max_seq_length deve ser do tipo int'\n",
        "      assert max_seq_length > 3, 'max_seq_length deve ser maior do que 3'\n",
        "\n",
        "      sentence_data = []\n",
        "      sentence_length = max_seq_length + 1\n",
        "      self.qtd_sequencia = 0\n",
        "      tamanho_batch = 50\n",
        "      num_batch_entrada = math.ceil(len(texts)/tamanho_batch) # inteiro acima para pegar último batch parcial de sentenças\n",
        "      print(f\" len(texts) {len(texts)}; tamanho_batch {tamanho_batch};  num_batch_entrada {num_batch_entrada} \")\n",
        "      print(f\" max_seq_length {max_seq_length}; Mas salvando sentence_length {sentence_length}\")\n",
        "      for ndx_batch in range(num_batch_entrada): \n",
        "        # ipdb.set_trace(context=6)\n",
        "        #if ndx_batch % 100 == 0:\n",
        "        #    print(F'\\tInicio Mydataset ndx_batch+1: {ndx_batch+1}')\n",
        "        batch_texto_numericalizado = tokenizer.batch_encode_plus(texts[ndx_batch*tamanho_batch:ndx_batch*tamanho_batch+tamanho_batch],\n",
        "                                                                 return_attention_mask=False, return_token_type_ids = False, add_special_tokens=True).input_ids ## já retorna sos\n",
        "\n",
        "        # concatenando conforme sugerido em https://huggingface.co/course/chapter7/6?fw=pt \n",
        "        #   \"A more efficient way to prepare the data is to join all the tokenized samples \n",
        "        #    in a batch with an eos_token_id token in between, and then perform the chunking \n",
        "        #    on the concatenated sequences. \"\n",
        "        lista_tokens_concatenadas = list(itertools.chain.from_iterable(batch_texto_numericalizado))\n",
        "        num_sentencas = math.floor(len(lista_tokens_concatenadas)/sentence_length)  # eliminando a última com pads\n",
        "        # if ndx_batch % 100 == 0:\n",
        "        #    print(F'\\t === num_sentencas:{num_sentencas} ')\n",
        "        for cnt_sentenca_batch in range(num_sentencas): \n",
        "          texto_numericalizado = lista_tokens_concatenadas[cnt_sentenca_batch*sentence_length:cnt_sentenca_batch*sentence_length + sentence_length]\n",
        "          sentence_data.append(texto_numericalizado)\n",
        "          self.qtd_sequencia += 1\n",
        "        if ndx_batch % 100 == 0:\n",
        "            print(F'\\t === ndx_batch+1: {ndx_batch+1}  self.qtd_sequencia: {self.qtd_sequencia}; Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "      print(F'\\tVou converter lista para tensor;  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "      self.data_tensor = torch.tensor(sentence_data).long()\n",
        "      print(F'\\tConvertido: lista para tensor;  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "      print(f\"Carregado dataset com {self.qtd_sequencia} sentenças\")\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.qtd_sequencia\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if idx > self.qtd_sequencia:\n",
        "        raise Exception(\"Tentativa de ler além do limite\")\n",
        "      return self.data_tensor[idx][:-1], self.data_tensor[idx][1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqF-LbXgrpFP"
      },
      "source": [
        "## Carregando modelo e tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1InUqeoncLP0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer,  AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'facebook/opt-125m'\n",
        "# requer Embedding dimensions of the model (1024) don't match the embedding dimensions of the document store (768). Initiate ElasticsearchDocumentStore again with arg embedding_dim=1024.\n",
        "# nome_modelo = \"unicamp-dl/ptt5-base-pt-msmarco-100k-v2\"\n",
        "# nome_modelo = \"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
        "\n",
        "NOME_CAMINHO_MODELO = \"/home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/\" + MODEL_NAME\n",
        "\n",
        "assert os.path.exists(NOME_CAMINHO_MODELO), f\"Path para {NOME_CAMINHO_MODELO} não existe!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfn2hCybb9FA",
        "outputId": "daed14b1-c622-4786-c179-625c1b4d1ce8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"/home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading file vocab.json\n",
            "loading file merges.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file /home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"/home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading configuration file /home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"/home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading weights file /home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "All the weights of OPTForCausalLM were initialized from the model checkpoint at /home/borela/fontes/deep_learning_em_buscas_unicamp/modelo/facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Por quê model.config.vocab_size (50272) != tokenizer.vocab_size (50265). Não deveriam ser iguais?\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(NOME_CAMINHO_MODELO)\n",
        "model = AutoModelForCausalLM.from_pretrained(NOME_CAMINHO_MODELO).to(hparam['device'])\n",
        "print(f'Por quê model.config.vocab_size ({model.config.vocab_size}) != tokenizer.vocab_size ({tokenizer.vocab_size}). Não deveriam ser iguais?')\n",
        "# tokenizer.vocab_size == 50265\n",
        "# model.config.vocab_size == 50272"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "X238N2gSOcUW"
      },
      "outputs": [],
      "source": [
        "hparam['vocab_size']= model.config.vocab_size # tokenizer.vocab_size\n",
        "# tokenizer.vocab_size == 50265\n",
        "# model.config.vocab_size == 50272\n",
        "# Por quê???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wew-gFbWeBTq"
      },
      "source": [
        "## Teste da implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SKwP0mWxA_c8"
      },
      "outputs": [],
      "source": [
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza pelas manhãs', 'Meu Pai, que mora em Brasília, gosta muito de correr no parque e está de dieta agora.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRyWTs9oyX8",
        "outputId": "8f2a3de9-2f87-4999-ed0c-886adc69c3b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2, 717, 257, 821, 2603, 139, 263, 9240, 8564], [2, 717, 2560, 821, 13334, 475, 6439, 139, 263, 3137, 254, 9366, 11188, 281, 313, 298, 17682, 29], [2, 5096, 257, 27039, 6, 1192, 14628, 102, 2841, 17128, 1977, 14190, 6, 821, 13334, 475, 6439, 139, 263, 9240, 8564, 117, 2242, 3407, 364, 3304, 1526, 263, 5626, 102, 5951, 4330, 4]]\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.batch_encode_plus(dummy_texts, return_attention_mask=False, return_token_type_ids = False, add_special_tokens=True).input_ids ## já retorna cls e sep\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZKGtVJHJaiz",
        "outputId": "aaf071ce-7358-44fb-ff42-675d8ca876ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(itertools.chain.from_iterable(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pWLNl32csty",
        "outputId": "38847b2b-bf74-4676-a9bf-d077190a6341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 717, 257, 821, 2603, 139, 263, 9240, 8564, 2, 717, 2560, 821, 13334, 475, 6439, 139, 263, 3137, 254, 9366, 11188, 281, 313, 298, 17682, 29, 2, 5096, 257, 27039, 6, 1192, 14628, 102, 2841, 17128, 1977, 14190, 6, 821, 13334, 475, 6439, 139, 263, 9240, 8564, 117, 2242, 3407, 364, 3304, 1526, 263, 5626, 102, 5951, 4330, 4]\n"
          ]
        }
      ],
      "source": [
        "print(list(itertools.chain.from_iterable(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdjhH-mlo43Z",
        "outputId": "5a6be239-e176-45c2-91dd-397b6f639cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('</s>', '<pad>')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(2), tokenizer.decode(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC6ziNOp7Juy",
        "outputId": "60cfc9d7-cbfb-4c3e-c32a-8f959d1ce35d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2, 717, 2560, 821, 13334, 475, 6439, 139, 263, 3137, 254, 9366, 11705, 3304, 1526, 263, 5626, 102, 5951, 4330]]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.batch_encode_plus([\"Ela gosta muito de comer pizza mas está de dieta agora\"]).input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN52aFtTl219",
        "outputId": "143dd3c2-7a91-48cc-ae79-05fc3c0e6dc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['</s>', '<pad>']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP4hcZfRl9Je",
        "outputId": "bdf98441-86c5-4a65-cf00-ddefb8b4c837"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(None,\n",
              " None,\n",
              " '</s>',\n",
              " 1,\n",
              " {'bos_token': '</s>',\n",
              "  'eos_token': '</s>',\n",
              "  'unk_token': '</s>',\n",
              "  'pad_token': '<pad>'})"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.cls_token_id, tokenizer.sep_token_id ,tokenizer.eos_token, tokenizer.pad_token_id , tokenizer.special_tokens_map "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se1_OZ3Jnjto",
        "outputId": "73e291f9-0382-40a2-975b-49cebad345c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " len(texts) 3; tamanho_batch 50;  num_batch_entrada 1 \n",
            " max_seq_length 9; Mas salvando sentence_length 10\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 6; Momento: [2023-Mar-28 17:14:39]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 17:14:39]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 17:14:39]\n",
            "Carregado dataset com 6 sentenças\n"
          ]
        }
      ],
      "source": [
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvBD9vuGe6Zz",
        "outputId": "d4932aa8-3b03-40c5-eaa3-b3ebbcd3cb1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, [9, 14])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dummy_texts), [len(txt) for txt in tokenizer(dummy_texts)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AER4E0JCexae",
        "outputId": "fc77c3ba-eb15-4a74-d3b1-33bdc435399a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dummy_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2BuTp4Dv8D8",
        "outputId": "616199b7-2b65-4e12-c61a-4fa5f69a989f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passou no assert de tamanho do dataset\n"
          ]
        }
      ],
      "source": [
        "# math.floor(len(list(itertools.chain.from_iterable(tokens))) / max_seq_length)\n",
        "# eliminando a última não completa\n",
        "assert len(dummy_dataset) == 6\n",
        "print('passou no assert de tamanho do dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NKR7NIdVnjtp"
      },
      "outputs": [],
      "source": [
        "first_batch_input, first_batch_target = next(iter(dummy_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiPSWmndJcuj",
        "outputId": "cbe2d54f-3037-43fc-afc2-3aacdc5be24f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[    2,   717,   257,   821,  2603,   139,   263,  9240,  8564],\n",
              "         [  717,  2560,   821, 13334,   475,  6439,   139,   263,  3137],\n",
              "         [ 9366, 11188,   281,   313,   298, 17682,    29,     2,  5096],\n",
              "         [27039,     6,  1192, 14628,   102,  2841, 17128,  1977, 14190],\n",
              "         [  821, 13334,   475,  6439,   139,   263,  9240,  8564,   117],\n",
              "         [ 3407,   364,  3304,  1526,   263,  5626,   102,  5951,  4330]]),\n",
              " tensor([[  717,   257,   821,  2603,   139,   263,  9240,  8564,     2],\n",
              "         [ 2560,   821, 13334,   475,  6439,   139,   263,  3137,   254],\n",
              "         [11188,   281,   313,   298, 17682,    29,     2,  5096,   257],\n",
              "         [    6,  1192, 14628,   102,  2841, 17128,  1977, 14190,     6],\n",
              "         [13334,   475,  6439,   139,   263,  9240,  8564,   117,  2242],\n",
              "         [  364,  3304,  1526,   263,  5626,   102,  5951,  4330,     4]]))"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "first_batch_input, first_batch_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "N0xhZcFhVJ01"
      },
      "outputs": [],
      "source": [
        "correct_first_batch_input = torch.LongTensor(\n",
        "        [[    2,   717,   257,   821,  2603,   139,   263,  9240,  8564],\n",
        "         [  717,  2560,   821, 13334,   475,  6439,   139,   263,  3137],\n",
        "         [ 9366, 11188,   281,   313,   298, 17682,    29,     2,  5096],\n",
        "         [27039,     6,  1192, 14628,   102,  2841, 17128,  1977, 14190],\n",
        "         [  821, 13334,   475,  6439,   139,   263,  9240,  8564,   117],\n",
        "         [ 3407,   364,  3304,  1526,   263,  5626,   102,  5951,  4330]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([[  717,   257,   821,  2603,   139,   263,  9240,  8564,     2],\n",
        "         [ 2560,   821, 13334,   475,  6439,   139,   263,  3137,   254],\n",
        "         [11188,   281,   313,   298, 17682,    29,     2,  5096,   257],\n",
        "         [    6,  1192, 14628,   102,  2841, 17128,  1977, 14190,     6],\n",
        "         [13334,   475,  6439,   139,   263,  9240,  8564,   117,  2242],\n",
        "         [  364,  3304,  1526,   263,  5626,   102,  5951,  4330,     4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhfLZAyMVUki",
        "outputId": "4a906e75-b6b5-49ec-c989-ee85d2e49229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passou no assert ajustado de dataset.\n"
          ]
        }
      ],
      "source": [
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert ajustado de dataset.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4Cw7LANFEQ",
        "outputId": "424328ae-2eb5-40ae-c83d-9c39571d53f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_sentencas: 250000\n",
            "hparam['max_seq_length']: 100\n"
          ]
        }
      ],
      "source": [
        "#@title Seleção datasets\n",
        "hparam['num_sentenca_train'] = 249800 #@param [800, 249800] {type:'raw'}\n",
        "hparam['num_sentenca_valid'] = 100 # 100\n",
        "hparam['num_sentenca_test'] = 100 # 100\n",
        "hparam['max_seq_length'] = 100 #@param [9, 50, 100, 250] {type:'raw'}\n",
        "total_sentencas = hparam['num_sentenca_train']+hparam['num_sentenca_valid']+hparam['num_sentenca_test']\n",
        "print(f\"total_sentencas: {total_sentencas}\")\n",
        "print(f\"hparam['max_seq_length']: {hparam['max_seq_length']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyq5iuj_NFEQ",
        "outputId": "80d50cc5-a8a5-49b8-fd37-2355d7a59d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/dataset_max_seq_100_text_\n"
          ]
        }
      ],
      "source": [
        "# prefixo_nome_diretorio = '/content/drive/My Drive/treinamento/202301_IA368DD/aula4/'\n",
        "prefixo_nome_diretorio = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/'\n",
        "\n",
        "infixo_nome= 'dataset_max_seq_'+ str(hparam['max_seq_length'])+'_text_'\n",
        "print(prefixo_nome_diretorio + infixo_nome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVTsirIiqJUB",
        "outputId": "082d20fa-1596-48fc-a5ab-85ae097adb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "#@title Boolean fields\n",
        "datasets_carregados_previamente = True #@param {type:\"boolean\"}\n",
        "\n",
        "print(datasets_carregados_previamente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "FUqNOdVlGEYd"
      },
      "outputs": [],
      "source": [
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "manqs7PdtOjQ"
      },
      "outputs": [],
      "source": [
        "if datasets_carregados_previamente:\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_test'])+'_test.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  test_dataset = torch.load(buffer)\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_valid'])+'_valid.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  valid_dataset = torch.load(buffer)\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_train'])+'_train.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  train_dataset = torch.load(buffer)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqEpL-vBtOjR",
        "outputId": "3e311b73-5ded-4d48-ee40-8a729f281b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-28 07:10:59--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.222.16, 172.217.173.80, 142.250.218.176, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.222.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1230909256 (1,1G) [text/plain]\n",
            "Saving to: ‘sample-1gb.txt’\n",
            "\n",
            "sample-1gb.txt      100%[===================>]   1,15G  12,0MB/s    in 97s     \n",
            "\n",
            "2023-03-28 07:12:37 (12,1 MB/s) - ‘sample-1gb.txt’ saved [1230909256/1230909256]\n",
            "\n",
            "250000\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  !wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
        "  texts = open('sample-1gb.txt').readlines() \n",
        "  assert total_sentencas <= len(texts), f\"total sentencas deve ser <= len(texts)\"\n",
        "  # texts = texts[:total]  \n",
        "  print(len(texts)) # 250000 \n",
        "\n",
        "  # carga total para treino:\n",
        "  # hparam['num_sentenca_train'] = total_sentencas - (hparam['num_sentenca_valid'] + hparam['num_sentenca_test'])\n",
        "  # train_texts = texts[:hparam['num_sentenca_train'] ]\n",
        "  # hparam['num_sentenca_train'] = len(train_texts)\n",
        "  # carga parcial para treino:\n",
        "  \n",
        "  #ipdb.set_trace(context=6)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5uOibekNNNl",
        "outputId": "ea6d8a6a-afe2-4ca4-838d-83408cfa615d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carregando valid_dataset\n",
            " len(texts) 100; tamanho_batch 50;  num_batch_entrada 2 \n",
            " max_seq_length 100; Mas salvando sentence_length 101\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 1155; Momento: [2023-Mar-28 07:13:36]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 07:13:36]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 07:13:36]\n",
            "Carregado dataset com 2036 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  valid_texts = texts[-(hparam['num_sentenca_valid'] + hparam['num_sentenca_test']):-hparam['num_sentenca_test']]\n",
        "  print(\"carregando valid_dataset\")\n",
        "  valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(valid_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_valid'])+'_valid.pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xkTjjo1NNNm",
        "outputId": "fb3a486b-76c4-4949-9ae4-649a904b11f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carregando test_dataset\n",
            " len(texts) 100; tamanho_batch 50;  num_batch_entrada 2 \n",
            " max_seq_length 100; Mas salvando sentence_length 101\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 612; Momento: [2023-Mar-28 07:13:37]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 07:13:38]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 07:13:38]\n",
            "Carregado dataset com 1143 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  print(\"carregando test_dataset\")\n",
        "  test_texts = texts[-hparam['num_sentenca_test']:]  \n",
        "  test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(test_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_test'])+'_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhKHOXM5NNNm",
        "outputId": "8c6ee036-0ce2-4740-a92f-53aa16e77777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carregando train_dataset\n",
            " len(texts) 249800; tamanho_batch 50;  num_batch_entrada 4996 \n",
            " max_seq_length 100; Mas salvando sentence_length 101\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 983; Momento: [2023-Mar-28 07:13:39]\n",
            "\t === ndx_batch+1: 101  self.qtd_sequencia: 91674; Momento: [2023-Mar-28 07:14:03]\n",
            "\t === ndx_batch+1: 201  self.qtd_sequencia: 184472; Momento: [2023-Mar-28 07:14:26]\n",
            "\t === ndx_batch+1: 301  self.qtd_sequencia: 269829; Momento: [2023-Mar-28 07:14:47]\n",
            "\t === ndx_batch+1: 401  self.qtd_sequencia: 349774; Momento: [2023-Mar-28 07:15:06]\n",
            "\t === ndx_batch+1: 501  self.qtd_sequencia: 438233; Momento: [2023-Mar-28 07:15:27]\n",
            "\t === ndx_batch+1: 601  self.qtd_sequencia: 520048; Momento: [2023-Mar-28 07:15:47]\n",
            "\t === ndx_batch+1: 701  self.qtd_sequencia: 607997; Momento: [2023-Mar-28 07:16:08]\n",
            "\t === ndx_batch+1: 801  self.qtd_sequencia: 690246; Momento: [2023-Mar-28 07:16:27]\n",
            "\t === ndx_batch+1: 901  self.qtd_sequencia: 773065; Momento: [2023-Mar-28 07:16:47]\n",
            "\t === ndx_batch+1: 1001  self.qtd_sequencia: 861577; Momento: [2023-Mar-28 07:17:08]\n",
            "\t === ndx_batch+1: 1101  self.qtd_sequencia: 946372; Momento: [2023-Mar-28 07:17:28]\n",
            "\t === ndx_batch+1: 1201  self.qtd_sequencia: 1036756; Momento: [2023-Mar-28 07:17:49]\n",
            "\t === ndx_batch+1: 1301  self.qtd_sequencia: 1125859; Momento: [2023-Mar-28 07:18:10]\n",
            "\t === ndx_batch+1: 1401  self.qtd_sequencia: 1211218; Momento: [2023-Mar-28 07:18:30]\n",
            "\t === ndx_batch+1: 1501  self.qtd_sequencia: 1293932; Momento: [2023-Mar-28 07:18:50]\n",
            "\t === ndx_batch+1: 1601  self.qtd_sequencia: 1382274; Momento: [2023-Mar-28 07:19:10]\n",
            "\t === ndx_batch+1: 1701  self.qtd_sequencia: 1470410; Momento: [2023-Mar-28 07:19:31]\n",
            "\t === ndx_batch+1: 1801  self.qtd_sequencia: 1558553; Momento: [2023-Mar-28 07:19:53]\n",
            "\t === ndx_batch+1: 1901  self.qtd_sequencia: 1648029; Momento: [2023-Mar-28 07:20:14]\n",
            "\t === ndx_batch+1: 2001  self.qtd_sequencia: 1736045; Momento: [2023-Mar-28 07:20:34]\n",
            "\t === ndx_batch+1: 2101  self.qtd_sequencia: 1825312; Momento: [2023-Mar-28 07:20:55]\n",
            "\t === ndx_batch+1: 2201  self.qtd_sequencia: 1913278; Momento: [2023-Mar-28 07:21:15]\n",
            "\t === ndx_batch+1: 2301  self.qtd_sequencia: 2001890; Momento: [2023-Mar-28 07:21:36]\n",
            "\t === ndx_batch+1: 2401  self.qtd_sequencia: 2083366; Momento: [2023-Mar-28 07:21:55]\n",
            "\t === ndx_batch+1: 2501  self.qtd_sequencia: 2166289; Momento: [2023-Mar-28 07:22:14]\n",
            "\t === ndx_batch+1: 2601  self.qtd_sequencia: 2253462; Momento: [2023-Mar-28 07:22:34]\n",
            "\t === ndx_batch+1: 2701  self.qtd_sequencia: 2339392; Momento: [2023-Mar-28 07:22:54]\n",
            "\t === ndx_batch+1: 2801  self.qtd_sequencia: 2422973; Momento: [2023-Mar-28 07:23:13]\n",
            "\t === ndx_batch+1: 2901  self.qtd_sequencia: 2504709; Momento: [2023-Mar-28 07:23:32]\n",
            "\t === ndx_batch+1: 3001  self.qtd_sequencia: 2587017; Momento: [2023-Mar-28 07:23:52]\n",
            "\t === ndx_batch+1: 3101  self.qtd_sequencia: 2674605; Momento: [2023-Mar-28 07:24:12]\n",
            "\t === ndx_batch+1: 3201  self.qtd_sequencia: 2763928; Momento: [2023-Mar-28 07:24:33]\n",
            "\t === ndx_batch+1: 3301  self.qtd_sequencia: 2855708; Momento: [2023-Mar-28 07:24:54]\n",
            "\t === ndx_batch+1: 3401  self.qtd_sequencia: 2937757; Momento: [2023-Mar-28 07:25:13]\n",
            "\t === ndx_batch+1: 3501  self.qtd_sequencia: 3024346; Momento: [2023-Mar-28 07:25:32]\n",
            "\t === ndx_batch+1: 3601  self.qtd_sequencia: 3117154; Momento: [2023-Mar-28 07:25:53]\n",
            "\t === ndx_batch+1: 3701  self.qtd_sequencia: 3201939; Momento: [2023-Mar-28 07:26:13]\n",
            "\t === ndx_batch+1: 3801  self.qtd_sequencia: 3293849; Momento: [2023-Mar-28 07:26:35]\n",
            "\t === ndx_batch+1: 3901  self.qtd_sequencia: 3376576; Momento: [2023-Mar-28 07:26:54]\n",
            "\t === ndx_batch+1: 4001  self.qtd_sequencia: 3466226; Momento: [2023-Mar-28 07:27:15]\n",
            "\t === ndx_batch+1: 4101  self.qtd_sequencia: 3547979; Momento: [2023-Mar-28 07:27:33]\n",
            "\t === ndx_batch+1: 4201  self.qtd_sequencia: 3634378; Momento: [2023-Mar-28 07:27:53]\n",
            "\t === ndx_batch+1: 4301  self.qtd_sequencia: 3726063; Momento: [2023-Mar-28 07:28:14]\n",
            "\t === ndx_batch+1: 4401  self.qtd_sequencia: 3806184; Momento: [2023-Mar-28 07:28:32]\n",
            "\t === ndx_batch+1: 4501  self.qtd_sequencia: 3893959; Momento: [2023-Mar-28 07:28:52]\n",
            "\t === ndx_batch+1: 4601  self.qtd_sequencia: 3985865; Momento: [2023-Mar-28 07:29:13]\n",
            "\t === ndx_batch+1: 4701  self.qtd_sequencia: 4076239; Momento: [2023-Mar-28 07:29:33]\n",
            "\t === ndx_batch+1: 4801  self.qtd_sequencia: 4164857; Momento: [2023-Mar-28 07:29:56]\n",
            "\t === ndx_batch+1: 4901  self.qtd_sequencia: 4254642; Momento: [2023-Mar-28 07:30:16]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-28 07:30:33]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-28 07:30:46]\n",
            "Carregado dataset com 4328981 sentenças\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'drive' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_467182/2291082637.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefixo_nome_diretorio\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minfixo_nome\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_sentenca_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_train.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_and_unmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mdrive.flush_and_unmount\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All changes made in this colab session should now be visible in Drive.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  assert hparam['num_sentenca_train'] <=  total_sentencas - (hparam['num_sentenca_valid'] + hparam['num_sentenca_test']), f\"Dados de treino não podem conter dados de validação/teste\"\n",
        "  train_texts = texts[:hparam['num_sentenca_train'] ]\n",
        "\n",
        "  print(\"carregando train_dataset\")\n",
        "  train_dataset = MyDataset(texts=train_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(train_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_train'])+'_train.pt')\n",
        "\n",
        "  # drive.flush_and_unmount()\n",
        "  # print('All changes made in this colab session should now be visible in Drive.')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqQfYzEBtOjS",
        "outputId": "4ed7c200-cb75-4b86-a6fb-c6765548654b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training examples: 4328981\n",
            "valid examples: 2036\n",
            "test examples: 1143\n"
          ]
        }
      ],
      "source": [
        "hparam['train_size'] = len(train_dataset) \n",
        "hparam['valid_size'] = len(valid_dataset) \n",
        "hparam['test_size'] = len(test_dataset) \n",
        "\n",
        "\n",
        "print(f\"training examples: {hparam['train_size']}\")\n",
        "print(f\"valid examples: {hparam['valid_size']}\")\n",
        "print(f\"test examples: {hparam['test_size']}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYIgLS4tOjT"
      },
      "source": [
        "max_seq_length=100, train_senteces = 800\n",
        "\n",
        "training examples: 14573\n",
        "valid examples: 2036\n",
        "test examples: 1143\n",
        "\n",
        "\n",
        "max_seq_length=100, train_senteces = 249800\n",
        "\n",
        "training examples: 4328981\n",
        "valid examples: 2036\n",
        "test examples: 1143\n",
        "\n",
        "\n",
        "max_seq_length=50, train_senteces = 249800\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm6_PTH2i98e"
      },
      "source": [
        "# Teste do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-QD1mZkMM9b",
        "outputId": "8df3c78f-a2cc-44ff-815a-17d59ebbdfc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 47.75\n",
            " used 18.4\n",
            " free 0.47\n",
            " cached 47.93\n",
            " buffers 0.55\n",
            "/nGPU\n",
            "Tue Mar 28 17:16:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "|  0%   49C    P8    38W / 370W |   2785MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "|    0   N/A  N/A    491803      C   ...vs/treinamento/bin/python     2721MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44wOmt6BqC52",
        "outputId": "a650ca6f-6639-4d54-9ef3-db18c3ac2a4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bos_token',\n",
              " 'eos_token',\n",
              " 'unk_token',\n",
              " 'sep_token',\n",
              " 'pad_token',\n",
              " 'cls_token',\n",
              " 'mask_token',\n",
              " 'additional_special_tokens']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2, None, 2048)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.pad_token_id, model.config.eos_token_id, model.config.sep_token_id, model.config.max_position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McvufVnXGlBt",
        "outputId": "56d3cc04-9ed2-4f35-afe5-2932720d5d8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_workers_dataloader': 0,\n",
              " 'device': device(type='cuda', index=0),\n",
              " 'vocab_size': 50272,\n",
              " 'num_sentenca_train': 249800,\n",
              " 'num_sentenca_valid': 100,\n",
              " 'num_sentenca_test': 100,\n",
              " 'max_seq_length': 100,\n",
              " 'train_size': 4328981,\n",
              " 'valid_size': 2036,\n",
              " 'test_size': 1143,\n",
              " 'batch_size': 2}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hparam['batch_size'] = 2\n",
        "hparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgeqpYyoHV6-",
        "outputId": "256625f3-50dd-49a5-d644-83f1de5e029a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_y.shape: torch.Size([2, 100]), sample_y: tensor([[35351,   366,  3840, 13151, 11474, 17833,  2072, 23473,   629,   281,\n",
            "           263,  8446,  1975,   493,  3381,  4214,  3137,  1021,   563,   139,\n",
            "          3840, 15551,  5873,     4,  3232,  5502,   379,  1021,   257,   475,\n",
            "          5655, 26437,  1499,  1526,  1069,   366, 40274,   424,    12,  1090,\n",
            "            10,   139,  2784,   100,   364,    10,     6,   117,   475, 17010,\n",
            "         11332,     6,  7252,  5133,  1977,    90, 25588,     6, 28312,  5563,\n",
            "           181,  4636, 37895,  1210,   271,   364, 29063,   257,   853,    10,\n",
            "         10306,  1001,  8344, 21647,    23,  6472,  5739,   109, 13736,  2527,\n",
            "           563,   139,  3840, 15551, 11474,   126,  7252,   586,   102,  3840,\n",
            "         15551, 11474,  3840, 26437,  1499,  1526,  1069,   366,   263,  1717],\n",
            "        [10969,  1916,  6757, 10071,  3381,  4214,     4,  4556,   139, 14039,\n",
            "         18416,    12,  1090,   116,  2884,   102,   283,  3381,   271,     6,\n",
            "          1521,   242,  1021,  5402,  8541,  3938,   263,  2628,   102,  6757,\n",
            "         10071,  3381,  4214,     6,  1192, 22936,  1943,  1526,    25,   856,\n",
            "          8367,   281,   263, 14237,  3900,  3381,  4214,   109, 15551,  5873,\n",
            "           364,  6821,  1526,  1021,   842,   257,  8541,  3938,  3137,  1021,\n",
            "          2784,   100,     4,   221, 22500,  7794,    10, 22706,   102,  3381,\n",
            "          4214,  3840,   740,  2095, 26012,  7450,   364,  1236, 29998,  1021,\n",
            "         18215,  5511,   139,    36, 32503,   808,  8367,   281,   295,  4214,\n",
            "           579,  4214,  1931,  1023, 19926,  3840, 26012,   428,  3985,  2841]])\n",
            "sample_x.shape: torch.Size([2, 100]), sample_x: tensor([[    2, 35351,   366,  3840, 13151, 11474, 17833,  2072, 23473,   629,\n",
            "           281,   263,  8446,  1975,   493,  3381,  4214,  3137,  1021,   563,\n",
            "           139,  3840, 15551,  5873,     4,  3232,  5502,   379,  1021,   257,\n",
            "           475,  5655, 26437,  1499,  1526,  1069,   366, 40274,   424,    12,\n",
            "          1090,    10,   139,  2784,   100,   364,    10,     6,   117,   475,\n",
            "         17010, 11332,     6,  7252,  5133,  1977,    90, 25588,     6, 28312,\n",
            "          5563,   181,  4636, 37895,  1210,   271,   364, 29063,   257,   853,\n",
            "            10, 10306,  1001,  8344, 21647,    23,  6472,  5739,   109, 13736,\n",
            "          2527,   563,   139,  3840, 15551, 11474,   126,  7252,   586,   102,\n",
            "          3840, 15551, 11474,  3840, 26437,  1499,  1526,  1069,   366,   263],\n",
            "        [ 1916, 10969,  1916,  6757, 10071,  3381,  4214,     4,  4556,   139,\n",
            "         14039, 18416,    12,  1090,   116,  2884,   102,   283,  3381,   271,\n",
            "             6,  1521,   242,  1021,  5402,  8541,  3938,   263,  2628,   102,\n",
            "          6757, 10071,  3381,  4214,     6,  1192, 22936,  1943,  1526,    25,\n",
            "           856,  8367,   281,   263, 14237,  3900,  3381,  4214,   109, 15551,\n",
            "          5873,   364,  6821,  1526,  1021,   842,   257,  8541,  3938,  3137,\n",
            "          1021,  2784,   100,     4,   221, 22500,  7794,    10, 22706,   102,\n",
            "          3381,  4214,  3840,   740,  2095, 26012,  7450,   364,  1236, 29998,\n",
            "          1021, 18215,  5511,   139,    36, 32503,   808,  8367,   281,   295,\n",
            "          4214,   579,  4214,  1931,  1023, 19926,  3840, 26012,   428,  3985]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "sample_x, sample_y = next(iter(DataLoader(valid_dataset,batch_size=hparam['batch_size'])))\n",
        "sample_x = sample_x.to(hparam['device'])\n",
        "print(f\"sample_y.shape: {sample_y.shape}, sample_y: {sample_y}\")\n",
        "print(f\"sample_x.shape: {sample_x.shape}, sample_x: {sample_x}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "0KzkMqPk8aU0"
      },
      "outputs": [],
      "source": [
        "saida = model(sample_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7ISrblKorF4",
        "outputId": "e3fbae92-10bd-4ded-c203-2f8629202503"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 50272])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saida['logits'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "J0Ct-TaTIICk"
      },
      "outputs": [],
      "source": [
        "assert saida['logits'].shape[2] == hparam['vocab_size'], \"Saída[2] deveria ser do tamanho do vocabulário do tokenizador\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "mfP4ntZRqia4"
      },
      "outputs": [],
      "source": [
        "assert saida['logits'].shape[1] == hparam['max_seq_length'], \"Saída[1] deveria ser do tamanho de max_seq_length\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ60n9LEr5jp",
        "outputId": "c98bfef7-7d90-4c73-84ec-96ab71da4099"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 50272)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.pad_token_id, model.config.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_rYB_7aI2_x"
      },
      "source": [
        "## Perplexidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "f2jgDEsvBBsB"
      },
      "outputs": [],
      "source": [
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    # muda de torch.Size([2, 100, vocabsize])\n",
        "    # para torch.Size([200, vocabsize])  \n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    # e target de torch.Size([2, seq_length])\n",
        "    # para torch.Size([2*seq_length])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "\n",
        "    # calculando perplexidade \n",
        "    return torch.exp(loss), loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl8FwXjt_esf",
        "outputId": "8a15012e-a41a-40f3-dc66-b0a8565d8fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se não tivesse pré-treino perplexity: 50272\n",
            "my perplexity:              30\n",
            "my loss:              3\n"
          ]
        }
      ],
      "source": [
        "my_perplexity, my_loss = perplexity(logits=saida['logits'], target=sample_y.to(hparam['device']), ignore_token_id=model.config.pad_token_id)\n",
        "\n",
        "\n",
        "print(f'Se não tivesse pré-treino perplexity: {model.config.vocab_size}')\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'my loss:              {int(my_loss)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "-SjDJnoOQiqD"
      },
      "outputs": [],
      "source": [
        "del saida, sample_x, sample_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiJtrsqPnE_l"
      },
      "source": [
        "# Treinamento e Validação \n",
        "\n",
        "  Masked Language Modelling (MLM trainning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12HRQXz_NHIA"
      },
      "source": [
        "## Funções auxiliares "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNBajDdAaIDh"
      },
      "source": [
        "### De geração de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "sTGkH-VHV3uv"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "DHbd9sLSURwj"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Sw9V0JU9URwk"
      },
      "outputs": [],
      "source": [
        "# Fonte da 1a versão da função abaixo foi o colega Leandro Carísio\n",
        "# aprimorada para ter os parâmetros e retornar uma tupla a ser mostrada durante o treinamento\n",
        "def continuar_frase_pipeline(frase, parm_model, max_length=120, top_p:int=1, temperature:int=0, top_k:int=50, repetition_penalty:float=1.5):\n",
        "  \"\"\"\n",
        "  temperature: controla a diversidade da amostragem estocástica, permitindo que você obtenha resultados mais criativos. Um valor mais alto resulta em textos mais imprevisíveis, enquanto um valor mais baixo resulta em textos mais conservadores.\n",
        "\n",
        "  top_k: limita o número de tokens considerados durante a amostragem estocástica, selecionando apenas os k tokens mais prováveis em cada passo. Isso pode ajudar a evitar resultados inesperados ou fora de contexto.\n",
        "\n",
        "  top_p (também conhecido como nucleus sampling): limita a soma cumulativa das probabilidades dos tokens durante a amostragem estocástica, excluindo os tokens menos prováveis ​​até que a soma atinja uma proporção pré-determinada. Isso pode ajudar a evitar resultados inesperados ou fora de contexto.\n",
        "\n",
        "  repetition_penalty: o fator de penalização a ser aplicado a palavras repetidas no texto gerado.\n",
        "  \"\"\"\n",
        "  params = {'max_length':max_length, 'top_p ': top_p, 'temperature':temperature, 'top_k':top_k,  'repetition_penalty':repetition_penalty}\n",
        "  device_generator = 0 if hparam['device'].type == 'cuda' else None\n",
        "  generator = pipeline('text-generation', model=parm_model, tokenizer=tokenizer, device=device_generator)\n",
        "  output = generator(frase, max_length=max_length, top_p = top_p, top_k=top_k, temperature=temperature,  repetition_penalty=repetition_penalty)\n",
        "  return output[0]['generated_text'], params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U5snaxJMGeO"
      },
      "source": [
        "### De treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "M3XfDRtAMUFw"
      },
      "outputs": [],
      "source": [
        "def validation_step(input, target, parm_model):\n",
        "    saida = parm_model(input)\n",
        "    # muda de torch.Size([2, 100, vocabsize])\n",
        "    # para torch.Size([200, vocabsize])    \n",
        "    logits = saida['logits'].reshape(-1, saida['logits'].shape[-1])\n",
        "    # e target de torch.Size([2, seq_length])\n",
        "    # para torch.Size([2*seq_length])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, ignore_index=parm_model.config.pad_token_id)\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uwMnuznWtiAd"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "os0oYoIScCN0"
      },
      "outputs": [],
      "source": [
        "def add_param_weight_decay(net, l2_value, skip_list=()):\n",
        "  \"\"\"\n",
        "    A weight decay penalty of 10−4 was used in the Brown experiments \n",
        "    and a weight decay of 10−5 was used in the APNews experiments  (not applied to bias)\n",
        "    fonte: https://raberrytv.wordpress.com/2017/10/29/pytorch-weight-decay-made-easy/\n",
        "  \"\"\"\n",
        "  decay, no_decay = [], []\n",
        "  for name, param in net.named_parameters():\n",
        "    # if not param.requires_grad: continue # frozen weights\t\t            \n",
        "    if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list: no_decay.append(param)\n",
        "    else: decay.append(param)\n",
        "  return [{'params': no_decay, 'weight_decay': 0.}, {'params': decay, 'weight_decay': l2_value}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "UkNIPj_LuMJO"
      },
      "outputs": [],
      "source": [
        "def treina_modelo (parm_model, parm_loader_train, parm_loader_valid, parm_loader_test, hparam:dict, parm_se_apenas_uma_validacao:bool=False, parm_se_gera_rastro:bool=True, parm_verbose:bool = True, parm_intervalo_print = 10):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  global prefixo_nome_diretorio\n",
        "  if parm_se_gera_rastro:\n",
        "    rastro_neptune = NeptuneRastroRun(hparam, parm_lista_tag= gera_tag_rastro_experiencia_treino(parm_aula='aula4', hparam=hparam) )\n",
        "  try:\n",
        "    path_modelo = f'{prefixo_nome_diretorio}best_model_len_{hparam[\"max_seq_length\"]}_inicio_treino_{time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}.pt'\n",
        "    train_losses = []\n",
        "    n_examples = 0\n",
        "    step_avaliacao = 0\n",
        "    best_perplexidade_validacao = float('inf')\n",
        "    best_step = 0\n",
        "    history = []\n",
        "    parm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_ppl = np.exp(np.average([\n",
        "            validation_step(input_tmp.to(hparam['device']), target_tmp.to(hparam['device']), parm_model)\n",
        "            for input_tmp, target_tmp in parm_loader_valid]))\n",
        "    metrica_rastro = {\"valid/perplexidade\": valid_ppl}  \n",
        "    history.append(metrica_rastro)\n",
        "    if parm_se_gera_rastro:\n",
        "      rastro_neptune.salva_metrica(metrica_rastro)\n",
        "    print(f'hparam: {hparam}')\n",
        "    print(f'Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} Métricas iniciais em validação: {metrica_rastro} Serão treinadas {hparam[\"max_examples\"]} amostras')\n",
        "\n",
        "    time_inicio_treino = time.time()\n",
        "    ultimo_step_treinado = 0\n",
        "    se_continua_execucao = True\n",
        "    parm_model.train()\n",
        "    qtd_step_sem_melhor_metrica = 0\n",
        "    while n_examples < hparam['max_examples'] and se_continua_execucao:\n",
        "        # for qtd_param_update, (input, target) in enumerate(tqdm(parm_loader_train, desc=f\"Training {100*n_examples/hparam['max_examples']:.3f}%\", leave=False)):  \n",
        "        for qtd_param_update, (input, target) in enumerate(parm_loader_train):  \n",
        "        # for qtd_param_update in range(hparam['max_examples']):  \n",
        "            # input, target = next(iter(parm_loader_train))\n",
        "            ultimo_step_treinado += 1 \n",
        "            saida = parm_model(input.to(hparam['device']))\n",
        "            # muda de torch.Size([2, 100, vocabsize])\n",
        "            # para torch.Size([200, vocabsize])    \n",
        "            logits = saida['logits'].reshape(-1, saida['logits'].shape[-1])\n",
        "            # e target de torch.Size([2, seq_length])\n",
        "            # para torch.Size([2*seq_length])\n",
        "            target = target.reshape(-1).to(hparam['device'])\n",
        "            loss = nn.functional.cross_entropy(logits, target, ignore_index=parm_model.config.pad_token_id)            \n",
        "            # ipdb.set_trace(context=4)\n",
        "            hparam['optimizer'].zero_grad()            \n",
        "            fator_corte_loss = max(hparam['fator_corte_loss_maximo'], n_examples/hparam['max_examples'])\n",
        "            loss = hparam['criterion'](logits, target) * fator_corte_loss   # ajustando para diminuir a redução na loss quando perto do fim do treino\n",
        "            loss.backward()\n",
        "            hparam['optimizer'].step()\n",
        "            hparam['scheduler'].step()  # DÚVIDA: melhor fazer por step treino ou por step validação?  Esse último não impactou mudança.              \n",
        "            loss_batch = loss.item()/fator_corte_loss # desfazendo fator_corte_loss para não refletir na perplexidade\n",
        "            train_losses.append(loss_batch) \n",
        "            n_examples += len(input)  # Increment of batch size\n",
        "\n",
        "            if ultimo_step_treinado % hparam['eval_every_steps'] == 0:\n",
        "                train_ppl = np.exp(np.average(train_losses))\n",
        "                parm_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    valid_ppl = np.exp(np.average([\n",
        "                        validation_step(input_tmp.to(hparam['device']), target_tmp.to(hparam['device']), parm_model)\n",
        "                        for input_tmp, target_tmp in parm_loader_valid]))\n",
        "\n",
        "                train_losses = []\n",
        "\n",
        "                metrica_rastro = {\"train/perplexidade\": train_ppl,\n",
        "                                  \"train/loss\": loss_batch, \n",
        "                                  \"train/n_examples\": n_examples, \n",
        "                                  \"train/learning_rate\": hparam[\"optimizer\"].param_groups[1][\"lr\"],\n",
        "                                  \"valid/perplexidade\": valid_ppl}  \n",
        "                history.append(metrica_rastro)\n",
        "                if parm_se_gera_rastro:\n",
        "                  rastro_neptune.salva_metrica(metrica_rastro)\n",
        "\n",
        "                sufixo_msg = \"\"\n",
        "                \n",
        "                # Salvando o melhor modelo de acordo com a loss de validação\n",
        "                if valid_ppl < best_perplexidade_validacao:\n",
        "                    best_model_dict = parm_model.state_dict()\n",
        "                    best_perplexidade_validacao = valid_ppl\n",
        "                    best_step = ultimo_step_treinado\n",
        "                    sufixo_msg += f\" novo best valid {valid_ppl}\"\n",
        "\n",
        "                    # salva quando encontrado best_step\n",
        "                    # se não houve melhoria em 1 step anterior\n",
        "                    if qtd_step_sem_melhor_metrica >= 1:                    \n",
        "                      torch.save(parm_model, path_modelo)    \n",
        "                      sufixo_msg += f\"; modelo salvo em {path_modelo}\"\n",
        "                      print(sufixo_msg)\n",
        "\n",
        "                    qtd_step_sem_melhor_metrica = 0\n",
        "\n",
        "                    frase_inicio = \"Praticar esportes é \"\n",
        "                    frase_final = continuar_frase_pipeline(frase_inicio, parm_model = parm_model)\n",
        "                    print(f\"Testando modelo com perplexidade menor. Frase gerada: {frase_final}\")                    \n",
        "                   \n",
        "                    # print('best model')\n",
        "                elif hparam['early_stop'] <= (ultimo_step_treinado - best_step):\n",
        "                    print(f\"Parando por critério de early_stop no step {ultimo_step_treinado} sendo best_step {best_step} e ealy_stop {hparam['early_stop']}\")\n",
        "                    se_continua_execucao = False\n",
        "                    break\n",
        "                else:\n",
        "                    qtd_step_sem_melhor_metrica +=1\n",
        "                if parm_se_apenas_uma_validacao:\n",
        "                    se_continua_execucao = False\n",
        "                    break\n",
        "                if parm_intervalo_print > 0:\n",
        "                    # if (ultimo_step_treinado)%(parm_intervalo_print*hparam['eval_every_steps']) == 0: \n",
        "                    print(f'Step: {ultimo_step_treinado} Amostras:{n_examples:d} de um total de {int(hparam[\"max_examples\"])} ({100*n_examples/hparam[\"max_examples\"]:.3f}%)')\n",
        "                    print(f'Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} lr: {hparam[\"optimizer\"].param_groups[1][\"lr\"]:.5e} Train loss: {loss_batch:.4f} perplexidade: {train_ppl:.4f} Validação perplexidade: {valid_ppl:.4f} {sufixo_msg}')\n",
        "\n",
        "                parm_model.train()\n",
        "\n",
        "\n",
        "            if n_examples >= hparam['max_examples']:              \n",
        "                break    \n",
        "            \n",
        "        \n",
        "\n",
        "\n",
        "    # calculando tempo gasto e médio por step\n",
        "    tempo_treino = time.time() - time_inicio_treino   \n",
        "    print(f\"Tempo gasto total {tempo_treino:9.5f}, steps: {ultimo_step_treinado}, tempo por step {tempo_treino/ultimo_step_treinado:9.5f}\")\n",
        "    \n",
        "    print(f'Final: Step: {ultimo_step_treinado} Amostras:{n_examples:d}  {100*n_examples/hparam[\"max_examples\"]:.3f}%  Momento: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")} lr:{hparam[\"optimizer\"].param_groups[1][\"lr\"]:.5e} Train loss: {loss_batch:.4f} Train perplexidade: {train_ppl:.4f} Validação perplexidade: {valid_ppl:.4f} ')\n",
        "\n",
        "\n",
        "    parm_model.load_state_dict(best_model_dict)\n",
        "    parm_model.to(hparam['device'])\n",
        "    torch.save(parm_model, path_modelo)    \n",
        "    print(f\"Modelo com melhor resultado em validação (step {best_step}) salvo após treino em {path_modelo}\")\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_ppl = np.exp(np.average([\n",
        "            validation_step(input_tmp.to(hparam['device']), target_tmp.to(hparam['device']), parm_model)\n",
        "            for input_tmp, target_tmp in parm_loader_test\n",
        "        ]))\n",
        "\n",
        "\n",
        "    metrica_rastro = {\"test/perplexidade\": test_ppl}      \n",
        "    print(f\" Resultado com dados de teste para modelo treinado: {metrica_rastro}\")\n",
        "    if parm_se_gera_rastro:\n",
        "      rastro_neptune.run_neptune[\"context/tempo_treino\"] = tempo_treino\n",
        "      rastro_neptune.run_neptune[\"context/tempo_treino_por_step\"] = tempo_treino/ultimo_step_treinado\n",
        "      rastro_neptune.run_neptune[\"valid/best_step\"] = best_step\n",
        "      rastro_neptune.salva_metrica(metrica_rastro)\n",
        "      #rastro_neptune.gera_grafico_modelo(parm_loader_train, parm_model)    \n",
        "\n",
        "\n",
        "    frase_inicio = \"Praticar esportes é \"\n",
        "    frase_final = continuar_frase_pipeline(frase_inicio, parm_model)\n",
        "    print(f\"Frase inicio: {frase_inicio}\")\n",
        "    print(f\"Frase final gerada: {frase_final}\")\n",
        "\n",
        "  finally:\n",
        "    if parm_se_gera_rastro:\n",
        "      rastro_neptune.stop()\n",
        "\n",
        "\n",
        "  return {\"perplexidade_test\":test_ppl, \"perplexidade_treino\":train_ppl, \"best_perplexidade_validacao\":best_perplexidade_validacao,  \"best_step\": best_step} #, \"best_model_dict\": best_model_dict}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IpynnWi-SoTi"
      },
      "source": [
        "Limpa o cache da memória da GPU\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "PZhfJhoC0Xmk"
      },
      "outputs": [],
      "source": [
        "def ajusta_parametro_grid(hparam, combinacao_parametro, model, se_treina_poucos_dados:bool=False):\n",
        "  parametro_esperado_grid = ( \"batch_size\", \n",
        "                             \"num_epochs\", \"learning_rate\", \"fator_corte_loss_maximo\", \n",
        "                             'decrease_factor_lr', 'weight_decay') #  'percent_unfreeze_embeddings') #, 'percent_unfreeze_embeddings')\n",
        "  if not model:\n",
        "    raise Exception(\"Necessário informar model!\")                            \n",
        "  for nome_parametro in parametro_esperado_grid:\n",
        "      if nome_parametro not in combinacao_parametro:\n",
        "          raise NotImplementedError(f'Gride de parâmetros está incompleto, não contem {nome_parametro}')\n",
        "      hparam[nome_parametro] = combinacao_parametro[nome_parametro]\n",
        "  for nome_parametro in combinacao_parametro:\n",
        "      if nome_parametro not in parametro_esperado_grid:\n",
        "          raise NotImplementedError(f'Gride de parâmetros está com parâmetro adicional não tratado: {nome_parametro}')\n",
        "      hparam[nome_parametro] = combinacao_parametro[nome_parametro]\n",
        "  hparam['num_workers_dataloader'] = 0\n",
        "  lambdalr = lambda qtd_param_update: 1/(1 + qtd_param_update * hparam['decrease_factor_lr'] )\n",
        "  hparam['drop_last'] = True\n",
        "  # The drop_last=True parameter ignores the last batch \n",
        "  # (when the number of examples in your dataset is not divisible by your batch_size ) \n",
        "  train_loader = DataLoader(train_dataset, batch_size=hparam['batch_size'], shuffle=True, drop_last=hparam['drop_last'], num_workers=hparam['num_workers_dataloader'])\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=hparam['batch_size'], shuffle=False, drop_last=hparam['drop_last'], num_workers=hparam[\"num_workers_dataloader\"])\n",
        "  test_loader = DataLoader(test_dataset, batch_size=hparam['batch_size'], shuffle=False, drop_last=hparam['drop_last'], num_workers=hparam['num_workers_dataloader'])\n",
        "  hparam['train_size'] = len(train_dataset) \n",
        "  hparam['valid_size'] = len(valid_dataset) \n",
        "  hparam['test_size'] = len(test_dataset) \n",
        "  if se_treina_poucos_dados:\n",
        "    train_loader = [next(iter(train_loader))] # para overfit com poucos dados (1 batch)\n",
        "    hparam['max_examples'] = hparam['num_epochs'] * hparam['batch_size']  \n",
        "    hparam['percentual_eval_every_steps'] = 0.0025\n",
        "    # a cada percentual do total\n",
        "    hparam['eval_every_steps'] = math.ceil(hparam['percentual_eval_every_steps'] * (hparam['max_examples'] / hparam['batch_size']))  \n",
        "  else:\n",
        "    hparam['max_examples'] = hparam['num_epochs'] * hparam['train_size'] \n",
        "    hparam['percentual_eval_every_steps'] = 0.0025\n",
        "    # a cada percentual do total    \n",
        "    hparam['eval_every_steps'] = int(hparam['percentual_eval_every_steps'] * (hparam['max_examples'] / hparam['batch_size']))  \n",
        "  hparam['early_stop'] = 10 * hparam['eval_every_steps']\n",
        "  hparam['criterion'] = torch.nn.CrossEntropyLoss()\n",
        "  inicializa_seed(123)\n",
        "  # model = model.float()\n",
        "  # print(f\"model.embedding_layer.weight.requires_grad: {model.embedding_layer.weight.requires_grad}\")\n",
        "\n",
        "  hparam['num_params'] = count_parameters(model)\n",
        "  print(f\"Number of model parameters: {hparam['num_params']}\")\n",
        "  # hparam['learning_rate'] =  3e-5 # 1e-3\n",
        "  # hparam['weight_decay'] = 1e-4\n",
        "  params = add_param_weight_decay(model, hparam['weight_decay'])\n",
        "  hparam['amsgrad']=False\n",
        "  hparam['optimizer'] = torch.optim.Adam(params, lr=hparam['learning_rate'], weight_decay= hparam['weight_decay'], amsgrad=hparam['amsgrad'])\n",
        "  hparam['scheduler'] = torch.optim.lr_scheduler.LambdaLR(hparam['optimizer'], lr_lambda=lambdalr, verbose=False)\n",
        "  # hparam['lista_tag_rastro_experiencia_treino'] =  gera_tag_rastro_experiencia_treino(parm_aula='aula7', hparam=hparam) \n",
        "  return hparam, model,train_loader,valid_loader,test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "-OKGe-cu9GkN"
      },
      "outputs": [],
      "source": [
        "def treina_grid(hparam, gridparam, model, parm_se_apenas_uma_validacao:bool=False, parm_se_gera_rastro:bool=True, se_treina_poucos_dados:bool=False): \n",
        "  if not model:\n",
        "    raise Exception(\"Necessário informar ou model ou classe_modelo, não os dois!\")                            \n",
        "\n",
        "  keys, values = zip(*gridparam.items())\n",
        "  lista_combinacao_grid = [dict(zip(keys, v)) for v in itertools.product(*values)]  \n",
        "  total_combinacao = len(lista_combinacao_grid)\n",
        "  print(f\"Serão {total_combinacao} experimentações\")\n",
        "  qtd_experimento = 1\n",
        "  lista_resultado = []\n",
        "  # for cnt_combinacao, combinacao in enumerate(tqdm(lista_combinacao_grid, desc=f\"Experimento {qtd_experimento}/{total_combinacao}\")):\n",
        "  for cnt_combinacao, combinacao in enumerate(lista_combinacao_grid):\n",
        "    print(f\"\\n\\nNUM: {qtd_experimento}/{total_combinacao} : {combinacao} \")\n",
        "    hparam, model, train_loader, valid_loader, test_loader = ajusta_parametro_grid(hparam, combinacao, model, se_treina_poucos_dados=se_treina_poucos_dados)\n",
        "    #ipdb.set_trace(context=4)\n",
        "    resultado = treina_modelo(model, parm_loader_train=train_loader, parm_loader_valid=valid_loader,\n",
        "                          parm_loader_test=test_loader, hparam=hparam,\n",
        "                          parm_se_apenas_uma_validacao=parm_se_apenas_uma_validacao,\n",
        "                          parm_se_gera_rastro=parm_se_gera_rastro, parm_verbose=True, parm_intervalo_print=1)\n",
        "    qtd_experimento += 1\n",
        "    print(cnt_combinacao, resultado)\n",
        "    lista_resultado.append[(cnt_combinacao, resultado)]\n",
        "  return lista_resultado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1RESg_Jb8vs"
      },
      "source": [
        "## Experimentos de treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxs2QdSY3da"
      },
      "source": [
        "### Testando em poucos dados (Overfit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Pwep987wSLIx"
      },
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[10],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[8],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }                           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5tYXy4Y8Spu",
        "outputId": "5ff661ce-ed88-4844-a351-a79a66035b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 10, 'fator_corte_loss_maximo': 1, 'batch_size': 8, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'train_size': 14573, 'valid_size': 2036, 'test_size': 1143, 'batch_size': 8, 'num_epochs': 10, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'max_examples': 80, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 1, 'early_stop': 7, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f3051da7c70>}\n",
            "Momento: [2023-Mar-28 03:49:46] Métricas iniciais em validação: {'valid/perplexidade': 38.5976437671499} Serão treinadas 80 amostras\n",
            "Step: 1 Amostras:8  10.000%  Momento: [2023-Mar-28 03:50:05] lr: 9.99999e-05 Train loss: 3.5271 perplexidade: 34.0236 Validação perplexidade: 43.4348  novo best valid 43.43478546803648\n",
            "Step: 2 Amostras:16  20.000%  Momento: [2023-Mar-28 03:50:23] lr: 9.99998e-05 Train loss: 2.7963 perplexidade: 16.3835 Validação perplexidade: 45.6706 \n",
            "Step: 3 Amostras:24  30.000%  Momento: [2023-Mar-28 03:50:42] lr: 9.99997e-05 Train loss: 2.1973 perplexidade: 9.0005 Validação perplexidade: 52.5099 \n",
            "Step: 4 Amostras:32  40.000%  Momento: [2023-Mar-28 03:51:01] lr: 9.99996e-05 Train loss: 1.7663 perplexidade: 5.8490 Validação perplexidade: 56.6818 \n",
            "Step: 5 Amostras:40  50.000%  Momento: [2023-Mar-28 03:51:21] lr: 9.99995e-05 Train loss: 1.4200 perplexidade: 4.1373 Validação perplexidade: 62.5680 \n",
            "Step: 6 Amostras:48  60.000%  Momento: [2023-Mar-28 03:51:41] lr: 9.99994e-05 Train loss: 1.1248 perplexidade: 3.0795 Validação perplexidade: 69.6467 \n",
            "Step: 7 Amostras:56  70.000%  Momento: [2023-Mar-28 03:52:00] lr: 9.99993e-05 Train loss: 0.8776 perplexidade: 2.4051 Validação perplexidade: 75.7386 \n",
            "Parando por critério de early_stop no step 8 sendo best_step 1 e ealy_stop 7\n",
            "Tempo gasto total 153.14141, steps: 8, tempo por step  19.14268\n",
            "Final: Step: 8 Amostras:64  80.000%  Momento: [2023-Mar-28 03:52:19] lr:9.99992e-05 Train loss: 0.6845 Train perplexidade: 1.9828 Validação perplexidade: 80.5822 \n",
            "Modelo com melhor resultado em validação (step 1) salvo após treino em /content/drive/My Drive/treinamento/202301_IA368DD/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 03:49:29].pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Resultado com dados de teste para modelo treinado: {'test/perplexidade': 71.7844570425778}\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ills, mas não há esporte nenhum. Por conta-se, pois não sabem por eisso. Quatro seguinte, por ei, passou por esporte nenhum. Conta-se que estava entendendo o que temos na manhã de offinga-se. Conta-se que estava, cuspindo o cor\n"
          ]
        }
      ],
      "source": [
        "treina_grid(hparam, gridparam, model, parm_se_gera_rastro = False, se_treina_poucos_dados=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCGzCsRe8Sf9",
        "outputId": "0a3a88a8-99b0-4ccb-a9a1-9cbeb73f42c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 22.53\n",
            " used 4.35\n",
            " free 10.5\n",
            " cached 12.06\n",
            " buffers 0.42\n",
            "/nGPU\n",
            "Tue Mar 28 03:52:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0    30W /  70W |   4431MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PVZTTkn7it"
      },
      "source": [
        "### Treinado por 1 época em poucos dados (mas sem overfitar)\n",
        "\n",
        "\n",
        "hparam['num_sentenca_train'] = 800"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZjdE8fHXwql",
        "outputId": "4db3fe11-6b50-43c8-d31b-bd495708de36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(hparam['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPEp1SNUX1Ft",
        "outputId": "37ad97cb-73f3-4756-8e0a-af628d5bab36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 27.33\n",
            " available 21.8\n",
            " used 5.08\n",
            " free 9.23\n",
            " cached 12.6\n",
            " buffers 0.43\n",
            "/nGPU\n",
            "Tue Mar 28 04:00:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    27W /  70W |   5263MiB / 15360MiB |     13%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinando em todos os dados\n",
        "\n",
        "\n",
        "hparam['num_sentenca_train'] = 249800"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Batch size 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAto1eZydxNq",
        "outputId": "72383039-f7f0-4059-9cb4-484444f42e94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /home/borela/fontes/relevar-busca/modelo/facebook/opt-125m/config.json\n",
            "Model config OPTConfig {\n",
            "  \"_name_or_path\": \"/home/borela/fontes/relevar-busca/modelo/facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "loading weights file /home/borela/fontes/relevar-busca/modelo/facebook/opt-125m/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "All the weights of OPTForCausalLM were initialized from the model checkpoint at /home/borela/fontes/relevar-busca/modelo/facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(NOME_CAMINHO_MODELO).to(hparam['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPj2HsMsdxNr",
        "outputId": "42e5de56-90e2-4ae2-93e9-505e2f3b9834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 45.97\n",
            " used 20.26\n",
            " free 3.45\n",
            " cached 43.22\n",
            " buffers 0.42\n",
            "/nGPU\n",
            "Tue Mar 28 08:23:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 67%   58C    P2   127W / 370W |   6283MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "|    0   N/A  N/A    467182      C   .../relevar-busca/bin/python     6219MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "bWyPky-Qd887"
      },
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[3],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[8],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_workers_dataloader': 0,\n",
              " 'device': device(type='cuda', index=0),\n",
              " 'vocab_size': 50272,\n",
              " 'num_sentenca_train': 249800,\n",
              " 'num_sentenca_valid': 100,\n",
              " 'num_sentenca_test': 100,\n",
              " 'max_seq_length': 100,\n",
              " 'batch_size': 8,\n",
              " 'num_epochs': 3,\n",
              " 'learning_rate': 0.0001,\n",
              " 'fator_corte_loss_maximo': 1,\n",
              " 'decrease_factor_lr': 1e-06,\n",
              " 'weight_decay': 0.0001,\n",
              " 'drop_last': True,\n",
              " 'train_size': 4328981,\n",
              " 'valid_size': 2036,\n",
              " 'test_size': 1143,\n",
              " 'max_examples': 12986943,\n",
              " 'percentual_eval_every_steps': 0.0025,\n",
              " 'eval_every_steps': 4058,\n",
              " 'early_stop': 40580,\n",
              " 'criterion': CrossEntropyLoss(),\n",
              " 'num_params': 125239296,\n",
              " 'amsgrad': False,\n",
              " 'optimizer': Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     initial_lr: 0.0001\n",
              "     lr: 9.995352161245022e-05\n",
              "     weight_decay: 0.0\n",
              " \n",
              " Parameter Group 1\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     initial_lr: 0.0001\n",
              "     lr: 9.995352161245022e-05\n",
              "     weight_decay: 0.0001\n",
              " ),\n",
              " 'scheduler': <torch.optim.lr_scheduler.LambdaLR at 0x7fc5f14cb0d0>}"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ['NEPTUNE_ALLOW_SELF_SIGNED_CERTIFICATE'] = 'TRUE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rayM8tB9d888",
        "outputId": "f510b233-cdf8-4ae3-f0e9-4b0649f9fa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 3, 'fator_corte_loss_maximo': 1, 'batch_size': 8, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/ipykernel_launcher.py:54: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'tuple'>).\n",
            "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
            "        for dictionaries or collections that contain unsupported values.\n",
            "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
            "/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/ipykernel_launcher.py:54: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
            "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
            "        for dictionaries or collections that contain unsupported values.\n",
            "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 8, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 4058, 'early_stop': 40580, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc5b7e27210>}\n",
            "Momento: [2023-Mar-28 08:24:02] Métricas iniciais em validação: {'valid/perplexidade': 38.597947302471454} Serão treinadas 12986943 amostras\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal para cantecer os outros. Na terça-feira, as diferentes, além da grande câmara da Rua Grande, para o espanhol, devem fazer todos os carros a partir da hora do lançamento.\n",
            "\n",
            "Step: 4058 Amostras:32464 de um total de 12986943.000 (0.250%)\n",
            "Momento: [2023-Mar-28 08:42:34] lr: 9.95958e-05 Train loss: 3.3789 perplexidade: 31.0682 Validação perplexidade: 29.5213  novo best valid 29.521281106490353\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal! Mas o time da torcida não saiu mais pouco dessa atração mais alto. Tbm, na campanha de campo, quatro treinadores fofaram o gol por lá em qualquer lugar, não têm tudo na torcida. Uma coisa é o uso do time para a torcida (pela\n",
            "Step: 8116 Amostras:64928 de um total de 12986943.000 (0.500%)\n",
            "Momento: [2023-Mar-28 09:00:59] lr: 9.91949e-05 Train loss: 3.4555 perplexidade: 26.0827 Validação perplexidade: 27.2876  novo best valid 27.287553370398204\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é Ânimas e, só podemos fazer esse trabalho. De modo que nenhum país não tem vários exemplos, não deveria ter um dia mais no Brasil, por isso, então quem a vinculava estava lá e agora o galo vai mesmo mais de 15 minutos depois que\n",
            "Step: 12174 Amostras:97392 de um total de 12986943.000 (0.750%)\n",
            "Momento: [2023-Mar-28 09:19:25] lr: 9.87972e-05 Train loss: 3.4262 perplexidade: 24.4423 Validação perplexidade: 26.0850  novo best valid 26.084954714522095\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é étnicos desde que cabeçados do putebol mundial. Um dos mais frutos do jogo, o Sport atualiza campeonato no próximo dia 5, o Gráfico acaba sendo eliminado. Tanto no futebol, mas no empate das competições, o clássico não deve ser um\n",
            "Step: 16232 Amostras:129856 de um total de 12986943.000 (1.000%)\n",
            "Momento: [2023-Mar-28 09:37:52] lr: 9.84027e-05 Train loss: 3.1712 perplexidade: 23.6379 Validação perplexidade: 25.3287  novo best valid 25.32865067832858\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ética: a maioria das vezes ainda acontece sem câmeras como a da câmera. Os veículos são caracterizados por alto número de pontos de válvulas sistêmicas, como a percussão de um motor, a fazê-los e a altura do dia a dia, ent\n",
            "Step: 20290 Amostras:162320 de um total de 12986943.000 (1.250%)\n",
            "Momento: [2023-Mar-28 09:56:14] lr: 9.80113e-05 Train loss: 2.9209 perplexidade: 23.0882 Validação perplexidade: 24.8664  novo best valid 24.86643511492557\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ético com os personagens de suas equações e tecnologia, mas isso não é o único esporte brasileiro com os técnicos naturais, mas essas criaturas desafiosas também são consideradas por atletas pouco selecionados por esta temporada - e que em f\n",
            "Step: 24348 Amostras:194784 de um total de 12986943.000 (1.500%)\n",
            "Momento: [2023-Mar-28 10:14:38] lr: 9.76231e-05 Train loss: 3.0265 perplexidade: 22.5698 Validação perplexidade: 24.3971  novo best valid 24.39712410506446\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ética para esportes deveria ser adotado antes dos alunos, principalmente para quem sabe máxima de gostar da história do torcedor de formação, fazendo com que o jogador precisa fazer todo o jogador em fazendas da vida do jogador do adversário está certo em ficção,\n",
            "Step: 28406 Amostras:227248 de um total de 12986943.000 (1.750%)\n",
            "Momento: [2023-Mar-28 10:33:04] lr: 9.72379e-05 Train loss: 3.0327 perplexidade: 22.2165 Validação perplexidade: 24.1975  novo best valid 24.1975078084483\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ingerir em geral ou jogar em casa quando chegar atrás de uma página de TV e jogar em um lugar que estava líquido em campo com ele? É possível vermos que o seu \"homem é uma opção, por exemplo.\" Esse é o caso de que o Brasil poderá ter que até ir\n",
            "Step: 32464 Amostras:259712 de um total de 12986943.000 (2.000%)\n",
            "Momento: [2023-Mar-28 10:51:27] lr: 9.68557e-05 Train loss: 2.9620 perplexidade: 21.7637 Validação perplexidade: 23.8227  novo best valid 23.82272204787049\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ________!!!! Entendem que existe razão econômica em apaixonar um povo de algo desafioso mesmo tive de ser povo. Com muita ciência, as passearam pela sucessão, pelo povo, pelo povo para ver o Estado ser um povo democrático. Então, também fo\n",
            "Step: 36522 Amostras:292176 de um total de 12986943.000 (2.250%)\n",
            "Momento: [2023-Mar-28 11:09:52] lr: 9.64765e-05 Train loss: 3.1157 perplexidade: 21.7368 Validação perplexidade: 23.4503  novo best valid 23.450251486964806\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal que não há no mais? Na época, o peritano não estava mau, mas hoje, está muito bom\". Na avaliação feita para a categoria, o ano é marcado por decisões do torneio regional de futebol, que já acontece com a própria equipe. Tudo most\n",
            "Step: 40580 Amostras:324640 de um total de 12986943.000 (2.500%)\n",
            "Momento: [2023-Mar-28 11:28:17] lr: 9.61003e-05 Train loss: 2.9930 perplexidade: 21.3568 Validação perplexidade: 23.3368  novo best valid 23.336786206270347\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é irmão de uma família pra código de vida, só fica não! Onde então quem quiser... não é um público onde tivemos a relação com aquele que faz e não dá. Mas se achar que esse país ainda não temos dificuldades para fazer… Como\n",
            "Step: 44638 Amostras:357104 de um total de 12986943.000 (2.750%)\n",
            "Momento: [2023-Mar-28 11:46:42] lr: 9.57269e-05 Train loss: 3.0270 perplexidade: 21.1319 Validação perplexidade: 23.0003  novo best valid 23.00030853837098\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ético em relação às suas competências – com ajuda de alunos educadores, ajuda a algumas pessoas a compartilhar a aparência da cintura e da calor –, por sua vez, acompanhar a avaliação do seu potencial de um conjunto que se vai\n",
            "Step: 48696 Amostras:389568 de um total de 12986943.000 (3.000%)\n",
            "Momento: [2023-Mar-28 12:05:10] lr: 9.53565e-05 Train loss: 3.3240 perplexidade: 20.9523 Validação perplexidade: 22.7990  novo best valid 22.798993973551696\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ética, não é é suficiente. Já não temos tempo para lutar, e é uma luz muito estética, mas é muito importante. São pessoas que atravessa o fotografo quando há colegas, mas são pessoas que vão atuar por fotografar, com as\n",
            "Step: 52754 Amostras:422032 de um total de 12986943.000 (3.250%)\n",
            "Momento: [2023-Mar-28 12:23:35] lr: 9.49890e-05 Train loss: 2.9204 perplexidade: 20.7871 Validação perplexidade: 22.5772  novo best valid 22.577157502248223\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegalidade. E pode ter maior motivo no vôlei que for de Fuso, ter uma das faltas mais importantes do próximo. Fica com o resultado que as faltas mais simpáticas são as equipes classificadas de estresse, mas há alguma melhor alternativa para o torneiro.\n",
            "Step: 56812 Amostras:454496 de um total de 12986943.000 (3.500%)\n",
            "Momento: [2023-Mar-28 12:41:59] lr: 9.46242e-05 Train loss: 2.8195 perplexidade: 20.5827 Validação perplexidade: 22.4371  novo best valid 22.43710419279474\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal apenas com o calendário de nascimento. A criação de casos como as amostras de vinil é uma série de carregamentos próximos ao cair de uma chuva mais cura. E todo o conteúdo, como se poderia provar, parecia simples, mas um\n",
            "Step: 60870 Amostras:486960 de um total de 12986943.000 (3.750%)\n",
            "Momento: [2023-Mar-28 13:00:42] lr: 9.42623e-05 Train loss: 2.9319 perplexidade: 20.4692 Validação perplexidade: 22.3347  novo best valid 22.334745917716276\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "All 0 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-22/metadata\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_467182/2716319437.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtreina_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparm_se_gera_rastro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_treina_poucos_dados\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mtreina_grid\u001b[0m \u001b[0;34m= <function treina_grid at 0x7fc591a755f0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mhparam\u001b[0m \u001b[0;34m= {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 8, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 4058, 'early_stop': 40580, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.417623575702154e-05\n    weight_decay: 0.0\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.417623575702154e-05\n    weight_decay: 0.0001\n), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc5b7e27210>}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mgridparam\u001b[0m \u001b[0;34m= {'learning_rate': [0.0001], 'num_epochs': [3], 'fator_corte_loss_maximo': [1], 'batch_size': [8], 'decrease_factor_lr': [1e-06], 'weight_decay': [0.0001]}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel\u001b[0m \u001b[0;34m= OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_se_gera_rastro\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mse_treina_poucos_dados\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_467182/2743148588.py\u001b[0m in \u001b[0;36mtreina_grid\u001b[0;34m(hparam={'amsgrad': False, 'batch_size': 8, 'criterion': CrossEntropyLoss(), 'decrease_factor_lr': 1e-06, 'device': device(type='cuda', index=0), 'drop_last': True, 'early_stop': 40580, 'eval_every_steps': 4058, 'fator_corte_loss_maximo': 1, 'learning_rate': 0.0001, ...}, gridparam={'batch_size': [8], 'decrease_factor_lr': [1e-06], 'fator_corte_loss_maximo': [1], 'learning_rate': [0.0001], 'num_epochs': [3], 'weight_decay': [0.0001]}, model=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), parm_se_apenas_uma_validacao=False, parm_se_gera_rastro=True, se_treina_poucos_dados=False)\u001b[0m\n\u001b[1;32m     16\u001b[0m                           \u001b[0mparm_loader_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                           \u001b[0mparm_se_apenas_uma_validacao\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparm_se_apenas_uma_validacao\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                           parm_se_gera_rastro=parm_se_gera_rastro, parm_verbose=True, parm_intervalo_print=1)\n\u001b[0m        \u001b[0;36mparm_se_gera_rastro\u001b[0m \u001b[0;34m= True\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_verbose\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_intervalo_print\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mqtd_experimento\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_467182/956702019.py\u001b[0m in \u001b[0;36mtreina_modelo\u001b[0;34m(parm_model=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), parm_loader_train=<torch.utils.data.dataloader.DataLoader object>, parm_loader_valid=<torch.utils.data.dataloader.DataLoader object>, parm_loader_test=<torch.utils.data.dataloader.DataLoader object>, hparam={'amsgrad': False, 'batch_size': 8, 'criterion': CrossEntropyLoss(), 'decrease_factor_lr': 1e-06, 'device': device(type='cuda', index=0), 'drop_last': True, 'early_stop': 40580, 'eval_every_steps': 4058, 'fator_corte_loss_maximo': 1, 'learning_rate': 0.0001, ...}, parm_se_apenas_uma_validacao=False, parm_se_gera_rastro=True, parm_verbose=True, parm_intervalo_print=1)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# input, target = next(iter(parm_loader_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0multimo_step_treinado\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0msaida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36msaida\u001b[0m \u001b[0;34m= CausalLMOutputWithPast(loss=None, logits=tensor([[[-7.0608e-01, -7.0608e-01, 4.7824e+00,  ..., -7.0608e-01, -7.0608e-01, -7.0608e-01],\n         [-1.2103e+00, -1.2103e+00, 1.0985e-01,  ..., -1.2103e+00, -1.2103e+00, -1.2103e+00],\n         [-1.2950e+00, -1.2950e+00, 3.0314e+00,  ..., -1.2950e+00, -1.2950e+00, -1.2950e+00],\n         ...,\n         [-1.0290e+00, -1.0290e+00, 1.3842e+00,  ..., -1.0290e+00, -1.0290e+00, -1.0290e+00],\n         [-1.1941e+00, -1.1941e+00, -2.6586e+00,  ..., -1.1941e+00, -1.1941e+00, -1.1941e+00],\n         [-1.2801e+00, -1.2801e+00, -1.6287e+00,  ..., -1.2801e+00, -1.2801e+00, -1.2801e+00]],\n\n        [[-7.6565e-01, -7.6565e-01, 5.1192e+00,  ..., -7.6565e-01, -7.6565e-01, -7.6565e-01],\n         [-1.5436e+00, -1.5436e+00, -2.2337e+00,  ..., -1.5436e+00, -1.5436e+00, -1.5436e+00],\n         [-1.1917e+00, -1.1917e+00, -2.1827e+00,  ..., -1.1917e+00, -1.1917e+00, -1.1917e+00],\n         ...,\n         [-3.7363e-01, -3.7363e-01, 2.1150e+00,  ..., -3.7363e-01, -3.7363e-01, -3.7363e-01],\n         [-7.2628e-01, -7.2628e-01, 2.3529e+00,  ..., -7.2628e-01, -7.2628e-01, -7.2628e-01],\n         [-7.6331e-01, -7.6331e-01, 1.9496e+00,  ..., -7.6331e-01, -7.6331e-01, -7.6331e-01]],\n\n        [[-1.0333e+00, -1.0333e+00, 6.0654e+00,  ..., -1.0333e+00, -1.0333e+00, -1.0333e+00],\n         [-1.3155e+00, -1.3155e+00, 1.8820e+00,  ..., -1.3155e+00, -1.3155e+00, -1.3155e+00],\n         [-7.5010e-01, -7.5010e-01, 3.7633e-01,  ..., -7.5010e-01, -7.5010e-01, -7.5010e-01],\n         ...,\n         [-1.3440e+00, -1.3440e+00, 2.7969e-01,  ..., -1.3440e+00, -1.3440e+00, -1.3440e+00],\n         [-5.4762e-01, -5.4762e-01, 4.6957e-01,  ..., -5.4762e-01, -5.4762e-01, -5.4762e-01],\n         [-5.2106e-01, -5.2106e-01, 2.1507e+00,  ..., -5.2106e-01, -5.2106e-01, -5.2106e-01]],\n\n        ...,\n\n        [[-8.7288e-01, -8.7288e-01, 6.6141e+00,  ..., -8.7288e-01, -8.7288e-01, -8.7288e-01],\n         [-6.1964e-01, -6.1964e-01, 2.8585e+00,  ..., -6.1964e-01, -6.1964e-01, -6.1964e-01],\n         [-1.4803e+00, -1.4803e+00, 1.4309e+00,  ..., -1.4803e+00, -1.4803e+00, -1.4803e+00],\n         ...,\n         [-7.8691e-01, -7.8691e-01, 5.4914e-01,  ..., -7.8691e-01, -7.8691e-01, -7.8691e-01],\n         [-5.6097e-01, -5.6097e-01, 1.2765e+00,  ..., -5.6097e-01, -5.6097e-01, -5.6097e-01],\n         [-4.1946e-01, -4.1946e-01, 1.9005e+00,  ..., -4.1946e-01, -4.1946e-01, -4.1946e-01]],\n\n        [[-1.0547e+00, -1.0547e+00, 7.3389e+00,  ..., -1.0547e+00, -1.0547e+00, -1.0547e+00],\n         [-7.6265e-01, -7.6265e-01, 1.1648e+00,  ..., -7.6265e-01, -7.6265e-01, -7.6265e-01],\n         [-1.5372e+00, -1.5372e+00, 1.3710e+00,  ..., -1.5372e+00, -1.5372e+00, -1.5372e+00],\n         ...,\n         [-3.6936e-01, -3.6936e-01, -8.7053e-01,  ..., -3.6936e-01, -3.6936e-01, -3.6936e-01],\n         [-9.6853e-01, -9.6853e-01, 4.7362e-01,  ..., -9.6853e-01, -9.6853e-01, -9.6853e-01],\n         [-1.5109e+00, -1.5109e+00, 2.4677e-01,  ..., -1.5109e+00, -1.5109e+00, -1.5109e+00]],\n\n        [[-1.0679e+00, -1.0679e+00, 5.6145e+00,  ..., -1.0679e+00, -1.0679e+00, -1.0679e+00],\n         [-6.4194e-01, -6.4194e-01, 2.3226e+00,  ..., -6.4194e-01, -6.4194e-01, -6.4194e-01],\n         [-5.2669e-01, -5.2669e-01, 2.7015e+00,  ..., -5.2669e-01, -5.2669e-01, -5.2669e-01],\n         ...,\n         [-6.0808e-01, -6.0808e-01, 1.9227e+00,  ..., -6.0808e-01, -6.0808e-01, -6.0808e-01],\n         [-9.4951e-01, -9.4951e-01, 6.7377e-01,  ..., -9.4951e-01, -9.4951e-01, -9.4951e-01],\n         [-4.0772e-01, -4.0772e-01, 5.5599e-01,  ..., -4.0772e-01, -4.0772e-01, -4.0772e-01]]],\n       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-1.1077e-01,  4.4679e-02, -4.3729e-01,  ..., -2.9026e-02,\n            5.0028e-02, -2.3363e-01],\n          [ 1.0634e-01,  3.1913e-01,  6.1606e-02,  ...,  1.3365e-01,\n            8.8287e-02,  3.5002e-01],\n          [ 5.6920e-01,  6.7280e-01, -2.0207e-01,  ..., -2.4644e-01,\n           -9.7644e-04,  5.8110e-01],\n          ...,\n          [ 5.8162e-01, -3.5591e-02,  1.1925e+00,  ..., -7.0258e-01,\n            4.4694e-02,  1.2763e+00],\n          [-3.7696e-01, -5.8708e-02,  7.7259e-01,  ...,  1.4867e-01,\n           -8.9071e-03, -4.0889e-02],\n          [-4.8397e-01,  3.3452e-01,  1.4432e+00,  ...,  6.1309e-01,\n            1.7408e-01,  4.3058e-02]],\n\n         [[-3.2242e-01, -1.9511e-02,  2.5050e-01,  ...,  2.8960e-01,\n           -4.3686e-01, -9.0104e-02],\n          [-2.7875e-01, -1.5179e-01, -1.1016e-01,  ..., -1.0981e-01,\n            5.6770e-02,  1.9821e-01],\n          [-1.9721e-01, -5.6815e-02, -6.2016e-02,  ..., -5.7042e-02,\n           -3.8568e-02,  1.8851e-01],\n          ...,\n          [ 1.1715e+00,  8.4175e-01, -3.9740e-01,  ..., -4.4310e-01,\n            3.8855e-01,  1.0478e-01],\n          [ 1.1047e-01,  4.5577e-02, -7.2761e-02,  ..., -5.9751e-02,\n           -7.9142e-02,  7.1446e-02],\n          [ 3.8700e-01,  2.2083e-01, -1.6022e-01,  ..., -1.6367e-01,\n            3.7830e-02,  1.3692e-01]],\n\n         [[-2.5236e-03, -7.5596e-02, -1.8949e-01,  ..., -1.3994e-01,\n           -9.6206e-02, -9.5201e-02],\n          [ 3.9623e-02,  1.5275e-01, -1.4670e-01,  ..., -1.0440e-01,\n           -9.9692e-02, -9.4111e-02],\n          [ 6.5354e-02,  1.5510e-01, -1.0256e-01,  ..., -8.2511e-02,\n           -9.5895e-02, -9.6462e-02],\n          ...,\n          [ 1.1054e-01,  1.5605e-01, -4.9903e-02,  ..., -3.8112e-02,\n           -9.5063e-02, -9.5988e-02],\n          [ 6.9731e-02,  7.1657e-02, -9.5053e-02,  ..., -7.7475e-02,\n           -9.7220e-02, -9.4126e-02],\n          [ 8.5510e-02,  1.6097e-01, -1.1215e-01,  ..., -6.0238e-02,\n           -9.7587e-02, -9.3738e-02]],\n\n         ...,\n\n         [[-1.1283e-01,  9.3038e-02, -1.0065e-01,  ...,  2.7576e-01,\n            1.6104e-02, -1.0911e-01],\n          [-5.6875e-01,  8.5421e-02,  2.6449e-01,  ...,  1.1069e-01,\n            4.0395e-02,  2.9411e-01],\n          [-4.4992e-01,  8.5120e-02,  1.2231e-01,  ...,  2.2003e-01,\n            4.2014e-02,  2.0307e-01],\n          ...,\n          [ 9.6776e-01,  8.1695e-02, -9.1967e-02,  ..., -1.6724e-01,\n            5.4214e-02, -2.6720e-01],\n          [ 3.3503e-01,  1.0637e-01,  1.2567e-01,  ..., -9.9932e-02,\n            3.8951e-02, -2.4462e-02],\n          [ 5.3365e-01,  1.0193e-01,  7.0972e-03,  ..., -1.6969e-02,\n            2.6716e-02, -1.7716e-01]],\n\n         [[ 1.1111e-01,  1.2582e-01,  4.3105e-02,  ...,  2.5020e-03,\n            1.0049e-01,  7.0532e-02],\n          [ 6.0015e-02,  6.3753e-02,  3.1107e-02,  ...,  2.5610e-03,\n            5.6932e-02,  4.1793e-02],\n          [ 4.4380e-02,  4.0111e-02,  3.8027e-02,  ...,  2.5243e-03,\n            4.6685e-02,  4.1123e-02],\n          ...,\n          [-2.0106e-01, -2.3295e-01, -9.3967e-02,  ...,  2.5613e-03,\n           -1.7946e-01, -1.4446e-01],\n          [-9.9054e-02, -1.1491e-01, -4.9531e-02,  ...,  2.5623e-03,\n           -8.8486e-02, -7.5070e-02],\n          [-9.3394e-02, -1.1051e-01, -4.0387e-02,  ...,  2.5442e-03,\n           -8.1976e-02, -6.6981e-02]],\n\n         [[-3.1494e-03, -8.9097e-03,  5.7380e-03,  ..., -7.8918e-04,\n           -2.6602e-03, -3.0400e-03],\n          [-3.3491e-03, -7.1675e-03,  3.6413e-03,  ..., -4.6086e-03,\n            1.2551e-03, -8.1986e-03],\n          [-4.7364e-03, -7.2898e-03,  4.0226e-03,  ...,  9.0114e-04,\n           -4.3177e-03, -3.4579e-03],\n          ...,\n          [-1.9323e-03, -6.7007e-03,  4.6892e-03,  ..., -5.3504e-03,\n            2.0028e-03, -8.9545e-03],\n          [-2.3659e-03, -7.4121e-03,  4.6034e-03,  ..., -5.4061e-03,\n            2.0331e-03, -7.6230e-03],\n          [-1.4423e-03, -8.1993e-03,  5.6258e-03,  ..., -5.8459e-03,\n            2.5181e-03, -1.0552e-02]]],\n\n\n        [[[ 1.1091e-01, -3.8038e-01, -2.9492e-02,  ...,  1.3806e-02,\n            1.7791e-01, -9.8286e-02],\n          [-1.5772e-03,  1.2380e-01,  7.1917e-02,  ...,  1.0771e-01,\n            1.2862e-01,  2.1104e-01],\n          [ 6.1405e-01,  4.8482e-01, -1.6800e-01,  ..., -1.7003e-01,\n            4.8944e-02,  6.4986e-01],\n          ...,\n          [ 5.1775e-02,  2.1604e-01,  3.5210e-01,  ..., -3.7935e-01,\n           -2.0209e-02,  4.9406e-01],\n          [-2.8258e-01,  4.0804e-01,  6.9488e-01,  ...,  7.1289e-02,\n           -2.0270e-01,  3.9324e-02],\n          [-4.6592e-01,  3.3148e-01,  1.3750e+00,  ...,  6.5626e-01,\n            8.6987e-02,  6.2123e-02]],\n\n         [[ 3.3301e-01,  5.4262e-01,  2.7953e-01,  ...,  2.4790e-01,\n           -2.7584e-01, -2.7330e-01],\n          [-2.4437e-01, -6.4792e-02, -3.7007e-02,  ..., -3.3605e-02,\n           -9.2591e-03,  1.1704e-01],\n          [-3.2306e-01, -1.9076e-01, -9.8882e-02,  ..., -9.4829e-02,\n            1.5480e-04,  2.0895e-01],\n          ...,\n          [ 3.0824e-01,  1.7564e-01, -2.2780e-01,  ..., -2.2023e-01,\n            8.8589e-02,  1.6998e-01],\n          [ 3.2847e-02, -8.2305e-02, -1.1064e-01,  ..., -8.3630e-02,\n           -9.6106e-02,  1.7355e-01],\n          [-7.8682e-02, -9.6385e-02, -9.3618e-02,  ..., -5.1419e-02,\n           -1.3283e-01,  1.6117e-01]],\n\n         [[-6.9423e-02, -1.8103e-01, -2.9065e-01,  ..., -1.9507e-01,\n           -9.4539e-02, -9.5154e-02],\n          [ 1.0761e-02,  9.8333e-02, -1.7421e-01,  ..., -1.3232e-01,\n           -9.8259e-02, -9.4280e-02],\n          [ 6.0067e-02,  1.6306e-01, -1.0933e-01,  ..., -8.7281e-02,\n           -9.8864e-02, -9.5476e-02],\n          ...,\n          [ 9.7283e-02,  1.1986e-01, -5.9733e-02,  ..., -5.2182e-02,\n           -9.6758e-02, -9.5183e-02],\n          [ 1.1141e-01,  1.2925e-01, -4.0737e-02,  ..., -4.0773e-02,\n           -9.5189e-02, -9.5557e-02],\n          [ 1.3730e-01,  2.0680e-01, -3.9643e-02,  ..., -1.4024e-02,\n           -9.9100e-02, -9.3496e-02]],\n\n         ...,\n\n         [[ 2.7258e-01,  8.3781e-02, -2.4668e-01,  ...,  2.4538e-01,\n            4.3394e-02, -2.8617e-01],\n          [-3.2554e-01,  1.0665e-01,  2.3964e-01,  ...,  4.3065e-02,\n            4.4802e-02,  2.1558e-01],\n          [-6.1155e-01,  7.9995e-02,  2.6062e-01,  ...,  1.5353e-01,\n            7.0486e-02,  3.5312e-01],\n          ...,\n          [ 3.5940e-01,  7.8656e-02,  8.4890e-02,  ..., -8.6270e-02,\n            3.8027e-02, -3.2073e-02],\n          [ 1.4940e-01,  8.8723e-02,  3.5643e-02,  ...,  3.8819e-02,\n           -4.7992e-03, -6.0832e-02],\n          [ 6.4978e-02,  9.3092e-02,  1.6999e-01,  ...,  1.7169e-02,\n            2.8527e-02,  4.5232e-02]],\n\n         [[ 2.4820e-01,  2.8445e-01,  9.6936e-02,  ...,  2.4567e-03,\n            2.2244e-01,  1.6162e-01],\n          [ 8.0505e-02,  8.7949e-02,  3.6009e-02,  ...,  2.5307e-03,\n            7.4777e-02,  5.4078e-02],\n          [ 3.7253e-02,  3.3646e-02,  3.1100e-02,  ...,  2.5680e-03,\n            3.9165e-02,  3.3210e-02],\n          ...,\n          [-1.6228e-01, -1.8862e-01, -7.3577e-02,  ...,  2.5960e-03,\n           -1.4431e-01, -1.1620e-01],\n          [-1.6891e-01, -1.9814e-01, -6.8308e-02,  ...,  2.5790e-03,\n           -1.4895e-01, -1.1641e-01],\n          [-1.5994e-01, -1.8830e-01, -6.0649e-02,  ...,  2.6058e-03,\n           -1.4045e-01, -1.0857e-01]],\n\n         [[-2.7461e-03, -9.1827e-03,  6.2119e-03,  ..., -2.8631e-03,\n           -5.2167e-04, -7.3477e-03],\n          [-3.9333e-03, -6.7686e-03,  3.4402e-03,  ..., -1.9294e-03,\n           -1.4671e-03, -5.2958e-03],\n          [-3.7553e-03, -7.3539e-03,  3.5834e-03,  ..., -3.8263e-03,\n            4.5759e-04, -6.7202e-03],\n          ...,\n          [-1.5407e-03, -7.0746e-03,  4.3224e-03,  ..., -8.6611e-03,\n            5.3244e-03, -1.0030e-02],\n          [-2.5662e-03, -8.1362e-03,  5.8112e-03,  ..., -4.1110e-03,\n            7.4147e-04, -8.0974e-03],\n          [-2.1703e-03, -6.4450e-03,  3.1366e-03,  ..., -9.0064e-03,\n            5.6560e-03, -9.3232e-03]]],\n\n\n        [[[-8.7064e-02, -1.7067e-01, -3.8286e-01,  ..., -1.2505e-02,\n            1.1458e-01, -2.9816e-01],\n          [ 2.1827e-02,  4.9088e-01, -5.6248e-02,  ...,  6.7847e-02,\n            2.6352e-02,  2.3539e-01],\n          [ 5.2767e-01,  4.9262e-01, -1.5175e-01,  ..., -1.7723e-01,\n            3.2259e-02,  5.8013e-01],\n          ...,\n          [ 8.5408e-02,  3.2372e-01,  1.7925e-01,  ..., -4.5386e-01,\n           -2.4531e-02,  5.1746e-01],\n          [-3.2621e-01,  1.2792e-01,  9.4934e-01,  ...,  1.8520e-01,\n           -1.1006e-01,  7.6841e-02],\n          [-4.8827e-01,  5.1933e-01,  1.1312e+00,  ...,  4.9464e-01,\n            6.6332e-02,  2.7169e-02]],\n\n         [[ 2.2478e-03,  2.0370e-01,  2.5344e-01,  ...,  2.5479e-01,\n           -3.4761e-01, -1.6246e-01],\n          [-4.7853e-01, -2.5324e-01, -5.6699e-02,  ..., -1.5449e-02,\n           -7.8395e-02,  2.3480e-01],\n          [-2.8486e-01, -1.0475e-01, -5.8251e-02,  ..., -5.2546e-02,\n           -4.1121e-02,  1.6672e-01],\n          ...,\n          [ 1.1606e-01, -2.7730e-04, -2.2784e-01,  ..., -2.0289e-01,\n            2.9905e-02,  2.0568e-01],\n          [ 1.0552e-01, -5.7836e-02, -1.5846e-01,  ..., -1.4297e-01,\n           -3.9298e-02,  1.5427e-01],\n          [ 1.4268e-01,  4.1251e-02, -1.4706e-01,  ..., -1.3453e-01,\n           -3.6401e-02,  1.6493e-01]],\n\n         [[-5.6973e-02, -1.6109e-01, -2.6872e-01,  ..., -1.8543e-01,\n           -9.6228e-02, -9.4697e-02],\n          [ 5.2361e-02,  1.6721e-01, -1.1617e-01,  ..., -9.7052e-02,\n           -9.7715e-02, -9.5351e-02],\n          [ 3.6756e-02,  1.2517e-01, -1.3067e-01,  ..., -1.1073e-01,\n           -9.7057e-02, -9.5760e-02],\n          ...,\n          [ 1.2142e-01,  1.6126e-01, -2.9184e-02,  ..., -3.0244e-02,\n           -9.8916e-02, -9.4450e-02],\n          [ 1.2941e-01,  1.4585e-01, -2.8318e-02,  ..., -2.1457e-02,\n           -9.7240e-02, -9.4428e-02],\n          [ 9.1425e-02,  1.4742e-01, -9.3244e-02,  ..., -5.6807e-02,\n           -9.6216e-02, -9.4482e-02]],\n\n         ...,\n\n         [[ 1.6942e-01,  8.8850e-02, -1.5762e-01,  ...,  1.9928e-01,\n            4.2234e-02, -1.9715e-01],\n          [-6.9022e-01,  9.5102e-02,  2.1788e-01,  ...,  2.0513e-01,\n            9.2627e-03,  2.6511e-01],\n          [-4.1558e-01,  9.4697e-02,  2.2893e-01,  ...,  1.2490e-01,\n            4.8881e-02,  2.7341e-01],\n          ...,\n          [ 2.0214e-01,  8.3157e-02,  1.3368e-01,  ..., -5.2473e-02,\n            4.4889e-02,  4.8270e-02],\n          [ 1.2761e-01,  7.5998e-02,  1.3995e-01,  ..., -4.2446e-02,\n            2.7924e-02,  3.7293e-02],\n          [ 2.5944e-01,  1.0404e-01,  2.2965e-03,  ...,  7.0697e-02,\n            3.1924e-03, -1.2041e-01]],\n\n         [[ 1.9527e-01,  2.2410e-01,  7.4310e-02,  ...,  2.4943e-03,\n            1.7476e-01,  1.2490e-01],\n          [ 1.6375e-02,  1.2680e-02,  9.9887e-03,  ...,  2.5512e-03,\n            1.8283e-02,  1.2360e-02],\n          [ 3.7897e-02,  3.4103e-02,  3.3256e-02,  ...,  2.5221e-03,\n            3.9978e-02,  3.4479e-02],\n          ...,\n          [-1.9882e-01, -2.3014e-01, -9.1971e-02,  ...,  2.6174e-03,\n           -1.7751e-01, -1.4283e-01],\n          [-1.8434e-01, -2.1266e-01, -8.4970e-02,  ...,  2.6213e-03,\n           -1.6501e-01, -1.3341e-01],\n          [-9.8251e-02, -1.1789e-01, -3.5754e-02,  ...,  2.5254e-03,\n           -8.4958e-02, -6.6204e-02]],\n\n         [[-2.0845e-03, -9.1792e-03,  6.0091e-03,  ..., -5.0008e-03,\n            1.6339e-03, -8.5074e-03],\n          [-4.4475e-03, -6.8201e-03,  3.6797e-03,  ...,  7.5603e-04,\n           -4.1955e-03, -2.6145e-03],\n          [-4.4440e-03, -7.4381e-03,  3.9372e-03,  ..., -7.9689e-04,\n           -2.6127e-03, -4.0554e-03],\n          ...,\n          [-1.7447e-03, -7.1916e-03,  4.4383e-03,  ..., -7.6986e-03,\n            4.3492e-03, -8.9603e-03],\n          [-1.1428e-03, -7.7264e-03,  4.8816e-03,  ..., -1.0613e-02,\n            7.3137e-03, -1.2994e-02],\n          [-2.8949e-03, -8.0171e-03,  5.5340e-03,  ..., -2.7555e-03,\n           -6.2390e-04, -6.9509e-03]]],\n\n\n        ...,\n\n\n        [[[-1.0048e-01, -1.9790e-01, -3.2272e-01,  ...,  1.3944e-02,\n            1.3118e-01, -2.8134e-01],\n          [ 1.3773e-01,  4.6876e-01, -6.5512e-02,  ..., -1.0182e-02,\n            5.3250e-02,  3.6159e-01],\n          [ 4.9230e-01,  7.0525e-01, -3.1556e-01,  ..., -2.4532e-01,\n           -9.3481e-03,  4.7859e-01],\n          ...,\n          [-4.9527e-02,  5.1701e-01,  2.9385e-02,  ..., -4.7212e-01,\n           -6.5458e-02,  3.5906e-01],\n          [-3.2968e-01,  9.4484e-02,  1.0765e+00,  ...,  1.6895e-01,\n           -8.9886e-02,  8.6396e-02],\n          [-4.7534e-01,  4.5321e-01,  1.2656e+00,  ...,  5.0142e-01,\n            1.1290e-01, -2.1659e-02]],\n\n         [[-3.6580e-02,  1.5391e-01,  2.2139e-01,  ...,  2.3206e-01,\n           -3.4406e-01, -1.2980e-01],\n          [-1.0981e-01, -1.4461e-02, -9.3170e-02,  ..., -1.1420e-01,\n            8.9383e-02,  1.6966e-01],\n          [-4.0016e-01, -2.3397e-01, -3.7857e-02,  ..., -1.7908e-02,\n           -9.0336e-02,  2.2713e-01],\n          ...,\n          [-7.7431e-02, -1.1648e-01, -1.6435e-01,  ..., -1.0863e-01,\n           -8.7705e-02,  2.4835e-01],\n          [ 4.4183e-01,  1.6767e-01, -1.8520e-01,  ..., -2.0627e-01,\n            7.3136e-02,  1.2682e-01],\n          [ 3.0231e-01,  1.5883e-01, -1.4989e-01,  ..., -1.4978e-01,\n            9.1190e-03,  1.6051e-01]],\n\n         [[-2.3025e-02, -1.1858e-01, -2.2903e-01,  ..., -1.5384e-01,\n           -9.6520e-02, -9.4676e-02],\n          [ 1.4461e-02,  1.1892e-01, -1.7219e-01,  ..., -1.2856e-01,\n           -9.6042e-02, -9.5578e-02],\n          [ 5.3551e-02,  1.4763e-01, -1.1026e-01,  ..., -9.4874e-02,\n           -9.6664e-02, -9.6559e-02],\n          ...,\n          [ 1.0324e-01,  1.4112e-01, -4.3827e-02,  ..., -4.9323e-02,\n           -9.7484e-02, -9.5459e-02],\n          [ 9.8953e-02,  9.0568e-02, -6.8064e-02,  ..., -4.8377e-02,\n           -9.6039e-02, -9.4998e-02],\n          [ 7.3277e-02,  1.2352e-01, -1.2108e-01,  ..., -7.1800e-02,\n           -9.5520e-02, -9.4475e-02]],\n\n         ...,\n\n         [[ 4.7318e-02,  8.0485e-02, -1.0358e-01,  ...,  2.1195e-01,\n            3.7680e-02, -1.3220e-01],\n          [-3.8244e-01,  8.8152e-02,  9.4003e-02,  ...,  1.5438e-01,\n            1.9107e-02,  1.2157e-01],\n          [-5.4255e-01,  9.0749e-02,  1.5560e-01,  ...,  2.1245e-01,\n            3.5204e-02,  2.3987e-01],\n          ...,\n          [ 1.1441e-01,  8.6298e-02,  8.9904e-02,  ...,  1.9840e-02,\n            1.3648e-02,  1.0232e-02],\n          [ 3.7032e-01,  8.0513e-02,  2.5181e-02,  ..., -4.1121e-02,\n            2.3388e-02, -9.9144e-02],\n          [ 4.0906e-01,  1.0323e-01, -1.0024e-02,  ...,  3.6380e-02,\n            2.6296e-03, -1.6088e-01]],\n\n         [[ 1.6446e-01,  1.8870e-01,  6.1089e-02,  ...,  2.5213e-03,\n            1.4719e-01,  1.0389e-01],\n          [ 1.0789e-01,  1.1677e-01,  5.5575e-02,  ...,  2.5062e-03,\n            1.0112e-01,  7.7949e-02],\n          [ 2.0762e-02,  1.6055e-02,  1.9270e-02,  ...,  2.5269e-03,\n            2.3460e-02,  1.9164e-02],\n          ...,\n          [-1.8436e-01, -2.1527e-01, -8.4455e-02,  ...,  2.5935e-03,\n           -1.6354e-01, -1.3079e-01],\n          [-1.3047e-01, -1.5245e-01, -5.8596e-02,  ...,  2.5818e-03,\n           -1.1567e-01, -9.3696e-02],\n          [-6.7234e-02, -8.2228e-02, -2.1693e-02,  ...,  2.5326e-03,\n           -5.7189e-02, -4.4896e-02]],\n\n         [[-1.7354e-03, -8.7941e-03,  5.5826e-03,  ..., -6.7573e-03,\n            3.4081e-03, -9.9609e-03],\n          [-4.7336e-03, -7.1602e-03,  4.2189e-03,  ...,  6.8925e-04,\n           -4.0879e-03, -5.1345e-03],\n          [-5.0618e-03, -7.5371e-03,  4.1758e-03,  ...,  1.9884e-03,\n           -5.4170e-03, -2.7378e-03],\n          ...,\n          [-1.8484e-03, -7.7126e-03,  5.1949e-03,  ..., -5.5725e-03,\n            2.1914e-03, -7.2800e-03],\n          [-1.5870e-03, -7.9221e-03,  5.3825e-03,  ..., -8.0659e-03,\n            4.7484e-03, -1.1533e-02],\n          [-2.3089e-03, -8.3131e-03,  5.7685e-03,  ..., -4.1473e-03,\n            7.9353e-04, -8.8269e-03]]],\n\n\n        [[[-5.7140e-02,  2.9285e-03, -4.1194e-01,  ..., -4.1403e-02,\n            4.7195e-02, -2.3063e-01],\n          [ 1.8489e-01,  2.3770e-01,  2.2766e-01,  ...,  1.8614e-02,\n            1.8937e-01,  4.9656e-01],\n          [ 4.8723e-01,  5.9376e-01, -1.7230e-01,  ..., -1.9654e-01,\n           -1.0856e-02,  5.1317e-01],\n          ...,\n          [ 4.2245e-02,  2.9205e-01,  2.2100e-01,  ..., -4.1823e-01,\n            7.3623e-03,  5.2938e-01],\n          [-2.1460e-01,  2.0592e-01,  1.0647e+00,  ...,  8.7019e-02,\n           -1.1788e-01,  1.0297e-01],\n          [-5.5630e-01,  1.4173e-01,  9.3020e-01,  ...,  4.1743e-01,\n            1.9747e-01, -1.6174e-01]],\n\n         [[-1.4231e-01,  1.0229e-01,  2.2712e-01,  ...,  2.4825e-01,\n           -3.7963e-01, -1.0657e-01],\n          [ 3.1561e-01,  3.0448e-01, -1.7055e-01,  ..., -2.3554e-01,\n            2.9678e-01,  1.3697e-01],\n          [-4.2736e-01, -2.2766e-01, -3.8645e-02,  ..., -5.3836e-03,\n           -1.0893e-01,  2.3004e-01],\n          ...,\n          [ 1.8049e-01,  3.0554e-02, -2.4460e-01,  ..., -2.2658e-01,\n            5.6827e-02,  2.1278e-01],\n          [ 5.3206e-01,  2.6705e-01, -1.5584e-01,  ..., -1.7431e-01,\n            4.8991e-02,  1.0384e-01],\n          [ 7.2473e-02,  9.0463e-02, -5.7543e-02,  ..., -3.7919e-02,\n           -1.0210e-01,  7.0311e-02]],\n\n         [[-2.7381e-02, -1.0936e-01, -2.2560e-01,  ..., -1.6084e-01,\n           -9.5251e-02, -9.5144e-02],\n          [ 8.9039e-03,  1.3376e-01, -1.8843e-01,  ..., -1.3216e-01,\n           -9.6452e-02, -9.5273e-02],\n          [ 8.0136e-02,  1.7971e-01, -7.6241e-02,  ..., -7.1574e-02,\n           -9.5854e-02, -9.6761e-02],\n          ...,\n          [ 1.1249e-01,  1.3510e-01, -4.5563e-02,  ..., -3.6679e-02,\n           -9.7166e-02, -9.5041e-02],\n          [ 9.1481e-02,  9.8967e-02, -7.5194e-02,  ..., -5.6071e-02,\n           -9.4456e-02, -9.5582e-02],\n          [ 5.5999e-02,  9.6229e-02, -1.2814e-01,  ..., -9.0184e-02,\n           -9.6612e-02, -9.3905e-02]],\n\n         ...,\n\n         [[-4.7676e-02,  8.6222e-02, -1.5088e-01,  ...,  2.7311e-01,\n            2.9126e-02, -1.5695e-01],\n          [-1.5503e-01,  8.8784e-02,  8.1389e-02,  ...,  8.4489e-02,\n            2.8954e-02,  7.4708e-02],\n          [-5.6306e-01,  8.7318e-02,  1.6814e-01,  ...,  2.0875e-01,\n            4.2885e-02,  2.4937e-01],\n          ...,\n          [ 2.6267e-01,  7.8761e-02,  1.1527e-01,  ..., -6.5555e-02,\n            4.1308e-02,  1.9283e-02],\n          [ 4.1254e-01,  8.2994e-02, -6.8388e-02,  ...,  2.6092e-02,\n            1.4271e-02, -1.8347e-01],\n          [ 4.2052e-01,  1.2606e-01,  1.1754e-01,  ..., -7.6236e-02,\n            3.3049e-02, -6.2967e-02]],\n\n         [[ 1.6238e-01,  1.8434e-01,  6.5631e-02,  ...,  2.4951e-03,\n            1.4662e-01,  1.0610e-01],\n          [ 1.3161e-01,  1.4415e-01,  6.5602e-02,  ...,  2.4952e-03,\n            1.2225e-01,  9.3826e-02],\n          [ 5.5258e-03, -3.5200e-03,  1.7685e-02,  ...,  2.5454e-03,\n            1.1180e-02,  1.2518e-02],\n          ...,\n          [-1.7015e-01, -1.9904e-01, -7.1827e-02,  ...,  2.6096e-03,\n           -1.5042e-01, -1.1876e-01],\n          [-1.1541e-01, -1.3698e-01, -4.8127e-02,  ...,  2.5490e-03,\n           -1.0097e-01, -8.0178e-02],\n          [-7.3623e-02, -8.6177e-02, -3.5934e-02,  ...,  2.5318e-03,\n           -6.5220e-02, -5.6159e-02]],\n\n         [[-3.0578e-03, -8.5933e-03,  5.5507e-03,  ..., -2.6669e-03,\n           -7.4084e-04, -5.8030e-03],\n          [-4.7169e-03, -6.9038e-03,  3.9655e-03,  ...,  2.4076e-05,\n           -3.4066e-03, -6.0014e-03],\n          [-4.7243e-03, -7.6370e-03,  4.2420e-03,  ...,  9.3261e-04,\n           -4.3800e-03, -1.8096e-03],\n          ...,\n          [-1.1117e-03, -7.6907e-03,  4.9837e-03,  ..., -9.3000e-03,\n            5.9731e-03, -1.0784e-02],\n          [-3.0649e-03, -8.2444e-03,  6.0318e-03,  ..., -2.9437e-03,\n           -4.3263e-04, -7.5245e-03],\n          [-2.4836e-03, -7.6626e-03,  4.6701e-03,  ..., -5.1205e-03,\n            1.7443e-03, -7.4034e-03]]],\n\n\n        [[[-9.8511e-02, -5.3360e-02, -3.9085e-01,  ...,  3.9089e-02,\n            9.2797e-02, -2.6422e-01],\n          [ 9.9777e-02,  3.6336e-01,  9.5982e-02,  ...,  1.4122e-01,\n            8.6345e-02,  3.6132e-01],\n          [ 5.6700e-01,  5.4018e-01, -1.0000e-01,  ..., -2.0710e-01,\n            9.5461e-02,  5.5670e-01],\n          ...,\n          [ 5.6532e-02,  2.4588e-01,  2.5381e-01,  ..., -4.0640e-01,\n           -1.5844e-03,  5.2698e-01],\n          [-3.1425e-01,  3.4784e-01,  7.0381e-01,  ...,  3.5800e-02,\n           -9.8045e-02, -1.6003e-02],\n          [-5.6669e-01,  3.7592e-01,  1.3572e+00,  ...,  6.4140e-01,\n            1.4147e-01,  1.0478e-03]],\n\n         [[-2.2682e-01,  2.4738e-02,  2.3662e-01,  ...,  2.6143e-01,\n           -3.9702e-01, -1.0414e-01],\n          [-1.6434e-01, -1.0158e-01, -1.4435e-01,  ..., -1.6017e-01,\n            1.1658e-01,  2.0951e-01],\n          [ 2.9930e-02,  4.9765e-02, -8.4420e-02,  ..., -1.1286e-01,\n            5.9804e-02,  1.8040e-01],\n          ...,\n          [ 4.0314e-02, -3.8661e-02, -2.2498e-01,  ..., -1.9322e-01,\n            7.9555e-03,  2.0371e-01],\n          [ 8.7145e-03, -1.1879e-01, -1.0605e-01,  ..., -7.8108e-02,\n           -1.1241e-01,  1.7210e-01],\n          [ 1.3583e-01, -1.2952e-02, -1.9033e-01,  ..., -1.8686e-01,\n            1.3621e-02,  1.8825e-01]],\n\n         [[-2.1846e-02, -1.0183e-01, -2.2171e-01,  ..., -1.5475e-01,\n           -9.8301e-02, -9.3957e-02],\n          [ 3.3378e-02,  1.4860e-01, -1.5583e-01,  ..., -1.1004e-01,\n           -9.8274e-02, -9.4676e-02],\n          [ 2.3211e-02,  9.9897e-02, -1.5871e-01,  ..., -1.1957e-01,\n           -9.5290e-02, -9.6788e-02],\n          ...,\n          [ 1.2644e-01,  1.5409e-01, -2.3061e-02,  ..., -2.5648e-02,\n           -9.7929e-02, -9.4884e-02],\n          [ 1.3144e-01,  1.6432e-01, -1.4026e-02,  ..., -2.3257e-02,\n           -9.7407e-02, -9.4821e-02],\n          [ 1.1225e-01,  1.8407e-01, -7.0182e-02,  ..., -3.7652e-02,\n           -9.8602e-02, -9.3616e-02]],\n\n         ...,\n\n         [[-7.8027e-02,  8.3773e-02, -6.0655e-02,  ...,  2.3435e-01,\n            3.4501e-02, -7.6254e-02],\n          [-5.3366e-01,  8.4353e-02,  2.2107e-01,  ...,  1.3005e-01,\n            3.1894e-02,  2.5624e-01],\n          [-3.1518e-01,  8.6699e-02,  7.7079e-02,  ...,  1.9980e-01,\n            3.8554e-02,  1.5009e-01],\n          ...,\n          [ 1.4900e-01,  7.8354e-02,  1.6168e-01,  ..., -6.0912e-02,\n            4.3857e-02,  7.4623e-02],\n          [ 1.4430e-01,  8.7679e-02,  5.2063e-02,  ...,  3.2982e-02,\n            5.9216e-03, -4.2034e-02],\n          [ 1.3402e-01,  9.1375e-02,  1.2351e-01,  ...,  1.5133e-02,\n            2.2572e-02, -1.5312e-03]],\n\n         [[ 1.4578e-01,  1.6632e-01,  5.6761e-02,  ...,  2.5149e-03,\n            1.3110e-01,  9.3135e-02],\n          [ 8.5327e-02,  9.2175e-02,  4.1856e-02,  ...,  2.5426e-03,\n            7.9989e-02,  5.9864e-02],\n          [ 9.6969e-02,  1.0129e-01,  5.7076e-02,  ...,  2.4860e-03,\n            9.3270e-02,  7.5469e-02],\n          ...,\n          [-2.0188e-01, -2.3388e-01, -9.1767e-02,  ...,  2.6282e-03,\n           -1.8003e-01, -1.4417e-01],\n          [-1.7180e-01, -2.0034e-01, -7.6914e-02,  ...,  2.5687e-03,\n           -1.5235e-01, -1.2126e-01],\n          [-1.5010e-01, -1.7573e-01, -6.2995e-02,  ...,  2.5945e-03,\n           -1.3264e-01, -1.0521e-01]],\n\n         [[-2.1835e-03, -9.3306e-03,  6.0233e-03,  ..., -4.5447e-03,\n            1.1547e-03, -6.9664e-03],\n          [-3.5638e-03, -7.2516e-03,  4.0136e-03,  ..., -3.6198e-03,\n            2.6866e-04, -8.3244e-03],\n          [-4.5675e-03, -7.4923e-03,  4.2951e-03,  ...,  1.0112e-03,\n           -4.4161e-03, -4.2759e-03],\n          ...,\n          [-6.0479e-04, -6.8344e-03,  3.8220e-03,  ..., -1.1284e-02,\n            7.9735e-03, -1.2129e-02],\n          [-4.2392e-03, -8.9514e-03,  6.6116e-03,  ...,  1.0638e-03,\n           -4.5067e-03, -2.5092e-03],\n          [-1.2819e-03, -8.1392e-03,  5.3426e-03,  ..., -8.8789e-03,\n            5.5703e-03, -1.1859e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-4.7286e-02, -7.8047e-02,  5.4222e-03,  ...,  2.1631e-02,\n            4.6684e-02, -3.0969e-02],\n          [ 3.8656e-03,  2.1606e-03, -5.5619e-02,  ..., -1.3081e-02,\n           -2.1564e-02, -4.4159e-02],\n          [-9.0929e-02, -5.8052e-02,  1.8834e-01,  ...,  9.7145e-03,\n            4.7834e-02, -9.0251e-03],\n          ...,\n          [-1.8099e-01,  1.0726e-01,  8.0413e-02,  ...,  6.5492e-03,\n           -1.7766e-01,  9.8587e-02],\n          [ 1.5946e-03,  3.4863e-02,  7.4268e-02,  ...,  2.5754e-02,\n            1.5108e-02, -3.4940e-02],\n          [ 7.2043e-02,  1.9942e-01, -5.6472e-02,  ...,  1.1666e-02,\n           -1.0627e-01,  6.2892e-02]],\n\n         [[ 1.1331e-02, -1.3104e-02,  1.4995e-02,  ..., -2.4844e-02,\n            9.8249e-02,  1.6033e-02],\n          [ 5.1766e-02,  3.9269e-02, -1.7387e-01,  ..., -5.1197e-03,\n            3.5269e-03, -2.3531e-02],\n          [-1.4559e-02, -9.1426e-02, -2.1409e-01,  ...,  1.4554e-02,\n           -9.2873e-03, -7.7069e-02],\n          ...,\n          [-3.7492e-02, -4.4829e-02, -6.8266e-02,  ..., -6.0849e-02,\n            1.0353e-02,  1.1395e-01],\n          [-2.2251e-02,  5.0315e-02,  1.8092e-02,  ..., -4.3554e-02,\n            8.0356e-02, -6.3356e-03],\n          [-2.4225e-02,  1.1103e-01, -3.0029e-01,  ..., -5.2761e-02,\n           -2.9360e-02, -2.6968e-02]],\n\n         [[ 4.5161e-02, -3.2560e-02, -3.0969e-02,  ...,  2.9089e-02,\n           -6.7895e-02,  3.8940e-02],\n          [ 2.0145e-02,  6.0475e-02, -2.5225e-02,  ..., -7.0217e-03,\n           -7.3252e-03,  3.0002e-02],\n          [ 1.0708e-01, -8.1810e-02,  2.4272e-02,  ...,  2.7974e-02,\n           -2.4780e-02, -1.6441e-02],\n          ...,\n          [-4.2518e-02,  4.4632e-03, -2.2754e-02,  ..., -1.9260e-02,\n            3.3296e-02, -5.6484e-02],\n          [ 2.9044e-02,  3.7656e-03,  2.0230e-02,  ..., -2.3113e-02,\n           -2.2485e-02, -1.6207e-02],\n          [ 6.5169e-02, -3.3614e-02, -6.1587e-02,  ..., -2.6931e-02,\n           -2.5825e-02,  3.6249e-02]],\n\n         ...,\n\n         [[-2.7479e-03,  8.8392e-03, -4.3631e-02,  ...,  8.0860e-02,\n            1.0324e-01,  5.3151e-02],\n          [-4.3058e-03, -1.8341e-02, -9.4395e-03,  ..., -4.2181e-02,\n            2.7651e-02, -2.9743e-02],\n          [-6.5543e-02,  2.0093e-02, -3.6348e-02,  ...,  3.8037e-02,\n            1.2647e-01,  4.7822e-02],\n          ...,\n          [-9.2420e-03, -1.5703e-02, -5.4603e-03,  ...,  4.8969e-03,\n           -1.0424e-01, -3.5122e-02],\n          [-2.1829e-02,  1.8137e-02, -1.3569e-02,  ..., -2.8602e-02,\n            1.8794e-01, -3.2371e-02],\n          [-9.6601e-03,  2.4216e-02,  6.5867e-02,  ...,  5.0225e-02,\n           -1.8940e-03,  7.6008e-02]],\n\n         [[ 7.0604e-04, -9.9432e-03, -5.4384e-02,  ...,  2.2712e-02,\n            7.0347e-03,  2.7105e-04],\n          [-8.4082e-03,  1.5086e-02, -3.2164e-02,  ...,  2.4412e-02,\n           -4.1517e-02, -8.1201e-03],\n          [ 5.5985e-02, -9.4820e-04, -2.3018e-02,  ..., -6.2146e-02,\n           -2.1211e-02,  3.0621e-02],\n          ...,\n          [ 1.4319e-02, -3.4983e-02, -5.4188e-02,  ...,  3.1548e-02,\n            5.5682e-02,  2.7625e-02],\n          [ 3.5582e-02, -6.0371e-02, -4.1094e-02,  ...,  3.5103e-02,\n            6.6442e-02, -5.0581e-02],\n          [-5.4354e-03, -3.3041e-02, -3.7931e-02,  ...,  2.7952e-02,\n            2.2688e-02, -3.0247e-02]],\n\n         [[ 7.0100e-03,  2.6501e-02,  4.0591e-03,  ...,  3.5254e-03,\n            1.2644e-03, -5.3663e-03],\n          [-3.1991e-02, -5.3880e-02,  2.1779e-02,  ...,  8.5264e-03,\n            3.2182e-02,  4.4755e-02],\n          [-1.9916e-02,  2.1417e-03, -5.6584e-02,  ..., -3.2680e-02,\n           -2.4700e-02, -2.7475e-02],\n          ...,\n          [ 3.1447e-02, -4.8833e-02,  3.4253e-02,  ..., -5.5083e-02,\n           -3.0669e-02, -8.1289e-03],\n          [ 7.1612e-03, -5.5725e-02,  4.2344e-02,  ...,  1.9319e-02,\n            1.6518e-02,  1.5318e-02],\n          [-2.1152e-02, -7.1643e-03, -3.1048e-02,  ..., -2.7176e-02,\n           -9.7172e-03,  1.1809e-01]]],\n\n\n        [[[-7.6772e-02, -7.6999e-02,  2.9758e-02,  ..., -2.7703e-04,\n            3.4755e-02,  6.9573e-03],\n          [-3.0802e-02,  2.2520e-02,  8.3301e-02,  ...,  8.3431e-03,\n            1.3026e-02, -3.7104e-02],\n          [-3.8520e-02,  9.8362e-03,  1.6017e-02,  ...,  5.1539e-03,\n           -2.5873e-02, -9.2536e-02],\n          ...,\n          [ 4.9490e-02,  1.0856e-01,  4.7471e-02,  ..., -2.0457e-01,\n            2.8222e-02, -3.3469e-02],\n          [-1.3563e-02, -5.6748e-02, -6.2085e-02,  ...,  1.1352e-01,\n            3.3800e-02,  6.7028e-02],\n          [ 5.2530e-02,  8.4214e-02, -4.4725e-02,  ..., -5.6822e-02,\n            1.9527e-02, -1.8669e-02]],\n\n         [[-3.3069e-03, -2.8265e-03,  7.0667e-02,  ...,  1.1381e-02,\n           -3.9490e-04,  3.5849e-02],\n          [ 1.9528e-02,  7.2370e-02, -3.8940e-02,  ..., -1.8059e-02,\n            3.9335e-02, -1.8660e-02],\n          [-4.3676e-02, -4.1145e-02, -9.1782e-03,  ...,  3.1288e-02,\n            9.1922e-02, -5.1407e-02],\n          ...,\n          [-9.2965e-03,  1.8951e-03, -5.1769e-01,  ..., -2.9874e-02,\n           -1.8320e-02, -8.0548e-02],\n          [ 6.8457e-02, -1.8072e-02,  2.3464e-01,  ..., -5.2979e-02,\n            9.1452e-02, -2.8294e-02],\n          [-5.7244e-03, -6.8105e-04,  2.8927e-02,  ..., -5.8041e-02,\n            8.0459e-02, -1.6335e-01]],\n\n         [[ 7.3863e-03, -1.7942e-02, -1.4248e-02,  ..., -4.0681e-03,\n           -8.0181e-03,  8.4410e-03],\n          [ 4.2333e-02,  1.6819e-02,  3.2532e-02,  ..., -1.6331e-02,\n           -2.3597e-02, -8.4890e-04],\n          [ 5.5423e-02, -3.0507e-02, -4.1926e-03,  ..., -3.0016e-02,\n            1.4748e-02, -2.3795e-02],\n          ...,\n          [ 1.6175e-02,  6.4043e-02, -1.5707e-02,  ..., -1.3874e-02,\n           -2.3858e-02, -1.7013e-02],\n          [-1.1551e-01, -2.6699e-02, -2.2773e-02,  ..., -5.4512e-02,\n           -4.1981e-02, -8.5464e-02],\n          [ 2.5749e-03, -4.1372e-02,  4.8288e-02,  ..., -1.0906e-02,\n            3.4969e-03, -1.1376e-02]],\n\n         ...,\n\n         [[ 7.1075e-03,  7.0915e-03, -3.3610e-04,  ..., -1.3905e-02,\n            6.1741e-02, -2.9401e-02],\n          [-2.8358e-02,  2.0021e-02,  1.3744e-03,  ..., -6.7906e-02,\n            1.4889e-01, -3.8639e-02],\n          [-3.0141e-03,  6.4651e-02, -1.4606e-02,  ..., -1.0608e-02,\n           -3.0954e-02,  1.3914e-03],\n          ...,\n          [-3.1075e-02, -4.4512e-02, -1.0308e-01,  ...,  3.6822e-02,\n            1.3679e-01, -2.8032e-02],\n          [-3.1287e-02,  2.7576e-03, -6.5132e-02,  ...,  1.3208e-02,\n            7.4406e-02, -6.9537e-03],\n          [-1.8677e-02, -7.5357e-03, -8.5171e-02,  ...,  2.3963e-03,\n            3.9595e-02,  5.1053e-02]],\n\n         [[-9.7115e-03,  2.3629e-03,  2.2882e-02,  ...,  4.4521e-03,\n            7.5420e-04, -1.3876e-02],\n          [ 2.6236e-02, -5.0810e-02, -4.0793e-02,  ...,  3.8329e-02,\n            7.7332e-03, -2.3670e-02],\n          [ 1.8586e-02,  4.7980e-02, -1.9597e-02,  ...,  2.0600e-02,\n           -1.7757e-02,  1.9083e-02],\n          ...,\n          [ 9.6520e-03, -8.2122e-03, -7.2520e-02,  ..., -2.3850e-02,\n            1.0159e-01, -6.5301e-02],\n          [-9.1559e-04, -4.8940e-02, -4.8195e-02,  ..., -2.6480e-02,\n            5.1107e-02, -4.9055e-02],\n          [ 7.5998e-03,  1.4037e-03, -3.7938e-02,  ...,  2.6496e-02,\n            6.5994e-02, -3.4603e-02]],\n\n         [[-1.0722e-02,  2.5706e-02,  1.1355e-02,  ..., -9.1575e-03,\n           -8.0174e-03,  7.2957e-03],\n          [ 2.0017e-03, -4.5275e-02,  3.3667e-02,  ...,  2.5794e-02,\n            2.2865e-02,  1.6461e-02],\n          [-1.9842e-02, -5.5315e-02,  3.0803e-02,  ..., -1.2028e-04,\n            2.2224e-02,  9.3615e-03],\n          ...,\n          [ 2.6473e-02, -3.2755e-02,  1.1002e-02,  ..., -1.0230e-02,\n           -1.7649e-02, -2.9693e-02],\n          [ 6.6177e-03,  5.3806e-02, -7.2994e-02,  ..., -1.2844e-02,\n           -2.4117e-02,  6.2656e-02],\n          [ 8.1063e-03, -3.7223e-04,  6.2357e-03,  ...,  5.4035e-03,\n            7.8975e-02,  7.7049e-02]]],\n\n\n        [[[-5.2540e-03,  1.6439e-02,  1.7175e-02,  ...,  3.2975e-02,\n           -9.4956e-04, -1.0958e-02],\n          [ 1.6356e-02, -4.7196e-02, -3.4378e-04,  ..., -4.9071e-02,\n           -4.1181e-02, -3.5543e-02],\n          [-6.6299e-03, -1.3149e-02,  4.5836e-02,  ...,  6.9349e-02,\n           -1.8078e-02, -4.6258e-02],\n          ...,\n          [ 3.4896e-02,  9.9193e-02,  1.7692e-02,  ...,  3.5532e-02,\n           -7.8660e-02,  9.5939e-02],\n          [-3.2221e-03, -1.7800e-02,  1.6320e-02,  ..., -6.3220e-02,\n            2.3951e-02,  9.6133e-03],\n          [ 6.2488e-03, -4.1139e-02, -1.4981e-01,  ...,  5.5717e-03,\n            8.0901e-02, -2.8042e-02]],\n\n         [[ 3.9877e-02,  1.4830e-02,  1.5325e-01,  ..., -1.4238e-02,\n            4.1622e-02,  2.3710e-02],\n          [ 2.2595e-02,  2.6349e-02, -1.3778e-01,  ..., -7.0282e-02,\n           -5.6993e-03, -6.7101e-02],\n          [-3.5173e-03, -2.8095e-03, -7.4093e-02,  ...,  4.9879e-02,\n           -2.3868e-02, -1.8472e-02],\n          ...,\n          [-6.0885e-02, -2.6171e-02, -7.8012e-02,  ...,  4.6385e-03,\n            7.3845e-02,  8.6438e-03],\n          [-6.8627e-02, -1.5068e-02, -7.6895e-02,  ..., -5.4170e-02,\n           -6.4334e-02, -3.3383e-02],\n          [ 3.7723e-02, -1.6429e-02,  5.2653e-02,  ..., -1.9423e-02,\n            1.0996e-03, -1.7452e-01]],\n\n         [[ 7.9603e-02, -4.6283e-03,  2.7169e-02,  ..., -3.5206e-02,\n            2.9368e-03, -1.5397e-02],\n          [ 1.5012e-02,  5.7301e-02, -2.2629e-02,  ...,  9.2313e-03,\n           -3.8500e-02,  7.6426e-02],\n          [ 2.1476e-02,  1.0020e-01, -4.5594e-02,  ..., -2.5198e-02,\n           -5.0366e-02, -4.8471e-02],\n          ...,\n          [ 8.3808e-02, -1.0817e-02,  1.8092e-02,  ..., -3.8152e-03,\n            3.5584e-02, -1.6657e-03],\n          [-5.1935e-02,  4.6574e-03,  2.3194e-02,  ..., -5.2004e-02,\n           -7.0292e-03,  1.3915e-02],\n          [ 3.1808e-02, -1.8419e-03, -6.1387e-02,  ..., -3.4751e-02,\n           -4.7955e-02,  5.0526e-02]],\n\n         ...,\n\n         [[ 3.6333e-02,  1.7545e-02, -1.9599e-02,  ..., -9.8678e-03,\n            9.8737e-02, -1.4564e-02],\n          [-9.6419e-03, -2.2158e-02,  4.0177e-03,  ...,  3.1534e-02,\n            3.9429e-03,  4.5335e-02],\n          [-4.1093e-02,  6.3789e-03,  2.6989e-02,  ..., -2.0191e-02,\n           -5.9179e-02,  1.8127e-02],\n          ...,\n          [-2.0320e-02,  3.5174e-02, -9.5517e-02,  ...,  7.2234e-03,\n           -1.3363e-01,  5.4552e-03],\n          [ 3.9274e-03, -4.5106e-02, -3.5482e-02,  ...,  3.5592e-02,\n            6.3879e-03,  2.0917e-03],\n          [-9.7069e-02,  1.6530e-02,  6.0967e-02,  ...,  9.3254e-02,\n            4.5286e-02,  5.6556e-02]],\n\n         [[-2.2292e-02,  2.4939e-02, -7.3142e-03,  ...,  3.4027e-02,\n           -3.4537e-02, -7.0406e-03],\n          [-2.4504e-02, -1.2127e-02, -1.2349e-04,  ...,  7.1274e-03,\n           -7.3531e-03,  3.8398e-02],\n          [ 1.8557e-02, -9.7018e-03, -4.1535e-02,  ...,  7.6111e-02,\n           -4.3476e-02,  1.6159e-02],\n          ...,\n          [ 6.1837e-02,  7.7769e-03, -5.2791e-02,  ...,  5.5880e-02,\n            4.7337e-02, -3.9523e-02],\n          [ 1.2279e-02, -1.7500e-02, -9.6386e-04,  ...,  2.9538e-02,\n            6.8362e-02, -6.6177e-02],\n          [ 3.0820e-02, -8.1823e-02,  1.8523e-03,  ...,  1.9743e-02,\n            3.1907e-02, -5.5917e-02]],\n\n         [[-9.2209e-03,  2.7264e-02,  2.7358e-02,  ...,  4.7814e-03,\n           -7.6787e-03,  1.5133e-02],\n          [ 7.5282e-03,  7.2146e-04,  1.7511e-02,  ..., -8.2349e-03,\n            4.2529e-02, -2.6556e-03],\n          [ 1.9799e-02, -1.3791e-02,  1.0677e-02,  ...,  2.2716e-02,\n            2.2724e-02,  6.3806e-02],\n          ...,\n          [ 1.1025e-02, -5.2825e-02,  2.1440e-02,  ...,  4.0782e-03,\n            9.5855e-03,  5.4082e-02],\n          [-2.7003e-03, -6.5251e-02, -2.6693e-02,  ..., -9.2388e-03,\n            5.8101e-02,  2.0726e-02],\n          [ 1.3178e-02,  1.8063e-02,  6.2366e-03,  ...,  2.6856e-03,\n           -1.3382e-02,  1.0409e-01]]],\n\n\n        ...,\n\n\n        [[[-9.3881e-03, -4.7778e-02,  2.3684e-02,  ...,  2.1651e-02,\n            3.1156e-02, -5.6247e-02],\n          [ 5.3087e-02,  1.0966e-01, -5.1679e-02,  ...,  4.5062e-02,\n           -2.3665e-02,  7.5171e-02],\n          [-2.3233e-02,  6.6914e-02, -8.7120e-02,  ...,  2.9152e-02,\n           -1.0323e-01,  2.5592e-02],\n          ...,\n          [-3.3896e-02,  6.5263e-02, -4.4336e-04,  ..., -3.1821e-02,\n           -1.5400e-02, -2.6014e-02],\n          [ 2.8275e-01,  1.2143e-01,  1.7213e-01,  ..., -5.6195e-02,\n            2.8361e-03, -3.0044e-02],\n          [-5.7929e-02, -5.8531e-02,  7.6947e-02,  ...,  8.7300e-02,\n           -5.0746e-02,  4.3939e-02]],\n\n         [[ 1.3043e-02,  1.1293e-02, -1.1044e-01,  ...,  1.5029e-02,\n            2.5388e-02,  1.8284e-02],\n          [ 1.1054e-02, -2.4236e-02, -2.0518e-02,  ..., -1.2561e-02,\n           -6.7370e-02,  2.9842e-02],\n          [-3.3131e-02, -8.8598e-02, -1.9538e-02,  ...,  1.8544e-03,\n            4.6572e-02,  6.9002e-03],\n          ...,\n          [ 2.3249e-02, -9.9220e-02, -1.4022e-01,  ..., -4.0067e-02,\n            5.3998e-02,  2.8934e-02],\n          [-1.3476e-01, -1.0358e-01, -3.0688e-01,  ..., -3.3701e-02,\n           -5.0024e-03,  6.9832e-03],\n          [ 7.9161e-02,  3.8402e-02,  2.4976e-01,  ..., -4.0743e-02,\n           -6.4309e-02,  1.0603e-02]],\n\n         [[ 4.9440e-04, -2.6092e-02, -1.9364e-02,  ..., -2.6860e-02,\n           -3.1095e-02, -8.0910e-03],\n          [-5.9521e-02,  2.6117e-02, -6.1489e-02,  ...,  1.9824e-02,\n           -5.9074e-02, -2.0389e-02],\n          [ 2.5446e-02, -7.0669e-02,  1.9791e-02,  ...,  4.0569e-02,\n            3.2041e-02,  5.3355e-02],\n          ...,\n          [-4.8430e-02, -8.6877e-02, -3.2459e-02,  ...,  5.2966e-03,\n            2.2903e-02, -6.6410e-03],\n          [ 1.2315e-03,  2.3766e-02,  3.8706e-02,  ..., -1.4124e-03,\n           -7.5160e-02,  8.7438e-03],\n          [-4.8423e-02,  6.7291e-02,  7.2306e-03,  ...,  1.1652e-01,\n           -4.9091e-02,  1.4628e-01]],\n\n         ...,\n\n         [[ 8.2653e-04,  8.4164e-03,  3.5615e-03,  ..., -8.1404e-03,\n            4.5837e-02,  1.4547e-02],\n          [-1.1623e-01,  2.8919e-02,  5.1862e-02,  ..., -9.1235e-03,\n            7.0960e-02,  2.7687e-03],\n          [-3.3489e-02, -1.9355e-02, -4.5418e-03,  ...,  2.5405e-02,\n           -9.3077e-03,  3.6225e-02],\n          ...,\n          [-3.8619e-03, -3.3761e-02, -1.1182e-01,  ...,  9.8763e-03,\n            8.8627e-03, -8.8922e-02],\n          [-2.6035e-02, -3.0331e-02, -4.3264e-03,  ...,  3.9098e-02,\n            7.9812e-02,  6.9120e-02],\n          [ 3.1568e-02, -2.3858e-02, -3.0154e-02,  ..., -1.4617e-02,\n           -1.4297e-01,  2.5992e-02]],\n\n         [[-1.7205e-02, -2.3408e-02,  1.1341e-03,  ..., -2.0631e-04,\n            1.5307e-02, -4.0298e-02],\n          [-2.7173e-03, -6.4457e-02, -6.8830e-02,  ..., -8.8505e-03,\n            1.9471e-02, -2.2603e-02],\n          [-1.6446e-03,  9.8585e-04, -2.8815e-02,  ..., -1.8795e-02,\n           -3.7425e-02, -1.9813e-02],\n          ...,\n          [ 1.3812e-02, -4.1252e-02, -4.5758e-02,  ...,  8.6599e-06,\n            5.9769e-02, -1.2236e-01],\n          [-2.5566e-02,  3.3628e-02, -1.2480e-02,  ..., -2.7473e-02,\n            4.4704e-02, -4.8222e-02],\n          [ 9.5434e-03,  4.1593e-02,  1.4327e-02,  ..., -5.5927e-02,\n           -2.8630e-03,  7.3060e-03]],\n\n         [[-3.0703e-03,  1.7760e-02,  6.2061e-03,  ...,  3.2168e-03,\n            2.5090e-02,  6.5354e-04],\n          [ 5.4993e-03,  3.9762e-02, -4.3578e-03,  ..., -5.0264e-02,\n           -2.0366e-02,  9.0074e-03],\n          [-3.5969e-02,  6.0044e-02,  1.2481e-02,  ..., -2.8959e-02,\n           -2.6152e-02, -4.0969e-02],\n          ...,\n          [-4.0703e-02,  1.7830e-02,  4.0870e-02,  ..., -3.7794e-02,\n           -2.4428e-02, -1.7206e-02],\n          [ 1.3418e-02, -2.6250e-02, -2.2817e-02,  ..., -5.2305e-03,\n           -2.7767e-03,  4.2373e-03],\n          [ 3.4793e-03, -1.9227e-02, -1.9181e-02,  ..., -1.5946e-02,\n            3.6947e-02,  2.0575e-03]]],\n\n\n        [[[-1.5102e-02, -1.0283e-01,  8.5936e-02,  ..., -2.0024e-02,\n           -5.0246e-02,  6.4767e-02],\n          [-4.7523e-02, -2.2252e-02, -8.0689e-02,  ..., -4.3187e-02,\n            6.9320e-02,  1.4313e-01],\n          [-9.3297e-03,  2.6010e-02,  5.1125e-02,  ...,  6.2260e-02,\n           -4.9560e-02,  1.0651e-02],\n          ...,\n          [ 3.1094e-02, -7.3768e-02, -1.3487e-02,  ..., -1.2877e-01,\n            5.2492e-02,  8.0016e-02],\n          [ 3.1436e-02,  4.2324e-02, -7.1589e-02,  ..., -2.4691e-02,\n           -1.7725e-01, -8.3043e-02],\n          [ 4.5232e-02, -1.8752e-02, -2.2433e-02,  ...,  1.0277e-02,\n            2.9775e-02, -2.6479e-02]],\n\n         [[-7.8658e-03, -2.4266e-03,  6.2905e-02,  ..., -1.5014e-02,\n           -3.9783e-03, -4.9833e-03],\n          [ 2.9601e-02,  2.2296e-02,  1.4688e-01,  ...,  7.6743e-03,\n            3.0213e-02,  5.1383e-03],\n          [ 2.7924e-02, -3.2262e-02, -1.1016e-01,  ..., -5.0222e-02,\n            7.0240e-02, -5.6474e-02],\n          ...,\n          [-4.6460e-02,  5.6826e-03, -1.7485e-01,  ...,  8.0421e-03,\n            2.8282e-02,  1.0899e-01],\n          [ 1.3377e-02, -8.2107e-02, -1.2025e-01,  ..., -5.0317e-02,\n            2.1950e-01, -1.1074e-01],\n          [-1.1163e-04,  1.2838e-01,  2.1860e-02,  ..., -1.7329e-02,\n           -3.3917e-02, -7.0411e-02]],\n\n         [[ 1.2755e-02,  2.3734e-02, -1.7598e-02,  ...,  2.5524e-02,\n           -4.3804e-02,  3.7010e-03],\n          [ 5.5990e-03,  1.1741e-02,  3.8471e-04,  ..., -2.3868e-02,\n           -1.6992e-02,  5.7528e-02],\n          [ 5.2684e-02, -2.5519e-02, -1.1698e-02,  ...,  3.1332e-02,\n            1.5014e-02, -1.2301e-02],\n          ...,\n          [-3.9830e-02,  1.2790e-02, -6.8275e-02,  ..., -1.0027e-01,\n            9.1938e-04, -6.9835e-02],\n          [-6.6620e-02,  2.4038e-02, -4.1791e-02,  ...,  2.2031e-02,\n           -2.8545e-03,  7.7608e-02],\n          [ 2.3542e-02, -6.0456e-02, -1.9594e-03,  ..., -2.4694e-02,\n           -2.7684e-02, -2.3717e-02]],\n\n         ...,\n\n         [[ 1.3209e-02,  4.2259e-03, -1.8280e-02,  ...,  9.3592e-03,\n            2.7660e-02, -1.3285e-02],\n          [-6.9013e-02, -2.4784e-02,  4.4857e-02,  ..., -5.1299e-03,\n           -8.4337e-03, -2.6999e-02],\n          [-5.2476e-02,  2.9788e-02, -3.1458e-02,  ...,  8.8712e-03,\n           -5.9293e-02,  1.5564e-02],\n          ...,\n          [ 1.9459e-02,  1.5668e-02, -8.7560e-02,  ...,  8.4211e-03,\n           -8.1644e-02,  8.2460e-03],\n          [-1.1869e-02,  4.5535e-02,  3.1998e-02,  ...,  8.7698e-03,\n            1.2413e-01, -3.8996e-02],\n          [-1.6452e-02,  6.4471e-02,  1.7251e-02,  ..., -1.6852e-02,\n           -1.2656e-02, -2.6434e-02]],\n\n         [[-9.7273e-03,  3.2848e-03,  3.9021e-02,  ..., -1.9412e-02,\n           -3.5083e-02,  2.0446e-02],\n          [ 4.3633e-03, -3.3039e-02, -1.4262e-02,  ...,  2.0909e-02,\n           -2.5953e-02,  4.4959e-02],\n          [ 6.7220e-03,  3.8932e-02, -1.1804e-02,  ...,  7.4905e-03,\n           -7.5677e-03,  5.4593e-02],\n          ...,\n          [-1.1640e-02,  2.1943e-02, -6.7986e-02,  ...,  3.1021e-02,\n            8.2886e-02, -3.0385e-02],\n          [ 9.7302e-03, -2.9902e-02, -2.8131e-03,  ..., -1.7925e-03,\n            3.6328e-02, -5.0659e-02],\n          [ 1.1599e-02, -7.9560e-02, -2.0108e-02,  ...,  1.3427e-02,\n            1.1377e-02, -2.4566e-02]],\n\n         [[ 1.0727e-02,  6.5934e-03, -1.5039e-03,  ...,  6.1945e-03,\n            5.4793e-03, -1.8245e-02],\n          [ 7.9157e-03, -1.9761e-02,  1.9331e-03,  ...,  4.8894e-03,\n           -2.1381e-02,  1.2910e-02],\n          [ 2.5048e-02,  2.7552e-02, -4.3903e-04,  ...,  4.9577e-03,\n           -1.9078e-02, -8.1221e-02],\n          ...,\n          [ 4.5814e-02, -8.0135e-02,  1.5030e-02,  ...,  1.5970e-02,\n            2.5563e-02,  7.4306e-02],\n          [ 2.5990e-03,  3.5749e-02, -2.5399e-02,  ..., -4.0551e-02,\n           -5.2564e-02, -1.0578e-02],\n          [ 1.0246e-02, -3.7649e-02,  1.3440e-02,  ...,  2.3466e-02,\n            5.9235e-02,  5.2050e-02]]],\n\n\n        [[[-3.6645e-03, -4.8499e-02, -1.6979e-02,  ..., -2.1645e-02,\n            2.1175e-02, -6.9931e-02],\n          [ 1.1544e-01, -3.7723e-02,  6.8506e-02,  ...,  5.3487e-03,\n           -9.6557e-02,  3.1470e-02],\n          [ 1.0555e-01,  1.4520e-01, -3.3384e-02,  ..., -1.2351e-01,\n           -3.4509e-02, -1.6377e-01],\n          ...,\n          [ 3.3694e-02, -4.3296e-02, -8.5605e-02,  ...,  7.0288e-02,\n            3.4494e-02,  1.2963e-02],\n          [-9.8524e-02,  3.1441e-02, -5.1032e-02,  ..., -6.7901e-02,\n           -9.9278e-02, -4.7950e-02],\n          [ 8.8052e-02, -3.7795e-02, -1.4675e-01,  ..., -1.0938e-01,\n            1.4047e-01,  9.9893e-02]],\n\n         [[-3.7591e-02, -3.6988e-03, -3.5202e-02,  ...,  1.6941e-03,\n            5.2757e-02,  6.3734e-02],\n          [-2.5453e-02,  1.2658e-02, -1.5620e-01,  ..., -2.8923e-03,\n           -2.6739e-02, -4.0795e-02],\n          [-7.3946e-02, -4.8606e-02, -4.5241e-02,  ...,  1.6064e-02,\n           -9.0242e-03, -5.4137e-02],\n          ...,\n          [ 1.7363e-02,  2.3588e-02, -1.3169e-01,  ..., -2.4045e-02,\n           -1.3958e-01,  1.9033e-02],\n          [-3.6811e-03, -1.2585e-01,  4.4058e-02,  ..., -3.0393e-03,\n            8.5472e-02, -8.0106e-02],\n          [-6.1307e-02, -6.8352e-02, -5.1006e-02,  ...,  1.8940e-02,\n           -6.3603e-02, -1.4718e-02]],\n\n         [[-2.2123e-02, -4.9005e-02,  4.1954e-03,  ..., -2.6949e-02,\n           -1.1930e-02, -2.7865e-02],\n          [-6.1825e-02,  1.3437e-02,  1.2043e-02,  ..., -1.9357e-03,\n           -2.4523e-02,  1.1796e-02],\n          [-6.9389e-03, -1.6714e-02,  4.3791e-03,  ...,  7.3857e-03,\n           -4.3349e-02, -4.6141e-02],\n          ...,\n          [-1.8191e-02,  2.7534e-02, -5.4357e-02,  ..., -6.3243e-02,\n           -1.7092e-02, -4.8247e-02],\n          [-3.9343e-02,  1.7133e-02,  2.8595e-02,  ...,  3.9609e-02,\n           -3.5293e-02,  1.5970e-02],\n          [ 1.5135e-02,  8.0727e-02, -9.6332e-03,  ..., -4.2504e-02,\n           -1.2386e-03,  2.0142e-02]],\n\n         ...,\n\n         [[ 3.2372e-02,  3.9895e-02, -2.9418e-02,  ...,  1.5828e-02,\n            3.2782e-02, -4.7478e-02],\n          [-6.5308e-03,  1.6517e-03,  6.0130e-03,  ..., -2.5717e-02,\n            5.5959e-02, -1.8909e-02],\n          [-1.9415e-02,  2.8131e-02,  3.7014e-02,  ...,  3.6472e-02,\n           -4.2812e-02,  1.1940e-02],\n          ...,\n          [-4.2166e-03, -2.9394e-02, -1.5994e-02,  ...,  1.2140e-02,\n           -5.4823e-02, -5.6401e-02],\n          [-1.2198e-01, -3.3147e-02, -1.3446e-02,  ..., -2.4598e-03,\n            1.0171e-01, -2.0409e-02],\n          [ 2.4962e-02, -6.1895e-02,  3.0726e-02,  ...,  4.9426e-02,\n           -2.7492e-02, -1.1259e-02]],\n\n         [[-1.5471e-02,  1.8312e-02, -1.7860e-02,  ...,  2.8336e-02,\n           -6.2596e-04, -3.3125e-02],\n          [-3.6229e-02, -5.1000e-03,  2.6782e-03,  ..., -4.6669e-03,\n           -2.8571e-02, -1.4087e-02],\n          [-9.0067e-03,  1.3061e-02, -3.6259e-03,  ...,  1.2426e-02,\n           -6.7170e-02,  2.9405e-02],\n          ...,\n          [ 6.9867e-03, -1.8875e-02, -1.0156e-01,  ...,  6.1567e-02,\n            5.9989e-02, -2.9406e-02],\n          [-2.6983e-02, -5.2961e-02, -1.0140e-02,  ...,  1.5542e-02,\n            5.9169e-02, -3.5485e-02],\n          [-4.9879e-02, -3.0230e-02, -8.5079e-05,  ..., -4.1181e-02,\n            2.2337e-02, -3.9665e-02]],\n\n         [[-4.8132e-03, -1.4073e-02,  2.6353e-03,  ...,  2.6004e-03,\n            2.2998e-02,  2.6035e-02],\n          [ 2.1397e-02, -2.3592e-02, -2.2316e-03,  ..., -9.0873e-03,\n            5.0937e-02, -2.0277e-02],\n          [-2.1859e-03,  4.0250e-02,  2.5000e-02,  ..., -2.2612e-02,\n           -2.4296e-02, -3.4245e-02],\n          ...,\n          [ 3.3572e-02, -8.8905e-02,  4.8515e-02,  ...,  3.8278e-02,\n           -1.8270e-02,  3.6083e-02],\n          [-1.1873e-02,  4.9110e-02,  7.7116e-02,  ...,  5.3908e-02,\n            2.4004e-02,  3.5244e-02],\n          [ 5.2311e-02, -3.8368e-02,  1.7422e-02,  ...,  1.9405e-02,\n            4.4826e-02,  6.7564e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[ 1.1805e-02, -6.6965e-03, -1.4343e-01,  ...,  1.6860e-02,\n           -1.5927e-02,  3.9402e-01],\n          [ 1.8206e-02, -8.8080e-03,  3.7514e-01,  ...,  1.6321e-02,\n           -1.7234e-03, -1.3700e+00],\n          [ 2.4485e-02, -9.6316e-03,  2.9582e-01,  ...,  1.5582e-02,\n            1.7172e-03, -1.0139e+00],\n          ...,\n          [-1.3623e-02,  1.2254e-02,  1.6939e-01,  ...,  1.8937e-02,\n           -3.1150e-02, -3.6236e-01],\n          [-1.4751e-02,  1.2976e-02,  1.5906e-01,  ...,  1.8935e-02,\n           -3.1236e-02, -3.6577e-01],\n          [-2.1522e-02,  1.3156e-02,  1.4581e-01,  ...,  1.9483e-02,\n           -3.6136e-02, -2.9281e-01]],\n\n         [[ 4.3801e-01,  2.4209e-01, -3.2583e-01,  ...,  2.9293e-01,\n            3.2322e-01, -3.2365e-01],\n          [-6.8243e-01, -3.7809e-01,  5.1062e-01,  ..., -4.8174e-01,\n           -5.1849e-01,  5.1806e-01],\n          [-4.9945e-01, -2.7041e-01,  3.7033e-01,  ..., -3.5048e-01,\n           -3.7734e-01,  3.7695e-01],\n          ...,\n          [-4.6415e-01, -2.5308e-01,  3.4489e-01,  ..., -3.2772e-01,\n           -3.5179e-01,  3.5117e-01],\n          [-4.9216e-01, -2.7544e-01,  3.6974e-01,  ..., -3.5199e-01,\n           -3.7668e-01,  3.7606e-01],\n          [-4.8187e-01, -2.6747e-01,  3.6104e-01,  ..., -3.4347e-01,\n           -3.6796e-01,  3.6734e-01]],\n\n         [[-2.3733e-01,  2.8371e-01,  2.5956e-01,  ...,  2.6936e-01,\n           -3.1782e-01,  3.8317e-01],\n          [ 3.4461e-01, -3.8862e-01, -3.4310e-01,  ..., -3.6381e-01,\n            4.1843e-01, -5.1730e-01],\n          [ 2.4317e-01, -2.7393e-01, -2.3859e-01,  ..., -2.5485e-01,\n            2.9442e-01, -3.6969e-01],\n          ...,\n          [ 2.7117e-01, -3.0258e-01, -2.6676e-01,  ..., -2.8324e-01,\n            3.2336e-01, -3.9869e-01],\n          [ 2.7518e-01, -3.0619e-01, -2.7067e-01,  ..., -2.8700e-01,\n            3.2683e-01, -4.0209e-01],\n          [ 2.7325e-01, -3.0500e-01, -2.6894e-01,  ..., -2.8552e-01,\n            3.2596e-01, -4.0155e-01]],\n\n         ...,\n\n         [[ 2.7648e-01,  2.7508e-01,  9.1519e-02,  ..., -6.3726e-02,\n            2.5644e-01,  2.0167e-01],\n          [-2.1905e-01, -3.1084e-01, -5.7883e-01,  ...,  4.1098e-01,\n           -7.2264e-01, -5.3289e-01],\n          [-1.3255e-01, -2.1099e-01, -4.6663e-01,  ...,  3.2757e-01,\n           -5.6699e-01, -4.1135e-01],\n          ...,\n          [-1.3283e-01, -2.0777e-01, -4.6041e-01,  ...,  3.2878e-01,\n           -5.4994e-01, -4.0297e-01],\n          [-1.4422e-01, -2.2021e-01, -4.7373e-01,  ...,  3.3990e-01,\n           -5.6572e-01, -4.1692e-01],\n          [-1.4320e-01, -2.1952e-01, -4.7318e-01,  ...,  3.3876e-01,\n           -5.6512e-01, -4.1639e-01]],\n\n         [[ 3.0188e-01,  4.1554e-01, -5.3763e-01,  ..., -7.5290e-01,\n           -4.7968e-01,  1.5211e-01],\n          [-9.5757e-01, -1.4041e+00,  3.9988e-01,  ...,  1.4649e+00,\n           -6.3510e-02,  1.3491e-01],\n          [-8.8459e-01, -8.4779e-01,  8.5795e-02,  ...,  1.0329e+00,\n           -6.3177e-01,  1.1260e-01],\n          ...,\n          [ 3.7251e-01, -3.7333e-01,  5.8290e-01,  ...,  2.2819e-01,\n            1.2741e+00,  1.1272e-01],\n          [ 5.3164e-01, -3.9857e-01,  7.4634e-01,  ...,  2.3479e-01,\n            1.7949e+00,  1.4056e-01],\n          [ 5.5033e-01, -2.9920e-01,  6.2721e-01,  ...,  1.0927e-01,\n            1.6515e+00,  1.3429e-01]],\n\n         [[-4.4964e-01,  5.2801e-01, -4.5847e-01,  ...,  6.0005e-01,\n            7.6857e-02,  1.0421e-01],\n          [ 1.1734e+00, -6.1451e-01,  4.7849e-01,  ..., -6.0044e-01,\n            6.0442e-02,  1.1568e-01],\n          [ 9.4379e-01, -3.8008e-01,  3.3235e-01,  ..., -3.8531e-01,\n            7.0658e-02,  1.0763e-01],\n          ...,\n          [ 4.2975e-01, -5.2591e-01,  2.2898e-01,  ..., -5.0957e-01,\n            7.0269e-02,  1.0670e-01],\n          [ 4.1711e-01, -5.6984e-01,  2.2461e-01,  ..., -5.4384e-01,\n            8.6276e-02,  9.7179e-02],\n          [ 2.2864e-01, -6.3318e-01,  2.0895e-01,  ..., -5.9634e-01,\n            1.0827e-01,  7.5676e-02]]],\n\n\n        [[[ 1.1984e-02, -6.6097e-03, -1.4539e-01,  ...,  1.6856e-02,\n           -1.6023e-02,  4.0038e-01],\n          [ 1.6548e-02, -8.0893e-03,  3.4152e-01,  ...,  1.6222e-02,\n           -1.6786e-03, -1.2481e+00],\n          [ 1.6149e-02, -9.3883e-03,  3.4193e-01,  ...,  1.6198e-02,\n           -3.9154e-03, -1.1484e+00],\n          ...,\n          [-1.8705e-02,  1.5059e-02,  1.7228e-01,  ...,  1.9492e-02,\n           -3.4134e-02, -3.6081e-01],\n          [-1.2570e-02,  1.3435e-02,  1.8605e-01,  ...,  1.8971e-02,\n           -2.8569e-02, -4.6466e-01],\n          [-2.0062e-02,  1.6379e-02,  2.0359e-01,  ...,  1.9535e-02,\n           -3.6743e-02, -5.0861e-01]],\n\n         [[ 4.4150e-01,  2.4407e-01, -3.2846e-01,  ...,  2.9536e-01,\n            3.2586e-01, -3.2630e-01],\n          [-6.6025e-01, -3.6750e-01,  4.9498e-01,  ..., -4.6748e-01,\n           -5.0272e-01,  5.0228e-01],\n          [-5.5172e-01, -3.0681e-01,  4.1376e-01,  ..., -3.9197e-01,\n           -4.2093e-01,  4.2054e-01],\n          ...,\n          [-5.4112e-01, -3.0090e-01,  4.0542e-01,  ..., -3.8475e-01,\n           -4.1264e-01,  4.1197e-01],\n          [-5.3856e-01, -2.9798e-01,  4.0226e-01,  ..., -3.8171e-01,\n           -4.0949e-01,  4.0884e-01],\n          [-6.2681e-01, -3.5277e-01,  4.7219e-01,  ..., -4.4750e-01,\n           -4.7976e-01,  4.7912e-01]],\n\n         [[-2.3917e-01,  2.8580e-01,  2.6146e-01,  ...,  2.7134e-01,\n           -3.2009e-01,  3.8590e-01],\n          [ 3.3115e-01, -3.7304e-01, -3.2914e-01,  ..., -3.4913e-01,\n            4.0146e-01, -4.9701e-01],\n          [ 2.5525e-01, -2.8789e-01, -2.5109e-01,  ..., -2.6798e-01,\n            3.0977e-01, -3.8886e-01],\n          ...,\n          [ 2.9514e-01, -3.3089e-01, -2.9172e-01,  ..., -3.0965e-01,\n            3.5478e-01, -4.3812e-01],\n          [ 3.0027e-01, -3.3671e-01, -2.9702e-01,  ..., -3.1518e-01,\n            3.6101e-01, -4.4528e-01],\n          [ 3.4329e-01, -3.8472e-01, -3.4120e-01,  ..., -3.6103e-01,\n            4.1254e-01, -5.0567e-01]],\n\n         ...,\n\n         [[ 2.7809e-01,  2.7696e-01,  9.3653e-02,  ..., -6.5268e-02,\n            2.5952e-01,  2.0400e-01],\n          [-2.0377e-01, -2.9304e-01, -5.5880e-01,  ...,  3.9630e-01,\n           -6.9554e-01, -5.1126e-01],\n          [-1.5714e-01, -2.3832e-01, -4.9644e-01,  ...,  3.5151e-01,\n           -6.0568e-01, -4.4300e-01],\n          ...,\n          [-1.6802e-01, -2.4842e-01, -5.0601e-01,  ...,  3.6270e-01,\n           -6.1263e-01, -4.5227e-01],\n          [-1.6304e-01, -2.4397e-01, -5.0208e-01,  ...,  3.5760e-01,\n           -6.1009e-01, -4.4871e-01],\n          [-2.1203e-01, -2.9866e-01, -5.6196e-01,  ...,  4.0525e-01,\n           -6.8778e-01, -5.1247e-01]],\n\n         [[ 3.0642e-01,  4.1948e-01, -5.3827e-01,  ..., -7.5972e-01,\n           -4.7914e-01,  1.5171e-01],\n          [-7.5297e-01, -1.2392e+00,  5.0792e-01,  ...,  1.3056e+00,\n            2.2815e-01,  1.4604e-01],\n          [-9.3959e-01, -9.9839e-01,  2.3771e-01,  ...,  1.1788e+00,\n           -4.5408e-01,  1.1660e-01],\n          ...,\n          [ 4.3252e-01, -4.9494e-01,  6.9160e-01,  ...,  3.3631e-01,\n            1.5679e+00,  1.0053e-01],\n          [ 3.2530e-01, -6.1263e-01,  6.6681e-01,  ...,  3.6506e-01,\n            1.4042e+00,  9.9502e-02],\n          [ 4.5175e-01, -6.8528e-01,  9.2911e-01,  ...,  4.6714e-01,\n            2.0449e+00,  9.1171e-02]],\n\n         [[-4.5441e-01,  5.3304e-01, -4.6226e-01,  ...,  6.0522e-01,\n            7.7008e-02,  1.0409e-01],\n          [ 1.1389e+00, -6.0669e-01,  4.6440e-01,  ..., -6.1040e-01,\n            7.8628e-02,  1.0245e-01],\n          [ 9.2299e-01, -6.0838e-01,  4.1933e-01,  ..., -5.9767e-01,\n            8.4473e-02,  9.6392e-02],\n          ...,\n          [ 4.3108e-01, -6.9467e-01,  2.8780e-01,  ..., -6.4450e-01,\n            7.3984e-02,  1.0315e-01],\n          [ 5.4853e-01, -5.9082e-01,  2.8313e-01,  ..., -5.7332e-01,\n            7.7208e-02,  1.0031e-01],\n          [ 5.5841e-01, -7.5751e-01,  3.4418e-01,  ..., -7.3272e-01,\n            6.1821e-02,  1.1382e-01]]],\n\n\n        [[[ 1.1673e-02, -6.6351e-03, -1.4495e-01,  ...,  1.6878e-02,\n           -1.6093e-02,  4.0125e-01],\n          [ 1.9277e-02, -8.3771e-03,  3.2302e-01,  ...,  1.5793e-02,\n            1.6145e-03, -1.1645e+00],\n          [ 2.3961e-02, -7.4250e-03,  3.4988e-01,  ...,  1.5776e-02,\n            1.5136e-03, -1.1845e+00],\n          ...,\n          [-1.9881e-02,  1.5346e-02,  2.0745e-01,  ...,  1.9370e-02,\n           -3.4592e-02, -4.6772e-01],\n          [-2.3384e-02,  1.4073e-02,  1.6343e-01,  ...,  1.9589e-02,\n           -3.6664e-02, -3.3042e-01],\n          [-1.2491e-02,  1.0119e-02,  1.1964e-01,  ...,  1.8855e-02,\n           -2.7718e-02, -2.9817e-01]],\n\n         [[ 4.4241e-01,  2.4460e-01, -3.2915e-01,  ...,  2.9602e-01,\n            3.2656e-01, -3.2699e-01],\n          [-6.0558e-01, -3.3465e-01,  4.5287e-01,  ..., -4.2806e-01,\n           -4.6038e-01,  4.5994e-01],\n          [-5.9422e-01, -3.2743e-01,  4.4363e-01,  ..., -4.1919e-01,\n           -4.5103e-01,  4.5065e-01],\n          ...,\n          [-5.6927e-01, -3.1603e-01,  4.2602e-01,  ..., -4.0372e-01,\n           -4.3337e-01,  4.3273e-01],\n          [-5.1036e-01, -2.8493e-01,  3.8311e-01,  ..., -3.6421e-01,\n           -3.9016e-01,  3.8952e-01],\n          [-4.0644e-01, -2.2208e-01,  3.0241e-01,  ..., -2.8854e-01,\n           -3.0901e-01,  3.0842e-01]],\n\n         [[-2.3993e-01,  2.8665e-01,  2.6224e-01,  ...,  2.7215e-01,\n           -3.2100e-01,  3.8696e-01],\n          [ 2.9893e-01, -3.3729e-01, -2.9611e-01,  ..., -3.1493e-01,\n            3.6312e-01, -4.5179e-01],\n          [ 2.8847e-01, -3.2594e-01, -2.8542e-01,  ..., -3.0395e-01,\n            3.5120e-01, -4.3867e-01],\n          ...,\n          [ 3.0913e-01, -3.4586e-01, -3.0593e-01,  ..., -3.2419e-01,\n            3.7051e-01, -4.5604e-01],\n          [ 2.6728e-01, -2.9980e-01, -2.6312e-01,  ..., -2.7996e-01,\n            3.2140e-01, -3.9896e-01],\n          [ 2.3057e-01, -2.5758e-01, -2.2516e-01,  ..., -2.4015e-01,\n            2.7520e-01, -3.4236e-01]],\n\n         ...,\n\n         [[ 2.7840e-01,  2.7734e-01,  9.4089e-02,  ..., -6.5572e-02,\n            2.6015e-01,  2.0448e-01],\n          [-1.7114e-01, -2.5563e-01, -5.1692e-01,  ...,  3.6477e-01,\n           -6.3768e-01, -4.6598e-01],\n          [-1.7062e-01, -2.5530e-01, -5.1679e-01,  ...,  3.6422e-01,\n           -6.3837e-01, -4.6596e-01],\n          ...,\n          [-1.8146e-01, -2.6363e-01, -5.2294e-01,  ...,  3.7573e-01,\n           -6.3627e-01, -4.7052e-01],\n          [-1.5669e-01, -2.3440e-01, -4.8937e-01,  ...,  3.5196e-01,\n           -5.8617e-01, -4.3357e-01],\n          [-1.0910e-01, -1.8055e-01, -4.2967e-01,  ...,  3.0579e-01,\n           -5.0526e-01, -3.6941e-01]],\n\n         [[ 3.0592e-01,  4.2426e-01, -5.4097e-01,  ..., -7.6069e-01,\n           -4.8272e-01,  1.5169e-01],\n          [-8.2860e-01, -1.1640e+00,  3.3196e-01,  ...,  1.2718e+00,\n           -1.1560e-01,  1.3501e-01],\n          [-9.6269e-01, -1.1144e+00,  2.8995e-01,  ...,  1.2648e+00,\n           -3.8254e-01,  1.0373e-01],\n          ...,\n          [ 4.0034e-01, -4.8077e-01,  7.6385e-01,  ...,  4.0407e-01,\n            1.6048e+00,  9.8884e-02],\n          [ 4.5715e-01, -4.8186e-01,  7.1865e-01,  ...,  2.9596e-01,\n            1.6015e+00,  7.8926e-02],\n          [ 2.8434e-01, -4.3622e-01,  4.9508e-01,  ...,  2.3230e-01,\n            1.1146e+00,  1.0348e-01]],\n\n         [[-4.5965e-01,  5.3360e-01, -4.6317e-01,  ...,  6.0589e-01,\n            7.6951e-02,  1.0410e-01],\n          [ 1.0317e+00, -4.8895e-01,  3.8941e-01,  ..., -4.7442e-01,\n            7.4082e-02,  1.0396e-01],\n          [ 1.0809e+00, -5.5295e-01,  4.3758e-01,  ..., -5.5551e-01,\n            6.4659e-02,  1.1349e-01],\n          ...,\n          [ 4.8266e-01, -7.5322e-01,  3.4985e-01,  ..., -7.5237e-01,\n            6.3360e-02,  1.1413e-01],\n          [ 4.3634e-01, -7.0529e-01,  2.9753e-01,  ..., -6.6756e-01,\n            6.9288e-02,  1.0800e-01],\n          [ 4.5702e-01, -4.1755e-01,  1.7888e-01,  ..., -4.0093e-01,\n            7.6593e-02,  1.0091e-01]]],\n\n\n        ...,\n\n\n        [[[ 1.2088e-02, -6.6704e-03, -1.4522e-01,  ...,  1.6859e-02,\n           -1.5951e-02,  3.9882e-01],\n          [ 1.6584e-02, -1.0451e-02,  3.0943e-01,  ...,  1.6244e-02,\n           -2.1085e-04, -1.1208e+00],\n          [ 2.7422e-02, -1.0710e-02,  3.6722e-01,  ...,  1.5335e-02,\n            5.6557e-03, -1.2797e+00],\n          ...,\n          [-1.4060e-02,  1.3082e-02,  1.6347e-01,  ...,  1.9075e-02,\n           -3.1139e-02, -3.7721e-01],\n          [-1.5200e-02,  1.6485e-02,  1.9280e-01,  ...,  1.9420e-02,\n           -3.4022e-02, -4.3645e-01],\n          [-9.7093e-03,  1.0682e-02,  1.4393e-01,  ...,  1.8763e-02,\n           -2.7810e-02, -3.6306e-01]],\n\n         [[ 4.4143e-01,  2.4400e-01, -3.2840e-01,  ...,  2.9530e-01,\n            3.2580e-01, -3.2623e-01],\n          [-6.0548e-01, -3.3152e-01,  4.5078e-01,  ..., -4.2546e-01,\n           -4.5830e-01,  4.5789e-01],\n          [-6.2343e-01, -3.4368e-01,  4.6555e-01,  ..., -4.3955e-01,\n           -4.7309e-01,  4.7272e-01],\n          ...,\n          [-4.9011e-01, -2.7061e-01,  3.6599e-01,  ..., -3.4798e-01,\n           -3.7300e-01,  3.7234e-01],\n          [-5.3693e-01, -2.9872e-01,  4.0233e-01,  ..., -3.8188e-01,\n           -4.0951e-01,  4.0887e-01],\n          [-4.5511e-01, -2.5278e-01,  3.4081e-01,  ..., -3.2478e-01,\n           -3.4762e-01,  3.4701e-01]],\n\n         [[-2.3938e-01,  2.8601e-01,  2.6167e-01,  ...,  2.7155e-01,\n           -3.2029e-01,  3.8608e-01],\n          [ 3.0360e-01, -3.4324e-01, -3.0107e-01,  ..., -3.2031e-01,\n            3.6998e-01, -4.6108e-01],\n          [ 3.1466e-01, -3.5522e-01, -3.1234e-01,  ..., -3.3191e-01,\n            3.8254e-01, -4.7503e-01],\n          ...,\n          [ 2.7776e-01, -3.1020e-01, -2.7359e-01,  ..., -2.9041e-01,\n            3.3169e-01, -4.0883e-01],\n          [ 2.9428e-01, -3.2952e-01, -2.9075e-01,  ..., -3.0851e-01,\n            3.5299e-01, -4.3523e-01],\n          [ 2.5599e-01, -2.8596e-01, -2.5126e-01,  ..., -2.6724e-01,\n            3.0568e-01, -3.7819e-01]],\n\n         ...,\n\n         [[ 2.7790e-01,  2.7676e-01,  9.3440e-02,  ..., -6.5088e-02,\n            2.5924e-01,  2.0377e-01],\n          [-1.7902e-01, -2.6514e-01, -5.2792e-01,  ...,  3.7228e-01,\n           -6.5438e-01, -4.7810e-01],\n          [-1.9067e-01, -2.7811e-01, -5.4214e-01,  ...,  3.8361e-01,\n           -6.7257e-01, -4.9322e-01],\n          ...,\n          [-1.4294e-01, -2.1986e-01, -4.7422e-01,  ...,  3.3842e-01,\n           -5.6870e-01, -4.1799e-01],\n          [-1.7549e-01, -2.5598e-01, -5.1343e-01,  ...,  3.7010e-01,\n           -6.1846e-01, -4.5945e-01],\n          [-1.2436e-01, -1.9770e-01, -4.4876e-01,  ...,  3.2066e-01,\n           -5.3179e-01, -3.9005e-01]],\n\n         [[ 3.0377e-01,  4.1899e-01, -5.4118e-01,  ..., -7.5922e-01,\n           -4.8627e-01,  1.5126e-01],\n          [-8.5016e-01, -1.1325e+00,  2.3094e-01,  ...,  1.1364e+00,\n           -3.0742e-01,  1.3191e-01],\n          [-1.0597e+00, -1.0901e+00,  1.8408e-01,  ...,  1.2973e+00,\n           -6.1775e-01,  1.1021e-01],\n          ...,\n          [ 3.4851e-01, -4.9388e-01,  6.2499e-01,  ...,  3.0964e-01,\n            1.3107e+00,  1.0653e-01],\n          [ 3.5086e-01, -5.4550e-01,  6.5803e-01,  ...,  3.4854e-01,\n            1.4283e+00,  1.0308e-01],\n          [ 2.6960e-01, -5.1678e-01,  5.4576e-01,  ...,  2.4883e-01,\n            1.1296e+00,  1.1322e-01]],\n\n         [[-4.5113e-01,  5.3521e-01, -4.6153e-01,  ...,  6.0654e-01,\n            7.7049e-02,  1.0395e-01],\n          [ 1.0296e+00, -4.8976e-01,  3.9580e-01,  ..., -4.7937e-01,\n            7.6882e-02,  1.0079e-01],\n          [ 1.1374e+00, -4.9262e-01,  4.2927e-01,  ..., -5.0603e-01,\n            6.8360e-02,  1.1019e-01],\n          ...,\n          [ 4.9105e-01, -5.5869e-01,  2.5878e-01,  ..., -5.5058e-01,\n            7.2653e-02,  1.0467e-01],\n          [ 4.6910e-01, -6.7134e-01,  2.8748e-01,  ..., -6.2805e-01,\n            7.5881e-02,  1.0153e-01],\n          [ 5.1778e-01, -4.8022e-01,  2.2200e-01,  ..., -4.5862e-01,\n            7.9020e-02,  9.8995e-02]]],\n\n\n        [[[ 1.2099e-02, -6.4388e-03, -1.2435e-01,  ...,  1.6833e-02,\n           -1.7128e-02,  3.4817e-01],\n          [ 1.8942e-02, -1.3043e-02,  3.6242e-01,  ...,  1.5905e-02,\n            2.0668e-03, -1.3947e+00],\n          [ 3.1799e-02, -1.1412e-02,  3.4712e-01,  ...,  1.4992e-02,\n            8.7558e-03, -1.3216e+00],\n          ...,\n          [-1.7955e-02,  1.5628e-02,  1.9498e-01,  ...,  1.9252e-02,\n           -3.3565e-02, -3.9534e-01],\n          [-1.1142e-02,  1.1327e-02,  1.5611e-01,  ...,  1.8706e-02,\n           -2.9072e-02, -3.5492e-01],\n          [-1.2215e-02,  1.2543e-02,  1.0481e-01,  ...,  1.8861e-02,\n           -3.1957e-02, -1.5868e-01]],\n\n         [[ 4.1782e-01,  2.3011e-01, -3.1030e-01,  ...,  2.7826e-01,\n            3.0760e-01, -3.0803e-01],\n          [-6.8530e-01, -3.7631e-01,  5.1086e-01,  ..., -4.8133e-01,\n           -5.1876e-01,  5.1834e-01],\n          [-6.2864e-01, -3.4551e-01,  4.6872e-01,  ..., -4.4234e-01,\n           -4.7629e-01,  4.7592e-01],\n          ...,\n          [-5.2565e-01, -2.9413e-01,  3.9497e-01,  ..., -3.7556e-01,\n           -4.0209e-01,  4.0143e-01],\n          [-4.8648e-01, -2.6782e-01,  3.6279e-01,  ..., -3.4476e-01,\n           -3.6977e-01,  3.6914e-01],\n          [-3.7784e-01, -2.0981e-01,  2.8273e-01,  ..., -2.7076e-01,\n           -2.8913e-01,  2.8855e-01]],\n\n         [[-2.2355e-01,  2.6795e-01,  2.4532e-01,  ...,  2.5445e-01,\n           -3.0072e-01,  3.6283e-01],\n          [ 3.5799e-01, -4.0603e-01, -3.5740e-01,  ..., -3.7949e-01,\n            4.3855e-01, -5.4406e-01],\n          [ 3.0576e-01, -3.4689e-01, -3.0356e-01,  ..., -3.2332e-01,\n            3.7475e-01, -4.6891e-01],\n          ...,\n          [ 2.8620e-01, -3.2018e-01, -2.8238e-01,  ..., -2.9972e-01,\n            3.4273e-01, -4.2251e-01],\n          [ 2.7065e-01, -3.0317e-01, -2.6650e-01,  ..., -2.8333e-01,\n            3.2478e-01, -4.0241e-01],\n          [ 2.1638e-01, -2.3913e-01, -2.0998e-01,  ..., -2.2351e-01,\n            2.5396e-01, -3.1447e-01]],\n\n         ...,\n\n         [[ 2.6772e-01,  2.6468e-01,  7.9610e-02,  ..., -5.5355e-02,\n            2.3983e-01,  1.8864e-01],\n          [-2.1545e-01, -3.0929e-01, -5.7921e-01,  ...,  4.0690e-01,\n           -7.3068e-01, -5.3476e-01],\n          [-1.8358e-01, -2.7147e-01, -5.3599e-01,  ...,  3.7643e-01,\n           -6.6886e-01, -4.8750e-01],\n          ...,\n          [-1.6824e-01, -2.4732e-01, -5.0353e-01,  ...,  3.6319e-01,\n           -6.0382e-01, -4.4863e-01],\n          [-1.4263e-01, -2.1950e-01, -4.7390e-01,  ...,  3.3813e-01,\n           -5.6945e-01, -4.1778e-01],\n          [-9.3472e-02, -1.6145e-01, -4.0752e-01,  ...,  2.9099e-01,\n           -4.7325e-01, -3.4507e-01]],\n\n         [[ 2.7885e-01,  3.5775e-01, -5.0399e-01,  ..., -7.0197e-01,\n           -4.6232e-01,  1.4884e-01],\n          [-1.1401e+00, -1.5175e+00,  2.9815e-01,  ...,  1.5256e+00,\n           -3.9317e-01,  1.3024e-01],\n          [-1.1800e+00, -1.3106e+00,  2.8424e-01,  ...,  1.5077e+00,\n           -5.5846e-01,  1.1549e-01],\n          ...,\n          [ 4.9197e-01, -5.6932e-01,  8.0106e-01,  ...,  3.1808e-01,\n            1.7416e+00,  1.0566e-01],\n          [ 3.9805e-01, -4.1729e-01,  6.2209e-01,  ...,  2.3676e-01,\n            1.3684e+00,  1.0401e-01],\n          [ 6.5177e-01, -1.2696e-01,  6.4093e-01,  ..., -3.8864e-02,\n            1.7767e+00,  1.4770e-01]],\n\n         [[-4.0265e-01,  5.1285e-01, -4.3821e-01,  ...,  5.8221e-01,\n            7.7115e-02,  1.0375e-01],\n          [ 1.2054e+00, -5.1271e-01,  4.3973e-01,  ..., -5.1091e-01,\n            7.4184e-02,  1.0361e-01],\n          [ 1.2998e+00, -4.4495e-01,  4.4771e-01,  ..., -4.8111e-01,\n            6.7103e-02,  1.1084e-01],\n          ...,\n          [ 4.5918e-01, -6.4926e-01,  2.7663e-01,  ..., -6.2750e-01,\n            7.3949e-02,  1.0434e-01],\n          [ 4.3897e-01, -5.1185e-01,  2.2100e-01,  ..., -5.0951e-01,\n            8.7158e-02,  9.2688e-02],\n          [ 1.1820e-01, -3.6323e-01,  4.2444e-02,  ..., -3.2201e-01,\n            9.9421e-02,  8.9489e-02]]],\n\n\n        [[[ 1.1471e-02, -6.8225e-03, -1.4514e-01,  ...,  1.6884e-02,\n           -1.6137e-02,  4.0158e-01],\n          [ 6.1867e-03, -9.0610e-03,  3.4406e-01,  ...,  1.7032e-02,\n           -7.7414e-03, -1.1515e+00],\n          [ 2.3230e-02, -1.0694e-02,  2.7300e-01,  ...,  1.5772e-02,\n            1.9787e-03, -9.3100e-01],\n          ...,\n          [-1.9266e-02,  1.6198e-02,  2.2202e-01,  ...,  1.9551e-02,\n           -3.5013e-02, -5.3954e-01],\n          [-2.2643e-02,  1.2836e-02,  1.8710e-01,  ...,  1.9477e-02,\n           -3.2321e-02, -4.4823e-01],\n          [-2.2629e-02,  1.6460e-02,  2.1695e-01,  ...,  1.9833e-02,\n           -3.6965e-02, -5.3694e-01]],\n\n         [[ 4.4255e-01,  2.4463e-01, -3.2924e-01,  ...,  2.9608e-01,\n            3.2664e-01, -3.2707e-01],\n          [-6.2282e-01, -3.4744e-01,  4.6761e-01,  ..., -4.4214e-01,\n           -4.7516e-01,  4.7474e-01],\n          [-4.7312e-01, -2.5474e-01,  3.4997e-01,  ..., -3.3138e-01,\n           -3.5685e-01,  3.5648e-01],\n          ...,\n          [-6.0535e-01, -3.4122e-01,  4.5640e-01,  ..., -4.3274e-01,\n           -4.6386e-01,  4.6323e-01],\n          [-5.8051e-01, -3.2939e-01,  4.3879e-01,  ..., -4.1703e-01,\n           -4.4612e-01,  4.4545e-01],\n          [-6.2803e-01, -3.5693e-01,  4.7512e-01,  ..., -4.5070e-01,\n           -4.8266e-01,  4.8202e-01]],\n\n         [[-2.4017e-01,  2.8693e-01,  2.6249e-01,  ...,  2.7241e-01,\n           -3.2131e-01,  3.8733e-01],\n          [ 3.0442e-01, -3.4363e-01, -3.0180e-01,  ..., -3.2089e-01,\n            3.7002e-01, -4.6009e-01],\n          [ 2.2472e-01, -2.5343e-01, -2.1967e-01,  ..., -2.3525e-01,\n            2.7246e-01, -3.4401e-01],\n          ...,\n          [ 3.2652e-01, -3.6633e-01, -3.2404e-01,  ..., -3.4334e-01,\n            3.9305e-01, -4.8360e-01],\n          [ 3.2253e-01, -3.6107e-01, -3.1977e-01,  ..., -3.3866e-01,\n            3.8679e-01, -4.7444e-01],\n          [ 3.3626e-01, -3.7781e-01, -3.3419e-01,  ..., -3.5408e-01,\n            4.0568e-01, -4.9891e-01]],\n\n         ...,\n\n         [[ 2.7819e-01,  2.7714e-01,  9.3903e-02,  ..., -6.5359e-02,\n            2.6001e-01,  2.0430e-01],\n          [-1.9175e-01, -2.7808e-01, -5.4094e-01,  ...,  3.8495e-01,\n           -6.6696e-01, -4.9114e-01],\n          [-1.2109e-01, -1.9698e-01, -4.5028e-01,  ...,  3.1672e-01,\n           -5.4297e-01, -3.9325e-01],\n          ...,\n          [-2.0461e-01, -2.8984e-01, -5.5179e-01,  ...,  3.9814e-01,\n           -6.7343e-01, -5.0127e-01],\n          [-1.8202e-01, -2.6545e-01, -5.2573e-01,  ...,  3.7597e-01,\n           -6.4056e-01, -4.7392e-01],\n          [-2.1427e-01, -3.0106e-01, -5.6437e-01,  ...,  4.0743e-01,\n           -6.9013e-01, -5.1484e-01]],\n\n         [[ 3.0573e-01,  4.2487e-01, -5.4063e-01,  ..., -7.6264e-01,\n           -4.8315e-01,  1.5175e-01],\n          [-8.1154e-01, -1.0993e+00,  3.1891e-01,  ...,  1.1806e+00,\n           -1.3865e-01,  1.1102e-01],\n          [-8.8163e-01, -7.7697e-01,  3.4245e-02,  ...,  8.9978e-01,\n           -7.2986e-01,  1.0904e-01],\n          ...,\n          [ 3.3095e-01, -6.2868e-01,  8.0463e-01,  ...,  5.3153e-01,\n            1.6522e+00,  9.4137e-02],\n          [ 4.5802e-01, -6.7795e-01,  7.3348e-01,  ...,  3.3567e-01,\n            1.6515e+00,  9.4642e-02],\n          [ 3.8949e-01, -6.1829e-01,  8.4727e-01,  ...,  4.5774e-01,\n            1.8165e+00,  1.0371e-01]],\n\n         [[-4.5617e-01,  5.3059e-01, -4.6112e-01,  ...,  6.0249e-01,\n            7.6950e-02,  1.0410e-01],\n          [ 9.3220e-01, -6.2076e-01,  4.2161e-01,  ..., -5.8278e-01,\n            8.1718e-02,  9.6979e-02],\n          [ 8.9327e-01, -3.6694e-01,  3.0739e-01,  ..., -3.5766e-01,\n            7.2306e-02,  1.0514e-01],\n          ...,\n          [ 5.0478e-01, -8.1052e-01,  3.5714e-01,  ..., -7.6289e-01,\n            7.0763e-02,  1.0582e-01],\n          [ 4.8852e-01, -6.2392e-01,  2.7475e-01,  ..., -5.8752e-01,\n            9.3146e-02,  8.5818e-02],\n          [ 5.7595e-01, -7.8610e-01,  3.5496e-01,  ..., -7.3852e-01,\n            8.4418e-02,  9.2888e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-1.9793e-03,  2.6758e-03, -1.5176e-04,  ...,  3.0906e-03,\n            3.2075e-03,  4.8003e-03],\n          [-1.2848e-02, -4.4365e-02,  2.7738e-02,  ..., -1.4877e-02,\n            1.1315e-02, -4.4251e-03],\n          [ 1.5508e-02, -1.2412e-03,  1.5363e-02,  ..., -8.7690e-03,\n           -7.2659e-03,  3.5371e-02],\n          ...,\n          [-4.3733e-02, -2.0554e-03,  4.0823e-02,  ...,  4.5650e-02,\n            2.0127e-02, -5.7223e-03],\n          [-7.8647e-04,  2.3715e-02,  4.8956e-02,  ...,  3.1876e-02,\n           -2.0588e-02,  1.9078e-02],\n          [-1.2397e-02,  2.2756e-02,  1.9218e-02,  ...,  1.4431e-02,\n           -4.2506e-02,  4.1722e-02]],\n\n         [[-1.5453e-03,  1.7401e-04,  1.1769e-03,  ...,  9.0832e-04,\n           -1.5207e-04, -8.4701e-04],\n          [-7.9849e-03,  2.6640e-02, -6.3731e-02,  ..., -3.8745e-02,\n           -1.6054e-03,  2.2326e-02],\n          [ 1.5270e-02,  1.8329e-02, -8.8076e-03,  ...,  1.0532e-02,\n            1.7852e-02,  4.6446e-02],\n          ...,\n          [-2.6852e-02,  6.3593e-03,  2.7917e-02,  ...,  1.4592e-02,\n           -6.4271e-03, -2.4593e-02],\n          [ 3.2325e-03,  3.2915e-03,  9.0647e-03,  ...,  1.4237e-02,\n           -1.8789e-02,  2.4117e-02],\n          [-3.9541e-02,  4.7189e-02, -3.4740e-02,  ..., -2.8946e-02,\n            2.2422e-02, -3.1345e-03]],\n\n         [[ 7.7517e-03, -1.0633e-03,  3.6742e-03,  ...,  1.2551e-03,\n            1.1982e-03, -2.1499e-03],\n          [-9.5875e-02, -9.6757e-03,  1.9541e-02,  ..., -1.2141e-03,\n            1.0595e-02,  1.8986e-02],\n          [-1.4483e-03, -1.6472e-02,  8.4351e-03,  ...,  3.9603e-02,\n            3.0720e-03, -2.5561e-02],\n          ...,\n          [ 3.6794e-02,  1.9655e-03, -1.3722e-02,  ...,  2.2571e-02,\n           -3.3859e-02,  8.3448e-04],\n          [-1.7343e-02,  3.7676e-02, -1.8111e-02,  ...,  4.1909e-02,\n            1.3872e-02,  1.6244e-02],\n          [ 3.3630e-02,  2.4599e-02, -2.5876e-02,  ...,  1.2388e-02,\n           -9.3091e-03, -3.3230e-02]],\n\n         ...,\n\n         [[ 3.9316e-04,  3.5756e-04,  1.7241e-03,  ..., -3.5250e-03,\n            2.7736e-03, -4.6596e-04],\n          [ 5.4090e-02, -4.9953e-02, -5.8135e-02,  ..., -8.2082e-03,\n            4.0288e-03,  1.4323e-02],\n          [ 1.5596e-02,  8.8889e-03, -8.0963e-02,  ..., -2.2924e-02,\n            5.4970e-02,  1.4214e-02],\n          ...,\n          [-5.6706e-02,  3.5108e-02,  3.2224e-02,  ..., -3.8860e-02,\n           -5.6206e-02, -2.2527e-02],\n          [-7.9043e-03,  4.5698e-03,  4.8564e-02,  ..., -1.9719e-02,\n           -2.0519e-02,  1.7223e-03],\n          [-4.3554e-02,  1.2575e-02,  2.0761e-02,  ..., -6.0387e-03,\n            2.5597e-02, -1.2018e-02]],\n\n         [[ 8.5504e-03,  2.0663e-03, -3.0617e-03,  ...,  2.9981e-03,\n            7.4640e-02,  3.7714e-03],\n          [ 2.6934e-02,  6.1935e-02,  3.8534e-02,  ..., -2.7161e-02,\n           -1.9361e-01,  4.8917e-03],\n          [-1.1238e-02, -2.7557e-02,  5.7176e-02,  ...,  1.0759e-02,\n            2.0614e-02, -4.8355e-02],\n          ...,\n          [-3.4741e-02, -3.0393e-02,  7.9345e-03,  ...,  5.4270e-03,\n           -3.3255e-02,  1.1222e-02],\n          [-4.9144e-02, -2.5364e-03,  8.0379e-03,  ..., -1.2610e-02,\n           -3.6149e-02, -8.0798e-03],\n          [-1.0182e-02,  3.8016e-02,  2.3663e-02,  ...,  6.7551e-02,\n            1.1788e-01, -2.0127e-02]],\n\n         [[-2.2487e-03,  4.3629e-03,  4.5532e-03,  ..., -2.8200e-03,\n            2.6430e-03, -3.4304e-03],\n          [ 1.6660e-03,  5.2212e-02, -7.4283e-02,  ...,  4.9676e-02,\n            5.4612e-02,  1.1479e-02],\n          [-6.1504e-03,  4.2080e-02,  1.2390e-02,  ...,  4.3362e-02,\n            1.0816e-02, -1.5726e-03],\n          ...,\n          [ 8.1946e-02, -1.1622e-01,  6.3599e-02,  ...,  1.3292e-02,\n           -6.0426e-02, -2.6193e-02],\n          [ 1.3624e-01, -2.3562e-03,  3.8853e-04,  ...,  1.6620e-02,\n           -1.0749e-01, -9.4146e-04],\n          [ 3.5602e-02, -5.7780e-03,  3.6611e-02,  ...,  2.7211e-02,\n           -1.3461e-01, -5.1638e-03]]],\n\n\n        [[[-8.8994e-04,  5.8889e-03, -6.2962e-04,  ...,  1.8788e-03,\n            3.8897e-03,  4.5320e-03],\n          [ 1.6777e-02, -2.1870e-02,  3.0642e-02,  ..., -4.1769e-02,\n            1.1438e-02, -2.5833e-02],\n          [ 6.3804e-02, -1.7977e-02,  2.2831e-02,  ..., -7.9899e-02,\n           -4.3486e-02, -7.2998e-02],\n          ...,\n          [-1.3309e-02, -1.0141e-02, -1.3271e-02,  ...,  2.7405e-02,\n           -2.2729e-02,  7.0001e-02],\n          [ 2.6834e-02,  3.2962e-03, -5.9088e-02,  ...,  9.5453e-03,\n           -4.7826e-03,  2.0399e-02],\n          [-1.4668e-03, -3.0272e-02,  3.2531e-02,  ...,  2.4692e-02,\n           -3.6952e-02,  4.2890e-02]],\n\n         [[-1.5355e-04, -3.4226e-04,  1.5066e-03,  ...,  1.7568e-03,\n           -9.8330e-04, -1.3149e-03],\n          [-1.7334e-03,  8.9869e-03, -4.1807e-02,  ..., -2.7286e-02,\n           -2.7330e-02,  1.0342e-03],\n          [ 3.9175e-03, -6.8713e-03, -2.2936e-02,  ..., -7.5853e-02,\n           -1.3169e-02, -2.1148e-02],\n          ...,\n          [-3.8685e-04,  1.7011e-02, -2.4350e-02,  ..., -3.0854e-02,\n           -1.3143e-03, -2.4718e-02],\n          [-1.2639e-02,  5.2445e-02, -2.4025e-02,  ...,  1.1365e-02,\n           -4.5253e-03,  5.7331e-02],\n          [-2.0644e-02, -2.6634e-02,  4.3563e-03,  ...,  2.8189e-02,\n           -2.4346e-02, -8.3288e-03]],\n\n         [[ 1.0769e-02,  1.1266e-03,  2.6680e-03,  ...,  1.3685e-03,\n            1.2729e-03, -1.8639e-03],\n          [-6.2083e-02,  1.1927e-02,  7.1009e-03,  ...,  2.0496e-02,\n            2.3240e-02,  3.0060e-02],\n          [-4.4014e-02, -3.1815e-02,  8.3532e-03,  ...,  3.3014e-02,\n           -2.0195e-02,  1.8144e-02],\n          ...,\n          [ 3.9352e-03,  3.3368e-02, -1.9956e-02,  ..., -1.2568e-02,\n           -1.5166e-02, -2.4644e-02],\n          [ 1.4503e-04,  2.3926e-02, -1.0433e-02,  ...,  2.3855e-03,\n            2.3188e-02, -6.5716e-02],\n          [-1.1063e-02,  4.1786e-02, -1.7404e-02,  ..., -2.7454e-02,\n           -2.5300e-02,  7.8376e-03]],\n\n         ...,\n\n         [[-1.0434e-03,  2.2537e-03,  3.9005e-03,  ..., -4.4361e-03,\n            1.7575e-03, -6.2748e-04],\n          [ 1.4776e-02, -3.4330e-02,  3.3167e-02,  ..., -4.1174e-03,\n           -3.4797e-02,  5.1846e-03],\n          [ 1.3924e-02, -5.7054e-02, -3.7628e-03,  ...,  1.7570e-02,\n           -1.4650e-02, -3.7527e-03],\n          ...,\n          [ 3.0256e-02,  1.0231e-02,  2.8818e-02,  ...,  2.3142e-02,\n            1.3511e-02,  4.1667e-02],\n          [-6.4944e-03,  2.8678e-02, -8.1819e-03,  ..., -2.0606e-03,\n            6.8208e-02,  2.0581e-02],\n          [ 1.1141e-02, -1.4573e-02,  2.0830e-02,  ..., -7.8110e-03,\n           -1.4577e-02,  3.1915e-02]],\n\n         [[ 6.9104e-03,  1.9547e-03, -4.0290e-03,  ...,  3.3393e-03,\n            7.1524e-02,  1.1726e-03],\n          [-3.6075e-02,  2.8826e-02,  3.7236e-02,  ..., -2.8434e-02,\n           -1.8530e-01, -3.3561e-02],\n          [-1.6059e-02,  4.1454e-02, -7.1654e-03,  ..., -6.2145e-02,\n           -1.4255e-01, -2.6396e-02],\n          ...,\n          [ 1.6332e-02,  1.8595e-03, -1.9437e-02,  ..., -3.3783e-03,\n           -8.7219e-02, -1.0159e-01],\n          [ 2.5891e-02, -1.9274e-02,  3.4112e-02,  ...,  6.1153e-03,\n            1.3963e-01, -5.8687e-02],\n          [ 2.4813e-02, -2.4144e-02,  1.5543e-02,  ...,  1.0755e-03,\n           -1.3490e-02, -2.1481e-02]],\n\n         [[-2.0633e-03,  4.1931e-03,  3.8440e-03,  ..., -1.5304e-03,\n            4.5845e-03, -4.5423e-03],\n          [ 3.8525e-02,  1.1145e-02, -3.7449e-02,  ...,  5.4559e-02,\n           -5.4318e-02, -8.4742e-04],\n          [-2.9278e-02,  2.1433e-02, -3.4012e-02,  ...,  4.2689e-02,\n           -9.7895e-02,  4.5778e-02],\n          ...,\n          [-9.7331e-02, -5.7809e-02, -5.4189e-03,  ..., -1.6350e-02,\n           -3.3263e-02,  8.0532e-03],\n          [-1.5121e-02, -3.4609e-02, -1.9466e-02,  ..., -1.9357e-03,\n            6.7202e-02,  6.7218e-03],\n          [-6.6708e-02, -5.3173e-02,  1.5419e-03,  ...,  1.2250e-02,\n            8.3639e-02,  4.6620e-04]]],\n\n\n        [[[-1.2476e-03,  4.7318e-03,  2.7614e-04,  ...,  2.1343e-03,\n            2.3159e-03,  6.4910e-03],\n          [ 1.7478e-02, -4.8706e-02,  3.1760e-02,  ..., -4.2523e-03,\n           -3.3079e-02,  2.0545e-02],\n          [ 5.4797e-02, -2.3573e-02,  3.1071e-03,  ..., -5.2159e-03,\n           -2.8161e-02, -2.1676e-02],\n          ...,\n          [-3.4152e-03, -6.9686e-03,  4.7495e-02,  ...,  4.5691e-03,\n           -2.1026e-02,  1.2460e-02],\n          [-1.0538e-02, -6.3757e-03,  3.7226e-02,  ...,  1.8354e-02,\n           -5.4028e-02,  6.5241e-02],\n          [-3.7132e-02, -2.2809e-02, -1.8260e-02,  ...,  3.8981e-02,\n           -4.3098e-02,  4.2456e-02]],\n\n         [[ 1.6858e-04, -5.1221e-04,  5.5179e-04,  ...,  1.6789e-03,\n           -3.9203e-04, -9.9759e-05],\n          [ 1.9838e-02,  6.6711e-03,  3.6084e-03,  ...,  3.4767e-02,\n            2.5339e-02,  3.2652e-02],\n          [-1.8001e-02,  4.7830e-02,  1.0187e-02,  ..., -8.1151e-03,\n            3.1160e-02, -1.0914e-02],\n          ...,\n          [ 2.3952e-02, -8.1847e-03, -1.1552e-02,  ..., -2.7806e-02,\n           -4.4235e-03, -6.3927e-03],\n          [-2.4981e-03,  5.8340e-03, -2.7342e-02,  ..., -1.5784e-02,\n           -3.0298e-02, -3.3763e-02],\n          [-6.4620e-03,  2.6437e-02, -5.0266e-02,  ..., -1.9760e-03,\n           -1.5011e-02,  2.5482e-02]],\n\n         [[ 1.0458e-02,  1.0992e-03,  1.4276e-03,  ...,  8.2627e-04,\n            6.9914e-04, -1.7074e-03],\n          [-1.1575e-03,  2.7444e-02,  3.1592e-02,  ...,  3.5108e-02,\n            4.7684e-02,  4.2930e-02],\n          [-4.3544e-02,  1.8920e-02,  1.2692e-02,  ..., -6.0622e-03,\n            1.3449e-02,  1.4487e-02],\n          ...,\n          [ 4.7018e-03,  2.4905e-02, -1.7330e-02,  ...,  1.7572e-02,\n           -2.8602e-02, -2.8178e-02],\n          [ 1.0136e-02,  6.1193e-02, -3.8407e-02,  ..., -3.6059e-02,\n            1.1851e-02,  6.0831e-03],\n          [ 8.5518e-03,  5.6658e-02, -3.5789e-02,  ..., -1.5683e-02,\n            3.7833e-02, -4.1954e-02]],\n\n         ...,\n\n         [[-3.7173e-04,  2.1266e-03,  2.4810e-03,  ..., -5.3063e-03,\n            3.6574e-03,  1.6936e-03],\n          [ 9.3019e-03,  1.1207e-03,  1.0036e-02,  ..., -5.0516e-02,\n           -5.1882e-03, -1.5528e-02],\n          [ 6.5137e-03,  1.2244e-02, -1.9879e-02,  ..., -3.1141e-02,\n           -1.1490e-02, -4.1213e-02],\n          ...,\n          [-1.2284e-02,  2.4139e-02,  2.7027e-02,  ..., -2.7011e-03,\n           -2.1510e-02,  2.6039e-02],\n          [-7.1027e-03,  6.6249e-02,  6.1190e-02,  ..., -2.4874e-02,\n           -1.7516e-02,  4.6628e-02],\n          [-4.0664e-02,  5.0344e-02,  4.7447e-02,  ..., -6.4763e-03,\n            2.3607e-02,  1.3920e-02]],\n\n         [[ 8.2473e-03,  2.1268e-03, -4.3260e-03,  ...,  4.1041e-03,\n            7.7745e-02,  1.0882e-03],\n          [-1.7952e-02, -4.2736e-02,  3.8453e-02,  ...,  4.7659e-02,\n           -1.0883e-01, -1.4218e-02],\n          [-3.9615e-02, -2.9171e-02,  5.9262e-04,  ..., -2.9449e-02,\n           -1.1433e-01,  1.4203e-02],\n          ...,\n          [ 4.2797e-03, -2.4860e-02, -2.3665e-02,  ...,  2.4682e-02,\n           -2.0376e-02, -3.5586e-02],\n          [ 9.3352e-03,  3.2862e-02,  3.4108e-03,  ..., -1.0844e-02,\n           -5.1205e-02, -4.9083e-02],\n          [-2.2398e-03, -2.4570e-02,  2.7409e-02,  ...,  6.0194e-02,\n            1.0952e-02, -1.0025e-01]],\n\n         [[-1.6807e-03,  3.7449e-03,  4.5903e-03,  ..., -2.8462e-03,\n            3.5788e-03, -4.1812e-03],\n          [-1.7309e-02,  4.8998e-03, -5.6516e-02,  ...,  7.6237e-02,\n            1.5996e-02,  7.6613e-03],\n          [-4.9970e-02,  6.9113e-02, -1.3658e-02,  ...,  3.4699e-02,\n           -5.9379e-03,  4.6013e-02],\n          ...,\n          [-6.2514e-03, -4.0856e-02,  1.5712e-02,  ..., -4.7181e-03,\n           -1.8418e-02,  2.2987e-03],\n          [-4.1065e-02, -3.1287e-02, -4.9273e-02,  ...,  5.8249e-03,\n            1.5404e-02, -6.0404e-03],\n          [-3.1664e-02, -3.4073e-03,  1.1910e-02,  ...,  1.9753e-03,\n            5.3879e-02, -5.9921e-02]]],\n\n\n        ...,\n\n\n        [[[-1.5752e-03,  5.1957e-03, -8.5913e-04,  ...,  2.1886e-03,\n            4.5904e-03,  5.4870e-03],\n          [-3.9368e-03, -3.8067e-02, -6.3734e-03,  ..., -2.2036e-03,\n           -1.3432e-02, -5.9652e-03],\n          [ 3.5078e-02,  7.0370e-03,  7.0823e-03,  ..., -7.8886e-02,\n            4.7051e-02, -5.3843e-02],\n          ...,\n          [ 1.8501e-02, -2.5251e-02, -7.5566e-03,  ...,  2.1066e-02,\n           -2.8284e-02,  4.3193e-02],\n          [ 2.9123e-02, -1.2590e-02, -4.6674e-03,  ...,  2.7559e-02,\n           -4.9601e-02,  3.7368e-02],\n          [-3.9981e-02, -1.5563e-02, -1.4927e-02,  ...,  2.7824e-02,\n           -2.1692e-02,  5.0192e-02]],\n\n         [[ 2.5502e-04, -1.5668e-03, -8.2724e-04,  ...,  1.1326e-03,\n           -1.4812e-03, -7.8056e-04],\n          [-7.1355e-03,  7.1503e-03, -7.3012e-02,  ...,  7.5611e-03,\n           -6.4857e-02,  4.0160e-02],\n          [-3.4218e-03,  7.6992e-03, -7.6413e-03,  ..., -3.0331e-03,\n            1.9494e-02,  1.5596e-02],\n          ...,\n          [ 1.3929e-02, -8.8262e-03,  1.3731e-02,  ...,  3.5719e-03,\n           -1.3324e-02,  1.5179e-02],\n          [-1.4392e-02,  2.5082e-02,  2.0629e-02,  ..., -4.2762e-03,\n            3.2993e-02, -2.5849e-02],\n          [-3.7045e-02,  5.4820e-02, -2.3918e-02,  ...,  1.2239e-03,\n            4.9645e-03, -1.4356e-02]],\n\n         [[ 1.0658e-02,  3.6041e-04,  2.1723e-03,  ..., -4.5289e-04,\n            9.5514e-04, -2.4288e-03],\n          [-1.7069e-02, -1.5327e-02, -1.5983e-03,  ..., -1.3846e-02,\n            5.1135e-02, -1.2563e-02],\n          [-5.5197e-02, -1.2475e-02,  9.2324e-03,  ...,  5.7690e-03,\n           -5.9941e-03,  1.2961e-02],\n          ...,\n          [ 2.7536e-03, -1.1859e-02,  3.0056e-02,  ...,  3.7245e-02,\n           -1.8652e-02, -5.1781e-02],\n          [-4.1832e-03,  2.4226e-02,  3.0010e-03,  ..., -1.3738e-02,\n           -1.6820e-02, -3.5582e-02],\n          [-3.0038e-02,  3.5307e-02, -2.6743e-02,  ..., -3.1232e-02,\n           -7.9100e-03, -4.0431e-02]],\n\n         ...,\n\n         [[-7.6465e-04,  3.5911e-03,  3.9658e-03,  ..., -5.6328e-03,\n            3.2806e-03,  1.3483e-03],\n          [-2.1893e-05,  2.9919e-02,  2.8547e-02,  ..., -2.2000e-02,\n            3.5996e-03,  2.1069e-02],\n          [ 1.0504e-02, -2.1826e-02,  4.6492e-04,  ..., -6.1364e-03,\n           -5.2734e-03, -3.8162e-02],\n          ...,\n          [ 2.6682e-02,  1.2307e-03,  4.5629e-03,  ...,  2.2696e-02,\n            4.7461e-02,  2.1113e-02],\n          [ 9.0182e-04,  3.6300e-02, -1.4127e-03,  ...,  9.4783e-03,\n            4.5875e-02,  2.1763e-02],\n          [-6.4707e-04,  2.2514e-02,  1.5649e-02,  ..., -5.9000e-03,\n            2.5700e-02,  2.4091e-02]],\n\n         [[ 5.2969e-03,  4.6081e-03, -4.2026e-03,  ...,  5.2181e-03,\n            7.4419e-02,  1.5383e-03],\n          [-4.4868e-02,  6.7166e-02,  3.1850e-02,  ..., -1.4115e-02,\n           -1.1308e-01, -2.5913e-02],\n          [-3.6989e-02, -2.1412e-02,  1.1491e-02,  ...,  1.7171e-02,\n           -5.0079e-02, -6.2044e-02],\n          ...,\n          [ 7.7732e-02, -5.6908e-02,  8.3594e-03,  ...,  7.0241e-03,\n           -1.7938e-02, -1.4982e-02],\n          [ 4.5086e-02, -4.2100e-02, -3.9030e-02,  ...,  1.0780e-02,\n           -2.2411e-01, -3.1125e-03],\n          [ 1.0254e-01, -5.1150e-02,  4.7781e-02,  ...,  5.0652e-02,\n           -1.4214e-01, -4.2355e-02]],\n\n         [[-3.0151e-03,  4.4996e-03,  4.9451e-03,  ..., -3.8932e-03,\n            7.3007e-03, -5.4993e-03],\n          [-9.3884e-03,  3.3339e-02,  2.5699e-03,  ...,  3.9412e-02,\n            4.6946e-02, -1.0606e-02],\n          [ 1.0784e-02, -4.6453e-02, -3.7896e-02,  ...,  1.0857e-02,\n           -6.7171e-03, -2.1781e-02],\n          ...,\n          [ 1.4485e-02, -1.0193e-01, -1.9354e-02,  ...,  7.8369e-04,\n           -4.6445e-03, -3.0903e-03],\n          [-2.3138e-03, -6.4989e-02, -3.0287e-02,  ..., -1.2014e-02,\n            1.6897e-02,  5.0790e-02],\n          [ 2.5076e-02, -1.1097e-01, -1.8942e-02,  ...,  4.9749e-02,\n            4.2642e-02, -1.9755e-02]]],\n\n\n        [[[-1.5150e-03,  2.6119e-03,  2.4455e-03,  ...,  1.4946e-03,\n            1.8508e-03,  6.6028e-03],\n          [ 1.0183e-03, -2.3795e-02, -4.0242e-02,  ...,  4.9083e-03,\n           -2.6174e-02, -1.2822e-02],\n          [ 3.9451e-02, -3.8273e-02,  8.9411e-03,  ..., -4.8083e-02,\n            1.5712e-02, -2.1217e-02],\n          ...,\n          [-3.9926e-02, -3.8733e-03, -2.0364e-02,  ...,  1.6609e-02,\n           -3.1393e-02,  2.5407e-02],\n          [-3.6061e-03,  2.6958e-02, -1.7468e-02,  ...,  3.4941e-02,\n            7.2476e-03,  1.6748e-02],\n          [-2.8563e-02,  4.1789e-02,  3.0643e-02,  ...,  3.5267e-02,\n            2.0082e-02,  1.2462e-02]],\n\n         [[ 3.4322e-04, -8.7104e-04,  1.1762e-03,  ...,  3.5621e-03,\n            7.2240e-04,  7.1406e-05],\n          [-2.6569e-02,  5.2512e-02, -7.2645e-02,  ..., -4.0599e-02,\n            4.2990e-03,  1.1754e-02],\n          [-1.5547e-02,  1.4434e-02, -3.0910e-02,  ...,  1.1932e-02,\n            3.5233e-02, -7.8093e-04],\n          ...,\n          [-5.5989e-03, -3.9406e-03, -2.7941e-02,  ..., -2.9843e-03,\n           -1.4959e-02, -3.6161e-02],\n          [-1.6065e-02,  6.0330e-03,  2.0845e-03,  ...,  1.3562e-02,\n            7.9020e-03, -1.5633e-02],\n          [-1.1359e-02, -2.1954e-02, -4.4280e-03,  ...,  1.3730e-02,\n           -2.9985e-02, -7.3204e-03]],\n\n         [[ 1.1397e-02,  1.8773e-04,  2.9631e-03,  ..., -8.0286e-04,\n            1.2294e-05, -6.4235e-03],\n          [-1.1076e-02,  3.1190e-02, -1.5903e-02,  ..., -4.9669e-03,\n            5.2421e-02,  2.0055e-02],\n          [-2.6004e-02,  1.7533e-02,  1.2260e-03,  ...,  8.8325e-03,\n            1.5037e-02, -6.4296e-03],\n          ...,\n          [-5.0762e-02,  3.4413e-02, -2.5435e-02,  ..., -2.9466e-02,\n            1.1897e-02,  4.3188e-03],\n          [ 3.3005e-02,  4.7042e-02, -1.2276e-02,  ..., -1.1181e-02,\n           -7.7854e-03, -3.3827e-02],\n          [-1.8784e-02,  4.0284e-02, -1.4491e-02,  ..., -3.2578e-03,\n            3.2124e-03,  1.6079e-02]],\n\n         ...,\n\n         [[-4.5985e-03,  4.0706e-03,  3.4747e-03,  ..., -4.9092e-03,\n            4.7782e-03,  1.5123e-03],\n          [ 5.9643e-03,  9.1503e-04,  2.8937e-02,  ..., -7.1554e-02,\n            1.6768e-02, -1.7813e-03],\n          [ 5.7575e-03,  2.8790e-02,  9.3944e-03,  ..., -2.3229e-02,\n           -1.2403e-02, -3.5542e-02],\n          ...,\n          [ 1.5760e-02,  1.2635e-02,  4.1567e-02,  ...,  1.2360e-02,\n           -2.3326e-02,  2.9291e-02],\n          [-2.6756e-03,  5.5986e-02,  4.4014e-02,  ..., -1.9016e-02,\n           -1.0865e-02, -9.7733e-03],\n          [-1.0476e-02,  1.0878e-02,  5.2477e-02,  ..., -1.4210e-03,\n           -3.7858e-02,  5.5325e-03]],\n\n         [[ 5.2283e-03,  2.6699e-03, -4.8377e-03,  ...,  6.0105e-03,\n            6.7771e-02,  1.6340e-03],\n          [-8.0147e-02,  7.5267e-03,  3.9303e-02,  ...,  2.1057e-02,\n           -4.6367e-02,  4.2192e-03],\n          [-4.7461e-02, -2.3164e-02, -8.2010e-04,  ..., -4.6007e-02,\n           -1.7009e-01, -2.7080e-02],\n          ...,\n          [ 2.8871e-02, -1.1172e-02,  1.6814e-02,  ...,  1.0192e-02,\n           -2.0442e-01, -5.8128e-02],\n          [ 2.1767e-02, -7.5882e-02,  5.4595e-02,  ...,  7.5478e-02,\n           -1.7975e-02, -2.7998e-02],\n          [ 2.5730e-02, -4.7867e-02,  9.4036e-03,  ...,  3.2207e-02,\n            3.4701e-02, -9.7836e-03]],\n\n         [[-1.7612e-03,  5.5272e-03,  6.5681e-03,  ..., -1.9756e-03,\n            8.7418e-03, -2.2640e-03],\n          [ 6.0487e-02,  2.9274e-02, -7.1629e-03,  ...,  7.1805e-02,\n            9.7056e-02,  1.7115e-02],\n          [-6.3348e-02,  2.2109e-02, -8.8544e-03,  ..., -7.8132e-03,\n            5.3775e-02, -6.7735e-03],\n          ...,\n          [-8.0187e-02,  1.6133e-02,  2.6435e-02,  ..., -3.0606e-02,\n           -1.3648e-02,  7.2457e-03],\n          [-4.3291e-02, -1.4755e-02,  5.2311e-02,  ...,  2.0278e-02,\n            2.1733e-02,  1.2601e-02],\n          [ 1.1938e-02,  3.1482e-02,  7.3338e-03,  ...,  5.3818e-02,\n            2.6654e-02, -6.9988e-03]]],\n\n\n        [[[-2.2364e-03,  3.1645e-03,  4.3289e-04,  ...,  5.0159e-04,\n            4.6704e-03,  4.1699e-03],\n          [ 1.0395e-02, -5.7599e-02,  3.1257e-02,  ..., -3.2270e-02,\n            1.5143e-02, -2.6933e-02],\n          [ 2.8190e-04, -1.2063e-02,  2.9151e-02,  ..., -7.9319e-03,\n            2.4319e-02, -5.3468e-02],\n          ...,\n          [-2.6750e-02, -7.9587e-03,  4.5349e-02,  ...,  6.9859e-02,\n           -4.4934e-02,  3.0268e-02],\n          [-1.9966e-03, -4.4943e-02, -5.3890e-04,  ...,  4.2940e-02,\n           -4.4942e-02,  3.4385e-02],\n          [-7.2289e-03, -1.5740e-02,  1.7551e-02,  ...,  5.5741e-02,\n           -3.6061e-02,  2.1247e-02]],\n\n         [[-7.1880e-04, -1.8874e-03, -2.6902e-04,  ...,  4.1134e-04,\n           -1.1369e-03, -3.9314e-03],\n          [-2.8177e-02, -8.7393e-03, -2.4474e-02,  ..., -2.1065e-02,\n           -4.1022e-03, -3.4080e-02],\n          [-6.3474e-03, -1.3764e-02, -7.4086e-03,  ...,  4.1792e-03,\n           -7.1863e-04, -8.4452e-03],\n          ...,\n          [ 1.2606e-02,  2.6392e-03,  1.6514e-02,  ..., -8.4130e-04,\n            6.9404e-03, -5.4034e-03],\n          [ 1.0886e-02, -3.4615e-03,  1.1032e-02,  ..., -9.5624e-04,\n            1.9316e-02, -1.8480e-02],\n          [-4.1250e-02,  3.2482e-02,  1.0718e-02,  ...,  4.4117e-03,\n            2.9133e-02, -1.3650e-02]],\n\n         [[ 8.4137e-03, -3.1252e-04,  1.9630e-03,  ..., -9.9681e-04,\n            4.3824e-05, -2.3937e-03],\n          [-1.5713e-02,  1.1317e-02,  8.0091e-04,  ..., -2.3409e-02,\n           -1.4905e-02,  5.0948e-03],\n          [-1.1899e-02, -3.2752e-02,  2.4480e-02,  ..., -2.9767e-02,\n            7.2729e-03, -4.3570e-02],\n          ...,\n          [ 1.2291e-02,  2.3719e-02,  5.2038e-03,  ...,  1.6133e-02,\n           -5.0091e-03,  8.0391e-03],\n          [ 2.0286e-02,  2.0463e-02, -1.7577e-02,  ...,  1.5970e-02,\n            2.3600e-02, -4.8740e-02],\n          [-1.6521e-02,  4.5473e-02, -2.3719e-02,  ..., -3.4516e-02,\n            2.6717e-04, -1.3709e-02]],\n\n         ...,\n\n         [[ 7.3501e-04,  1.7383e-03,  2.3881e-03,  ..., -2.5988e-03,\n            1.4820e-03,  1.7830e-03],\n          [ 1.1331e-02,  4.2996e-03,  5.4732e-03,  ..., -1.0237e-03,\n            6.3032e-03,  5.0664e-05],\n          [-1.5849e-02,  1.0272e-03, -3.2959e-02,  ..., -3.2345e-02,\n            2.6163e-03, -4.0754e-02],\n          ...,\n          [-8.7789e-03,  2.0184e-02,  3.9980e-02,  ..., -6.4583e-03,\n            5.8709e-03, -2.6117e-03],\n          [ 4.8392e-03,  3.3641e-02,  6.4927e-02,  ...,  1.4185e-03,\n            1.5520e-02, -8.2759e-04],\n          [-3.1444e-02,  2.0956e-02,  2.7575e-02,  ...,  3.6024e-03,\n            2.2890e-02, -2.0187e-02]],\n\n         [[ 8.0673e-03,  3.5675e-03, -5.0370e-03,  ...,  1.4502e-03,\n            7.2284e-02,  1.9922e-03],\n          [-6.5183e-03,  5.1851e-02, -1.5408e-02,  ..., -1.3739e-02,\n           -9.4750e-02, -2.9560e-02],\n          [ 2.6051e-03, -2.4114e-02, -4.2631e-02,  ..., -8.8000e-03,\n           -6.5391e-02, -4.7148e-02],\n          ...,\n          [-2.5326e-02,  2.3617e-03, -2.1298e-02,  ..., -2.6907e-02,\n           -1.3705e-01, -7.9098e-03],\n          [-2.3402e-02, -1.3458e-02, -2.0632e-03,  ...,  7.2447e-02,\n            7.5792e-02, -1.5347e-02],\n          [-2.8352e-02,  1.2696e-02, -3.5727e-03,  ...,  1.4697e-02,\n           -1.1266e-01, -3.8170e-02]],\n\n         [[-6.4788e-03,  6.7929e-03,  4.9211e-03,  ..., -2.7148e-03,\n            9.0439e-03, -2.6919e-03],\n          [ 1.8691e-02,  4.2845e-02, -2.6775e-02,  ...,  4.7813e-02,\n            1.0357e-01,  1.9318e-02],\n          [-4.3592e-02,  8.8304e-02,  5.1348e-02,  ...,  4.9564e-02,\n            9.0286e-02,  2.1566e-02],\n          ...,\n          [ 4.0303e-02, -3.2679e-02, -2.0532e-02,  ...,  8.9352e-03,\n            9.7429e-02,  2.7721e-02],\n          [ 9.2306e-02, -1.1348e-01, -1.9432e-03,  ...,  3.6433e-02,\n            1.4785e-02, -8.4139e-03],\n          [ 4.8571e-02, -9.1664e-03, -2.6849e-02,  ...,  1.7603e-02,\n            4.3196e-02, -8.1647e-03]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-1.4671e-01, -1.6080e-01, -4.4703e-02,  ..., -1.3974e-01,\n           -1.4520e-01, -2.8933e-01],\n          [ 5.4002e-01,  7.4051e-01,  1.9181e+00,  ...,  4.2331e-01,\n            5.1381e-01, -1.5021e+00],\n          [ 4.8576e-01,  6.5538e-01,  1.6924e+00,  ...,  3.8705e-01,\n            4.7653e-01, -1.4124e+00],\n          ...,\n          [ 4.8705e-01,  6.3699e-01,  1.1766e+00,  ...,  3.7485e-01,\n            4.2989e-01, -1.1879e+00],\n          [ 3.4161e-01,  4.8176e-01,  9.0680e-01,  ...,  2.2590e-01,\n            2.4314e-01, -8.8137e-01],\n          [ 4.8272e-01,  6.5236e-01,  1.2508e+00,  ...,  3.5701e-01,\n            4.1270e-01, -1.1850e+00]],\n\n         [[ 6.5085e-02, -6.9234e-02, -6.5263e-03,  ..., -7.4242e-02,\n            1.0405e-01, -1.1716e-01],\n          [-6.4727e-01,  7.3515e-01,  5.2059e-01,  ...,  6.3238e-01,\n           -4.6460e-01,  8.7966e-01],\n          [-5.8507e-01,  6.6272e-01,  4.7850e-01,  ...,  5.7084e-01,\n           -4.1808e-01,  7.8562e-01],\n          ...,\n          [-5.6722e-01,  6.4537e-01,  4.6606e-01,  ...,  5.5268e-01,\n           -4.0412e-01,  7.6508e-01],\n          [-5.1957e-01,  5.9418e-01,  4.2706e-01,  ...,  5.0519e-01,\n           -3.6312e-01,  7.0563e-01],\n          [-5.9848e-01,  6.8242e-01,  4.8586e-01,  ...,  5.8359e-01,\n           -4.2645e-01,  8.1454e-01]],\n\n         [[-2.9014e-01,  6.2394e-03, -1.9294e-01,  ..., -6.3191e-02,\n            8.3539e-03,  9.7681e-02],\n          [ 1.2189e+00,  4.4581e-02,  7.8315e-01,  ..., -1.1339e-01,\n            3.8291e-02, -2.9482e-01],\n          [ 9.6892e-01,  2.2671e-02,  5.6366e-01,  ..., -8.2595e-02,\n            1.3057e-02, -3.2378e-01],\n          ...,\n          [-4.6681e-02,  1.5248e-03, -1.1211e-01,  ..., -4.8610e-02,\n            1.0765e-02, -1.2502e+00],\n          [-1.2460e-01, -1.1575e-02, -1.4068e-01,  ..., -3.7019e-02,\n           -1.5998e-02, -1.4455e+00],\n          [ 5.1534e-02, -3.5186e-02, -3.6920e-02,  ..., -4.9298e-02,\n           -2.2989e-03, -1.2630e+00]],\n\n         ...,\n\n         [[ 1.3071e-01, -2.5651e-02, -1.6388e-01,  ..., -4.1731e-02,\n           -4.8842e-03,  3.8170e-03],\n          [-9.3577e-01,  2.1682e-01,  9.6492e-01,  ...,  3.3194e-01,\n           -5.0507e-03,  3.4401e-03],\n          [-8.0789e-01,  2.2088e-01,  8.2707e-01,  ...,  3.1722e-01,\n           -5.0138e-03,  3.6093e-03],\n          ...,\n          [-7.4603e-01,  3.3778e-01,  7.5547e-01,  ...,  4.1176e-01,\n           -4.4013e-03,  3.6912e-03],\n          [-7.5001e-01,  2.8937e-01,  7.6332e-01,  ...,  3.6744e-01,\n           -4.5450e-03,  3.4975e-03],\n          [-8.0129e-01,  3.1692e-01,  8.1614e-01,  ...,  4.0075e-01,\n           -4.6429e-03,  3.4013e-03]],\n\n         [[-2.7546e-02,  7.9853e-03, -2.6585e-02,  ..., -2.6804e-02,\n            4.6448e-01, -9.3319e-04],\n          [ 4.8018e-02,  1.1368e-02, -4.5303e-02,  ..., -2.3656e-02,\n           -2.1955e+00,  2.8430e-01],\n          [ 3.0721e-02,  2.2710e-02, -4.0697e-02,  ..., -1.0946e-02,\n           -1.5337e+00,  1.0670e-01],\n          ...,\n          [-1.5154e-02,  4.7826e-02, -5.8328e-02,  ..., -3.0467e-02,\n            1.4150e+00, -3.1856e-02],\n          [-6.6828e-02, -3.7147e-03, -5.1164e-02,  ..., -8.7393e-02,\n            1.1692e+00,  5.3264e-01],\n          [-7.1441e-03,  1.5382e-02, -5.4865e-02,  ..., -2.8966e-02,\n            1.2915e+00,  2.6619e-01]],\n\n         [[-1.3735e-01,  1.7961e-01, -3.5090e-01,  ..., -2.3148e-01,\n           -2.1824e-01,  2.2875e-01],\n          [ 4.8011e-01, -4.1053e-01,  3.3776e-01,  ...,  4.9198e-01,\n            4.1909e-01, -4.3643e-01],\n          [ 4.3451e-01, -3.6753e-01,  2.8519e-01,  ...,  4.3591e-01,\n            3.7159e-01, -3.8621e-01],\n          ...,\n          [ 4.2228e-01, -3.5574e-01,  2.7250e-01,  ...,  4.2303e-01,\n            3.5918e-01, -3.7327e-01],\n          [ 3.6427e-01, -2.9976e-01,  2.0937e-01,  ...,  3.5744e-01,\n            2.9969e-01, -3.1168e-01],\n          [ 4.3362e-01, -3.6589e-01,  2.8706e-01,  ...,  4.3915e-01,\n            3.7140e-01, -3.8673e-01]]],\n\n\n        [[[-1.4829e-01, -1.6308e-01, -4.9228e-02,  ..., -1.4080e-01,\n           -1.4639e-01, -2.8728e-01],\n          [ 3.9639e-01,  5.6296e-01,  1.5661e+00,  ...,  2.9498e-01,\n            3.5554e-01, -1.2300e+00],\n          [ 5.2937e-01,  7.3443e-01,  1.8645e+00,  ...,  4.0574e-01,\n            4.9339e-01, -1.4729e+00],\n          ...,\n          [ 5.6139e-01,  7.4327e-01,  1.4345e+00,  ...,  4.2923e-01,\n            4.9027e-01, -1.3062e+00],\n          [ 4.5101e-01,  6.0124e-01,  1.2000e+00,  ...,  3.4296e-01,\n            3.9769e-01, -1.1581e+00],\n          [ 5.8992e-01,  7.9427e-01,  1.6182e+00,  ...,  4.4561e-01,\n            5.0667e-01, -1.3584e+00]],\n\n         [[ 6.7047e-02, -7.1482e-02, -7.9384e-03,  ..., -7.6185e-02,\n            1.0558e-01, -1.1997e-01],\n          [-5.7375e-01,  6.4919e-01,  4.7107e-01,  ...,  5.5964e-01,\n           -4.0986e-01,  7.6732e-01],\n          [-6.5786e-01,  7.4762e-01,  5.2818e-01,  ...,  6.4284e-01,\n           -4.7291e-01,  8.9494e-01],\n          ...,\n          [-6.5460e-01,  7.4553e-01,  5.2844e-01,  ...,  6.3926e-01,\n           -4.7206e-01,  8.9178e-01],\n          [-5.6257e-01,  6.3808e-01,  4.6542e-01,  ...,  5.4828e-01,\n           -4.0261e-01,  7.5389e-01],\n          [-7.1453e-01,  8.1154e-01,  5.7535e-01,  ...,  6.9883e-01,\n           -5.2198e-01,  9.7026e-01]],\n\n         [[-2.9592e-01,  6.5653e-03, -1.9641e-01,  ..., -6.3023e-02,\n            8.2116e-03,  9.7969e-02],\n          [ 1.0112e+00,  1.5241e-02,  6.4137e-01,  ..., -7.8766e-02,\n           -7.7055e-04, -3.5891e-01],\n          [ 1.1460e+00,  1.4486e-02,  6.4957e-01,  ..., -8.9949e-02,\n            8.1880e-03, -4.5450e-01],\n          ...,\n          [ 3.0746e-01, -4.8667e-04,  1.4459e-01,  ..., -4.4180e-02,\n           -4.6313e-03, -1.1489e+00],\n          [ 9.4290e-02, -4.1223e-03, -2.4405e-02,  ..., -3.8731e-02,\n           -5.3247e-03, -1.1530e+00],\n          [ 2.3273e-01,  3.3922e-02,  9.3227e-02,  ..., -7.7502e-02,\n            2.4689e-02, -1.1681e+00]],\n\n         ...,\n\n         [[ 1.3357e-01, -2.5981e-02, -1.6694e-01,  ..., -4.2320e-02,\n           -4.8812e-03,  3.8221e-03],\n          [-7.7469e-01,  1.4349e-01,  7.9726e-01,  ...,  2.3925e-01,\n           -4.9076e-03,  3.4737e-03],\n          [-8.7976e-01,  1.9512e-01,  9.0760e-01,  ...,  2.9923e-01,\n           -4.9157e-03,  3.4336e-03],\n          ...,\n          [-9.1010e-01,  3.5619e-01,  9.3037e-01,  ...,  4.5315e-01,\n           -4.5609e-03,  3.5289e-03],\n          [-7.6013e-01,  3.0578e-01,  7.7263e-01,  ...,  3.8804e-01,\n           -4.6131e-03,  3.5439e-03],\n          [-9.8303e-01,  3.4208e-01,  1.0096e+00,  ...,  4.4765e-01,\n           -4.8168e-03,  3.0851e-03]],\n\n         [[-2.8059e-02,  8.6730e-03, -2.6210e-02,  ..., -2.7268e-02,\n            4.6415e-01, -6.1854e-03],\n          [-4.1190e-02, -2.4788e-02, -4.0736e-02,  ..., -6.5345e-02,\n           -2.4833e+00,  5.0385e-01],\n          [ 8.2904e-03, -1.6499e-02, -2.8760e-02,  ..., -1.8745e-02,\n           -2.0789e+00,  3.3298e-01],\n          ...,\n          [ 1.4981e-02,  1.2784e-02, -6.5017e-02,  ..., -1.9508e-02,\n            1.0777e+00,  1.5106e-01],\n          [-2.6327e-02,  2.4109e-02, -3.3439e-02,  ..., -5.2020e-02,\n            1.2646e+00,  7.8678e-02],\n          [-3.1846e-02, -1.8355e-02, -4.4879e-02,  ..., -4.1555e-02,\n            3.9876e-01,  4.8464e-01]],\n\n         [[-1.3899e-01,  1.8118e-01, -3.5276e-01,  ..., -2.3345e-01,\n           -2.1994e-01,  2.3054e-01],\n          [ 4.0391e-01, -3.3830e-01,  2.5108e-01,  ...,  4.0017e-01,\n            3.3995e-01, -3.5319e-01],\n          [ 4.6975e-01, -4.0006e-01,  3.2788e-01,  ...,  4.8230e-01,\n            4.0888e-01, -4.2631e-01],\n          ...,\n          [ 4.6457e-01, -3.9495e-01,  3.2298e-01,  ...,  4.7756e-01,\n            4.0374e-01, -4.2097e-01],\n          [ 4.1436e-01, -3.4831e-01,  2.6314e-01,  ...,  4.1296e-01,\n            3.5086e-01, -3.6449e-01],\n          [ 5.1626e-01, -4.4502e-01,  3.7882e-01,  ...,  5.3537e-01,\n            4.5658e-01, -4.7545e-01]]],\n\n\n        [[[-1.4825e-01, -1.6315e-01, -5.1252e-02,  ..., -1.4084e-01,\n           -1.4647e-01, -2.8644e-01],\n          [ 4.5434e-01,  6.2146e-01,  1.6433e+00,  ...,  3.5751e-01,\n            4.4112e-01, -1.3748e+00],\n          [ 5.1993e-01,  7.1107e-01,  1.7947e+00,  ...,  4.0463e-01,\n            4.9133e-01, -1.4471e+00],\n          ...,\n          [ 4.7275e-01,  6.3948e-01,  1.1596e+00,  ...,  3.4209e-01,\n            3.8612e-01, -1.1284e+00],\n          [ 4.8529e-01,  6.4811e-01,  1.2239e+00,  ...,  3.6383e-01,\n            4.1615e-01, -1.1799e+00],\n          [ 4.3582e-01,  5.8147e-01,  1.2122e+00,  ...,  3.3611e-01,\n            3.9197e-01, -1.1678e+00]],\n\n         [[ 6.7475e-02, -7.1906e-02, -8.3185e-03,  ..., -7.6615e-02,\n            1.0598e-01, -1.2043e-01],\n          [-5.8087e-01,  6.5652e-01,  4.7777e-01,  ...,  5.6680e-01,\n           -4.1658e-01,  7.7587e-01],\n          [-6.5197e-01,  7.3866e-01,  5.2840e-01,  ...,  6.3715e-01,\n           -4.7174e-01,  8.7982e-01],\n          ...,\n          [-6.0159e-01,  6.8724e-01,  4.8694e-01,  ...,  5.8653e-01,\n           -4.2797e-01,  8.2139e-01],\n          [-5.7703e-01,  6.5828e-01,  4.6995e-01,  ...,  5.6231e-01,\n           -4.0931e-01,  7.8456e-01],\n          [-5.4221e-01,  6.1504e-01,  4.4826e-01,  ...,  5.2816e-01,\n           -3.8482e-01,  7.2683e-01]],\n\n         [[-2.9691e-01,  6.5250e-03, -1.9907e-01,  ..., -6.3910e-02,\n            8.9514e-03,  9.7558e-02],\n          [ 1.0143e+00,  1.8587e-02,  7.3537e-01,  ..., -8.4189e-02,\n            1.4304e-02, -8.6184e-02],\n          [ 1.3866e+00,  2.3118e-02,  9.4869e-01,  ..., -8.1782e-02,\n            1.4847e-02, -1.9789e-01],\n          ...,\n          [-5.9676e-02, -3.1908e-03, -9.6412e-02,  ..., -5.6543e-02,\n            3.9731e-03, -1.3338e+00],\n          [ 3.3779e-01, -1.9564e-02,  1.8999e-01,  ..., -2.1773e-02,\n           -2.1611e-02, -1.0431e+00],\n          [ 2.2705e-01, -8.0319e-03,  6.5942e-02,  ..., -3.7839e-02,\n           -1.0979e-02, -8.9779e-01]],\n\n         ...,\n\n         [[ 1.3373e-01, -2.5507e-02, -1.6716e-01,  ..., -4.1967e-02,\n           -4.8794e-03,  3.8265e-03],\n          [-7.7933e-01,  1.8349e-01,  7.9937e-01,  ...,  2.7967e-01,\n           -4.7897e-03,  3.7679e-03],\n          [-8.9105e-01,  2.2022e-01,  9.1715e-01,  ...,  3.2531e-01,\n           -4.8926e-03,  3.5249e-03],\n          ...,\n          [-8.1447e-01,  3.1846e-01,  8.2983e-01,  ...,  4.0202e-01,\n           -4.3908e-03,  3.5346e-03],\n          [-7.9513e-01,  3.1912e-01,  8.0929e-01,  ...,  4.0209e-01,\n           -4.4958e-03,  3.5821e-03],\n          [-6.9964e-01,  2.8211e-01,  7.0939e-01,  ...,  3.5977e-01,\n           -4.7118e-03,  3.6857e-03]],\n\n         [[-2.7844e-02,  9.1950e-03, -2.6636e-02,  ..., -2.7058e-02,\n            4.7908e-01, -7.6962e-03],\n          [ 1.8224e-02,  2.5125e-02, -3.9601e-02,  ...,  2.1109e-03,\n           -1.9439e+00,  6.4806e-02],\n          [-1.4068e-02,  1.2807e-02, -1.7371e-02,  ..., -4.5422e-02,\n           -2.1563e+00,  2.4560e-01],\n          ...,\n          [-1.2094e-02,  5.2986e-03, -5.2643e-02,  ..., -4.5684e-02,\n            8.3775e-01,  3.0918e-01],\n          [-1.0916e-03,  2.1979e-02, -4.4921e-02,  ..., -2.4993e-02,\n            8.7357e-01,  1.5935e-01],\n          [-1.6337e-02,  3.5260e-02, -2.6869e-02,  ..., -2.0371e-02,\n            8.6107e-01,  2.6193e-03]],\n\n         [[-1.3936e-01,  1.8153e-01, -3.5314e-01,  ..., -2.3384e-01,\n           -2.2031e-01,  2.3092e-01],\n          [ 4.3668e-01, -3.7016e-01,  2.8609e-01,  ...,  4.3619e-01,\n            3.7339e-01, -3.8755e-01],\n          [ 4.6882e-01, -4.0003e-01,  3.2459e-01,  ...,  4.7793e-01,\n            4.0726e-01, -4.2378e-01],\n          ...,\n          [ 4.3215e-01, -3.6437e-01,  2.8589e-01,  ...,  4.3813e-01,\n            3.6999e-01, -3.8534e-01],\n          [ 3.9013e-01, -3.2365e-01,  2.4038e-01,  ...,  3.9097e-01,\n            3.2700e-01, -3.4106e-01],\n          [ 3.7111e-01, -3.0599e-01,  2.1727e-01,  ...,  3.6586e-01,\n            3.0690e-01, -3.1959e-01]]],\n\n\n        ...,\n\n\n        [[[-1.5249e-01, -1.6879e-01, -6.0433e-02,  ..., -1.4406e-01,\n           -1.5013e-01, -2.8021e-01],\n          [ 4.4615e-01,  6.0321e-01,  1.5867e+00,  ...,  3.5676e-01,\n            4.3706e-01, -1.3354e+00],\n          [ 5.1993e-01,  7.0848e-01,  1.9274e+00,  ...,  4.1646e-01,\n            5.1506e-01, -1.5193e+00],\n          ...,\n          [ 4.9307e-01,  6.5643e-01,  1.2599e+00,  ...,  3.7420e-01,\n            4.3121e-01, -1.2097e+00],\n          [ 5.7057e-01,  7.5272e-01,  1.4082e+00,  ...,  4.3714e-01,\n            5.0217e-01, -1.3263e+00],\n          [ 4.6645e-01,  6.1353e-01,  1.1635e+00,  ...,  3.5830e-01,\n            4.1366e-01, -1.1669e+00]],\n\n         [[ 7.2461e-02, -7.7579e-02, -1.1984e-02,  ..., -8.1556e-02,\n            1.0993e-01, -1.2749e-01],\n          [-5.6201e-01,  6.3435e-01,  4.6427e-01,  ...,  5.4816e-01,\n           -4.0195e-01,  7.4744e-01],\n          [-6.6649e-01,  7.4956e-01,  5.4704e-01,  ...,  6.5201e-01,\n           -4.8961e-01,  8.8519e-01],\n          ...,\n          [-5.7902e-01,  6.5924e-01,  4.7371e-01,  ...,  5.6437e-01,\n           -4.1267e-01,  7.8395e-01],\n          [-6.6275e-01,  7.5502e-01,  5.3375e-01,  ...,  6.4732e-01,\n           -4.7802e-01,  9.0398e-01],\n          [-5.5134e-01,  6.2702e-01,  4.5411e-01,  ...,  5.3702e-01,\n           -3.9130e-01,  7.4292e-01]],\n\n         [[-2.9922e-01,  7.3254e-03, -1.9723e-01,  ..., -6.3585e-02,\n            8.6767e-03,  1.0737e-01],\n          [ 1.0951e+00,  1.6105e-02,  7.7632e-01,  ..., -6.3125e-02,\n           -6.3057e-03, -6.7339e-02],\n          [ 1.2629e+00,  1.1181e-02,  8.0600e-01,  ..., -7.4799e-02,\n            8.8878e-03, -9.1791e-02],\n          ...,\n          [-2.0373e-02, -2.1954e-02, -3.9395e-02,  ..., -2.5707e-02,\n           -1.9560e-02, -1.1932e+00],\n          [ 2.0170e-01, -1.2262e-03,  7.2893e-02,  ..., -3.9586e-02,\n            4.4883e-03, -1.2595e+00],\n          [ 1.3983e-01, -6.8764e-03,  1.3597e-02,  ..., -3.7055e-02,\n           -2.7486e-03, -1.1633e+00]],\n\n         ...,\n\n         [[ 1.4113e-01, -2.8609e-02, -1.7492e-01,  ..., -4.5776e-02,\n           -4.8849e-03,  3.8240e-03],\n          [-7.2863e-01,  1.8513e-01,  7.4477e-01,  ...,  2.7471e-01,\n           -5.0245e-03,  3.5076e-03],\n          [-8.4145e-01,  1.8509e-01,  8.6663e-01,  ...,  2.8687e-01,\n           -5.1359e-03,  3.2874e-03],\n          ...,\n          [-7.9854e-01,  3.4247e-01,  8.1188e-01,  ...,  4.2315e-01,\n           -4.5147e-03,  3.3725e-03],\n          [-9.2125e-01,  3.7622e-01,  9.4129e-01,  ...,  4.7256e-01,\n           -4.3281e-03,  3.5420e-03],\n          [-7.3232e-01,  3.2039e-01,  7.4195e-01,  ...,  3.9698e-01,\n           -4.4970e-03,  3.6567e-03]],\n\n         [[-2.8092e-02,  8.5310e-03, -2.5712e-02,  ..., -2.6724e-02,\n            4.7439e-01, -5.3842e-03],\n          [ 1.2296e-02,  8.2626e-03, -2.7760e-02,  ..., -1.0056e-02,\n           -1.5886e+00,  1.5245e-01],\n          [-2.1760e-02,  2.4849e-03, -6.6033e-03,  ..., -4.8730e-02,\n           -2.3787e+00,  2.5309e-01],\n          ...,\n          [-8.0675e-03,  3.4264e-02, -2.9129e-02,  ..., -4.6212e-02,\n            1.3252e+00,  5.6595e-02],\n          [-3.6200e-02,  2.1732e-02, -3.4233e-02,  ..., -3.7766e-02,\n            1.0728e+00,  5.1893e-02],\n          [-2.2760e-03,  5.3043e-02, -3.3608e-02,  ..., -4.1832e-02,\n            1.3543e+00,  2.9122e-02]],\n\n         [[-1.4381e-01,  1.8579e-01, -3.5812e-01,  ..., -2.3907e-01,\n           -2.2491e-01,  2.3572e-01],\n          [ 3.9609e-01, -3.3053e-01,  2.4301e-01,  ...,  3.9197e-01,\n            3.3212e-01, -3.4535e-01],\n          [ 5.0600e-01, -4.3723e-01,  3.6120e-01,  ...,  5.1419e-01,\n            4.4434e-01, -4.6084e-01],\n          ...,\n          [ 4.3287e-01, -3.6569e-01,  2.8468e-01,  ...,  4.3597e-01,\n            3.7020e-01, -3.8494e-01],\n          [ 4.5447e-01, -3.8460e-01,  3.1371e-01,  ...,  4.6872e-01,\n            3.9386e-01, -4.1141e-01],\n          [ 3.9026e-01, -3.2459e-01,  2.3815e-01,  ...,  3.8754e-01,\n            3.2651e-01, -3.3975e-01]]],\n\n\n        [[[-1.5285e-01, -1.6931e-01, -6.5637e-02,  ..., -1.4453e-01,\n           -1.5086e-01, -2.7754e-01],\n          [ 4.8853e-01,  6.6474e-01,  1.7824e+00,  ...,  3.9018e-01,\n            4.7963e-01, -1.4239e+00],\n          [ 5.7115e-01,  7.7562e-01,  2.0563e+00,  ...,  4.5841e-01,\n            5.6037e-01, -1.5888e+00],\n          ...,\n          [ 5.0099e-01,  6.6741e-01,  1.2399e+00,  ...,  3.7567e-01,\n            4.2734e-01, -1.1895e+00],\n          [ 4.8416e-01,  6.3809e-01,  1.1619e+00,  ...,  3.6918e-01,\n            4.1750e-01, -1.1552e+00],\n          [ 1.6196e-01,  2.6025e-01,  4.5842e-01,  ...,  7.1795e-02,\n            6.4135e-02, -5.5859e-01]],\n\n         [[ 7.2248e-02, -7.7374e-02, -1.1700e-02,  ..., -8.1344e-02,\n            1.0967e-01, -1.2734e-01],\n          [-6.2046e-01,  7.0020e-01,  5.0777e-01,  ...,  6.0617e-01,\n           -4.4880e-01,  8.2900e-01],\n          [-6.9346e-01,  7.8250e-01,  5.6269e-01,  ...,  6.7860e-01,\n           -5.0775e-01,  9.3032e-01],\n          ...,\n          [-5.9774e-01,  6.8164e-01,  4.8571e-01,  ...,  5.8282e-01,\n           -4.2617e-01,  8.1288e-01],\n          [-5.9513e-01,  6.7524e-01,  4.8954e-01,  ...,  5.8050e-01,\n           -4.2860e-01,  7.9989e-01],\n          [-3.8897e-01,  4.4606e-01,  3.3156e-01,  ...,  3.7572e-01,\n           -2.5971e-01,  5.2170e-01]],\n\n         [[-2.9796e-01,  6.1526e-03, -1.9757e-01,  ..., -6.2754e-02,\n            8.2120e-03,  1.0695e-01],\n          [ 1.5148e+00, -4.9333e-04,  9.6045e-01,  ..., -6.3295e-02,\n           -4.3946e-03, -9.9155e-02],\n          [ 1.5115e+00,  2.5764e-02,  9.3841e-01,  ..., -7.8671e-02,\n            1.3259e-02, -1.5764e-01],\n          ...,\n          [ 1.7492e-01, -3.1821e-03,  6.8102e-02,  ..., -5.4656e-02,\n            6.7971e-03, -1.1616e+00],\n          [-3.8997e-02, -2.6860e-02, -4.6099e-02,  ..., -3.0024e-02,\n           -1.2417e-02, -1.1991e+00],\n          [-1.0040e-01,  2.9398e-03, -3.6622e-03,  ..., -5.7229e-02,\n            3.9140e-03, -1.1537e+00]],\n\n         ...,\n\n         [[ 1.4291e-01, -2.8058e-02, -1.7687e-01,  ..., -4.5681e-02,\n           -4.8845e-03,  3.8261e-03],\n          [-7.7762e-01,  1.8339e-01,  7.9831e-01,  ...,  2.7636e-01,\n           -5.2222e-03,  3.5277e-03],\n          [-9.1504e-01,  2.1046e-01,  9.4413e-01,  ...,  3.2104e-01,\n           -5.2242e-03,  3.3460e-03],\n          ...,\n          [-8.1952e-01,  3.2752e-01,  8.3493e-01,  ...,  4.1340e-01,\n           -4.3933e-03,  3.4717e-03],\n          [-7.6344e-01,  3.4761e-01,  7.7419e-01,  ...,  4.2449e-01,\n           -4.5707e-03,  3.5074e-03],\n          [-5.6106e-01,  2.4786e-01,  5.6355e-01,  ...,  3.0387e-01,\n           -4.7297e-03,  3.4237e-03]],\n\n         [[-2.8038e-02,  9.9280e-03, -2.5535e-02,  ..., -2.7241e-02,\n            4.7472e-01, -2.1022e-03],\n          [ 7.5671e-03,  3.5542e-03, -2.7747e-02,  ..., -1.6880e-02,\n           -1.7835e+00,  1.2384e-01],\n          [-2.1248e-02, -6.4057e-03, -9.8801e-03,  ..., -4.1803e-02,\n           -2.3250e+00,  2.6766e-01],\n          ...,\n          [ 6.6291e-03,  1.9587e-02, -7.1767e-02,  ..., -1.9323e-02,\n            1.1410e+00,  1.3749e-01],\n          [ 2.3610e-03,  3.9565e-02, -6.1561e-02,  ..., -1.5401e-02,\n            1.7137e+00,  2.6759e-02],\n          [-2.7084e-02, -2.9654e-02, -5.4390e-02,  ..., -5.7020e-02,\n            2.0094e+00,  4.7960e-01]],\n\n         [[-1.4381e-01,  1.8578e-01, -3.5813e-01,  ..., -2.3908e-01,\n           -2.2491e-01,  2.3573e-01],\n          [ 4.6609e-01, -3.9812e-01,  3.1923e-01,  ...,  4.7123e-01,\n            4.0386e-01, -4.1955e-01],\n          [ 5.3287e-01, -4.6219e-01,  3.9309e-01,  ...,  5.4855e-01,\n            4.7261e-01, -4.9106e-01],\n          ...,\n          [ 4.1896e-01, -3.5150e-01,  2.7179e-01,  ...,  4.2362e-01,\n            3.5656e-01, -3.7161e-01],\n          [ 4.3188e-01, -3.6492e-01,  2.8314e-01,  ...,  4.3419e-01,\n            3.6908e-01, -3.8358e-01],\n          [ 2.7560e-01, -2.1569e-01,  1.0861e-01,  ...,  2.5079e-01,\n            2.0766e-01, -2.1499e-01]]],\n\n\n        [[[-1.3518e-01, -1.4610e-01, -2.1639e-02,  ..., -1.3051e-01,\n           -1.3504e-01, -3.0560e-01],\n          [ 4.2160e-01,  5.7675e-01,  1.4547e+00,  ...,  3.2490e-01,\n            3.8992e-01, -1.2324e+00],\n          [ 4.8675e-01,  6.5734e-01,  1.7076e+00,  ...,  3.8868e-01,\n            4.7643e-01, -1.4258e+00],\n          ...,\n          [ 5.7225e-01,  7.6413e-01,  1.4593e+00,  ...,  4.3305e-01,\n            4.8902e-01, -1.2941e+00],\n          [ 5.4777e-01,  7.2518e-01,  1.4040e+00,  ...,  4.2104e-01,\n            4.8639e-01, -1.3023e+00],\n          [ 5.1035e-01,  6.8272e-01,  1.3044e+00,  ...,  3.8349e-01,\n            4.3636e-01, -1.2112e+00]],\n\n         [[ 5.3722e-02, -5.6302e-02,  1.7972e-03,  ..., -6.2981e-02,\n            9.5046e-02, -1.0105e-01],\n          [-5.2039e-01,  5.9237e-01,  4.2756e-01,  ...,  5.0641e-01,\n           -3.6396e-01,  7.0202e-01],\n          [-5.7440e-01,  6.5100e-01,  4.6967e-01,  ...,  5.6024e-01,\n           -4.0888e-01,  7.7161e-01],\n          ...,\n          [-6.7583e-01,  7.7019e-01,  5.4292e-01,  ...,  6.6026e-01,\n           -4.8807e-01,  9.2338e-01],\n          [-6.5421e-01,  7.4238e-01,  5.3295e-01,  ...,  6.3908e-01,\n           -4.7547e-01,  8.8388e-01],\n          [-6.1240e-01,  6.9770e-01,  4.9787e-01,  ...,  5.9739e-01,\n           -4.3888e-01,  8.3203e-01]],\n\n         [[-2.8960e-01,  6.0038e-03, -1.9279e-01,  ..., -6.3616e-02,\n            8.6405e-03,  7.8911e-02],\n          [ 1.0800e+00,  1.3138e-03,  8.3139e-01,  ..., -6.2621e-02,\n           -2.0893e-03, -1.4141e-01],\n          [ 9.7705e-01,  1.3749e-02,  5.3734e-01,  ..., -6.3157e-02,\n           -3.1227e-03, -2.7931e-01],\n          ...,\n          [ 4.6229e-01, -1.2384e-02,  2.5725e-01,  ..., -4.0831e-02,\n           -1.6484e-03, -1.2318e+00],\n          [ 3.0826e-01, -1.3640e-02,  1.7153e-01,  ..., -3.3700e-02,\n           -1.3978e-02, -1.1549e+00],\n          [ 2.3791e-01, -4.9333e-03,  1.9648e-01,  ..., -4.0900e-02,\n           -1.8931e-03, -1.0796e+00]],\n\n         ...,\n\n         [[ 1.1565e-01, -2.0212e-02, -1.4806e-01,  ..., -3.4731e-02,\n           -4.8701e-03,  3.8173e-03],\n          [-7.1686e-01,  1.9556e-01,  7.3206e-01,  ...,  2.7553e-01,\n           -4.8764e-03,  3.5549e-03],\n          [-7.6355e-01,  1.8641e-01,  7.8316e-01,  ...,  2.7674e-01,\n           -4.9567e-03,  3.7614e-03],\n          ...,\n          [-9.2459e-01,  3.5794e-01,  9.4651e-01,  ...,  4.5535e-01,\n           -4.6277e-03,  3.4200e-03],\n          [-8.4232e-01,  3.3157e-01,  8.5977e-01,  ...,  4.1947e-01,\n           -4.6287e-03,  3.6340e-03],\n          [-8.4329e-01,  3.3919e-01,  8.5988e-01,  ...,  4.2786e-01,\n           -4.5496e-03,  3.4917e-03]],\n\n         [[-2.5565e-02,  8.7070e-03, -2.7085e-02,  ..., -2.6247e-02,\n            4.5819e-01, -1.0596e-03],\n          [ 3.4204e-02,  1.7831e-02, -5.2835e-02,  ...,  1.1493e-02,\n           -1.6162e+00,  1.7942e-01],\n          [ 3.0775e-02,  4.0540e-02, -2.6881e-02,  ...,  6.0553e-03,\n           -1.9110e+00,  3.6006e-02],\n          ...,\n          [-3.9018e-02,  9.3456e-03, -1.8693e-02,  ..., -4.4486e-02,\n            9.4256e-01,  2.7053e-01],\n          [-3.0803e-03,  4.1403e-02, -2.7809e-02,  ..., -2.8698e-02,\n            1.2028e+00,  8.6305e-02],\n          [-8.4907e-03,  1.6817e-02, -3.5588e-02,  ..., -1.0074e-02,\n            1.2040e+00,  2.2552e-01]],\n\n         [[-1.2790e-01,  1.7061e-01, -3.4030e-01,  ..., -2.2031e-01,\n           -2.0847e-01,  2.1854e-01],\n          [ 3.7447e-01, -3.0944e-01,  2.2041e-01,  ...,  3.6892e-01,\n            3.1019e-01, -3.2284e-01],\n          [ 4.1944e-01, -3.5281e-01,  2.6931e-01,  ...,  4.1970e-01,\n            3.5628e-01, -3.7059e-01],\n          ...,\n          [ 4.8654e-01, -4.1612e-01,  3.4714e-01,  ...,  5.0283e-01,\n            4.2631e-01, -4.4438e-01],\n          [ 4.7695e-01, -4.0759e-01,  3.3463e-01,  ...,  4.8884e-01,\n            4.1592e-01, -4.3293e-01],\n          [ 4.3786e-01, -3.6988e-01,  2.9210e-01,  ...,  4.4468e-01,\n            3.7585e-01, -3.9145e-01]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-2.3367e-03, -2.8440e-03,  1.7376e-03,  ...,  1.4524e-03,\n            3.0027e-03,  5.0714e-04],\n          [ 1.4002e-02, -2.2730e-02, -1.2166e-03,  ...,  5.3749e-02,\n            8.7466e-03, -8.5443e-03],\n          [ 1.5568e-02, -1.3014e-02, -2.9296e-02,  ...,  3.1588e-02,\n            4.4023e-03,  5.3629e-02],\n          ...,\n          [ 1.5130e-02, -2.9964e-02, -7.9265e-03,  ...,  1.0067e-01,\n            4.8694e-02,  8.1297e-02],\n          [ 6.3752e-03,  1.6450e-02, -6.1122e-02,  ..., -9.4651e-03,\n           -3.0008e-02,  3.1138e-02],\n          [-1.3120e-02, -3.7145e-02,  4.4477e-04,  ..., -3.6449e-02,\n           -3.6575e-02,  5.4502e-03]],\n\n         [[ 6.6077e-04,  4.1335e-05, -1.7365e-03,  ..., -1.6535e-03,\n            1.0695e-03,  1.9920e-04],\n          [-3.5689e-02, -4.2135e-02, -6.9850e-03,  ...,  4.3083e-02,\n           -4.8027e-02,  6.1509e-04],\n          [-5.3646e-02,  7.6932e-03, -4.9740e-02,  ...,  5.3995e-02,\n           -3.1389e-02, -2.2037e-02],\n          ...,\n          [ 2.9236e-02, -3.4339e-02,  4.6010e-02,  ..., -2.6245e-02,\n            5.5893e-02, -1.6551e-02],\n          [-9.7518e-03,  2.8437e-02, -3.5936e-02,  ..., -5.6067e-03,\n           -3.0827e-02,  2.3716e-02],\n          [-5.1872e-02,  4.5729e-02, -1.1182e-02,  ..., -5.6094e-02,\n            5.7764e-02, -2.1677e-02]],\n\n         [[ 1.5410e-03, -1.7213e-03, -3.2749e-03,  ...,  3.5526e-03,\n           -3.5555e-03,  9.6038e-04],\n          [ 1.2855e-02, -9.4928e-02,  7.9947e-02,  ..., -7.8483e-02,\n           -3.6401e-01,  5.4202e-02],\n          [-1.4526e-02, -9.0485e-04,  1.0682e-01,  ..., -8.9607e-03,\n           -6.1264e-02,  6.4158e-02],\n          ...,\n          [ 1.0267e-01,  1.1724e-01,  5.2134e-02,  ..., -7.8260e-02,\n           -2.1642e-03, -7.7196e-02],\n          [-1.0431e-01,  1.4947e-01,  2.2446e-02,  ...,  3.2065e-02,\n            4.2484e-01, -2.6371e-02],\n          [ 8.2000e-02, -1.7047e-01,  7.5337e-02,  ...,  1.0539e-01,\n           -3.6471e-02,  8.8302e-03]],\n\n         ...,\n\n         [[ 1.0508e-04, -1.4821e-04, -1.1132e-03,  ...,  2.8018e-03,\n            2.5863e-03,  1.0809e-03],\n          [-5.6518e-02, -6.9394e-02,  1.1010e-03,  ...,  6.5823e-02,\n            8.4012e-03, -1.2338e-01],\n          [-8.5924e-03, -2.1499e-02,  2.0783e-02,  ..., -1.6898e-02,\n            2.5746e-02, -2.8607e-02],\n          ...,\n          [-1.0127e-02,  4.8740e-02, -7.3470e-04,  ...,  4.4570e-02,\n            1.4458e-02,  2.1481e-02],\n          [-1.0586e-02, -7.8385e-03,  1.2897e-02,  ...,  1.2774e-02,\n            4.5828e-03, -2.5831e-02],\n          [ 3.8208e-02, -6.8693e-02,  3.6235e-02,  ..., -3.3891e-02,\n            1.9107e-02,  2.7684e-02]],\n\n         [[-7.0145e-04,  1.9860e-03, -8.8562e-05,  ...,  5.9198e-03,\n            3.7491e-03,  3.1803e-04],\n          [ 5.6028e-03,  3.7051e-03,  1.9942e-02,  ..., -4.1703e-02,\n           -6.0778e-02, -2.4691e-02],\n          [-3.2813e-02, -1.3400e-03,  4.2530e-02,  ..., -6.0499e-02,\n           -1.7791e-02, -4.6997e-02],\n          ...,\n          [ 5.2988e-02,  2.8698e-02,  8.9848e-03,  ...,  4.7989e-03,\n           -5.4016e-03,  2.7702e-02],\n          [-3.5744e-02,  4.0715e-02,  2.9325e-02,  ..., -1.5734e-02,\n            2.1817e-02, -2.8042e-02],\n          [ 1.1415e-01,  2.7412e-03,  9.7348e-02,  ..., -2.8318e-02,\n            2.3167e-02,  3.9621e-02]],\n\n         [[ 8.9834e-04,  5.3358e-04,  4.0961e-03,  ..., -2.1531e-06,\n            3.1946e-03,  2.1499e-03],\n          [ 2.3572e-02, -9.9899e-03, -4.1551e-02,  ...,  4.0185e-02,\n           -3.1126e-02, -4.4143e-02],\n          [ 3.0576e-02, -4.1624e-02,  8.1602e-03,  ...,  6.5582e-02,\n           -8.8599e-03, -2.7609e-02],\n          ...,\n          [-1.0812e-02, -8.8173e-03,  3.1004e-02,  ..., -1.2603e-03,\n           -5.5684e-03, -5.0891e-02],\n          [-1.5484e-02, -5.5379e-02,  1.9460e-02,  ..., -8.9837e-02,\n           -3.9840e-02, -1.2414e-03],\n          [ 6.4878e-02, -1.4230e-02, -2.2684e-02,  ...,  7.5253e-02,\n            6.2331e-02,  5.7874e-02]]],\n\n\n        [[[-2.9002e-03, -2.9728e-03,  1.9037e-03,  ...,  1.3948e-03,\n            3.4657e-03,  1.9524e-03],\n          [-2.6546e-03,  8.3013e-03, -3.9628e-02,  ..., -8.5748e-02,\n           -5.0993e-02,  6.9867e-03],\n          [-2.6822e-02,  6.2409e-03, -5.6709e-03,  ..., -2.2020e-02,\n           -5.0899e-02, -3.0919e-02],\n          ...,\n          [ 2.0553e-02,  1.1162e-02, -9.9819e-03,  ..., -1.8076e-02,\n           -3.4711e-02, -4.8512e-02],\n          [-1.5429e-02,  3.1260e-02, -3.8893e-02,  ...,  4.4316e-02,\n           -1.6104e-02,  8.3861e-03],\n          [ 4.7827e-02,  3.2899e-02,  4.0706e-04,  ..., -3.8819e-03,\n           -3.2461e-02, -3.7989e-02]],\n\n         [[ 5.4897e-04,  1.7648e-03, -3.0589e-03,  ..., -4.7574e-03,\n            1.0103e-03,  1.2787e-03],\n          [-9.0643e-03,  2.7984e-02, -1.5812e-02,  ...,  5.3045e-02,\n           -7.9356e-02, -1.2652e-02],\n          [ 1.4768e-02, -4.5303e-03,  4.5299e-02,  ...,  9.4320e-02,\n           -1.0461e-02, -2.6667e-02],\n          ...,\n          [ 1.9228e-02, -5.7126e-02,  4.0291e-02,  ..., -4.8236e-02,\n            5.9357e-02,  5.6393e-03],\n          [ 4.5186e-02, -1.8854e-02, -7.1189e-02,  ..., -3.9111e-02,\n           -3.0927e-03,  1.6728e-02],\n          [ 3.3958e-02, -1.6313e-02,  1.7695e-02,  ..., -1.8004e-02,\n            2.5880e-02, -3.0381e-02]],\n\n         [[ 3.8106e-04, -6.6685e-03, -1.8430e-03,  ...,  3.2438e-03,\n           -2.4456e-03,  5.2321e-04],\n          [ 2.3403e-02, -8.7871e-02,  2.6673e-02,  ...,  5.0790e-02,\n           -1.2147e-01,  1.2055e-01],\n          [ 2.1656e-02, -1.7128e-01,  6.1019e-02,  ...,  3.4740e-02,\n            1.6335e-01,  2.8357e-02],\n          ...,\n          [-2.4438e-01, -1.9340e-01,  1.1677e-02,  ...,  1.1345e-02,\n           -2.6713e-02, -1.6420e-02],\n          [-8.8500e-02, -8.8149e-02,  1.8771e-02,  ..., -3.8095e-02,\n           -4.6311e-01, -8.8659e-03],\n          [-1.0535e-01,  2.3135e-01, -1.1260e-01,  ..., -6.5826e-03,\n            1.1492e+00,  4.8819e-02]],\n\n         ...,\n\n         [[ 1.6000e-03,  1.1811e-03, -1.0015e-03,  ...,  7.2339e-04,\n            1.7679e-03,  4.4910e-03],\n          [-4.8441e-02, -2.1032e-02, -2.4442e-02,  ...,  6.2648e-02,\n            7.7407e-03, -1.3404e-01],\n          [ 9.0081e-03, -6.0350e-02, -5.5934e-02,  ...,  2.8462e-03,\n            4.1976e-02, -6.8314e-02],\n          ...,\n          [ 3.1197e-02, -1.4642e-02, -3.3502e-02,  ..., -6.4688e-02,\n           -4.3721e-03,  4.5022e-02],\n          [ 3.0035e-02,  1.7874e-02,  1.8227e-02,  ..., -7.4779e-02,\n           -5.1340e-02,  5.8258e-02],\n          [ 2.9600e-02, -2.5502e-02, -2.1114e-02,  ...,  4.1000e-03,\n            1.2291e-03,  2.8927e-04]],\n\n         [[-1.0151e-03,  3.9496e-03,  3.9495e-04,  ...,  6.3420e-03,\n            3.5934e-03,  3.9423e-04],\n          [-3.7269e-02,  3.4390e-02,  2.8371e-02,  ..., -1.5517e-02,\n           -1.7906e-02, -2.2374e-02],\n          [ 3.2882e-02,  2.5762e-02,  5.2483e-02,  ...,  1.3096e-03,\n           -1.0446e-01, -1.3505e-02],\n          ...,\n          [ 1.3714e-02, -1.0148e-03,  3.8395e-02,  ..., -7.0661e-02,\n            3.3933e-02, -2.3575e-02],\n          [-4.4264e-02, -1.3850e-04, -7.1602e-02,  ..., -4.5077e-03,\n            5.8754e-04, -7.4955e-02],\n          [-9.2302e-02,  4.0891e-02, -2.0709e-02,  ...,  5.8531e-02,\n            1.4159e-02, -7.8016e-02]],\n\n         [[-4.9180e-04,  6.4928e-04,  4.3757e-03,  ..., -1.5371e-03,\n            2.0246e-03,  5.0378e-04],\n          [ 1.1958e-03, -6.7176e-02, -3.0110e-03,  ..., -3.0535e-02,\n           -4.3822e-02, -2.0606e-02],\n          [ 2.9835e-02, -7.6854e-03, -4.0773e-02,  ...,  2.3442e-02,\n            1.8265e-03,  6.0278e-02],\n          ...,\n          [-2.1327e-02, -2.9894e-02, -3.1595e-02,  ...,  4.5272e-02,\n           -1.4600e-02,  3.4084e-02],\n          [-1.6050e-02, -2.1649e-03, -4.0141e-02,  ...,  3.2166e-02,\n           -5.3817e-03, -1.1048e-02],\n          [ 7.3818e-02, -7.9945e-03,  4.5331e-02,  ..., -4.0416e-02,\n            3.0137e-02,  4.5173e-02]]],\n\n\n        [[[-3.9426e-03, -3.4957e-03,  4.2400e-03,  ...,  3.0273e-04,\n            5.0242e-03,  3.3483e-03],\n          [-1.2874e-03, -2.9849e-02, -1.2090e-02,  ..., -1.1261e-01,\n            1.0307e-02,  6.7067e-02],\n          [ 6.1841e-02, -1.1001e-03, -4.5832e-02,  ..., -6.9965e-02,\n            3.0472e-03,  5.5970e-02],\n          ...,\n          [ 2.5398e-02, -2.9120e-02, -3.2641e-02,  ...,  4.2244e-02,\n           -6.9946e-03,  4.3170e-03],\n          [ 1.2803e-02, -6.4248e-03,  2.4887e-02,  ...,  1.6691e-04,\n            7.9222e-03, -2.2536e-02],\n          [-1.1618e-02,  1.0015e-02,  3.6702e-02,  ...,  1.0143e-01,\n           -4.4789e-03,  1.7485e-02]],\n\n         [[ 1.8767e-04,  6.9252e-04, -3.2848e-03,  ..., -3.5804e-03,\n            8.3415e-04,  1.1811e-03],\n          [-8.6738e-02,  2.6024e-02, -4.1735e-02,  ...,  6.5544e-03,\n           -3.5973e-02, -1.3131e-02],\n          [-2.0158e-02, -3.7315e-02,  4.7807e-03,  ...,  8.6123e-02,\n           -6.8313e-02, -1.5869e-02],\n          ...,\n          [-1.8446e-02,  6.8589e-03,  6.6469e-02,  ..., -4.0045e-02,\n            3.8294e-02, -7.3033e-04],\n          [ 5.5633e-03, -3.6340e-02,  5.2784e-02,  ..., -2.4537e-02,\n            5.6460e-02, -9.3518e-02],\n          [-7.2887e-03, -2.8577e-02,  2.7574e-02,  ..., -6.8626e-03,\n            9.8562e-03, -4.3932e-02]],\n\n         [[ 2.7122e-03, -5.8981e-03, -5.7412e-04,  ...,  4.2030e-03,\n           -3.2525e-03,  2.0219e-03],\n          [-5.8427e-02, -2.3516e-01,  1.6388e-01,  ...,  5.9956e-02,\n            2.9185e-01,  1.1646e-02],\n          [ 2.9660e-02, -7.0415e-02,  2.9004e-03,  ..., -9.9611e-03,\n            3.4893e-01,  5.7867e-03],\n          ...,\n          [-2.0325e-01, -6.5605e-02,  7.0727e-02,  ...,  1.4600e-02,\n            3.8339e-01,  1.8680e-02],\n          [-8.2191e-02, -1.5630e-01, -6.9589e-02,  ...,  1.8798e-03,\n            2.6652e-01,  2.7505e-02],\n          [-5.0745e-02, -1.7099e-01,  4.0995e-02,  ...,  4.7944e-02,\n           -5.7396e-01, -7.3822e-02]],\n\n         ...,\n\n         [[ 2.4456e-03,  1.4015e-03, -2.0055e-03,  ...,  8.2413e-05,\n            2.3531e-03,  5.5864e-03],\n          [-6.5301e-03, -4.5524e-02, -3.0134e-02,  ..., -4.3323e-02,\n           -1.3382e-02, -4.6007e-02],\n          [-3.5204e-02, -5.6532e-03,  2.4363e-02,  ...,  4.4285e-02,\n           -5.8755e-02, -1.0053e-01],\n          ...,\n          [ 2.0328e-02, -4.8748e-02, -2.1521e-02,  ...,  1.2793e-02,\n            2.2133e-02,  7.7033e-03],\n          [ 6.6577e-02,  2.8320e-02,  2.2763e-02,  ..., -2.2315e-02,\n            2.2738e-02,  4.8650e-02],\n          [ 5.4068e-02,  1.5204e-02,  7.3957e-03,  ..., -2.6485e-02,\n            9.5279e-04,  6.7387e-02]],\n\n         [[ 1.3619e-04,  2.4017e-03, -4.7238e-04,  ...,  5.1949e-03,\n            3.4240e-03,  3.6065e-04],\n          [ 3.4832e-02,  1.5799e-02,  9.7172e-02,  ..., -3.9816e-02,\n           -2.5792e-02, -8.2310e-02],\n          [ 1.6423e-03, -7.0951e-03, -5.8996e-02,  ...,  5.2433e-02,\n            1.8583e-02, -9.3096e-03],\n          ...,\n          [ 7.0010e-02,  1.3915e-02,  5.5002e-02,  ..., -5.8422e-03,\n            4.5397e-02, -3.2350e-03],\n          [ 7.9208e-03, -5.7044e-02,  1.0896e-01,  ..., -7.3520e-02,\n            7.2396e-02, -8.4607e-02],\n          [ 9.8199e-02, -2.8611e-02,  7.2059e-02,  ..., -1.1960e-02,\n            1.8093e-02,  1.1162e-02]],\n\n         [[ 4.7635e-04,  5.5437e-04,  4.5117e-03,  ..., -1.8094e-03,\n            3.9410e-03,  6.2820e-04],\n          [ 3.5150e-02,  1.8418e-02, -5.8594e-02,  ...,  1.3060e-01,\n            5.9379e-02, -3.1812e-02],\n          [ 1.8942e-02, -1.1357e-02,  1.4337e-02,  ...,  9.7271e-03,\n            2.9468e-02, -3.9502e-02],\n          ...,\n          [ 3.4179e-02, -6.2380e-03, -2.4639e-02,  ...,  5.4013e-02,\n            4.3642e-02,  3.8808e-02],\n          [ 4.0987e-02, -3.7711e-02, -2.6207e-02,  ...,  2.6790e-02,\n            4.4072e-02,  2.7668e-02],\n          [-2.4216e-02,  5.8081e-03, -7.6426e-02,  ...,  4.6783e-02,\n            3.0041e-02,  2.0661e-02]]],\n\n\n        ...,\n\n\n        [[[-4.7350e-03, -6.5102e-04,  3.3425e-03,  ..., -1.3490e-03,\n            5.0004e-03,  9.6826e-04],\n          [-1.3078e-02,  3.5151e-02, -2.6946e-02,  ..., -7.9773e-03,\n            1.5368e-02, -4.3719e-02],\n          [ 5.1737e-02,  9.2366e-03, -2.1341e-02,  ..., -2.7128e-02,\n           -9.3571e-02, -1.3979e-02],\n          ...,\n          [ 1.7535e-03,  4.4916e-02,  5.4141e-03,  ...,  7.6544e-03,\n           -5.2882e-02, -2.9098e-02],\n          [-6.1077e-03,  2.2541e-02,  7.4645e-03,  ...,  5.3335e-02,\n            1.2832e-02,  7.3781e-03],\n          [-3.1976e-02,  2.8603e-02,  3.1337e-02,  ...,  2.8433e-02,\n           -1.6333e-03,  2.4059e-02]],\n\n         [[ 9.7317e-04,  1.7637e-03, -5.5934e-03,  ..., -6.1353e-03,\n            1.1676e-03,  1.1010e-04],\n          [ 6.8348e-03,  1.1058e-02, -3.3470e-02,  ..., -2.1397e-02,\n           -4.1753e-02, -2.1339e-02],\n          [-3.9310e-02,  2.2837e-02, -6.5836e-02,  ...,  1.8064e-02,\n           -4.7536e-02,  5.3433e-03],\n          ...,\n          [ 1.1604e-02, -3.7854e-02, -3.3604e-02,  ...,  2.3977e-03,\n            4.4625e-02, -1.2056e-02],\n          [ 4.5004e-03, -3.4380e-02, -9.1459e-03,  ..., -3.2937e-02,\n            2.3803e-02,  9.6301e-05],\n          [-4.4304e-03,  4.9390e-03, -3.4999e-03,  ..., -6.3737e-02,\n            1.9810e-02,  1.6521e-02]],\n\n         [[ 3.0992e-04, -5.4114e-03, -1.8650e-04,  ...,  4.8756e-03,\n           -1.5634e-02, -1.1739e-03],\n          [ 2.8569e-02, -5.6759e-02,  6.7659e-02,  ...,  4.3816e-02,\n           -5.8375e-01,  3.6862e-03],\n          [ 2.0087e-01, -1.6783e-02, -2.3471e-02,  ...,  1.7760e-02,\n            3.4886e-01,  5.1446e-04],\n          ...,\n          [-1.9366e-01, -2.1315e-03,  5.7371e-02,  ..., -2.9381e-02,\n            4.6910e-01,  2.1899e-02],\n          [-3.3441e-02,  1.2305e-01, -2.5806e-02,  ...,  1.2335e-01,\n           -1.3908e-01, -1.4553e-01],\n          [ 3.7632e-02, -7.5679e-02,  1.2477e-03,  ...,  9.7775e-02,\n           -4.3721e-01,  2.4901e-02]],\n\n         ...,\n\n         [[ 2.5884e-03,  2.9496e-03, -4.8045e-04,  ..., -2.9814e-03,\n            9.7524e-04,  7.1895e-03],\n          [-1.6792e-02,  2.2223e-02,  1.4165e-02,  ..., -1.0041e-01,\n           -2.7462e-02, -1.2916e-02],\n          [-3.2303e-02, -1.6042e-03,  1.1568e-02,  ..., -3.0566e-02,\n            1.2615e-02, -4.5851e-02],\n          ...,\n          [ 2.3873e-02,  2.0164e-03, -8.3959e-03,  ..., -4.9907e-02,\n           -7.6249e-03,  4.0612e-02],\n          [ 6.0453e-02,  1.5848e-02,  3.5536e-02,  ..., -2.3951e-02,\n           -1.6935e-02,  8.5162e-02],\n          [ 3.4550e-02,  3.3375e-02,  1.6138e-02,  ..., -1.3108e-02,\n           -9.9844e-03,  5.8186e-02]],\n\n         [[-3.0216e-03,  5.2467e-03, -4.0659e-03,  ...,  6.5260e-03,\n            1.9791e-03,  2.0842e-03],\n          [-3.6637e-02, -6.8864e-03, -8.5331e-04,  ..., -3.0659e-02,\n           -2.3022e-02, -7.8995e-03],\n          [-4.4608e-02, -2.0132e-02,  3.3297e-02,  ..., -8.2075e-02,\n           -4.0620e-03,  5.7874e-03],\n          ...,\n          [ 4.2600e-03, -3.1330e-02,  1.1009e-03,  ..., -3.0322e-02,\n           -1.7152e-02, -2.1710e-02],\n          [ 7.3141e-03, -3.1925e-02,  1.8441e-02,  ..., -9.7957e-02,\n            7.0434e-02, -3.0402e-02],\n          [ 7.7274e-03,  3.2645e-03,  6.4742e-02,  ...,  8.4666e-03,\n            7.6339e-04, -1.8618e-02]],\n\n         [[-7.9476e-04,  1.8567e-03,  4.8765e-03,  ..., -2.9923e-03,\n            2.1762e-03,  1.3090e-03],\n          [-2.5929e-02,  1.0726e-02, -4.6117e-02,  ...,  2.0550e-02,\n           -1.5989e-02, -9.4861e-03],\n          [-1.2654e-02,  1.2049e-02, -5.6884e-03,  ..., -2.0034e-02,\n           -3.5010e-02, -1.6878e-02],\n          ...,\n          [-1.1383e-02, -2.0531e-02, -5.7801e-02,  ..., -1.3288e-02,\n           -1.4468e-02,  6.0338e-02],\n          [ 2.8197e-03, -1.8131e-02, -1.0889e-02,  ..., -1.5298e-02,\n           -1.6038e-02,  3.2550e-02],\n          [-2.9304e-02,  2.4465e-02, -1.6126e-03,  ..., -2.3871e-02,\n           -2.5110e-02, -7.5689e-02]]],\n\n\n        [[[-4.9367e-03, -1.7195e-03,  7.5303e-04,  ..., -4.8160e-03,\n            2.1988e-03,  3.0610e-03],\n          [-4.7730e-03, -2.4319e-03, -2.4010e-02,  ..., -7.8874e-02,\n           -5.1933e-02,  1.8187e-02],\n          [ 3.1982e-02,  1.9064e-02, -3.3930e-02,  ..., -7.0582e-02,\n           -6.8391e-02, -1.9390e-02],\n          ...,\n          [-3.3763e-03, -2.8270e-02,  1.7288e-02,  ...,  4.8408e-03,\n            2.0479e-02, -3.6314e-02],\n          [ 3.7861e-02, -4.3276e-03, -1.0186e-02,  ...,  4.3370e-02,\n            5.6498e-03, -1.7771e-02],\n          [-3.2359e-02,  4.8081e-02, -5.6558e-02,  ..., -8.5000e-05,\n           -2.5119e-02,  2.4285e-02]],\n\n         [[-7.7912e-04,  5.2131e-03, -4.5964e-03,  ..., -5.6628e-03,\n            8.3097e-04, -1.8139e-04],\n          [-5.7359e-02, -2.8031e-02,  6.3166e-03,  ...,  5.3887e-02,\n           -2.0873e-02, -2.7856e-02],\n          [-6.7534e-02, -2.4433e-02, -2.0313e-02,  ...,  5.9385e-02,\n           -5.4176e-02,  2.2942e-02],\n          ...,\n          [-2.9451e-02, -3.1908e-02,  3.3100e-02,  ..., -3.4915e-02,\n            4.0281e-02, -5.8357e-03],\n          [ 2.2021e-02, -1.2272e-02, -4.3492e-03,  ..., -6.0077e-02,\n            5.6520e-02,  5.9054e-03],\n          [-1.3819e-02,  6.2276e-02, -3.9713e-02,  ..., -4.8202e-02,\n           -6.3499e-02,  4.7277e-02]],\n\n         [[ 9.1786e-04, -4.6278e-03, -1.7337e-03,  ...,  4.2554e-03,\n           -3.5210e-03,  4.3898e-03],\n          [ 5.0305e-02, -2.4658e-02,  1.2577e-01,  ...,  6.1780e-02,\n           -4.5119e-03,  5.4605e-02],\n          [ 1.2957e-01, -3.8331e-02, -3.7617e-02,  ...,  4.9547e-02,\n            4.1068e-01, -1.9582e-03],\n          ...,\n          [-2.1376e-01, -4.5899e-02,  1.4356e-02,  ...,  9.7526e-02,\n           -3.0391e-01, -1.1156e-01],\n          [-1.3527e-01, -9.7043e-02,  1.2768e-02,  ...,  9.4093e-02,\n           -2.0166e-01, -1.8867e-02],\n          [-1.0911e-01,  8.4660e-02, -5.3715e-02,  ...,  9.9988e-03,\n           -1.4028e-03,  2.6119e-02]],\n\n         ...,\n\n         [[ 7.7526e-04,  2.2531e-03, -2.4312e-03,  ..., -1.8322e-03,\n            2.3217e-03,  6.6214e-03],\n          [-5.2921e-02,  5.7483e-03,  2.2954e-02,  ...,  4.5704e-02,\n            1.0407e-02, -6.0809e-02],\n          [-3.6332e-02,  2.0758e-02,  1.2814e-02,  ..., -1.4085e-02,\n           -1.5169e-02, -1.5750e-02],\n          ...,\n          [ 7.4704e-02,  1.2624e-02,  1.2076e-02,  ..., -3.1094e-02,\n           -1.0148e-02,  5.2054e-02],\n          [-1.0339e-03, -6.8852e-03,  3.9147e-02,  ..., -5.8410e-02,\n           -3.9367e-02,  3.9084e-02],\n          [ 5.2026e-02,  1.6327e-02,  4.9213e-02,  ..., -4.0268e-02,\n           -4.1337e-02,  2.1859e-02]],\n\n         [[ 1.1854e-03,  5.4560e-03, -6.7241e-03,  ...,  3.5624e-03,\n            3.5527e-03,  1.4715e-03],\n          [-3.5124e-02, -2.6333e-02, -7.2978e-04,  ..., -2.7574e-02,\n           -9.5805e-02,  6.6157e-02],\n          [-6.3579e-02, -2.2894e-02,  2.8626e-02,  ...,  4.3804e-03,\n           -3.6779e-02, -1.0084e-02],\n          ...,\n          [ 3.8538e-02, -4.6237e-03,  8.2082e-02,  ..., -7.6512e-02,\n            3.9461e-02, -5.4443e-02],\n          [-2.0405e-02,  7.1958e-03,  5.6981e-02,  ...,  2.7416e-02,\n           -1.3495e-02,  1.7500e-02],\n          [-1.5229e-02,  5.4092e-02,  5.7553e-03,  ..., -1.2599e-03,\n           -1.4719e-03, -6.1221e-02]],\n\n         [[ 9.8836e-04,  3.8689e-03,  5.9799e-03,  ..., -4.2462e-03,\n            3.9857e-03,  1.9080e-03],\n          [ 4.1218e-02,  3.5481e-02, -3.1922e-02,  ...,  5.9757e-02,\n            1.9027e-02, -4.5219e-03],\n          [-3.4881e-02, -2.1884e-03, -7.0467e-02,  ...,  6.0355e-02,\n           -2.3660e-02, -1.9413e-02],\n          ...,\n          [ 3.9393e-03, -7.6757e-03, -1.4028e-02,  ...,  2.7433e-03,\n            1.5919e-02,  5.6106e-02],\n          [-3.7306e-02, -2.0555e-02, -4.0064e-02,  ..., -4.5273e-02,\n            9.6764e-03,  1.8248e-02],\n          [-2.8224e-02, -2.3868e-02,  1.3105e-02,  ..., -9.2285e-02,\n           -3.7216e-02, -1.8053e-02]]],\n\n\n        [[[-2.0750e-03, -3.6959e-03,  4.5417e-03,  ..., -9.3039e-04,\n            4.0395e-03,  1.4924e-03],\n          [-1.7235e-02,  5.6313e-03,  2.5016e-02,  ..., -1.3083e-02,\n           -8.3408e-03, -2.0455e-02],\n          [ 7.7940e-02, -4.9582e-02, -2.5932e-02,  ...,  6.0373e-02,\n            9.9164e-03,  2.9536e-02],\n          ...,\n          [ 5.8018e-03,  3.2974e-02,  3.3578e-02,  ...,  1.1992e-01,\n           -1.7494e-02, -1.7487e-02],\n          [-2.6839e-02,  2.0973e-02, -3.7759e-02,  ...,  3.8511e-02,\n           -4.0792e-02, -8.0037e-03],\n          [ 1.3129e-02,  1.7754e-02,  2.1705e-02,  ...,  1.7269e-02,\n            8.7889e-03, -6.9540e-02]],\n\n         [[ 2.3275e-03, -8.9255e-04, -7.8400e-04,  ..., -2.0058e-03,\n            1.9059e-03, -4.9767e-04],\n          [ 2.2803e-02, -3.8539e-02, -1.7813e-02,  ...,  8.6348e-03,\n            1.2241e-02, -8.8661e-02],\n          [-4.2651e-02, -3.1139e-02,  9.8964e-03,  ..., -6.6137e-03,\n            4.0607e-02, -1.3318e-02],\n          ...,\n          [-6.1750e-03, -5.6930e-02,  1.0115e-02,  ...,  2.7212e-02,\n            2.6147e-02, -3.6304e-02],\n          [-3.1823e-02, -5.1649e-02,  3.7814e-04,  ...,  3.0381e-02,\n            8.4295e-03, -3.5401e-02],\n          [-4.8909e-02, -5.5288e-02, -1.1196e-02,  ...,  1.1785e-02,\n            1.0226e-02, -4.9353e-02]],\n\n         [[ 2.2262e-03, -6.2733e-03, -3.4402e-03,  ...,  3.5200e-03,\n           -8.6531e-03,  1.9503e-04],\n          [-1.2355e-01,  3.0726e-02,  2.0251e-02,  ..., -4.4165e-02,\n           -1.0516e-01,  1.7199e-02],\n          [ 1.1273e-01, -2.6625e-02,  1.2858e-01,  ..., -4.6448e-02,\n           -1.7995e-01,  8.3244e-02],\n          ...,\n          [-8.3623e-02,  1.3048e-01,  1.5962e-02,  ...,  1.8184e-03,\n            1.3993e-01, -2.9894e-02],\n          [-7.6908e-02, -1.9725e-01, -8.3493e-04,  ...,  2.2342e-02,\n           -2.2428e-01, -1.2068e-01],\n          [-7.5592e-02,  8.8194e-02, -7.1276e-02,  ...,  7.2849e-02,\n           -4.0086e-01,  8.5893e-02]],\n\n         ...,\n\n         [[ 1.6390e-03,  3.1652e-03, -2.3915e-03,  ...,  2.3151e-03,\n            2.2578e-03,  4.5742e-03],\n          [-1.2864e-03,  1.4782e-02,  2.1418e-02,  ..., -6.3685e-02,\n           -3.8071e-02,  4.1705e-02],\n          [-1.7997e-02,  1.2959e-02,  2.3913e-03,  ..., -8.6827e-02,\n           -5.9644e-02,  4.3433e-03],\n          ...,\n          [ 8.4681e-03,  1.8099e-02,  2.7756e-02,  ..., -1.8954e-02,\n           -8.2276e-04,  4.9851e-02],\n          [ 2.9441e-02, -1.5083e-02,  4.2239e-02,  ..., -3.7506e-02,\n           -5.0778e-02,  2.4633e-02],\n          [ 3.5499e-02,  3.4793e-03,  6.8739e-02,  ..., -6.9471e-02,\n           -5.8629e-02,  6.8390e-02]],\n\n         [[-1.5944e-03,  1.2948e-03,  5.7899e-04,  ...,  4.0038e-03,\n            4.0495e-03,  6.7245e-04],\n          [-5.6909e-02, -5.5592e-02,  9.3875e-03,  ...,  1.1437e-02,\n           -4.0191e-02,  4.1410e-02],\n          [ 1.9162e-02,  4.0752e-02, -3.3890e-02,  ...,  5.2903e-02,\n           -3.3067e-02,  1.7158e-02],\n          ...,\n          [-4.7917e-02, -3.6239e-02,  5.6756e-02,  ...,  5.6546e-03,\n            6.9472e-02,  1.0626e-02],\n          [ 7.4070e-02, -2.1328e-02, -1.7586e-02,  ..., -4.8861e-02,\n            4.1948e-02, -2.6709e-02],\n          [ 4.0590e-04, -6.5867e-02,  3.3273e-02,  ...,  2.4208e-03,\n            4.0366e-02,  3.5695e-02]],\n\n         [[ 2.3976e-03,  1.5569e-03,  5.4857e-03,  ..., -4.1346e-03,\n            3.5392e-03,  3.2689e-03],\n          [ 8.7565e-02,  2.9426e-02,  5.2330e-02,  ..., -2.6273e-02,\n            5.1787e-02,  2.5512e-02],\n          [-1.5993e-03,  3.1182e-02, -8.1556e-02,  ...,  1.2832e-01,\n            3.6125e-02, -1.6087e-03],\n          ...,\n          [ 6.7797e-02, -1.6437e-02, -2.0408e-02,  ..., -4.2158e-02,\n            1.9506e-02,  1.9828e-02],\n          [ 1.5686e-02, -9.0795e-03, -5.7472e-02,  ...,  3.9567e-02,\n            4.6229e-02,  6.6012e-02],\n          [ 3.3276e-02,  2.0153e-03, -4.6776e-03,  ...,  5.8103e-04,\n            5.3910e-02,  7.3935e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-1.6986e-02, -3.7830e-02,  1.2475e-01,  ...,  1.4730e-01,\n            1.5922e-02,  1.0745e-01],\n          [ 8.9818e-02, -1.2890e+00, -1.5376e+00,  ..., -4.3397e-01,\n            8.1609e-02, -3.9015e-01],\n          [-2.4795e-01, -1.7863e+00, -1.3656e+00,  ..., -3.5267e-01,\n            6.8104e-02, -3.4446e-01],\n          ...,\n          [-4.4536e-01,  2.0983e-01,  5.6882e-01,  ..., -1.5012e+00,\n           -3.5372e-02, -1.3527e+00],\n          [ 5.2629e-01,  2.8850e-01,  8.6058e-01,  ..., -1.8475e+00,\n           -3.8725e-02, -1.7045e+00],\n          [-3.0566e-01, -2.7593e-01,  5.7245e-01,  ..., -1.7284e+00,\n           -4.2260e-02, -1.6161e+00]],\n\n         [[-1.7005e-01, -7.8063e-02, -6.4786e-02,  ..., -5.9505e-02,\n            4.0299e-02,  4.2126e-02],\n          [ 2.2499e+00, -5.8518e-01, -1.1134e-01,  ..., -2.1087e+00,\n           -1.3321e+00,  9.8395e-02],\n          [ 1.7741e+00, -8.7645e-01, -9.9156e-02,  ..., -1.6647e+00,\n           -1.0018e+00,  7.7104e-02],\n          ...,\n          [ 7.4697e-01, -8.0809e-04, -9.8294e-02,  ...,  8.1956e-01,\n           -4.1455e-01,  5.0391e-02],\n          [ 7.5642e-01,  1.0635e-01, -1.0903e-01,  ...,  6.8148e-01,\n           -6.7081e-01,  6.4083e-02],\n          [ 5.2619e-01, -4.3046e-01, -9.6992e-02,  ...,  9.4191e-01,\n           -2.1901e-01,  5.2574e-02]],\n\n         [[-6.6236e-02, -4.0218e-02, -3.7063e-02,  ..., -1.1612e-01,\n           -3.7894e-02,  2.3141e-02],\n          [ 5.5506e-01, -8.2199e-01, -7.4638e-01,  ..., -8.3179e-01,\n            5.6446e-01,  7.3140e-01],\n          [ 4.7375e-01, -7.1924e-01, -6.5329e-01,  ..., -7.3784e-01,\n            4.8570e-01,  6.3844e-01],\n          ...,\n          [ 4.8733e-01, -7.3591e-01, -6.6804e-01,  ..., -7.5255e-01,\n            4.9931e-01,  6.5290e-01],\n          [ 4.7580e-01, -7.2530e-01, -6.5697e-01,  ..., -7.4154e-01,\n            4.8770e-01,  6.4183e-01],\n          [ 4.3991e-01, -6.7567e-01, -6.1358e-01,  ..., -6.9759e-01,\n            4.5344e-01,  5.9853e-01]],\n\n         ...,\n\n         [[ 4.2168e-02,  1.1310e-01,  6.6684e-03,  ...,  1.2297e-01,\n            1.2296e-01, -5.3157e-02],\n          [-1.2773e+00,  2.3075e-02,  6.7588e-01,  ..., -1.5523e+00,\n            6.0985e-01, -2.1396e+00],\n          [-1.0917e+00,  2.0069e-01,  3.1237e-01,  ..., -1.1104e+00,\n            5.0470e-01, -1.5597e+00],\n          ...,\n          [-9.9689e-01,  2.1614e+00, -3.0839e-01,  ..., -1.1155e+00,\n            2.3826e+00,  5.7365e-01],\n          [-1.1787e+00,  1.6175e+00, -2.4462e-01,  ..., -1.2589e+00,\n            2.0020e+00,  3.3034e-01],\n          [-9.4794e-01,  1.9799e+00, -3.7007e-01,  ..., -8.3885e-01,\n            2.1237e+00,  4.1302e-01]],\n\n         [[-5.0545e-02, -1.4329e-01,  1.5708e-01,  ...,  3.6821e-01,\n            2.2036e-01,  5.9517e-01],\n          [ 4.8017e-01,  5.8562e-01, -5.5913e-01,  ..., -2.4202e-01,\n           -4.7197e-01, -1.9008e-01],\n          [ 4.0626e-01,  4.8379e-01, -4.5910e-01,  ..., -1.5687e-01,\n           -3.7538e-01, -8.0510e-02],\n          ...,\n          [ 4.2293e-01,  5.0027e-01, -4.7559e-01,  ..., -1.7368e-01,\n           -3.9279e-01, -9.8193e-02],\n          [ 4.2599e-01,  5.0500e-01, -4.8017e-01,  ..., -1.7734e-01,\n           -3.9716e-01, -1.0348e-01],\n          [ 3.8680e-01,  4.4932e-01, -4.2559e-01,  ..., -1.3153e-01,\n           -3.4451e-01, -4.3032e-02]],\n\n         [[ 2.1376e-02, -1.3082e-01, -8.8859e-02,  ..., -1.6248e-01,\n           -3.2432e-02,  3.6045e-02],\n          [-1.1096e-01,  5.9591e-01, -1.0721e+00,  ...,  7.1831e-01,\n           -8.2996e-02, -7.5088e-02],\n          [-9.7110e-02,  5.0022e-01, -9.4049e-01,  ...,  6.0106e-01,\n           -7.5851e-02, -6.3208e-02],\n          ...,\n          [-1.1354e-01,  5.2255e-01, -9.6802e-01,  ...,  6.2654e-01,\n           -8.7497e-02, -7.8737e-02],\n          [-1.1056e-01,  5.2875e-01, -9.7820e-01,  ...,  6.3513e-01,\n           -8.4743e-02, -7.5522e-02],\n          [-1.0037e-01,  4.5346e-01, -8.7696e-01,  ...,  5.4414e-01,\n           -7.7889e-02, -6.6995e-02]]],\n\n\n        [[[-2.2890e-02, -3.0801e-02,  1.3087e-01,  ...,  1.3905e-01,\n            1.5957e-02,  1.0009e-01],\n          [ 4.7157e-01, -1.5368e+00, -8.9521e-01,  ..., -6.7271e-01,\n            5.7593e-02, -6.6971e-01],\n          [-7.2048e-02, -1.1593e+00, -1.4526e+00,  ..., -4.2160e-01,\n            6.4873e-02, -4.0502e-01],\n          ...,\n          [-4.7752e-01, -1.8097e-01,  2.8108e-01,  ..., -1.5354e+00,\n           -3.0330e-02, -1.4233e+00],\n          [-3.8090e-01, -1.1133e-01,  5.2935e-01,  ..., -1.7350e+00,\n           -3.8084e-02, -1.5742e+00],\n          [ 3.0604e-01, -3.8942e-01,  2.3931e-01,  ..., -1.6079e+00,\n           -3.4949e-03, -1.4830e+00]],\n\n         [[-1.7246e-01, -6.8036e-02, -6.4098e-02,  ..., -5.9641e-02,\n            4.2758e-02,  4.1460e-02],\n          [ 1.8644e+00, -6.7968e-01, -1.0799e-01,  ..., -1.9130e+00,\n           -1.2912e+00,  9.1133e-02],\n          [ 1.9308e+00, -4.3755e-01, -8.5741e-02,  ..., -1.6583e+00,\n           -1.0855e+00,  7.1084e-02],\n          ...,\n          [ 1.0393e+00, -3.1308e-01, -9.0846e-02,  ...,  4.0179e-01,\n           -7.0369e-01,  5.4879e-02],\n          [ 6.6010e-01, -3.1273e-01, -8.7830e-02,  ...,  6.0933e-01,\n           -3.2574e-01,  3.5328e-02],\n          [ 1.0759e+00, -6.8292e-01, -1.1727e-01,  ...,  2.7186e-01,\n           -7.8796e-01,  7.4677e-02]],\n\n         [[-6.6727e-02, -3.9542e-02, -3.6472e-02,  ..., -1.1553e-01,\n           -3.8365e-02,  2.2553e-02],\n          [ 4.5143e-01, -6.9750e-01, -6.3121e-01,  ..., -7.1582e-01,\n            4.6334e-01,  6.1638e-01],\n          [ 5.0841e-01, -7.6569e-01, -6.9436e-01,  ..., -7.7934e-01,\n            5.1902e-01,  6.7939e-01],\n          ...,\n          [ 5.2301e-01, -7.7744e-01, -7.0695e-01,  ..., -7.9168e-01,\n            5.3429e-01,  6.9176e-01],\n          [ 4.4430e-01, -6.7705e-01, -6.1644e-01,  ..., -7.0040e-01,\n            4.5810e-01,  6.0150e-01],\n          [ 5.5168e-01, -8.1405e-01, -7.4002e-01,  ..., -8.2509e-01,\n            5.6200e-01,  7.2482e-01]],\n\n         ...,\n\n         [[ 4.6646e-02,  1.0511e-01,  7.2303e-04,  ...,  1.2313e-01,\n            1.0729e-01, -3.8650e-02],\n          [-1.1670e+00, -6.9080e-01,  3.9872e-01,  ..., -1.3053e+00,\n           -2.5922e-01, -1.7739e+00],\n          [-1.1110e+00, -1.3440e-01,  5.9989e-01,  ..., -1.3939e+00,\n            2.4684e-01, -1.9331e+00],\n          ...,\n          [-1.1717e+00,  2.2657e+00, -2.2158e-01,  ..., -1.1402e+00,\n            2.7057e+00,  2.9500e-01],\n          [-9.9017e-01,  2.0785e+00, -4.1883e-01,  ..., -8.7238e-01,\n            2.2608e+00,  5.2290e-01],\n          [-1.4197e+00,  2.2537e+00, -4.1375e-02,  ..., -1.2779e+00,\n            2.8676e+00, -3.0653e-01]],\n\n         [[-5.1073e-02, -1.4405e-01,  1.5783e-01,  ...,  3.6883e-01,\n            2.2109e-01,  5.9601e-01],\n          [ 4.1510e-01,  4.9354e-01, -4.6878e-01,  ..., -1.6614e-01,\n           -3.8494e-01, -9.0380e-02],\n          [ 4.4901e-01,  5.4215e-01, -5.1649e-01,  ..., -2.0604e-01,\n           -4.3100e-01, -1.4323e-01],\n          ...,\n          [ 4.3533e-01,  5.1605e-01, -4.9113e-01,  ..., -1.8729e-01,\n           -4.0796e-01, -1.1528e-01],\n          [ 3.8066e-01,  4.3817e-01, -4.1473e-01,  ..., -1.2330e-01,\n           -3.3405e-01, -3.0246e-02],\n          [ 4.7261e-01,  5.6703e-01, -5.4123e-01,  ..., -2.3013e-01,\n           -4.5630e-01, -1.6983e-01]],\n\n         [[ 2.1207e-02, -1.3140e-01, -8.7950e-02,  ..., -1.6326e-01,\n           -3.2593e-02,  3.5856e-02],\n          [-9.7529e-02,  5.0372e-01, -9.4704e-01,  ...,  6.0639e-01,\n           -7.8464e-02, -6.4218e-02],\n          [-9.9750e-02,  5.3533e-01, -9.9460e-01,  ...,  6.4753e-01,\n           -7.6609e-02, -6.5725e-02],\n          ...,\n          [-1.1772e-01,  5.4350e-01, -9.9708e-01,  ...,  6.5231e-01,\n           -8.7070e-02, -8.2171e-02],\n          [-1.0245e-01,  4.6283e-01, -8.8380e-01,  ...,  5.5209e-01,\n           -8.4362e-02, -6.9283e-02],\n          [-1.1552e-01,  5.8460e-01, -1.0541e+00,  ...,  7.0302e-01,\n           -8.4503e-02, -7.8713e-02]]],\n\n\n        [[[-2.7132e-02, -4.1053e-02,  1.2962e-01,  ...,  1.4429e-01,\n            1.5577e-02,  1.0355e-01],\n          [-4.0194e-01, -1.4107e+00, -1.0821e+00,  ..., -6.7689e-01,\n            3.8738e-02, -6.4371e-01],\n          [-4.1231e-01, -1.4611e+00, -1.6349e+00,  ..., -4.4394e-01,\n            6.8502e-02, -4.3672e-01],\n          ...,\n          [ 3.8358e-02, -7.5919e-03,  4.1084e-01,  ..., -1.5581e+00,\n           -2.5675e-02, -1.4334e+00],\n          [-4.8465e-01, -1.5577e-01,  2.7910e-01,  ..., -1.6954e+00,\n           -2.9370e-02, -1.5705e+00],\n          [-6.1715e-01,  9.2256e-03,  3.2807e-01,  ..., -1.5243e+00,\n           -4.4176e-02, -1.3656e+00]],\n\n         [[-1.7908e-01, -7.6159e-02, -6.3786e-02,  ..., -4.9934e-02,\n            4.5683e-02,  4.1256e-02],\n          [ 1.8486e+00, -6.0333e-01, -7.7189e-02,  ..., -1.6711e+00,\n           -1.0686e+00,  5.9047e-02],\n          [ 2.2601e+00, -7.6223e-01, -9.3483e-02,  ..., -2.0424e+00,\n           -1.4426e+00,  8.1052e-02],\n          ...,\n          [ 9.8701e-01, -2.9747e-01, -9.2469e-02,  ...,  6.0117e-01,\n           -6.5519e-01,  4.4660e-02],\n          [ 1.0865e+00, -4.9814e-01, -7.1956e-02,  ...,  3.6314e-01,\n           -7.2011e-01,  3.0728e-02],\n          [ 7.0731e-01, -3.9656e-01, -6.5721e-02,  ...,  4.5907e-01,\n           -3.9713e-01,  2.0529e-02]],\n\n         [[-6.8257e-02, -3.7646e-02, -3.4739e-02,  ..., -1.1377e-01,\n           -3.9850e-02,  2.0819e-02],\n          [ 4.9187e-01, -7.4064e-01, -6.7324e-01,  ..., -7.5795e-01,\n            5.0341e-01,  6.5840e-01],\n          [ 5.5696e-01, -8.2428e-01, -7.4844e-01,  ..., -8.3383e-01,\n            5.6635e-01,  7.3342e-01],\n          ...,\n          [ 5.0374e-01, -7.5696e-01, -6.8688e-01,  ..., -7.7153e-01,\n            5.1527e-01,  6.7165e-01],\n          [ 5.1655e-01, -7.7198e-01, -7.0100e-01,  ..., -7.8578e-01,\n            5.2776e-01,  6.8582e-01],\n          [ 3.9118e-01, -6.1332e-01, -5.5754e-01,  ..., -6.4108e-01,\n            4.0618e-01,  5.4269e-01]],\n\n         ...,\n\n         [[ 4.9509e-02,  1.1014e-01, -3.2278e-03,  ...,  1.3394e-01,\n            1.1241e-01, -3.4291e-02],\n          [-1.0178e+00,  1.1229e-01,  1.7166e-01,  ..., -1.1764e+00,\n            2.9452e-01, -1.2255e+00],\n          [-1.3048e+00,  1.4628e-01,  5.5561e-01,  ..., -1.4599e+00,\n            5.9526e-01, -1.9067e+00],\n          ...,\n          [-1.1347e+00,  1.9992e+00, -2.2609e-01,  ..., -1.0295e+00,\n            2.4062e+00,  1.1446e-01],\n          [-1.1641e+00,  2.2235e+00, -3.1383e-01,  ..., -1.0924e+00,\n            2.5511e+00,  2.9336e-01],\n          [-8.2537e-01,  2.1202e+00, -5.4637e-01,  ..., -7.8539e-01,\n            2.2961e+00,  7.8871e-01]],\n\n         [[-5.2525e-02, -1.4600e-01,  1.5975e-01,  ...,  3.7049e-01,\n            2.2294e-01,  5.9809e-01],\n          [ 4.2749e-01,  5.0828e-01, -4.8339e-01,  ..., -1.7945e-01,\n           -3.9923e-01, -1.0576e-01],\n          [ 4.8122e-01,  5.8371e-01, -5.5743e-01,  ..., -2.4190e-01,\n           -4.7081e-01, -1.8766e-01],\n          ...,\n          [ 4.4046e-01,  5.2460e-01, -4.9947e-01,  ..., -1.9385e-01,\n           -4.1596e-01, -1.2481e-01],\n          [ 4.3023e-01,  5.1089e-01, -4.8596e-01,  ..., -1.8210e-01,\n           -4.0278e-01, -1.1021e-01],\n          [ 3.3277e-01,  3.7692e-01, -3.5435e-01,  ..., -7.0049e-02,\n           -2.7520e-01,  3.4931e-02]],\n\n         [[ 2.1727e-02, -1.3361e-01, -8.5110e-02,  ..., -1.6585e-01,\n           -3.2278e-02,  3.6317e-02],\n          [-1.0542e-01,  5.1052e-01, -9.5319e-01,  ...,  6.1275e-01,\n           -8.5627e-02, -7.2074e-02],\n          [-1.1371e-01,  5.9999e-01, -1.0751e+00,  ...,  7.2178e-01,\n           -8.4475e-02, -7.7252e-02],\n          ...,\n          [-1.1114e-01,  5.3753e-01, -9.9319e-01,  ...,  6.4759e-01,\n           -8.4198e-02, -7.6223e-02],\n          [-1.2070e-01,  5.4091e-01, -9.9550e-01,  ...,  6.5023e-01,\n           -9.2343e-02, -8.5612e-02],\n          [-9.2582e-02,  4.0340e-01, -8.0593e-01,  ...,  4.8156e-01,\n           -8.4572e-02, -6.1922e-02]]],\n\n\n        ...,\n\n\n        [[[-2.3105e-02, -5.2390e-02,  1.3178e-01,  ...,  1.4964e-01,\n            1.6394e-02,  1.0849e-01],\n          [-4.7633e-01, -1.1810e+00, -1.2188e+00,  ..., -3.8574e-01,\n            3.8483e-02, -3.4563e-01],\n          [ 4.8617e-02, -1.1770e+00, -1.5981e+00,  ..., -4.0030e-01,\n            8.0430e-02, -3.3291e-01],\n          ...,\n          [-4.5728e-01, -2.3709e-01,  4.6242e-01,  ..., -1.7688e+00,\n           -4.6311e-02, -1.6206e+00],\n          [-5.7820e-01, -1.2836e-01,  2.7746e-01,  ..., -1.5980e+00,\n           -2.7037e-02, -1.4597e+00],\n          [-8.5177e-01, -3.4672e-01,  5.0825e-01,  ..., -1.7176e+00,\n           -5.5680e-02, -1.5813e+00]],\n\n         [[-1.9033e-01, -8.9861e-02, -6.3127e-02,  ..., -5.1623e-02,\n            5.3521e-02,  4.0486e-02],\n          [ 1.7145e+00, -6.2000e-01, -6.8115e-02,  ..., -1.6465e+00,\n           -1.0272e+00,  4.8601e-02],\n          [ 2.1339e+00, -5.0082e-01, -6.8537e-02,  ..., -2.2197e+00,\n           -1.1626e+00,  5.3343e-02],\n          ...,\n          [ 5.8062e-01, -6.3367e-01, -8.8852e-02,  ...,  8.6496e-01,\n           -3.2487e-01,  3.6887e-02],\n          [ 9.8404e-01, -5.1079e-01, -1.0196e-01,  ...,  3.9840e-01,\n           -6.4055e-01,  5.8399e-02],\n          [ 6.4941e-01, -5.1773e-01, -8.9124e-02,  ...,  6.6352e-01,\n           -3.8263e-01,  4.0960e-02]],\n\n         [[-7.2227e-02, -3.2618e-02, -3.0193e-02,  ..., -1.0919e-01,\n           -4.3698e-02,  1.6283e-02],\n          [ 3.9684e-01, -6.2209e-01, -5.6527e-01,  ..., -6.4903e-01,\n            4.1117e-01,  5.5057e-01],\n          [ 5.4856e-01, -8.1357e-01, -7.3887e-01,  ..., -8.2424e-01,\n            5.5816e-01,  7.2396e-01],\n          ...,\n          [ 4.4729e-01, -6.8327e-01, -6.2116e-01,  ..., -7.0521e-01,\n            4.6075e-01,  6.0615e-01],\n          [ 5.0074e-01, -7.4883e-01, -6.8126e-01,  ..., -7.6577e-01,\n            5.1274e-01,  6.6615e-01],\n          [ 4.4203e-01, -6.7581e-01, -6.1470e-01,  ..., -6.9869e-01,\n            4.5572e-01,  5.9974e-01]],\n\n         ...,\n\n         [[ 5.6800e-02,  9.8759e-02, -7.0697e-03,  ...,  1.4656e-01,\n            9.5986e-02, -3.2101e-02],\n          [-9.7110e-01,  2.7771e-01,  2.4486e-01,  ..., -1.0958e+00,\n            5.4709e-01, -1.1819e+00],\n          [-1.2831e+00,  2.4354e-02,  9.3001e-01,  ..., -1.7524e+00,\n            5.4118e-01, -2.4308e+00],\n          ...,\n          [-1.1193e+00,  2.4046e+00, -5.3821e-01,  ..., -8.0959e-01,\n            2.6468e+00,  5.9862e-01],\n          [-1.0846e+00,  2.0670e+00, -2.7817e-01,  ..., -1.0080e+00,\n            2.4215e+00,  3.0249e-01],\n          [-8.8015e-01,  1.9589e+00, -7.3554e-01,  ..., -7.2778e-01,\n            1.9536e+00,  1.0573e+00]],\n\n         [[-5.5886e-02, -1.5064e-01,  1.6430e-01,  ...,  3.7436e-01,\n            2.2735e-01,  6.0311e-01],\n          [ 3.4847e-01,  4.0069e-01, -3.7764e-01,  ..., -8.9014e-02,\n           -2.9692e-01,  9.9930e-03],\n          [ 4.8706e-01,  5.9278e-01, -5.6632e-01,  ..., -2.4914e-01,\n           -4.7893e-01, -1.9676e-01],\n          ...,\n          [ 3.8467e-01,  4.4714e-01, -4.2341e-01,  ..., -1.2939e-01,\n           -3.4229e-01, -4.0736e-02],\n          [ 4.1455e-01,  4.8764e-01, -4.6322e-01,  ..., -1.6351e-01,\n           -3.8091e-01, -8.4473e-02],\n          [ 3.6978e-01,  4.2574e-01, -4.0241e-01,  ..., -1.1180e-01,\n           -3.2199e-01, -1.7506e-02]],\n\n         [[ 2.2658e-02, -1.3805e-01, -7.9070e-02,  ..., -1.7124e-01,\n           -3.1933e-02,  3.7105e-02],\n          [-7.7608e-02,  4.0962e-01, -8.2130e-01,  ...,  4.9330e-01,\n           -7.3541e-02, -4.7670e-02],\n          [-1.1148e-01,  6.0683e-01, -1.0828e+00,  ...,  7.2921e-01,\n           -8.8275e-02, -7.5644e-02],\n          ...,\n          [-9.9929e-02,  4.6010e-01, -8.8527e-01,  ...,  5.5183e-01,\n           -8.0992e-02, -6.6920e-02],\n          [-1.1763e-01,  5.2031e-01, -9.6214e-01,  ...,  6.2203e-01,\n           -8.7089e-02, -8.2218e-02],\n          [-1.1293e-01,  4.6139e-01, -8.7855e-01,  ...,  5.4826e-01,\n           -8.8223e-02, -7.8866e-02]]],\n\n\n        [[[-2.3327e-02, -4.3254e-02,  1.2617e-01,  ...,  1.5292e-01,\n            1.5784e-02,  1.1215e-01],\n          [-3.5608e-01, -7.0510e-01, -1.6032e+00,  ..., -4.0495e-01,\n            5.1986e-02, -3.1790e-01],\n          [ 2.9784e-02, -5.5530e-01, -1.6906e+00,  ..., -3.8114e-01,\n            8.7383e-02, -2.7538e-01],\n          ...,\n          [-6.1392e-01, -1.3721e-02,  2.8392e-01,  ..., -1.5059e+00,\n           -2.0872e-02, -1.3750e+00],\n          [-6.8919e-01,  6.5420e-03,  5.5659e-01,  ..., -1.6421e+00,\n           -4.4670e-02, -1.4994e+00],\n          [ 3.4029e-01,  1.5119e-01,  9.8590e-01,  ..., -1.6710e+00,\n           -2.0748e-02, -1.5608e+00]],\n\n         [[-1.9286e-01, -8.2910e-02, -6.3631e-02,  ..., -4.4410e-02,\n            5.2422e-02,  4.1031e-02],\n          [ 2.0478e+00, -2.9494e-01, -1.0305e-01,  ..., -1.8655e+00,\n           -1.2076e+00,  8.4110e-02],\n          [ 2.1800e+00, -7.9891e-02, -8.9272e-02,  ..., -2.1245e+00,\n           -1.1947e+00,  7.4995e-02],\n          ...,\n          [ 1.0481e+00, -4.9650e-01, -1.0255e-01,  ...,  4.3458e-01,\n           -6.7710e-01,  6.2195e-02],\n          [ 5.8954e-01, -3.7565e-01, -9.0125e-02,  ...,  9.2555e-01,\n           -3.3446e-01,  4.0064e-02],\n          [ 4.0546e-01, -1.8519e-01, -1.0836e-01,  ...,  9.4438e-01,\n           -4.5778e-01,  6.2905e-02]],\n\n         [[-7.0495e-02, -3.4850e-02, -3.2194e-02,  ..., -1.1121e-01,\n           -4.2023e-02,  1.8279e-02],\n          [ 5.0710e-01, -7.6314e-01, -6.9241e-01,  ..., -7.7740e-01,\n            5.1779e-01,  6.7751e-01],\n          [ 5.6625e-01, -8.3462e-01, -7.5847e-01,  ..., -8.4398e-01,\n            5.7540e-01,  7.4354e-01],\n          ...,\n          [ 4.9150e-01, -7.4094e-01, -6.7266e-01,  ..., -7.5718e-01,\n            5.0343e-01,  6.5749e-01],\n          [ 4.2561e-01, -6.5741e-01, -5.9709e-01,  ..., -6.8094e-01,\n            4.3961e-01,  5.8207e-01],\n          [ 3.7229e-01, -5.9474e-01, -5.3865e-01,  ..., -6.2214e-01,\n            3.8737e-01,  5.2368e-01]],\n\n         ...,\n\n         [[ 5.2065e-02,  1.2812e-01, -3.2228e-03,  ...,  1.4131e-01,\n            1.3228e-01, -3.2578e-02],\n          [-1.2102e+00,  6.9295e-01,  6.6977e-01,  ..., -1.6724e+00,\n            1.2816e+00, -1.7041e+00],\n          [-1.3135e+00,  4.0152e-01,  1.0511e+00,  ..., -1.9650e+00,\n            9.8635e-01, -2.3761e+00],\n          ...,\n          [-1.1431e+00,  2.2911e+00, -2.3198e-01,  ..., -1.0528e+00,\n            2.7455e+00,  2.1247e-01],\n          [-9.3218e-01,  2.5600e+00, -4.4917e-01,  ..., -8.3752e-01,\n            2.7707e+00,  7.2345e-01],\n          [-9.8196e-01,  1.9445e+00, -2.8580e-01,  ..., -8.1394e-01,\n            2.3253e+00,  4.3344e-01]],\n\n         [[-5.4237e-02, -1.4844e-01,  1.6214e-01,  ...,  3.7249e-01,\n            2.2524e-01,  6.0073e-01],\n          [ 4.5002e-01,  5.4127e-01, -5.1569e-01,  ..., -2.0613e-01,\n           -4.3016e-01, -1.4166e-01],\n          [ 4.7642e-01,  5.8209e-01, -5.5558e-01,  ..., -2.3832e-01,\n           -4.6818e-01, -1.8632e-01],\n          ...,\n          [ 4.1127e-01,  4.8578e-01, -4.6126e-01,  ..., -1.6070e-01,\n           -3.7885e-01, -8.3365e-02],\n          [ 3.7224e-01,  4.2630e-01, -4.0312e-01,  ..., -1.1363e-01,\n           -3.2316e-01, -1.7747e-02],\n          [ 3.2606e-01,  3.6987e-01, -3.4730e-01,  ..., -6.3230e-02,\n           -2.6853e-01,  4.1532e-02]],\n\n         [[ 2.2140e-02, -1.3585e-01, -8.1980e-02,  ..., -1.6862e-01,\n           -3.2019e-02,  3.6687e-02],\n          [-1.0925e-01,  5.5136e-01, -1.0084e+00,  ...,  6.6222e-01,\n           -8.7387e-02, -7.4716e-02],\n          [-1.1660e-01,  6.1826e-01, -1.0974e+00,  ...,  7.4250e-01,\n           -9.0481e-02, -8.0195e-02],\n          ...,\n          [-1.0992e-01,  5.1381e-01, -9.6014e-01,  ...,  6.1821e-01,\n           -8.3121e-02, -7.5154e-02],\n          [-9.9311e-02,  4.5298e-01, -8.7288e-01,  ...,  5.4161e-01,\n           -8.1945e-02, -6.6437e-02],\n          [-8.3605e-02,  3.9611e-01, -8.0083e-01,  ...,  4.7562e-01,\n           -7.1980e-02, -5.2279e-02]]],\n\n\n        [[[-2.6151e-02, -3.0722e-02,  1.1619e-01,  ...,  1.1237e-01,\n            1.5651e-02,  7.6306e-02],\n          [-1.6200e-01, -1.0333e+00, -1.3096e+00,  ..., -4.7725e-01,\n            6.0678e-02, -4.6887e-01],\n          [-4.2352e-01, -1.3153e+00, -1.2916e+00,  ..., -3.5265e-01,\n            4.8787e-02, -3.2383e-01],\n          ...,\n          [-2.3833e-01,  4.8591e-01,  1.6310e-02,  ..., -1.4358e+00,\n           -5.8634e-03, -1.2796e+00],\n          [-3.3856e-01, -6.2852e-03,  3.2311e-01,  ..., -1.7053e+00,\n           -1.7184e-02, -1.5400e+00],\n          [-3.2723e-01,  1.7201e-01,  1.0239e-01,  ..., -1.4557e+00,\n           -1.1435e-02, -1.3124e+00]],\n\n         [[-1.1490e-01, -5.0682e-02, -6.5635e-02,  ..., -7.4827e-02,\n            9.5591e-03,  4.2733e-02],\n          [ 1.9797e+00, -4.4172e-01, -1.0160e-01,  ..., -1.6968e+00,\n           -1.2692e+00,  8.8946e-02],\n          [ 1.8245e+00, -4.5014e-01, -8.4411e-02,  ..., -1.6994e+00,\n           -1.0280e+00,  6.3297e-02],\n          ...,\n          [ 1.2104e+00, -1.8096e-02, -9.2354e-02,  ...,  3.0632e-01,\n           -8.0172e-01,  5.0360e-02],\n          [ 8.5810e-01, -2.2440e-01, -7.9710e-02,  ...,  6.1552e-01,\n           -4.0110e-01,  3.2395e-02],\n          [ 1.0403e+00, -2.5901e-01, -8.2097e-02,  ...,  3.9718e-01,\n           -6.1402e-01,  3.8779e-02]],\n\n         [[-4.6401e-02, -6.5296e-02, -5.9760e-02,  ..., -1.3902e-01,\n           -1.8667e-02,  4.5794e-02],\n          [ 4.7514e-01, -7.2621e-01, -6.5759e-01,  ..., -7.4229e-01,\n            4.8654e-01,  6.4262e-01],\n          [ 4.3661e-01, -6.7423e-01, -6.1179e-01,  ..., -6.9603e-01,\n            4.4951e-01,  5.9699e-01],\n          ...,\n          [ 5.2101e-01, -7.7946e-01, -7.0706e-01,  ..., -7.9193e-01,\n            5.3190e-01,  6.9182e-01],\n          [ 5.0400e-01, -7.5453e-01, -6.8576e-01,  ..., -7.7036e-01,\n            5.1578e-01,  6.7064e-01],\n          [ 5.0810e-01, -7.6138e-01, -6.9135e-01,  ..., -7.7602e-01,\n            5.1957e-01,  6.7616e-01]],\n\n         ...,\n\n         [[ 1.1522e-02,  1.0442e-01,  1.4323e-02,  ...,  6.6448e-02,\n            1.1606e-01, -6.6142e-02],\n          [-1.1642e+00,  1.5157e-01,  6.3390e-01,  ..., -1.4197e+00,\n            6.5608e-01, -1.8815e+00],\n          [-1.0065e+00, -3.0821e-01,  3.8805e-01,  ..., -1.2661e+00,\n           -6.9086e-02, -1.4932e+00],\n          ...,\n          [-1.2211e+00,  2.4021e+00,  5.8159e-03,  ..., -1.3686e+00,\n            2.9088e+00, -2.2249e-02],\n          [-1.0540e+00,  2.2274e+00, -2.4127e-01,  ..., -1.1402e+00,\n            2.4788e+00,  3.2527e-01],\n          [-1.1804e+00,  2.1616e+00, -2.2415e-02,  ..., -1.2440e+00,\n            2.5926e+00, -5.1599e-02]],\n\n         [[-3.3109e-02, -1.1950e-01,  1.3370e-01,  ...,  3.4822e-01,\n            1.9772e-01,  5.6955e-01],\n          [ 4.1761e-01,  5.0063e-01, -4.7559e-01,  ..., -1.7047e-01,\n           -3.9143e-01, -9.9237e-02],\n          [ 3.8734e-01,  4.5462e-01, -4.3060e-01,  ..., -1.3394e-01,\n           -3.4813e-01, -4.8289e-02],\n          ...,\n          [ 4.4818e-01,  5.3701e-01, -5.1155e-01,  ..., -2.0335e-01,\n           -4.2734e-01, -1.3854e-01],\n          [ 4.3644e-01,  5.1595e-01, -4.9111e-01,  ..., -1.8793e-01,\n           -4.0793e-01, -1.1453e-01],\n          [ 4.3111e-01,  5.1444e-01, -4.8936e-01,  ..., -1.8411e-01,\n           -4.0577e-01, -1.1427e-01]],\n\n         [[ 1.6582e-02, -1.0695e-01, -1.2114e-01,  ..., -1.3356e-01,\n           -3.4440e-02,  3.1954e-02],\n          [-9.2892e-02,  5.0585e-01, -9.5795e-01,  ...,  6.1370e-01,\n           -7.6533e-02, -6.0231e-02],\n          [-8.4395e-02,  4.5966e-01, -8.9031e-01,  ...,  5.5468e-01,\n           -7.4565e-02, -5.2894e-02],\n          ...,\n          [-1.1530e-01,  5.5422e-01, -1.0177e+00,  ...,  6.6888e-01,\n           -8.9747e-02, -8.0489e-02],\n          [-1.0959e-01,  5.3475e-01, -9.8717e-01,  ...,  6.4290e-01,\n           -8.4658e-02, -7.4697e-02],\n          [-1.1682e-01,  5.4514e-01, -1.0022e+00,  ...,  6.5596e-01,\n           -8.7124e-02, -8.1419e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[ 3.5699e-03, -1.3276e-03, -2.6139e-03,  ..., -3.3295e-04,\n            1.2529e-02, -1.5563e-03],\n          [-4.9353e-03, -6.1512e-02,  1.9969e-02,  ...,  1.0762e-03,\n           -2.6286e-01, -3.7185e-02],\n          [-9.2284e-02,  2.0157e-02,  1.0361e-02,  ..., -5.5360e-02,\n           -1.4337e-01,  1.7693e-02],\n          ...,\n          [-2.3769e-02,  4.2564e-02, -1.5580e-03,  ..., -2.5089e-02,\n            7.9824e-02,  3.9000e-02],\n          [ 5.3231e-02,  4.4765e-02, -5.4609e-02,  ...,  3.1799e-02,\n            2.6623e-01,  4.1441e-02],\n          [ 1.0239e-02,  2.5713e-02, -5.8048e-03,  ..., -3.9261e-02,\n            1.4538e-01,  1.5688e-02]],\n\n         [[ 1.3698e-03, -3.5058e-03, -3.6671e-03,  ..., -5.7848e-02,\n            2.5740e-03,  6.9855e-04],\n          [ 2.4298e-03,  5.4493e-03,  1.5157e-03,  ...,  1.1761e-01,\n           -1.1594e-02,  2.9409e-02],\n          [ 1.1094e-02,  6.6687e-02,  7.4764e-03,  ...,  8.5209e-02,\n           -1.6081e-03,  5.0331e-02],\n          ...,\n          [ 1.5394e-02,  2.9751e-02,  2.8640e-02,  ..., -5.0401e-01,\n           -2.3448e-02,  3.3314e-02],\n          [ 5.9952e-02,  3.4498e-02,  4.7117e-03,  ..., -4.8222e-01,\n           -3.6416e-02,  5.1652e-02],\n          [-1.0424e-02,  3.1821e-02, -1.1820e-02,  ..., -4.7965e-01,\n           -8.8172e-02,  4.6182e-02]],\n\n         [[-8.7374e-04,  2.5562e-03, -3.6138e-04,  ...,  7.9945e-04,\n           -8.4983e-05, -1.0819e-03],\n          [ 2.6371e-02, -3.3891e-02,  9.4288e-04,  ...,  1.4245e-02,\n           -3.7302e-02,  1.6595e-02],\n          [-1.7059e-02,  7.2129e-03,  1.0222e-02,  ..., -2.7795e-02,\n           -1.0097e-02, -4.1300e-03],\n          ...,\n          [ 9.7958e-03, -1.1270e-02,  4.9278e-02,  ...,  1.8032e-02,\n           -3.1010e-02,  5.5294e-02],\n          [ 5.3877e-02, -6.9252e-02,  4.9860e-04,  ...,  4.0360e-02,\n           -4.5829e-02,  5.9937e-02],\n          [-3.8616e-02,  8.1564e-03,  2.3412e-02,  ..., -4.6656e-02,\n           -3.8196e-02, -4.6021e-02]],\n\n         ...,\n\n         [[ 1.9681e-03,  3.9271e-03,  3.2417e-03,  ..., -6.5247e-03,\n           -2.8011e-04,  3.9548e-02],\n          [ 1.7171e-02, -1.4868e-01, -9.2087e-02,  ..., -1.8737e-01,\n            2.4787e-01,  1.0173e-02],\n          [ 1.8600e-01,  1.2696e-01, -2.8469e-02,  ..., -1.2624e-01,\n           -7.8752e-02, -3.2640e-01],\n          ...,\n          [ 1.4315e-01,  2.9409e-01,  3.2517e-02,  ..., -1.0794e-01,\n            9.1690e-02, -4.4612e-01],\n          [-2.9566e-01, -9.2850e-02,  1.6251e-01,  ...,  9.4857e-03,\n            2.2901e-01,  1.8036e-01],\n          [-6.8275e-03,  4.2725e-02, -7.7178e-03,  ..., -2.0869e-01,\n           -2.4674e-01, -1.3409e-01]],\n\n         [[ 5.3666e-04, -1.8807e-04, -1.4566e-03,  ..., -2.9589e-04,\n           -5.0810e-04,  1.6278e-04],\n          [-2.5714e-03,  4.0392e-02,  1.7900e-02,  ...,  5.4738e-02,\n            3.0905e-02,  1.9620e-02],\n          [-2.3432e-02, -1.2750e-02,  1.4968e-02,  ...,  8.5527e-03,\n            2.6720e-02,  8.7432e-03],\n          ...,\n          [-3.6258e-02, -3.9016e-03, -4.6473e-02,  ..., -2.4718e-02,\n            3.8034e-02, -4.7519e-02],\n          [-3.7285e-02, -1.0893e-04, -2.6761e-03,  ..., -4.1589e-02,\n           -8.6878e-03, -6.1313e-02],\n          [-9.7879e-03,  2.6642e-02, -8.5576e-03,  ...,  3.1203e-03,\n            3.9639e-03, -3.3508e-02]],\n\n         [[ 3.8767e-04,  1.7791e-04,  1.2866e-03,  ..., -1.9608e-03,\n           -8.5627e-05, -1.1844e-03],\n          [ 6.8056e-02,  5.4139e-02,  2.3636e-02,  ..., -1.6541e-02,\n           -8.1491e-04, -3.6351e-05],\n          [ 1.8507e-02, -2.2247e-02, -1.4733e-02,  ...,  2.5923e-02,\n            9.3096e-03,  3.8619e-02],\n          ...,\n          [-6.9088e-03, -1.3177e-02,  6.9110e-03,  ...,  3.0071e-02,\n            3.6899e-02,  6.6468e-02],\n          [ 6.3451e-02, -1.0143e-02,  3.2494e-02,  ...,  3.6208e-02,\n            4.6874e-02,  9.9501e-02],\n          [-3.7142e-03, -2.5250e-02,  1.3295e-02,  ...,  5.2276e-02,\n            5.5168e-03,  5.1797e-02]]],\n\n\n        [[[ 2.4650e-03, -1.0631e-03, -1.2137e-03,  ...,  3.1453e-05,\n            1.8040e-02, -1.3662e-03],\n          [ 4.7472e-02,  5.3908e-02, -1.0497e-02,  ...,  5.2391e-03,\n            1.5153e-01,  2.3112e-02],\n          [-3.7294e-02, -8.6372e-03,  4.2947e-03,  ..., -5.1616e-03,\n           -1.8549e-01, -5.7506e-02],\n          ...,\n          [-5.2531e-02, -2.1837e-02,  9.7759e-03,  ..., -1.6090e-02,\n            3.9086e-02,  4.8465e-02],\n          [-3.8028e-02,  5.0024e-03,  1.2228e-01,  ...,  1.9603e-02,\n           -4.4327e-02,  2.5850e-02],\n          [ 4.0041e-02,  1.6405e-02, -3.7309e-02,  ...,  6.1512e-02,\n            8.7835e-02,  6.6762e-03]],\n\n         [[ 8.9031e-04, -3.4318e-03, -2.6951e-03,  ..., -5.5506e-02,\n            2.3684e-03,  8.5305e-04],\n          [ 8.2848e-03,  1.0158e-02, -5.0302e-02,  ..., -4.3285e-03,\n            4.9807e-02, -2.7124e-03],\n          [ 7.0523e-03,  4.4617e-02,  1.7905e-02,  ...,  1.8425e-01,\n            1.6686e-03, -3.4927e-02],\n          ...,\n          [-1.6934e-02, -6.4037e-05, -2.3550e-02,  ..., -4.0779e-01,\n           -1.3654e-02, -1.9512e-02],\n          [-2.1349e-02,  1.8519e-02,  4.5332e-02,  ..., -5.1774e-01,\n           -3.8170e-02,  2.8561e-02],\n          [ 4.1169e-02, -9.0362e-03,  1.5832e-02,  ..., -4.4650e-01,\n            1.1105e-02,  3.3339e-02]],\n\n         [[-1.1545e-03,  3.3541e-03, -2.3265e-05,  ...,  5.0470e-04,\n            1.7125e-03, -3.1230e-04],\n          [ 4.9732e-02, -9.1742e-02, -2.0438e-02,  ...,  3.8508e-02,\n           -3.9243e-02,  3.5406e-02],\n          [ 2.3995e-02, -7.2787e-03,  4.8430e-02,  ..., -5.2771e-02,\n           -3.8046e-02, -5.0679e-02],\n          ...,\n          [ 1.0394e-02, -4.5662e-02,  4.1747e-02,  ..., -8.7144e-03,\n            2.4604e-02, -4.9586e-02],\n          [ 5.1260e-03, -9.6511e-03,  2.1095e-02,  ...,  1.0361e-02,\n           -5.1401e-03, -1.0263e-02],\n          [ 9.6338e-03, -6.9498e-02,  4.3425e-02,  ...,  4.4785e-02,\n           -1.1433e-02, -4.0822e-03]],\n\n         ...,\n\n         [[-9.3240e-04,  5.6419e-04,  1.7751e-04,  ..., -5.7057e-03,\n            2.9366e-03,  3.7179e-02],\n          [-7.9128e-02, -1.4391e-01,  1.0441e-01,  ..., -9.6579e-02,\n            1.9655e-01,  1.7485e-01],\n          [ 2.8589e-03, -5.2294e-02, -2.0461e-01,  ..., -5.2886e-02,\n           -8.2197e-02, -4.4726e-02],\n          ...,\n          [ 4.9583e-04,  2.9535e-02, -2.4558e-02,  ..., -2.5275e-01,\n           -3.0224e-02,  2.5765e-01],\n          [-2.6545e-01, -1.6896e-01, -1.8108e-02,  ..., -2.3747e-01,\n            3.0824e-02, -2.8733e-01],\n          [-3.2231e-01, -1.2735e-02,  2.5708e-01,  ..., -8.4910e-02,\n            6.0222e-02,  1.4424e-01]],\n\n         [[ 5.7026e-04, -1.5828e-03, -1.2277e-03,  ..., -7.0028e-04,\n           -2.7093e-04, -7.3284e-04],\n          [-2.8572e-04,  2.8920e-02, -6.3393e-03,  ..., -3.0459e-02,\n            2.1301e-02, -1.3904e-02],\n          [ 1.8904e-02,  4.1942e-02, -3.7624e-03,  ...,  1.9033e-02,\n            2.5935e-03,  2.9417e-03],\n          ...,\n          [ 7.4079e-03,  2.3856e-04, -1.3659e-02,  ...,  2.6736e-02,\n           -1.1308e-02, -4.5143e-02],\n          [-2.0652e-02, -2.1635e-02,  1.6339e-02,  ...,  1.0534e-02,\n           -5.4079e-02, -2.4733e-02],\n          [-5.5797e-02, -1.5674e-04,  7.2880e-03,  ...,  4.2720e-02,\n           -4.6399e-03, -7.0249e-02]],\n\n         [[-1.4236e-03,  8.5279e-04,  6.7817e-04,  ..., -8.5314e-04,\n           -4.0832e-04, -1.2364e-03],\n          [ 4.0574e-02,  1.0475e-03, -1.1712e-02,  ...,  8.2868e-03,\n           -2.6431e-02,  7.7082e-02],\n          [ 3.2556e-02, -2.5644e-02, -4.5003e-02,  ...,  1.6411e-02,\n           -6.8754e-02,  2.6129e-02],\n          ...,\n          [-2.2865e-02,  7.8406e-02,  5.3442e-02,  ..., -7.5423e-03,\n           -1.9764e-02,  1.5258e-02],\n          [ 2.3556e-02,  6.2487e-02,  6.9897e-02,  ..., -2.3022e-03,\n            3.0703e-02, -2.4807e-03],\n          [ 8.7726e-03,  2.7489e-02,  3.5748e-02,  ...,  4.8769e-02,\n            6.8925e-02,  5.4318e-02]]],\n\n\n        [[[ 2.1356e-03, -1.7683e-03, -1.8597e-03,  ...,  1.4436e-04,\n            2.0455e-02, -8.8761e-04],\n          [-8.2789e-02, -2.5168e-03,  4.8089e-02,  ..., -2.5549e-02,\n            3.1696e-02,  1.1241e-02],\n          [-7.3060e-03, -1.8425e-02,  3.9462e-02,  ...,  3.2143e-02,\n           -8.9903e-02, -4.1468e-02],\n          ...,\n          [ 7.2367e-02, -4.9966e-04,  3.4530e-02,  ...,  8.8283e-02,\n           -1.9944e-02,  2.5361e-02],\n          [-8.9543e-02, -2.5608e-02, -2.9506e-03,  ..., -2.2517e-02,\n           -1.6020e-01,  2.2303e-02],\n          [-1.0877e-02,  4.3659e-02,  7.1540e-02,  ...,  3.6180e-02,\n           -4.0412e-02, -7.2598e-04]],\n\n         [[ 1.0301e-03, -3.3589e-03, -3.0171e-03,  ..., -5.3257e-02,\n           -5.3178e-04,  1.1119e-03],\n          [-1.4110e-02,  6.6827e-02, -1.5967e-02,  ...,  1.8071e-01,\n           -1.0259e-02,  6.0953e-02],\n          [-3.2393e-02,  1.1888e-02,  4.5595e-02,  ...,  6.9694e-02,\n            1.3774e-02, -2.9333e-02],\n          ...,\n          [ 1.2911e-03, -3.7252e-03,  1.1188e-02,  ..., -4.3467e-01,\n           -3.3975e-02, -8.1263e-03],\n          [ 2.7174e-02, -1.9584e-02,  3.6192e-03,  ..., -2.8572e-01,\n           -6.8157e-03, -2.6369e-02],\n          [ 2.9925e-02, -2.1223e-04, -6.2062e-03,  ..., -4.3799e-01,\n           -4.0924e-02,  1.9142e-02]],\n\n         [[-2.0286e-03,  3.5708e-03, -3.7819e-04,  ..., -2.4057e-04,\n            1.1437e-03, -1.1615e-03],\n          [ 1.7837e-02, -7.2755e-03,  3.2065e-02,  ...,  8.7745e-03,\n           -3.5167e-02,  1.7628e-02],\n          [ 2.0532e-02, -5.2995e-02,  7.3368e-03,  ...,  3.3447e-03,\n           -4.0294e-02,  7.7449e-03],\n          ...,\n          [ 8.0446e-03, -5.4111e-03,  1.7188e-02,  ..., -1.2881e-02,\n           -1.4469e-02, -1.8551e-02],\n          [ 8.5435e-03, -2.8481e-02, -6.3920e-03,  ..., -2.2686e-02,\n            3.0563e-02, -5.4426e-02],\n          [ 1.2672e-02, -7.5431e-03, -1.3393e-02,  ...,  1.1318e-02,\n           -2.6586e-03, -4.2878e-02]],\n\n         ...,\n\n         [[ 2.5204e-04,  5.9429e-03,  5.2816e-04,  ..., -4.9256e-03,\n            1.3436e-04,  3.7819e-02],\n          [ 1.8262e-02, -2.0378e-01,  6.9308e-02,  ...,  5.1431e-02,\n           -2.4790e-01,  1.5653e-01],\n          [-7.3102e-02,  1.5346e-02,  4.9873e-03,  ..., -1.3573e-01,\n           -2.4959e-02,  3.3129e-01],\n          ...,\n          [-2.7080e-01, -3.0849e-01,  7.0610e-02,  ...,  6.1945e-02,\n            3.2376e-03, -1.5053e-01],\n          [-1.5077e-01,  4.7178e-02,  3.0740e-01,  ..., -1.6591e-01,\n           -4.2462e-02, -1.6645e-01],\n          [-4.8214e-01, -8.5058e-02,  2.1823e-02,  ..., -2.0792e-01,\n            1.9146e-02, -2.5411e-01]],\n\n         [[ 6.5269e-04, -1.0828e-03, -8.3129e-04,  ..., -1.1205e-03,\n           -1.0705e-03, -1.7179e-04],\n          [ 1.3241e-03,  4.2668e-03,  2.6897e-02,  ...,  3.3470e-02,\n            4.4640e-02,  1.4376e-02],\n          [-1.5103e-02,  3.4768e-02,  5.7376e-03,  ...,  2.1938e-02,\n            3.1818e-02,  2.3520e-02],\n          ...,\n          [ 5.1814e-03,  1.3728e-02,  2.5074e-02,  ..., -6.0898e-04,\n           -1.4257e-02, -5.0519e-02],\n          [ 3.3295e-03,  1.4064e-03, -1.1199e-02,  ..., -1.4116e-02,\n           -2.9400e-02, -2.9702e-02],\n          [-2.1483e-02, -1.0475e-02,  3.6194e-02,  ..., -3.2431e-02,\n           -3.9558e-02, -1.0814e-02]],\n\n         [[-7.2378e-04,  3.3047e-04,  4.2615e-04,  ..., -1.4231e-03,\n           -6.8620e-04, -1.1366e-03],\n          [ 5.0476e-02, -2.1616e-04, -4.0457e-02,  ..., -2.4583e-02,\n           -4.4639e-02, -3.7730e-02],\n          [ 4.7844e-02,  1.6861e-02, -3.4718e-02,  ..., -1.6727e-02,\n           -4.3159e-02,  1.3515e-02],\n          ...,\n          [ 1.6281e-02,  3.9321e-02,  2.0393e-02,  ...,  1.5157e-02,\n           -5.3941e-03,  2.6174e-02],\n          [-4.8236e-02,  4.5708e-02,  9.8724e-03,  ..., -2.1189e-02,\n           -7.0226e-02,  2.2694e-05],\n          [ 2.2407e-03,  4.1384e-02,  4.9743e-02,  ..., -5.2467e-02,\n           -2.5306e-02,  1.7730e-03]]],\n\n\n        ...,\n\n\n        [[[ 6.6891e-04,  3.0131e-04, -1.0868e-03,  ..., -2.9324e-05,\n            1.8057e-02, -2.7280e-04],\n          [-1.2759e-01,  7.0533e-03,  7.8092e-02,  ..., -9.9136e-02,\n           -4.0276e-02, -1.8458e-02],\n          [ 1.7800e-02,  1.7352e-02, -6.2298e-03,  ..., -3.6598e-04,\n           -1.9892e-01, -5.1612e-02],\n          ...,\n          [-8.0130e-02, -2.3513e-02, -3.9507e-02,  ..., -1.1367e-02,\n            3.8471e-02,  1.5156e-02],\n          [-3.5161e-02,  2.8026e-02, -8.7301e-04,  ...,  2.8102e-02,\n           -1.4412e-01,  2.2610e-02],\n          [-7.9398e-02,  4.2504e-02,  6.1674e-02,  ...,  8.3300e-03,\n           -1.4451e-01,  3.1273e-02]],\n\n         [[-4.8443e-05, -2.2414e-03, -5.7649e-04,  ..., -5.2222e-02,\n            1.2180e-03, -9.6332e-04],\n          [ 3.7233e-02, -2.4754e-02, -4.4143e-02,  ...,  1.2907e-01,\n            9.5138e-03,  2.4198e-02],\n          [-2.8130e-02,  5.0921e-02,  6.2102e-02,  ...,  1.2785e-01,\n           -2.1442e-02,  1.4975e-02],\n          ...,\n          [ 3.9925e-02,  2.3138e-02,  2.9764e-02,  ..., -4.8447e-01,\n           -2.3722e-02,  4.9587e-02],\n          [-9.6499e-03, -2.8175e-03, -6.6074e-03,  ..., -3.3701e-01,\n            2.8227e-03, -3.9894e-02],\n          [ 4.4765e-02,  2.5183e-02,  1.1313e-02,  ..., -4.3298e-01,\n           -7.2378e-02,  3.0285e-02]],\n\n         [[-1.9255e-03,  4.6882e-03, -1.1683e-03,  ...,  3.4100e-04,\n            2.2584e-03, -1.2007e-03],\n          [-5.7562e-03, -2.2954e-02, -1.6743e-02,  ..., -1.7835e-02,\n            6.5812e-03,  2.1592e-02],\n          [-4.0964e-02, -9.7612e-03, -1.3647e-02,  ...,  2.8221e-02,\n            1.2543e-03,  4.3615e-02],\n          ...,\n          [-1.3399e-02, -2.0424e-02,  3.0590e-02,  ...,  6.5149e-03,\n            3.0977e-03, -4.5183e-02],\n          [-2.9651e-03, -2.5018e-02,  1.1458e-02,  ..., -6.9786e-03,\n           -3.5593e-03, -4.8204e-02],\n          [-2.8903e-02, -1.1477e-02,  1.2010e-03,  ...,  3.8382e-02,\n            1.6593e-02, -1.7693e-02]],\n\n         ...,\n\n         [[-1.4105e-03,  1.3907e-02,  7.0469e-03,  ..., -1.1557e-02,\n           -9.9446e-04,  3.9716e-02],\n          [ 1.4705e-01, -1.4607e-01,  3.4290e-01,  ..., -2.5354e-01,\n           -2.3076e-01, -4.1929e-01],\n          [-9.8372e-03, -4.7997e-02,  2.8613e-01,  ...,  2.2385e-01,\n           -3.0028e-01,  2.9962e-03],\n          ...,\n          [-3.2134e-01, -2.0543e-01,  5.8434e-02,  ..., -1.3619e-01,\n           -3.6513e-02, -2.1925e-01],\n          [-1.7403e-01,  7.9662e-02,  6.8118e-02,  ..., -1.5203e-01,\n            8.5115e-02, -3.6447e-01],\n          [-3.1782e-02,  2.0627e-01, -8.9083e-02,  ..., -3.0694e-02,\n            2.9585e-02, -4.1105e-01]],\n\n         [[ 1.4504e-04, -2.2353e-03, -2.3309e-03,  ..., -1.3790e-03,\n           -1.4542e-03, -1.0368e-03],\n          [ 2.4927e-03, -3.2277e-02,  1.9009e-02,  ..., -6.2527e-02,\n            1.2095e-02,  4.6815e-02],\n          [-4.6608e-02,  1.4809e-03, -4.1413e-02,  ...,  1.7168e-02,\n            8.7781e-05, -5.0942e-02],\n          ...,\n          [-4.0416e-02, -2.5773e-02,  2.2096e-02,  ...,  7.8698e-03,\n           -1.7962e-02, -4.6426e-02],\n          [-3.3180e-02,  1.5195e-02,  5.0552e-02,  ..., -6.6159e-03,\n           -1.4825e-02, -4.8977e-03],\n          [-2.3409e-02, -3.0380e-02,  2.9501e-02,  ..., -2.6850e-03,\n           -2.2363e-02, -2.1053e-02]],\n\n         [[-1.8720e-03,  1.3468e-03,  7.1304e-04,  ..., -1.1530e-03,\n           -1.4133e-03, -2.2949e-04],\n          [ 2.6583e-03, -2.7608e-02,  7.3118e-03,  ...,  5.7094e-03,\n            1.6180e-03,  4.1725e-03],\n          [-5.4401e-02,  1.4788e-02,  3.7099e-02,  ...,  1.5466e-02,\n            3.3045e-02,  2.0598e-02],\n          ...,\n          [-1.6392e-02,  1.7141e-02,  4.8251e-02,  ...,  1.5122e-02,\n            1.2964e-02,  3.4620e-02],\n          [-2.2331e-02, -5.1463e-03,  3.0191e-02,  ...,  1.2759e-02,\n           -1.3494e-02,  3.4335e-02],\n          [-3.2352e-02,  4.7228e-02,  3.0473e-02,  ..., -3.2658e-03,\n           -6.9704e-04,  1.3299e-02]]],\n\n\n        [[[ 5.8497e-04, -3.3912e-04, -2.7919e-03,  ..., -4.1102e-04,\n            1.5127e-02, -1.8634e-03],\n          [-3.4368e-02, -7.5713e-03,  3.4452e-02,  ..., -6.1789e-02,\n           -1.0027e-01, -4.3539e-02],\n          [-4.0053e-02, -5.1332e-03,  4.1946e-02,  ..., -1.2628e-02,\n           -2.0761e-01, -1.1132e-02],\n          ...,\n          [ 3.4026e-03,  2.2082e-02,  8.0222e-02,  ...,  1.1779e-02,\n            1.0020e-01, -1.1402e-02],\n          [-3.7673e-02,  6.3360e-02,  5.8392e-02,  ..., -2.3912e-02,\n            1.0775e-01,  1.0816e-02],\n          [ 3.5979e-03,  2.8317e-02,  1.7195e-02,  ...,  2.9859e-02,\n            3.9976e-01, -8.6267e-03]],\n\n         [[-1.4831e-03, -2.1282e-03,  4.8881e-04,  ..., -5.7999e-02,\n            1.2275e-03, -1.4990e-03],\n          [ 1.8879e-02,  4.1739e-02, -1.9961e-02,  ...,  3.7025e-01,\n           -1.5178e-02,  3.8661e-02],\n          [-3.9901e-02,  6.6285e-02,  4.0407e-02,  ...,  2.3839e-01,\n           -1.8112e-02,  2.6579e-02],\n          ...,\n          [ 5.2564e-02, -3.4297e-02, -3.0355e-02,  ..., -2.4703e-01,\n           -1.9293e-02,  2.2082e-02],\n          [ 6.1542e-02, -2.5453e-02,  6.4982e-03,  ..., -4.8355e-01,\n           -4.1687e-02,  6.2646e-02],\n          [ 5.5519e-02, -8.8711e-03,  4.3735e-03,  ..., -6.3535e-01,\n           -7.4796e-03,  2.6448e-02]],\n\n         [[-3.3454e-03,  4.8980e-03, -3.1898e-04,  ..., -1.9954e-03,\n            1.7622e-03, -2.5852e-03],\n          [ 5.1398e-03, -2.9751e-02,  1.2302e-02,  ..., -9.1795e-03,\n            4.0730e-03, -1.5863e-02],\n          [-2.3308e-02, -4.6019e-02, -1.7086e-02,  ..., -8.9279e-03,\n            1.3092e-02, -1.5561e-02],\n          ...,\n          [-2.2897e-02, -4.0762e-02,  1.6762e-02,  ...,  2.2820e-02,\n            1.0194e-02, -1.7126e-02],\n          [-9.7542e-03, -3.0673e-02,  2.9334e-03,  ..., -1.3603e-04,\n           -2.8890e-02, -2.3405e-02],\n          [ 2.9032e-02, -5.2597e-02, -8.8432e-03,  ...,  3.3433e-02,\n           -3.7110e-02,  1.5203e-02]],\n\n         ...,\n\n         [[ 9.9367e-04,  6.1843e-03,  4.6389e-03,  ..., -6.5315e-03,\n           -5.3103e-04,  5.0386e-02],\n          [ 9.7986e-02,  7.1571e-02, -3.1719e-01,  ...,  9.1785e-03,\n            1.3963e-01, -1.3442e-01],\n          [ 1.6356e-01,  1.1737e-01,  7.0028e-02,  ...,  5.6660e-02,\n           -2.8627e-02,  1.6380e-01],\n          ...,\n          [-1.5283e-01,  2.3898e-02,  2.5209e-02,  ..., -3.1508e-01,\n            9.1333e-02, -2.4199e-01],\n          [-1.5120e-01, -1.4941e-01,  3.5393e-01,  ..., -3.4368e-01,\n           -2.3338e-01, -4.7540e-01],\n          [-1.4661e-01, -3.1653e-02,  2.3825e-01,  ..., -6.3692e-02,\n            1.5490e-01,  2.3799e-01]],\n\n         [[ 4.6676e-04, -1.9197e-03, -3.0077e-03,  ..., -2.1647e-04,\n            2.2760e-03, -1.2781e-03],\n          [-2.9621e-02,  5.2740e-03, -7.8372e-03,  ...,  2.9044e-02,\n            2.8689e-02,  1.4206e-02],\n          [-9.3325e-03,  2.5521e-02, -2.2466e-02,  ...,  2.7017e-02,\n            2.1074e-02,  3.9316e-02],\n          ...,\n          [-1.5578e-02,  2.0450e-02,  2.3888e-02,  ...,  1.4758e-02,\n            1.2394e-02, -7.6523e-03],\n          [-3.4834e-02, -3.7326e-03,  4.6750e-03,  ...,  5.0211e-03,\n           -1.7635e-03, -2.2267e-02],\n          [-3.6493e-02, -7.0874e-03, -9.7820e-03,  ..., -4.4753e-03,\n           -1.6221e-02, -5.1860e-02]],\n\n         [[-3.1661e-03, -2.4087e-04, -1.6304e-03,  ...,  3.1428e-04,\n           -1.0679e-03, -7.3758e-05],\n          [ 2.3395e-03,  3.4858e-02, -1.4272e-02,  ..., -7.6889e-03,\n           -2.5462e-02,  1.1762e-02],\n          [-1.5638e-03,  2.9067e-02,  2.4562e-02,  ..., -3.2547e-02,\n           -2.8325e-03,  1.5654e-02],\n          ...,\n          [ 5.6595e-03,  4.8276e-02,  4.7991e-02,  ...,  5.7173e-03,\n            2.0603e-02,  3.1140e-02],\n          [-1.0313e-02,  6.3322e-03,  1.8373e-02,  ...,  2.4141e-02,\n            1.1929e-02,  2.4607e-02],\n          [ 4.1399e-02,  1.4931e-02,  4.7289e-02,  ...,  8.9202e-03,\n            3.9514e-02,  6.5601e-02]]],\n\n\n        [[[ 8.9872e-04, -4.9069e-04,  1.0789e-04,  ...,  1.2741e-03,\n            1.7263e-02, -2.3005e-03],\n          [-9.1174e-02,  1.3302e-02,  3.5689e-02,  ..., -1.9581e-02,\n           -9.3849e-02,  3.0970e-03],\n          [-4.4277e-02, -3.6540e-03,  2.1709e-02,  ..., -2.9502e-02,\n           -2.1283e-01,  1.7505e-02],\n          ...,\n          [-7.0081e-03, -4.1149e-02, -1.7536e-02,  ...,  2.0116e-02,\n           -2.2377e-01, -1.0629e-02],\n          [ 3.2934e-02,  8.3778e-03, -2.9639e-02,  ...,  1.3939e-02,\n           -1.5665e-03,  2.0757e-02],\n          [-3.0708e-02,  5.5674e-03, -1.1543e-02,  ..., -1.4820e-02,\n           -2.8713e-02, -3.2586e-03]],\n\n         [[ 1.3386e-03, -5.0765e-03, -4.4182e-03,  ..., -5.1051e-02,\n            2.7218e-03, -1.3367e-04],\n          [-2.9893e-02,  1.4032e-02,  3.0989e-02,  ...,  2.5464e-01,\n           -8.1151e-03, -4.8727e-02],\n          [-3.7179e-03,  5.3319e-02,  2.1679e-02,  ...,  2.2516e-01,\n           -2.2909e-02, -1.5531e-02],\n          ...,\n          [ 5.7906e-02,  2.2564e-03,  9.5767e-04,  ..., -2.0520e-01,\n           -4.0913e-02, -5.2163e-02],\n          [-4.2048e-02,  1.5526e-02,  9.2694e-02,  ..., -5.5794e-01,\n           -3.7071e-02, -2.2616e-03],\n          [-2.1245e-02,  3.1693e-02,  1.5146e-02,  ..., -2.1467e-01,\n           -4.7523e-02, -2.5296e-02]],\n\n         [[-2.9659e-03,  2.6939e-03, -1.3544e-03,  ..., -4.0622e-04,\n            9.3947e-04, -2.7621e-03],\n          [ 2.9642e-02, -5.3817e-02, -3.6545e-02,  ...,  4.3366e-02,\n           -3.3366e-02,  4.6556e-02],\n          [ 6.1418e-02, -2.1654e-02,  3.7117e-02,  ..., -9.7637e-04,\n           -1.8474e-02,  4.4412e-02],\n          ...,\n          [ 2.1157e-02, -2.6739e-03,  1.1366e-02,  ..., -1.3144e-02,\n           -1.4329e-02, -2.2176e-02],\n          [-2.8047e-02,  5.0103e-02,  3.7098e-02,  ..., -3.3263e-02,\n            1.3897e-02, -1.5961e-02],\n          [-1.3342e-02,  1.0131e-02,  3.5242e-02,  ...,  4.7784e-03,\n            2.5832e-03, -1.8739e-02]],\n\n         ...,\n\n         [[ 1.0852e-03,  1.6334e-03,  2.3662e-03,  ..., -6.7898e-03,\n            3.3711e-04,  2.8953e-02],\n          [ 1.0389e-01, -8.4058e-02,  1.0368e-01,  ...,  1.1114e-01,\n            2.6529e-01, -2.3122e-02],\n          [ 6.9112e-02, -1.2458e-01, -2.5269e-01,  ...,  8.8334e-02,\n            2.0078e-01, -3.5254e-01],\n          ...,\n          [-2.2387e-01,  2.6161e-01,  2.3791e-01,  ...,  1.6452e-02,\n            1.5897e-01, -3.0186e-01],\n          [ 1.7930e-01,  2.0955e-01,  2.6464e-01,  ..., -1.1346e-02,\n            1.1591e-02, -3.5667e-01],\n          [ 1.0582e-01,  8.2677e-02,  2.6549e-01,  ..., -2.2834e-01,\n            1.3283e-01, -1.3761e-01]],\n\n         [[-4.0329e-04, -2.5785e-04, -1.8789e-03,  ..., -1.8857e-04,\n           -1.5287e-03, -4.0774e-04],\n          [ 3.0200e-03,  5.8396e-02,  1.7715e-02,  ...,  2.7501e-02,\n           -1.3534e-02,  3.3344e-02],\n          [ 9.6029e-03,  2.3690e-03,  7.0688e-02,  ..., -3.0035e-02,\n            6.8920e-03,  4.6293e-02],\n          ...,\n          [-5.7871e-03,  3.2573e-02,  1.3137e-02,  ...,  1.2132e-02,\n           -2.3085e-02, -2.6943e-02],\n          [ 2.1664e-02,  2.5485e-02,  1.3793e-02,  ...,  4.2689e-03,\n           -1.5734e-02,  9.1973e-03],\n          [ 8.0067e-04,  2.0602e-02,  3.5865e-02,  ...,  1.0748e-02,\n            4.2585e-03, -6.4950e-04]],\n\n         [[-4.1349e-06, -4.0766e-04,  3.2817e-04,  ..., -1.3807e-03,\n           -1.3491e-04,  3.5608e-04],\n          [ 4.2447e-02, -1.7848e-02, -2.5429e-02,  ..., -4.2510e-02,\n           -4.6701e-02, -1.2824e-02],\n          [-7.4472e-03, -3.8525e-02, -2.3964e-02,  ...,  4.6363e-02,\n           -1.8740e-02, -2.9166e-03],\n          ...,\n          [ 2.2825e-02, -1.8236e-02, -2.6603e-03,  ..., -5.9628e-03,\n           -1.8921e-02,  6.0965e-03],\n          [-1.8467e-02, -3.6663e-02, -1.2939e-02,  ...,  4.5895e-02,\n            2.1313e-02, -6.5063e-04],\n          [-3.8967e-02,  4.4501e-03, -9.4441e-03,  ...,  3.6433e-02,\n           -2.7654e-02, -1.1595e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[ 8.5260e-03, -5.7021e-01, -1.3953e-01,  ...,  1.0161e+00,\n            4.3756e-02,  8.1622e-02],\n          [ 3.1703e-03,  6.1238e-01, -7.0813e-02,  ..., -9.9243e-01,\n            1.3344e+00,  1.2796e-01],\n          [-4.1521e-03,  5.6004e-01, -7.0256e-02,  ..., -8.8611e-01,\n            1.5017e+00,  1.2911e-01],\n          ...,\n          [-1.5353e-02,  4.0604e-01, -8.5714e-02,  ..., -8.2312e-01,\n           -3.3354e-01,  1.0204e-01],\n          [-2.7673e-02,  4.9657e-01, -9.4866e-02,  ..., -8.1624e-01,\n            4.0144e-02,  1.0135e-01],\n          [-1.2539e-02,  3.9569e-01, -8.8339e-02,  ..., -8.3488e-01,\n           -1.4192e-01,  1.0972e-01]],\n\n         [[ 3.5833e-01,  5.7653e-02,  6.6976e-02,  ...,  4.2951e-01,\n           -2.8360e-01,  6.3215e-02],\n          [-7.7288e-01, -9.6139e-02, -2.6244e-01,  ..., -6.1043e-01,\n            4.6666e-01, -1.1251e-01],\n          [-6.7056e-01, -9.5680e-02, -2.4403e-01,  ..., -5.1718e-01,\n            4.0216e-01, -1.1034e-01],\n          ...,\n          [-5.9267e-01, -1.0215e-01, -2.3562e-01,  ..., -4.4992e-01,\n            3.5366e-01, -1.1485e-01],\n          [-5.6374e-01, -7.5269e-02, -2.0879e-01,  ..., -4.2224e-01,\n            3.2744e-01, -8.7717e-02],\n          [-5.6572e-01, -7.6040e-02, -2.0715e-01,  ..., -4.2290e-01,\n            3.2731e-01, -8.8133e-02]],\n\n         [[-1.5381e-02, -2.7123e-01, -4.6577e-01,  ..., -4.2684e-01,\n            3.2219e-01,  5.3098e-01],\n          [-5.0327e-02, -2.3558e-01,  5.3305e-01,  ...,  6.5030e-01,\n           -3.0314e-01, -7.6586e-01],\n          [-1.0600e-01, -3.0711e-01,  4.6516e-01,  ...,  5.9871e-01,\n           -4.2217e-01, -7.0720e-01],\n          ...,\n          [ 1.4160e-02, -4.9117e-01, -3.9609e-01,  ...,  2.8613e-02,\n            2.3886e-01, -3.6012e-01],\n          [ 3.8633e-02, -5.6408e-01, -6.7022e-01,  ..., -3.0200e-01,\n            9.0391e-01, -2.7210e-01],\n          [ 6.1005e-02, -8.3077e-01, -5.2666e-01,  ..., -1.7432e-01,\n            5.2391e-01, -2.9955e-01]],\n\n         ...,\n\n         [[ 2.5928e-01,  3.2929e-01,  3.7298e-01,  ...,  2.2046e-01,\n           -1.9610e-01,  3.7701e-01],\n          [-6.2473e-01, -5.7996e-01, -7.1820e-01,  ..., -4.4632e-01,\n            4.6681e-01, -8.4527e-01],\n          [-5.2550e-01, -4.7819e-01, -5.9710e-01,  ..., -3.7172e-01,\n            3.9270e-01, -7.1054e-01],\n          ...,\n          [-4.5799e-01, -4.1004e-01, -5.1792e-01,  ..., -3.1885e-01,\n            3.4024e-01, -6.2296e-01],\n          [-4.5017e-01, -4.0197e-01, -5.0739e-01,  ..., -3.1342e-01,\n            3.3485e-01, -6.1051e-01],\n          [-4.2700e-01, -3.7829e-01, -4.8005e-01,  ..., -2.9505e-01,\n            3.1659e-01, -5.8065e-01]],\n\n         [[-1.5357e-01, -1.5573e-01, -1.3735e-01,  ...,  8.8573e-02,\n           -4.4596e-02, -6.4953e-02],\n          [ 1.5988e+00,  4.9735e-01,  1.3776e+00,  ...,  5.7621e-01,\n            3.0069e-01,  4.8128e-01],\n          [ 2.0434e+00,  1.5734e-01, -3.6507e-01,  ..., -2.8538e-01,\n            4.4062e-01,  5.3145e-01],\n          ...,\n          [ 2.0811e-01,  3.0320e-01, -1.9890e+00,  ..., -2.8614e-02,\n            6.9650e-01, -5.0264e-01],\n          [ 4.0687e-01,  1.0799e+00,  6.0958e-01,  ...,  5.2189e-01,\n           -5.5785e-01, -3.2977e-01],\n          [ 5.1111e-01,  1.1383e+00, -6.7782e-01,  ...,  2.8729e-01,\n           -1.1975e-01,  1.5205e-01]],\n\n         [[ 3.2380e-01, -9.8533e-03, -3.0129e-02,  ..., -2.2944e-01,\n           -7.7900e-02,  6.0285e-06],\n          [-2.4704e+00, -1.9260e-02, -2.7682e-02,  ...,  7.7409e-01,\n            2.9448e-01, -4.8684e-03],\n          [-1.6240e+00,  4.0145e-02, -6.8348e-02,  ...,  1.0598e+00,\n            1.3886e-01, -6.6325e-02],\n          ...,\n          [ 1.3847e+00,  1.8102e-02, -6.7595e-02,  ...,  2.5838e+00,\n            1.1586e-01, -2.6192e-02],\n          [ 1.1044e+00, -2.1549e-02, -3.1875e-02,  ...,  2.4728e+00,\n            2.2054e-01,  6.8737e-03],\n          [ 1.3509e+00,  1.9722e-02, -5.0141e-02,  ...,  2.6410e+00,\n            3.1342e-02, -1.9109e-02]]],\n\n\n        [[[ 8.4092e-03, -5.6966e-01, -1.3962e-01,  ...,  1.0202e+00,\n            3.5089e-02,  8.1690e-02],\n          [-4.6418e-03,  5.4564e-01, -9.8741e-02,  ..., -7.2999e-01,\n            1.7046e+00,  9.7874e-02],\n          [-1.0263e-04,  5.3682e-01, -7.5162e-02,  ..., -8.7896e-01,\n            1.0727e+00,  1.4064e-01],\n          ...,\n          [-2.6988e-02,  4.2177e-01, -7.7417e-02,  ..., -9.2635e-01,\n            1.1059e-02,  1.2148e-01],\n          [-2.7927e-02,  4.5771e-01, -8.3726e-02,  ..., -9.3276e-01,\n           -1.4306e-01,  1.0601e-01],\n          [-2.2991e-02,  5.1920e-01, -9.0046e-02,  ..., -9.6392e-01,\n            6.8208e-01,  8.9436e-02]],\n\n         [[ 3.5711e-01,  5.7232e-02,  6.6227e-02,  ...,  4.2834e-01,\n           -2.8251e-01,  6.2737e-02],\n          [-6.5319e-01, -6.1766e-02, -2.1437e-01,  ..., -4.9821e-01,\n            3.7975e-01, -7.6924e-02],\n          [-7.0602e-01, -7.2835e-02, -2.2879e-01,  ..., -5.4756e-01,\n            4.1332e-01, -8.7886e-02],\n          ...,\n          [-6.0196e-01, -9.1206e-02, -2.2612e-01,  ..., -4.5739e-01,\n            3.5635e-01, -1.0365e-01],\n          [-5.8121e-01, -9.6464e-02, -2.2678e-01,  ..., -4.3783e-01,\n            3.4488e-01, -1.0845e-01],\n          [-6.1317e-01, -9.7910e-02, -2.3539e-01,  ..., -4.6816e-01,\n            3.6539e-01, -1.1096e-01]],\n\n         [[-1.4532e-02, -2.6804e-01, -4.7310e-01,  ..., -4.3226e-01,\n            3.2329e-01,  5.3806e-01],\n          [-4.6537e-02, -4.2372e-01,  3.2437e-01,  ...,  3.5610e-01,\n           -3.8794e-02, -6.0675e-01],\n          [-2.8163e-02, -3.6323e-01,  4.5120e-01,  ...,  5.3465e-01,\n           -8.3368e-02, -7.0963e-01],\n          ...,\n          [ 5.6920e-02, -7.4522e-01, -9.1981e-01,  ..., -6.0783e-01,\n            8.3841e-01, -7.1200e-03],\n          [ 6.2419e-02, -6.4241e-01, -8.0045e-01,  ..., -3.9099e-01,\n            5.5713e-01, -1.1935e-01],\n          [ 7.0553e-02, -8.0212e-01, -8.5715e-01,  ..., -5.1365e-01,\n            1.0854e+00, -2.0097e-01]],\n\n         ...,\n\n         [[ 2.5892e-01,  3.2894e-01,  3.7269e-01,  ...,  2.2007e-01,\n           -1.9571e-01,  3.7676e-01],\n          [-5.3981e-01, -4.9267e-01, -6.1447e-01,  ..., -3.8153e-01,\n            4.0237e-01, -7.3003e-01],\n          [-5.7774e-01, -5.3222e-01, -6.6304e-01,  ..., -4.0822e-01,\n            4.2889e-01, -7.8445e-01],\n          ...,\n          [-4.4305e-01, -3.9489e-01, -5.0044e-01,  ..., -3.0659e-01,\n            3.2805e-01, -6.0374e-01],\n          [-4.4828e-01, -3.9924e-01, -5.0091e-01,  ..., -3.1661e-01,\n            3.3812e-01, -6.0197e-01],\n          [-4.8182e-01, -4.3415e-01, -5.4476e-01,  ..., -3.3849e-01,\n            3.5976e-01, -6.5166e-01]],\n\n         [[-1.6862e-01, -1.5908e-01, -1.3528e-01,  ...,  7.6607e-02,\n           -4.4740e-02, -6.8665e-02],\n          [ 2.7894e+00,  6.4937e-01,  1.0708e+00,  ...,  1.0144e-01,\n           -9.1698e-01,  2.4435e-01],\n          [ 2.2517e+00,  1.5697e+00,  1.6082e+00,  ...,  6.6918e-01,\n           -3.2419e-01,  7.7416e-01],\n          ...,\n          [ 5.5883e-01,  1.9359e+00,  4.0236e-02,  ...,  3.7300e-01,\n            9.6423e-01, -1.4284e-01],\n          [ 2.4680e-01,  1.0735e+00, -1.2803e+00,  ..., -3.0698e-01,\n            1.6478e-01, -2.1925e-01],\n          [ 1.2753e+00,  2.0096e+00,  7.9504e-01,  ...,  4.7886e-01,\n           -1.0373e-01, -1.0879e-01]],\n\n         [[ 3.2857e-01, -1.0023e-02, -2.9893e-02,  ..., -2.3993e-01,\n           -7.8214e-02,  2.5051e-04],\n          [-2.0688e+00,  1.3261e-02, -7.4038e-02,  ...,  5.6338e-02,\n            2.8225e-01, -4.0713e-02],\n          [-2.3496e+00,  6.0152e-02, -1.0651e-01,  ...,  7.6028e-01,\n            1.6211e-01, -7.9702e-02],\n          ...,\n          [ 9.2669e-01, -3.8443e-02,  6.6554e-03,  ...,  2.7340e+00,\n            1.8176e-01,  2.1620e-02],\n          [ 1.7230e+00, -3.3312e-02, -1.8918e-02,  ...,  2.8162e+00,\n            8.6078e-02,  1.3407e-02],\n          [ 2.9215e-01, -1.5329e-02, -5.3515e-02,  ...,  2.8641e+00,\n            2.2424e-01, -1.1189e-02]]],\n\n\n        [[[ 8.7017e-03, -5.7370e-01, -1.3910e-01,  ...,  1.0263e+00,\n            4.3097e-02,  8.2500e-02],\n          [ 5.9847e-03,  4.6304e-01, -6.3928e-02,  ..., -7.9820e-01,\n            9.4481e-01,  1.6534e-01],\n          [ 1.0381e-02,  5.1935e-01, -6.7659e-02,  ..., -8.7295e-01,\n            1.5503e+00,  1.4379e-01],\n          ...,\n          [-1.2660e-02,  4.6415e-01, -8.4901e-02,  ..., -9.0236e-01,\n           -4.3049e-02,  1.0552e-01],\n          [-1.6912e-02,  5.0278e-01, -8.9542e-02,  ..., -1.0347e+00,\n            2.6295e-01,  1.0560e-01],\n          [-1.6805e-02,  3.7124e-01, -8.8903e-02,  ..., -7.7326e-01,\n            1.5679e-01,  1.1563e-01]],\n\n         [[ 3.6053e-01,  5.7776e-02,  6.7284e-02,  ...,  4.3146e-01,\n           -2.8480e-01,  6.3349e-02],\n          [-6.8117e-01, -9.0058e-02, -2.4344e-01,  ..., -5.2674e-01,\n            4.1060e-01, -1.0518e-01],\n          [-7.1521e-01, -9.5210e-02, -2.5591e-01,  ..., -5.5796e-01,\n            4.3194e-01, -1.1191e-01],\n          ...,\n          [-6.2401e-01, -9.9793e-02, -2.4106e-01,  ..., -4.7939e-01,\n            3.7396e-01, -1.1352e-01],\n          [-7.0030e-01, -1.0950e-01, -2.6511e-01,  ..., -5.4897e-01,\n            4.2715e-01, -1.2533e-01],\n          [-5.5448e-01, -9.3748e-02, -2.2553e-01,  ..., -4.1411e-01,\n            3.3432e-01, -1.0627e-01]],\n\n         [[-1.4684e-02, -2.7456e-01, -4.7219e-01,  ..., -4.3402e-01,\n            3.2418e-01,  5.3719e-01],\n          [-6.6150e-02, -3.5010e-01,  2.8955e-02,  ...,  1.1579e-01,\n           -8.0754e-02, -2.9150e-01],\n          [-5.0748e-02, -3.2920e-01,  2.1290e-01,  ...,  2.6557e-01,\n           -9.8752e-02, -5.1289e-01],\n          ...,\n          [ 6.6813e-02, -5.8339e-01, -4.8992e-01,  ..., -1.1176e-01,\n            6.7245e-01, -3.5045e-01],\n          [ 4.3955e-02, -7.6510e-01, -8.2047e-01,  ..., -4.5961e-01,\n            6.6994e-01, -8.2579e-02],\n          [-1.4787e-02, -9.0547e-01, -6.7889e-01,  ..., -3.8407e-01,\n            2.9181e-01, -8.8089e-02]],\n\n         ...,\n\n         [[ 2.6173e-01,  3.3182e-01,  3.7613e-01,  ...,  2.2216e-01,\n           -1.9779e-01,  3.8061e-01],\n          [-5.2853e-01, -4.8092e-01, -5.9938e-01,  ..., -3.7457e-01,\n            3.9550e-01, -7.1261e-01],\n          [-5.6854e-01, -5.2207e-01, -6.4829e-01,  ..., -4.0602e-01,\n            4.2681e-01, -7.6716e-01],\n          ...,\n          [-4.9474e-01, -4.4777e-01, -5.6168e-01,  ..., -3.4794e-01,\n            3.6919e-01, -6.7074e-01],\n          [-5.4123e-01, -4.9492e-01, -6.1496e-01,  ..., -3.8614e-01,\n            4.0721e-01, -7.2844e-01],\n          [-4.2765e-01, -3.7756e-01, -4.7469e-01,  ..., -2.9872e-01,\n            3.2015e-01, -5.7235e-01]],\n\n         [[-1.5916e-01, -1.4486e-01, -1.1227e-01,  ...,  8.5444e-02,\n           -4.2628e-02, -5.5262e-02],\n          [ 1.2839e+00,  1.0532e+00, -5.3008e-01,  ..., -4.1648e-01,\n           -3.3127e-01,  7.0134e-01],\n          [ 2.3142e+00,  8.4151e-01,  1.1914e+00,  ...,  2.8545e-01,\n           -2.0949e-02,  1.5053e-01],\n          ...,\n          [ 4.8914e-01,  1.4553e+00,  6.7393e-01,  ...,  7.3883e-01,\n            1.2832e-01, -9.0868e-02],\n          [ 6.9246e-01,  1.4330e+00,  3.6764e-01,  ...,  3.1518e-01,\n            8.1909e-01, -8.2417e-02],\n          [ 3.1954e-01,  9.7787e-01, -7.2973e-01,  ..., -1.5033e-01,\n            4.7703e-01,  8.8808e-02]],\n\n         [[ 3.3776e-01, -1.0115e-02, -2.9934e-02,  ..., -2.3637e-01,\n           -7.9399e-02,  3.9857e-04],\n          [-1.6015e+00,  3.8725e-03, -5.4370e-02,  ...,  7.8712e-01,\n            1.3516e-01, -2.9943e-02],\n          [-2.3980e+00, -3.2630e-02, -3.1963e-02,  ...,  6.9995e-01,\n            2.7330e-01,  1.8377e-04],\n          ...,\n          [ 7.3835e-01, -3.4884e-02, -4.1350e-03,  ...,  3.0321e+00,\n            1.1467e-01,  2.8510e-02],\n          [ 1.0964e+00, -1.0809e-02, -6.4591e-02,  ...,  2.7262e+00,\n            2.1196e-01, -1.3352e-02],\n          [ 1.0153e+00, -4.2948e-02,  3.0457e-03,  ...,  2.1295e+00,\n            1.1656e-01,  2.9204e-02]]],\n\n\n        ...,\n\n\n        [[[ 8.5843e-03, -5.7652e-01, -1.3973e-01,  ...,  1.0348e+00,\n            5.4888e-02,  8.1939e-02],\n          [ 9.3718e-03,  3.6708e-01, -8.8922e-02,  ..., -6.4158e-01,\n            1.0271e+00,  1.1863e-01],\n          [ 1.4254e-02,  6.2536e-01, -8.8126e-02,  ..., -9.9807e-01,\n            1.4593e+00,  9.4548e-02],\n          ...,\n          [-1.9121e-02,  4.2373e-01, -8.9718e-02,  ..., -8.7229e-01,\n           -1.4597e-02,  1.0408e-01],\n          [-2.5218e-02,  4.4376e-01, -9.0058e-02,  ..., -8.7797e-01,\n            6.5485e-02,  1.0187e-01],\n          [-2.1570e-02,  4.1814e-01, -8.6623e-02,  ..., -7.9451e-01,\n           -7.1378e-02,  1.1170e-01]],\n\n         [[ 3.6472e-01,  5.8300e-02,  6.8462e-02,  ...,  4.3537e-01,\n           -2.8756e-01,  6.3948e-02],\n          [-5.4939e-01, -5.7042e-02, -1.9249e-01,  ..., -4.0376e-01,\n            3.1589e-01, -7.0187e-02],\n          [-7.6450e-01, -9.8827e-02, -2.6529e-01,  ..., -6.0166e-01,\n            4.6239e-01, -1.1586e-01],\n          ...,\n          [-5.8920e-01, -1.0118e-01, -2.3405e-01,  ..., -4.4706e-01,\n            3.5411e-01, -1.1356e-01],\n          [-6.1851e-01, -1.0469e-01, -2.4622e-01,  ..., -4.7479e-01,\n            3.7640e-01, -1.1845e-01],\n          [-5.8489e-01, -9.8248e-02, -2.3632e-01,  ..., -4.4318e-01,\n            3.5405e-01, -1.1192e-01]],\n\n         [[-1.4384e-02, -2.8046e-01, -4.6784e-01,  ..., -4.3172e-01,\n            3.1799e-01,  5.3637e-01],\n          [-4.1380e-02, -2.9384e-01,  4.3643e-01,  ...,  5.3778e-01,\n           -3.7134e-01, -6.5079e-01],\n          [-7.0870e-02, -4.7994e-02,  6.8343e-01,  ...,  8.2781e-01,\n           -5.9933e-01, -8.3444e-01],\n          ...,\n          [ 3.8266e-02, -6.3053e-01, -1.1118e+00,  ..., -7.7882e-01,\n            1.2317e+00,  1.2598e-01],\n          [ 3.8581e-02, -6.9792e-01, -6.7722e-01,  ..., -3.5409e-01,\n            5.1391e-01, -1.2456e-01],\n          [ 1.1513e-02, -8.7597e-01, -6.8260e-01,  ..., -3.3341e-01,\n            3.8790e-01, -1.0740e-01]],\n\n         ...,\n\n         [[ 2.6479e-01,  3.3499e-01,  3.7999e-01,  ...,  2.2443e-01,\n           -2.0004e-01,  3.8495e-01],\n          [-4.4638e-01, -3.9663e-01, -5.0039e-01,  ..., -3.0906e-01,\n            3.3026e-01, -6.0303e-01],\n          [-6.0629e-01, -5.6083e-01, -6.9621e-01,  ..., -4.3156e-01,\n            4.5207e-01, -8.2166e-01],\n          ...,\n          [-4.5391e-01, -4.0529e-01, -5.0918e-01,  ..., -3.1852e-01,\n            3.3993e-01, -6.1132e-01],\n          [-4.9097e-01, -4.4293e-01, -5.5093e-01,  ..., -3.5050e-01,\n            3.7181e-01, -6.5601e-01],\n          [-4.4991e-01, -4.0064e-01, -5.0064e-01,  ..., -3.1981e-01,\n            3.4133e-01, -6.0026e-01]],\n\n         [[-1.5296e-01, -1.6072e-01, -1.3386e-01,  ...,  7.3320e-02,\n           -3.9768e-02, -6.3691e-02],\n          [ 1.8208e+00,  2.4712e-01, -1.0074e+00,  ..., -6.5144e-01,\n            1.7714e-01,  3.0490e-01],\n          [ 2.0688e+00, -6.3576e-01, -1.0084e+00,  ..., -6.2035e-02,\n           -3.3822e-01,  3.2579e-01],\n          ...,\n          [ 6.6596e-01,  1.9549e+00, -5.1300e-01,  ..., -2.0550e-01,\n            3.0918e-01,  3.0155e-01],\n          [ 7.9094e-02,  1.6644e+00,  4.2902e-01,  ...,  5.1038e-01,\n            1.5085e+00, -1.5403e-01],\n          [ 5.1949e-01,  7.6334e-01, -1.0654e+00,  ..., -2.0256e-01,\n            3.7645e-01, -3.0993e-01]],\n\n         [[ 3.3508e-01, -1.0503e-02, -2.9193e-02,  ..., -2.6411e-01,\n           -8.0341e-02,  8.0428e-04],\n          [-5.4197e-01, -8.5646e-02,  3.6902e-02,  ...,  6.6145e-01,\n            1.2639e-01,  4.3759e-02],\n          [-2.8215e+00,  1.7996e-02, -5.0695e-02,  ...,  8.7870e-01,\n            2.1951e-01, -4.4318e-02],\n          ...,\n          [ 1.1627e+00, -3.9049e-03, -3.5574e-02,  ...,  2.8685e+00,\n            7.8310e-02, -1.0545e-02],\n          [ 1.1734e+00, -5.7618e-02, -2.4285e-02,  ...,  2.3493e+00,\n            1.7116e-01,  3.2637e-02],\n          [ 1.7803e+00, -1.4191e-02, -3.6102e-02,  ...,  2.2523e+00,\n            9.2701e-02,  4.0003e-04]]],\n\n\n        [[[ 8.9253e-03, -5.8649e-01, -1.4038e-01,  ...,  1.0482e+00,\n            5.2227e-02,  8.0889e-02],\n          [ 1.2857e-02,  5.1927e-01, -7.6940e-02,  ..., -9.1919e-01,\n            8.7406e-01,  1.2404e-01],\n          [ 1.6063e-02,  6.3184e-01, -7.1539e-02,  ..., -1.1948e+00,\n            8.7329e-01,  1.2430e-01],\n          ...,\n          [-1.5330e-02,  4.5921e-01, -7.4538e-02,  ..., -8.6514e-01,\n            2.3997e-01,  1.3697e-01],\n          [-2.0978e-02,  3.7066e-01, -8.7104e-02,  ..., -9.0174e-01,\n           -2.0942e-01,  1.1648e-01],\n          [-3.0342e-02,  3.9147e-01, -1.0188e-01,  ..., -6.0363e-01,\n           -2.3648e-01,  1.0658e-01]],\n\n         [[ 3.7183e-01,  5.9324e-02,  7.0447e-02,  ...,  4.4178e-01,\n           -2.9214e-01,  6.5088e-02],\n          [-6.6862e-01, -8.9733e-02, -2.4329e-01,  ..., -5.1487e-01,\n            4.0180e-01, -1.0560e-01],\n          [-7.9807e-01, -1.0303e-01, -2.7315e-01,  ..., -6.3262e-01,\n            4.8309e-01, -1.2049e-01],\n          ...,\n          [-5.7240e-01, -8.6380e-02, -2.1906e-01,  ..., -4.3009e-01,\n            3.3624e-01, -9.8847e-02],\n          [-5.6561e-01, -9.2206e-02, -2.2034e-01,  ..., -4.2390e-01,\n            3.3233e-01, -1.0399e-01],\n          [-5.0268e-01, -6.7052e-02, -1.8918e-01,  ..., -3.6497e-01,\n            2.8588e-01, -7.7788e-02]],\n\n         [[-1.5637e-02, -2.8320e-01, -4.8603e-01,  ..., -4.4513e-01,\n            3.3287e-01,  5.5495e-01],\n          [-7.7116e-02, -1.9725e-02,  4.5178e-01,  ...,  6.2263e-01,\n           -4.0658e-01, -6.9940e-01],\n          [-5.8255e-02,  2.7148e-01,  4.9588e-01,  ...,  7.0842e-01,\n           -2.9568e-01, -7.6288e-01],\n          ...,\n          [ 4.8901e-02, -7.7106e-01, -7.0253e-01,  ..., -3.7993e-01,\n            6.4128e-01, -1.5665e-01],\n          [ 3.9134e-02, -8.3689e-01, -9.1752e-01,  ..., -5.3569e-01,\n            7.0342e-01, -1.2469e-02],\n          [ 7.6313e-02, -6.0148e-01, -6.4810e-01,  ..., -3.4944e-01,\n            8.3554e-01, -1.9906e-01]],\n\n         ...,\n\n         [[ 2.7091e-01,  3.4126e-01,  3.8749e-01,  ...,  2.2909e-01,\n           -2.0467e-01,  3.9341e-01],\n          [-5.3265e-01, -4.8517e-01, -6.0540e-01,  ..., -3.7676e-01,\n            3.9766e-01, -7.2025e-01],\n          [-6.4375e-01, -5.9945e-01, -7.4299e-01,  ..., -4.5971e-01,\n            4.8008e-01, -8.7442e-01],\n          ...,\n          [-4.4112e-01, -3.9256e-01, -4.9550e-01,  ..., -3.0737e-01,\n            3.2886e-01, -5.9692e-01],\n          [-4.3524e-01, -3.8632e-01, -4.8690e-01,  ..., -3.0447e-01,\n            3.2601e-01, -5.8666e-01],\n          [-3.8247e-01, -3.3247e-01, -4.2538e-01,  ..., -2.6129e-01,\n            2.8302e-01, -5.1985e-01]],\n\n         [[-1.6141e-01, -1.6481e-01, -1.4433e-01,  ...,  7.1901e-02,\n           -4.4986e-02, -7.1732e-02],\n          [ 1.3358e+00, -2.1208e-01, -3.0796e-01,  ..., -3.1560e-01,\n            1.0868e-01,  4.4908e-01],\n          [ 1.4047e+00, -2.6707e-01, -1.0914e+00,  ..., -1.2897e-01,\n           -3.6942e-01,  3.9438e-01],\n          ...,\n          [ 7.4837e-01,  1.4746e+00,  5.7827e-01,  ...,  2.7931e-01,\n            9.5278e-01,  2.2667e-01],\n          [ 5.2951e-01,  1.0157e+00, -1.5594e+00,  ..., -8.2701e-02,\n            1.7753e-01, -1.2269e-01],\n          [ 2.2487e-01,  8.2536e-01,  6.1065e-01,  ...,  7.2621e-01,\n           -9.1933e-01, -2.7702e-01]],\n\n         [[ 3.4828e-01, -9.2269e-03, -3.0282e-02,  ..., -2.5386e-01,\n           -8.4412e-02, -4.3205e-04],\n          [-1.7468e+00,  7.2992e-03, -3.2988e-02,  ...,  1.3171e+00,\n            1.5937e-01, -3.2538e-02],\n          [-3.0886e+00, -9.8120e-03, -2.6307e-02,  ...,  1.1290e+00,\n            1.8869e-01, -1.7274e-02],\n          ...,\n          [ 1.0454e+00, -3.5779e-02, -2.7046e-02,  ...,  2.6657e+00,\n            1.6707e-01,  1.6293e-02],\n          [ 1.8150e+00, -7.8445e-02,  3.8804e-02,  ...,  2.7955e+00,\n            8.4119e-02,  6.1944e-02],\n          [ 1.1979e+00, -3.5573e-02, -2.7046e-02,  ...,  2.1221e+00,\n            1.7951e-01,  1.9025e-02]]],\n\n\n        [[[ 8.2070e-03, -5.4971e-01, -1.3816e-01,  ...,  9.8154e-01,\n            2.6925e-02,  8.2648e-02],\n          [-7.4130e-04,  4.5467e-01, -9.0311e-02,  ..., -8.0076e-01,\n            1.3254e+00,  1.0095e-01],\n          [-2.8389e-03,  5.0052e-01, -7.5639e-02,  ..., -8.1037e-01,\n            1.2155e+00,  1.2263e-01],\n          ...,\n          [-2.0334e-02,  5.4820e-01, -8.3023e-02,  ..., -1.1278e+00,\n           -1.3789e-01,  1.0382e-01],\n          [-1.9184e-02,  3.6505e-01, -1.1718e-01,  ..., -1.1086e+00,\n           -1.1040e+00,  5.8767e-02],\n          [-1.7091e-02,  4.9807e-01, -9.4952e-02,  ..., -9.6372e-01,\n            1.2073e-02,  9.6489e-02]],\n\n         [[ 3.3484e-01,  5.4068e-02,  5.9525e-02,  ...,  4.0772e-01,\n           -2.6768e-01,  5.9115e-02],\n          [-6.7887e-01, -8.2730e-02, -2.4365e-01,  ..., -5.2558e-01,\n            4.0824e-01, -9.9685e-02],\n          [-6.8711e-01, -9.8084e-02, -2.5416e-01,  ..., -5.3255e-01,\n            4.1629e-01, -1.1426e-01],\n          ...,\n          [-7.3539e-01, -1.2092e-01, -2.8360e-01,  ..., -5.8323e-01,\n            4.5565e-01, -1.3775e-01],\n          [-6.0656e-01, -1.0560e-01, -2.4512e-01,  ..., -4.6379e-01,\n            3.6835e-01, -1.1959e-01],\n          [-6.3685e-01, -1.0722e-01, -2.5328e-01,  ..., -4.9174e-01,\n            3.8755e-01, -1.2188e-01]],\n\n         [[-1.4125e-02, -2.6549e-01, -4.6692e-01,  ..., -4.1785e-01,\n            3.2737e-01,  5.2242e-01],\n          [-1.5787e-02, -3.3446e-01,  3.5213e-01,  ...,  4.3345e-01,\n           -9.9310e-02, -5.9308e-01],\n          [-5.5802e-02, -2.7188e-01,  5.7128e-01,  ...,  6.5189e-01,\n           -4.3196e-01, -7.3352e-01],\n          ...,\n          [ 3.1653e-02, -4.0345e-01, -4.6634e-01,  ..., -3.3742e-02,\n            5.4021e-01, -4.0368e-01],\n          [ 1.7458e-02, -5.7997e-01, -4.1794e-01,  ...,  1.2832e-02,\n            3.5211e-01, -4.0220e-01],\n          [ 1.5379e-02, -5.1226e-01, -3.3880e-01,  ...,  4.1056e-02,\n            3.9770e-01, -4.4562e-01]],\n\n         ...,\n\n         [[ 2.4154e-01,  3.1102e-01,  3.5118e-01,  ...,  2.0696e-01,\n           -1.8268e-01,  3.5274e-01],\n          [-5.7072e-01, -5.2401e-01, -6.4735e-01,  ..., -4.0936e-01,\n            4.3012e-01, -7.6336e-01],\n          [-5.3989e-01, -4.9279e-01, -6.1462e-01,  ..., -3.8132e-01,\n            4.0216e-01, -7.3013e-01],\n          ...,\n          [-5.7465e-01, -5.2921e-01, -6.5507e-01,  ..., -4.1246e-01,\n            4.3339e-01, -7.7268e-01],\n          [-4.6094e-01, -4.1259e-01, -5.1826e-01,  ..., -3.2361e-01,\n            3.4500e-01, -6.2170e-01],\n          [-5.2313e-01, -4.7605e-01, -5.9087e-01,  ..., -3.7380e-01,\n            3.9493e-01, -7.0055e-01]],\n\n         [[-1.6221e-01, -1.3826e-01, -1.1911e-01,  ...,  9.8904e-02,\n           -4.2097e-02, -4.9100e-02],\n          [ 1.7543e+00,  7.8104e-01,  1.1617e+00,  ...,  1.8851e-01,\n            1.4241e-01,  7.0164e-01],\n          [ 1.8141e+00,  5.5105e-01, -6.5925e-01,  ..., -5.0298e-01,\n            1.9635e-01,  4.6697e-01],\n          ...,\n          [ 3.5386e-01,  1.0087e+00,  6.1454e-01,  ...,  5.9988e-01,\n            1.2129e+00, -1.1581e-02],\n          [ 5.9200e-01,  2.9516e-01, -1.1639e+00,  ...,  1.8343e-01,\n            1.1120e+00, -1.4119e-02],\n          [ 7.8736e-02,  1.1672e+00, -1.4029e-01,  ...,  2.2778e-01,\n            1.1396e+00, -2.1674e-02]],\n\n         [[ 3.1303e-01, -1.0040e-02, -3.0396e-02,  ..., -2.0387e-01,\n           -7.4954e-02, -8.5277e-06],\n          [-1.7179e+00,  9.1915e-02, -1.5570e-01,  ...,  1.1598e+00,\n            1.5608e-01, -1.1643e-01],\n          [-1.2649e+00,  9.8446e-02, -1.3418e-01,  ...,  6.7208e-01,\n            1.4565e-01, -1.1489e-01],\n          ...,\n          [ 7.2610e-01, -7.1671e-03, -4.3716e-02,  ...,  3.5225e+00,\n            1.9916e-01, -4.8579e-03],\n          [ 1.2089e+00, -2.8386e-02, -2.0679e-02,  ...,  3.3059e+00,\n            9.1680e-02,  1.3404e-02],\n          [ 1.2850e+00,  9.5001e-04, -7.1928e-02,  ...,  3.0729e+00,\n            1.1226e-01, -1.9934e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-2.8212e-03,  5.0874e-03,  2.4714e-03,  ..., -1.6492e-03,\n            4.2235e-03,  1.1211e-03],\n          [-1.4765e-02, -4.0602e-02, -3.5825e-02,  ..., -1.4037e-03,\n           -4.6966e-03,  2.1971e-02],\n          [ 5.9989e-02,  1.8237e-01, -1.7058e-01,  ...,  3.6338e-02,\n            7.8337e-02,  3.1525e-02],\n          ...,\n          [ 2.5640e-02, -1.5385e-02,  3.7176e-01,  ...,  3.5366e-02,\n           -1.2953e-02, -1.4923e-01],\n          [ 1.7658e-01,  4.0155e-02, -1.5364e-02,  ..., -5.0964e-02,\n            6.2141e-02,  4.2065e-02],\n          [ 4.8732e-02, -6.5212e-02,  8.8586e-02,  ..., -2.7965e-02,\n           -5.8375e-02, -3.0646e-02]],\n\n         [[-2.6676e-03,  3.9241e-04, -8.0906e-05,  ...,  4.5404e-03,\n            3.5899e-04,  1.1881e-03],\n          [ 3.1724e-02,  3.1507e-02, -1.0281e-01,  ...,  1.9035e-02,\n            8.8934e-02,  3.6245e-02],\n          [ 9.7881e-02, -7.1155e-02,  3.3595e-03,  ..., -2.2606e-02,\n            7.9076e-02,  3.5902e-02],\n          ...,\n          [-6.6678e-02, -1.4090e-02,  1.4221e-01,  ..., -1.3319e-01,\n           -1.6704e-01,  1.6990e-02],\n          [ 1.0859e-02, -6.1447e-02,  9.9084e-02,  ..., -1.2910e-02,\n           -7.5975e-02,  3.7045e-02],\n          [ 1.6672e-02, -9.2584e-02,  8.8114e-03,  ..., -3.1898e-02,\n           -1.8731e-04,  2.8174e-02]],\n\n         [[-4.0091e-03,  7.6313e-04,  1.6170e-03,  ...,  4.5280e-03,\n            1.2113e-02, -3.5248e-03],\n          [ 1.4856e-01,  6.5046e-02,  1.3080e-01,  ..., -5.6053e-01,\n           -4.9016e-01,  1.2051e-01],\n          [ 1.5653e-01,  8.2098e-02,  1.0613e-01,  ...,  8.7205e-02,\n            5.7131e-01,  2.2497e-02],\n          ...,\n          [-2.0571e-01, -1.3470e-01,  5.2410e-02,  ..., -5.9496e-02,\n            8.2723e-02,  1.0789e-01],\n          [ 1.5534e-01, -1.2113e-02,  7.2241e-02,  ..., -2.0446e-01,\n           -4.1867e-01,  9.0419e-02],\n          [ 1.0675e-01,  2.2899e-02,  8.0115e-02,  ..., -5.3410e-01,\n           -6.4234e-02,  1.6862e-01]],\n\n         ...,\n\n         [[-3.7312e-04,  2.4854e-03,  3.8970e-03,  ...,  1.2128e-04,\n           -2.7741e-04,  2.9875e-05],\n          [ 5.2323e-03,  3.6667e-02,  1.1274e-02,  ...,  5.8796e-02,\n            7.6901e-03,  4.9520e-02],\n          [ 9.2795e-02, -1.4123e-02, -5.0552e-02,  ..., -1.5446e-02,\n            1.2240e-03, -5.0515e-02],\n          ...,\n          [-2.3629e-02, -5.0004e-02,  3.2636e-02,  ...,  1.3638e-02,\n            9.9850e-03, -2.0646e-02],\n          [ 9.9571e-03, -4.6444e-02, -2.1932e-02,  ..., -1.8503e-02,\n           -2.6931e-02, -7.5720e-03],\n          [ 2.4552e-03, -8.5949e-02, -6.3234e-02,  ...,  1.2846e-02,\n           -3.0588e-02, -2.5642e-02]],\n\n         [[ 3.3883e-03,  6.2485e-04,  7.2851e-04,  ...,  1.0093e-02,\n            9.2284e-04,  4.4338e-02],\n          [ 9.6061e-02, -1.7187e-01, -4.7407e-01,  ...,  3.2522e-01,\n           -1.7824e-01,  9.2844e-01],\n          [-7.0470e-02,  1.3669e-01,  8.6827e-01,  ...,  3.0296e-01,\n           -3.3964e-01, -5.0779e-01],\n          ...,\n          [-3.3234e-01, -6.3083e-02,  3.1945e-01,  ..., -9.5926e-02,\n            3.6069e-02,  4.0928e-01],\n          [ 9.9675e-02,  8.8404e-02, -3.7428e-01,  ..., -4.2543e-02,\n           -4.0863e-01,  5.3532e-01],\n          [ 1.2304e-01, -8.1200e-02,  3.3291e-01,  ..., -8.5647e-02,\n           -9.9924e-02,  8.3046e-01]],\n\n         [[-6.0271e-03,  6.3592e-03,  2.3788e-03,  ...,  8.2948e-03,\n            7.4689e-03, -9.8104e-03],\n          [-1.9389e-01,  9.1085e-02,  1.5666e-01,  ...,  8.0033e-02,\n           -9.7269e-02,  2.9464e-02],\n          [ 7.3357e-01, -4.7625e-03,  2.6068e-01,  ..., -3.0547e-01,\n           -3.1745e-02, -2.0520e-02],\n          ...,\n          [-2.1614e-01,  6.5019e-02,  6.1057e-02,  ..., -3.6828e-01,\n           -1.7111e-01,  2.4216e-01],\n          [-2.0390e-01, -9.3919e-02, -9.7950e-02,  ...,  2.0802e-02,\n           -3.6151e-02,  1.3201e-01],\n          [ 1.1863e-01, -1.0614e-01, -1.0879e-01,  ..., -1.8655e-02,\n            1.5320e-01,  1.6312e-01]]],\n\n\n        [[[-2.6980e-03,  1.1892e-03, -1.6953e-03,  ..., -5.1242e-03,\n            2.9248e-03,  2.9264e-03],\n          [ 1.5214e-01, -1.2727e-03, -1.2574e-01,  ..., -1.1662e-02,\n           -9.3991e-02, -8.8759e-04],\n          [ 5.0489e-02, -1.2779e-01, -1.4526e-01,  ...,  5.3272e-02,\n           -1.2263e-01,  2.5669e-02],\n          ...,\n          [ 3.9713e-03,  9.4114e-02,  2.2680e-01,  ...,  3.3562e-02,\n           -1.2147e-02, -4.9018e-02],\n          [-4.1322e-02,  3.4778e-02, -3.7618e-02,  ...,  2.2817e-02,\n            9.0250e-02, -4.2764e-02],\n          [ 8.0828e-02,  1.4031e-02,  8.2237e-02,  ..., -4.0238e-02,\n            4.1403e-02, -9.0204e-02]],\n\n         [[-4.9367e-03, -2.2613e-04,  1.3481e-03,  ...,  3.8171e-03,\n           -1.8990e-03,  1.7804e-04],\n          [ 2.0474e-02, -2.4488e-02, -3.5139e-04,  ..., -3.8044e-03,\n            1.5271e-02,  2.9143e-02],\n          [ 7.7171e-02, -6.1259e-02,  1.0805e-02,  ..., -2.0050e-02,\n            5.7909e-03,  6.4442e-02],\n          ...,\n          [-7.0600e-02, -2.6414e-02,  6.6025e-02,  ...,  3.8332e-03,\n           -3.8174e-02,  3.8935e-02],\n          [-7.9220e-03,  1.1978e-03,  2.9736e-02,  ..., -7.4527e-02,\n           -1.2087e-02, -3.3287e-02],\n          [-2.4050e-02, -6.0868e-02, -1.1410e-02,  ...,  2.0448e-03,\n           -1.8428e-02,  3.5917e-02]],\n\n         [[-4.2035e-03, -9.4154e-04, -2.0943e-03,  ..., -2.4579e-03,\n            9.5686e-03, -3.1170e-03],\n          [-9.1620e-02,  7.0337e-02,  1.5344e-04,  ..., -1.9044e-01,\n           -1.3666e-02,  9.2686e-02],\n          [ 1.9079e-02,  1.2537e-01, -6.3886e-02,  ..., -8.3420e-01,\n           -2.6545e-01,  1.7820e-01],\n          ...,\n          [ 1.3887e-01, -1.0792e-01,  7.3255e-02,  ..., -3.9760e-01,\n           -3.0376e-01,  2.7675e-01],\n          [-1.1055e-01, -1.2851e-01,  4.1116e-02,  ...,  5.0946e-01,\n            1.2533e-01,  3.7930e-01],\n          [ 8.3403e-02, -5.7402e-02,  8.0183e-02,  ..., -8.1025e-02,\n           -3.0969e-01, -2.4441e-01]],\n\n         ...,\n\n         [[-3.4952e-04,  1.9788e-03,  4.0759e-03,  ..., -8.4324e-04,\n            7.0402e-05, -7.5194e-04],\n          [-3.1115e-02, -9.8139e-03,  1.1514e-02,  ..., -5.3184e-02,\n           -4.9600e-03, -1.5838e-02],\n          [-3.9634e-04, -7.1671e-03, -7.3116e-02,  ..., -5.3813e-02,\n           -2.9059e-02,  1.6457e-02],\n          ...,\n          [-6.4925e-02, -3.3369e-02,  7.8767e-03,  ...,  1.2084e-02,\n           -4.8482e-02,  7.6429e-02],\n          [ 1.7601e-02, -1.0317e-02, -1.1180e-02,  ..., -6.1756e-03,\n           -5.5042e-02, -6.0106e-02],\n          [-2.0944e-02, -9.2233e-02,  1.6094e-02,  ...,  1.5671e-02,\n           -1.0203e-02, -1.7460e-02]],\n\n         [[ 5.8596e-03, -6.3351e-03,  2.6003e-03,  ...,  3.2478e-03,\n            3.5822e-03,  4.1158e-02],\n          [ 3.5207e-02, -6.3087e-02, -3.4965e-01,  ..., -5.1603e-02,\n           -2.4400e-01, -4.9474e-02],\n          [-2.6614e-01, -5.0609e-01, -8.1702e-01,  ...,  2.5221e-01,\n           -5.0793e-01,  1.5426e-01],\n          ...,\n          [ 2.0473e-01, -3.4085e-01, -1.7241e-01,  ..., -3.6772e-01,\n            1.7013e-01, -2.5000e-02],\n          [ 4.2229e-03,  1.0711e-01,  3.4310e-01,  ...,  4.3575e-02,\n           -3.1126e-01, -5.1191e-01],\n          [-9.0677e-02, -7.6584e-02, -1.1523e-01,  ..., -3.0554e-01,\n            7.6482e-02,  4.2289e-01]],\n\n         [[-7.7925e-03,  6.0340e-03, -7.3734e-04,  ...,  1.0755e-02,\n            2.9849e-03, -6.6999e-03],\n          [-2.0010e-01,  2.8922e-01, -1.5434e-01,  ...,  3.7409e-02,\n            6.9255e-02, -1.5572e-01],\n          [-1.5435e-01,  2.3749e-01, -1.4997e-01,  ..., -2.2736e-02,\n            3.2346e-02, -1.4888e-01],\n          ...,\n          [ 4.0611e-02, -2.6158e-01, -2.1627e-01,  ...,  2.5662e-03,\n            1.6750e-01,  9.4787e-02],\n          [-1.2240e-01, -3.0907e-01,  2.7059e-01,  ...,  3.8763e-01,\n           -3.6345e-02, -2.6532e-01],\n          [-2.4634e-01, -2.7142e-01, -2.9602e-01,  ...,  1.1706e-01,\n            2.4938e-02, -1.4636e-01]]],\n\n\n        [[[-1.5861e-03, -1.1407e-03, -5.8705e-03,  ..., -3.4421e-03,\n            3.5679e-03,  3.4513e-03],\n          [ 2.1371e-02, -7.4400e-02, -4.4336e-01,  ..., -7.1804e-02,\n           -5.9341e-02,  1.7438e-01],\n          [ 1.5460e-02, -1.8022e-01, -2.1295e-01,  ...,  6.3450e-02,\n           -1.2446e-01,  7.0617e-02],\n          ...,\n          [ 1.1027e-01, -5.2428e-02,  8.4651e-03,  ..., -2.5115e-02,\n            8.5150e-02,  6.6400e-02],\n          [ 6.6244e-02,  5.5591e-02,  1.4166e-01,  ..., -4.4402e-02,\n            1.1932e-01,  7.7874e-02],\n          [-1.8653e-02,  1.1921e-01,  2.7160e-01,  ..., -5.6074e-03,\n            1.3545e-01,  6.6375e-02]],\n\n         [[-5.3781e-03, -1.4433e-05,  3.0894e-03,  ...,  2.7480e-03,\n           -3.1644e-03, -1.1108e-03],\n          [ 2.7886e-03,  6.8302e-03,  4.2681e-02,  ..., -4.3028e-02,\n           -3.1933e-02, -1.3200e-02],\n          [ 7.4338e-03, -1.5156e-03, -2.0520e-02,  ..., -1.0020e-01,\n           -6.6599e-03, -1.2112e-02],\n          ...,\n          [ 6.8778e-03, -5.0170e-02,  1.0712e-01,  ..., -1.4947e-02,\n           -3.6575e-02,  4.8964e-02],\n          [-4.6418e-02, -3.6767e-02,  4.8243e-02,  ...,  3.1835e-02,\n           -2.1250e-02,  5.1228e-02],\n          [-4.5892e-02, -3.0532e-02, -5.9257e-02,  ..., -2.0553e-03,\n            5.5123e-02,  3.8655e-02]],\n\n         [[-3.8277e-03, -1.1812e-03, -5.0135e-06,  ..., -5.6670e-03,\n            9.5107e-03, -6.0724e-04],\n          [-5.2948e-02, -9.5316e-03, -1.7140e-03,  ...,  3.8510e-01,\n           -7.8515e-02,  9.3605e-02],\n          [ 5.8395e-02, -1.7378e-02,  7.1451e-02,  ...,  2.6207e-02,\n            1.8749e-01, -5.4560e-02],\n          ...,\n          [ 1.8037e-01, -1.7303e-02,  2.3794e-02,  ..., -2.7507e-01,\n           -5.8483e-01,  2.2416e-02],\n          [ 2.2410e-01, -4.4777e-03,  2.3077e-02,  ...,  3.7058e-01,\n           -3.1082e-01,  1.4935e-01],\n          [-1.0505e-03,  3.4855e-02,  2.8867e-02,  ..., -8.4064e-01,\n           -2.2049e-01,  1.7744e-01]],\n\n         ...,\n\n         [[-4.8716e-04,  5.6077e-04,  4.4927e-03,  ..., -2.2726e-05,\n            2.4141e-03,  8.1268e-04],\n          [-3.6690e-02,  4.1741e-03,  3.9388e-03,  ...,  2.9449e-02,\n            1.7050e-03, -4.0207e-02],\n          [-1.2279e-02, -8.0213e-03,  1.7707e-02,  ...,  3.9185e-02,\n            4.5299e-02,  4.5573e-04],\n          ...,\n          [ 5.4985e-02, -4.7931e-02, -4.6790e-02,  ..., -6.0442e-03,\n           -2.2647e-02, -2.3393e-02],\n          [ 9.6787e-03, -7.4852e-02,  1.0869e-02,  ...,  1.0317e-02,\n            3.4251e-02, -2.0936e-02],\n          [ 2.8702e-02, -3.7802e-02,  1.3214e-04,  ..., -1.3910e-02,\n           -3.5789e-02, -2.9188e-02]],\n\n         [[ 9.5960e-03, -3.8402e-03,  4.6816e-03,  ...,  2.6518e-03,\n            1.2209e-03,  3.6631e-02],\n          [-8.3421e-02,  9.8709e-02,  2.8325e-01,  ..., -1.1877e-01,\n            8.8647e-02, -8.8709e-01],\n          [-1.1668e-01, -4.0809e-02,  3.4879e-01,  ..., -3.4375e-01,\n           -3.4429e-02, -2.7585e-01],\n          ...,\n          [ 4.9655e-02,  1.4720e-02, -1.2164e+00,  ...,  6.2621e-01,\n            2.3686e-01,  7.1715e-01],\n          [ 2.0432e-01, -3.3111e-01, -2.3418e-02,  ...,  8.2484e-03,\n            2.8519e-03, -8.6507e-01],\n          [ 7.6275e-03, -3.8926e-01, -5.1527e-01,  ..., -2.9274e-01,\n           -8.7958e-02, -8.5059e-01]],\n\n         [[-4.7571e-03,  6.0356e-03, -7.1524e-04,  ...,  1.2178e-02,\n            7.7574e-03, -8.0430e-03],\n          [ 3.6041e-01,  1.6022e-01,  2.7606e-01,  ...,  2.9381e-01,\n           -1.4734e-02, -5.9070e-02],\n          [ 1.4463e-01,  1.5148e-01, -3.9065e-02,  ...,  1.8539e-01,\n           -7.8423e-02, -1.5970e-01],\n          ...,\n          [-1.5248e-01,  2.5447e-01, -1.3954e-01,  ..., -9.2817e-03,\n           -3.1949e-01,  5.6547e-03],\n          [ 1.0258e-01,  9.8482e-02, -1.5448e-01,  ..., -1.8031e-01,\n           -2.6795e-01,  3.0650e-02],\n          [ 3.2688e-01, -1.1758e-01, -5.0625e-02,  ..., -1.1123e-01,\n           -4.2468e-01, -6.6005e-02]]],\n\n\n        ...,\n\n\n        [[[-1.8114e-03,  1.7303e-03, -3.7228e-03,  ..., -5.5881e-03,\n            2.5992e-03, -3.7564e-04],\n          [-1.5872e-02, -8.9005e-02,  2.9880e-01,  ..., -1.3151e-02,\n           -1.9025e-03,  9.6670e-02],\n          [ 3.0256e-02, -8.2892e-03, -1.0016e-01,  ..., -3.5269e-02,\n           -1.7155e-01, -5.2837e-02],\n          ...,\n          [ 5.8528e-02, -4.1893e-02,  1.7995e-01,  ..., -6.8318e-03,\n           -4.1201e-02, -1.0434e-01],\n          [ 1.3041e-02,  9.6813e-03,  2.3533e-01,  ...,  3.0614e-02,\n           -5.9910e-02, -1.3876e-01],\n          [ 4.3712e-02,  3.2437e-02,  4.4123e-01,  ..., -4.7676e-02,\n           -2.4082e-02, -3.5680e-02]],\n\n         [[-3.8414e-03,  1.5888e-04, -7.3014e-04,  ...,  5.1389e-03,\n            1.3256e-03, -1.4706e-03],\n          [ 1.5621e-03, -4.1681e-02,  4.7119e-02,  ..., -7.5561e-03,\n            6.9549e-02, -3.2740e-02],\n          [ 4.0244e-02,  1.0975e-02, -8.9441e-02,  ...,  3.1224e-02,\n            6.6910e-02, -6.2401e-02],\n          ...,\n          [ 2.7847e-02, -1.0493e-01,  4.4703e-02,  ..., -4.3522e-02,\n            8.8372e-04,  6.2892e-02],\n          [-4.6102e-02, -5.1953e-02, -6.2619e-02,  ..., -4.5612e-02,\n           -3.8223e-03, -3.5855e-02],\n          [-2.1709e-02, -8.7817e-02, -7.1946e-03,  ..., -7.4523e-02,\n           -5.3775e-02, -5.3213e-04]],\n\n         [[-7.0411e-03, -2.5603e-03, -1.1440e-03,  ..., -2.6203e-03,\n            5.0403e-03,  4.7393e-04],\n          [-1.7098e-01, -1.2087e-02,  1.2913e-02,  ...,  4.5173e-01,\n            2.3251e-01, -5.1387e-02],\n          [-1.8994e-01, -4.8155e-02,  2.4573e-02,  ..., -4.5138e-01,\n           -2.2572e-02,  2.0060e-01],\n          ...,\n          [-6.0015e-02, -1.3882e-01, -5.4577e-02,  ..., -6.4780e-01,\n            6.0197e-01,  6.5560e-02],\n          [ 3.0712e-01, -4.3819e-02, -2.0072e-02,  ..., -7.6457e-03,\n           -9.8081e-02, -1.0486e-01],\n          [-2.1914e-02, -9.7510e-02, -1.9990e-02,  ...,  7.4258e-01,\n            6.3685e-02,  2.1091e-01]],\n\n         ...,\n\n         [[ 2.3562e-03, -2.9635e-03,  3.6730e-03,  ..., -2.7838e-03,\n            4.0152e-03, -3.7146e-03],\n          [-1.4715e-03, -5.1843e-02, -9.1023e-03,  ..., -7.3549e-02,\n           -7.6687e-02, -9.6913e-02],\n          [-6.7459e-03,  1.7039e-03, -6.7103e-03,  ..., -7.9984e-02,\n           -1.0534e-01, -3.8590e-02],\n          ...,\n          [ 4.2764e-02, -6.5972e-02, -3.0657e-02,  ..., -1.8211e-02,\n           -1.3870e-02, -3.6220e-02],\n          [ 3.3670e-02, -1.1408e-01, -5.9324e-02,  ...,  4.7356e-02,\n            9.2798e-02,  5.9659e-02],\n          [ 6.3911e-02, -8.9889e-02, -3.4027e-02,  ...,  1.4503e-02,\n            5.3856e-02,  1.8267e-02]],\n\n         [[ 1.2931e-02, -5.7171e-04, -2.7237e-03,  ...,  8.9864e-03,\n            1.0822e-03,  4.5388e-02],\n          [ 3.5116e-01, -2.1022e-01,  2.3862e-01,  ...,  4.2309e-01,\n            1.8206e-02, -4.7085e-01],\n          [-3.2982e-01,  9.9782e-02,  4.2082e-01,  ..., -1.1578e-02,\n           -3.9459e-01,  6.1005e-01],\n          ...,\n          [-1.4190e-01, -9.6866e-02,  3.0365e-01,  ..., -1.1256e-01,\n            2.1121e-02, -1.0840e-02],\n          [ 1.0681e-01, -1.1565e-01, -3.1774e-01,  ..., -2.8997e-01,\n            4.3927e-01, -4.4518e-01],\n          [ 1.6871e-02,  9.1345e-02, -1.0608e-01,  ...,  2.1938e-01,\n           -1.3428e-01, -1.0197e+00]],\n\n         [[-9.0044e-03,  1.0115e-02, -1.9418e-03,  ...,  1.2072e-02,\n           -5.4394e-03, -7.3368e-03],\n          [-6.2184e-01,  3.0214e-01, -7.5039e-03,  ..., -2.6303e-01,\n            4.0741e-02, -8.4163e-01],\n          [-3.7602e-01,  9.1167e-02, -2.2047e-02,  ...,  1.8807e-02,\n           -1.9235e-01, -3.7937e-01],\n          ...,\n          [ 9.7293e-02, -2.5053e-01, -6.5612e-02,  ...,  1.2514e-02,\n            3.2742e-01, -1.2453e-01],\n          [ 2.9712e-02, -4.9406e-02, -2.8048e-01,  ..., -2.4490e-02,\n           -8.5066e-02, -8.1952e-02],\n          [-5.8531e-03, -1.8056e-01, -8.9457e-02,  ...,  1.1621e-01,\n           -6.3219e-02, -2.0983e-01]]],\n\n\n        [[[-1.1245e-03,  8.8377e-04, -6.3503e-03,  ..., -3.7549e-03,\n            2.6898e-03,  1.5211e-03],\n          [-2.9478e-02, -2.6201e-02,  5.3717e-02,  ...,  8.5446e-02,\n            1.0510e-02,  4.6979e-02],\n          [-3.4047e-02, -5.9252e-02, -3.1451e-01,  ...,  3.1847e-02,\n           -1.0963e-01, -2.7862e-02],\n          ...,\n          [ 1.4031e-02,  1.5022e-02,  2.5379e-01,  ..., -5.4872e-03,\n            5.8055e-02,  6.6442e-02],\n          [ 3.6279e-02, -9.4423e-02,  1.2728e-01,  ...,  1.8588e-02,\n           -6.5614e-02, -2.6842e-02],\n          [ 2.8646e-02, -3.6161e-02, -1.8886e-01,  ..., -4.6256e-02,\n           -5.2521e-02, -2.6096e-02]],\n\n         [[-4.5638e-05, -1.6618e-03,  4.4990e-03,  ...,  3.6992e-03,\n           -1.5447e-03, -3.9615e-03],\n          [ 6.2650e-02, -1.9552e-02,  3.0456e-03,  ...,  2.5119e-02,\n            1.6141e-02, -3.3173e-02],\n          [ 5.7506e-02, -3.9411e-02, -6.1453e-02,  ..., -1.9858e-03,\n            2.2286e-02,  2.2483e-02],\n          ...,\n          [ 2.4202e-02, -7.0404e-02,  2.4861e-02,  ...,  3.1376e-02,\n           -1.6811e-02,  5.1994e-02],\n          [-6.8289e-03,  7.7487e-04,  1.2315e-02,  ...,  9.4424e-03,\n           -4.3688e-03, -4.6188e-02],\n          [-3.5335e-02, -3.1079e-03,  1.0916e-02,  ...,  4.0597e-02,\n           -2.9066e-02,  3.7299e-02]],\n\n         [[-5.0494e-03, -3.0524e-03, -7.3872e-04,  ..., -7.6551e-03,\n            1.5058e-02, -5.0385e-03],\n          [-4.7809e-02,  6.0292e-02, -2.9050e-03,  ...,  1.9480e-01,\n            3.2800e-01, -4.8227e-02],\n          [-1.3295e-01,  8.9344e-02,  1.6061e-02,  ..., -1.9380e-01,\n            2.8686e-01,  1.7451e-01],\n          ...,\n          [ 8.4986e-02, -2.7645e-02,  4.9928e-02,  ...,  2.0358e-01,\n           -4.4424e-01,  7.2048e-02],\n          [-8.8909e-02, -1.4212e-01, -3.7943e-02,  ...,  4.5480e-01,\n           -1.6860e-01,  1.2103e-01],\n          [ 2.0659e-02, -2.0571e-02, -4.5453e-03,  ..., -3.8095e-01,\n           -5.1717e-01, -2.3146e-02]],\n\n         ...,\n\n         [[ 3.6108e-03, -4.0555e-03, -1.1284e-03,  ..., -2.2930e-03,\n            3.2200e-03, -3.5256e-03],\n          [ 3.4172e-02, -4.6470e-02,  1.1834e-02,  ..., -6.3672e-02,\n           -2.8466e-02, -6.8717e-02],\n          [-1.0060e-02,  1.1186e-02,  3.4265e-02,  ..., -5.4272e-02,\n           -7.0556e-02, -1.0511e-01],\n          ...,\n          [ 6.5964e-02, -2.5981e-02,  4.3589e-03,  ...,  1.5870e-02,\n            7.0993e-03, -1.1731e-03],\n          [ 4.3829e-02, -7.1584e-02, -2.8542e-02,  ...,  1.9858e-02,\n           -3.3280e-02,  1.5401e-02],\n          [ 6.9174e-03, -1.8630e-02,  1.5707e-02,  ...,  1.8784e-02,\n           -1.7635e-03, -1.9391e-02]],\n\n         [[ 1.0008e-02,  4.1171e-04,  9.6395e-03,  ...,  1.8760e-03,\n           -2.5346e-03,  3.7441e-02],\n          [ 2.2968e-01, -4.3372e-02,  5.3730e-01,  ...,  5.0836e-01,\n           -2.3572e-01, -2.9105e-01],\n          [-2.5047e-01, -3.7978e-02,  3.8669e-01,  ...,  8.1801e-03,\n           -9.9756e-02,  2.4973e-01],\n          ...,\n          [-2.4954e-01, -1.7667e-02, -8.4159e-01,  ...,  2.5269e-02,\n           -3.7410e-01, -2.9055e-01],\n          [-8.0766e-03,  6.0839e-02, -1.6041e-01,  ..., -3.6241e-01,\n            2.6399e-01, -4.8795e-01],\n          [-1.9918e-01, -8.6111e-02,  3.9424e-02,  ..., -3.8073e-02,\n            8.3760e-02,  5.5802e-01]],\n\n         [[-8.5446e-03, -2.2567e-03, -1.3319e-03,  ...,  5.6999e-03,\n            6.4859e-03, -1.3118e-02],\n          [ 6.0947e-02, -3.7162e-01,  1.5238e-01,  ..., -1.5207e-01,\n           -2.7800e-01, -1.8832e-03],\n          [-1.9361e-01, -2.5366e-02,  1.2499e-01,  ..., -2.4002e-02,\n           -1.5542e-01, -2.1007e-01],\n          ...,\n          [ 1.4502e-01,  2.1301e-02, -2.2926e-01,  ..., -6.7802e-02,\n            8.7235e-02,  1.2236e-01],\n          [-1.2249e-01,  1.1513e-01,  1.7022e-01,  ...,  1.0967e-01,\n           -2.0879e-01,  1.4471e-01],\n          [-1.3203e-01, -1.6507e-02, -1.5179e-01,  ..., -6.0402e-02,\n            4.7338e-02,  1.5917e-01]]],\n\n\n        [[[-2.7697e-03,  1.3171e-03,  2.9931e-03,  ..., -3.1847e-03,\n            2.2585e-03,  3.1906e-03],\n          [-2.3311e-02,  6.1643e-02,  4.3625e-02,  ...,  7.7311e-02,\n           -7.6211e-02,  7.0385e-02],\n          [-8.9307e-03,  8.6536e-02,  1.9407e-01,  ...,  7.7987e-02,\n           -4.9523e-02, -5.2309e-02],\n          ...,\n          [ 2.8080e-02,  5.7641e-02,  6.3179e-02,  ...,  2.6664e-02,\n            5.2566e-02,  4.3088e-03],\n          [ 1.2134e-01,  3.2208e-02, -3.9688e-01,  ...,  3.9197e-02,\n            1.6972e-01,  1.5939e-01],\n          [-6.9421e-02,  5.0131e-02,  2.2336e-01,  ...,  1.6188e-01,\n            1.0195e-01,  7.0733e-02]],\n\n         [[-2.3986e-03, -1.2273e-03,  6.2628e-04,  ...,  3.6334e-03,\n           -1.9242e-04, -3.6687e-04],\n          [ 1.2233e-02, -8.4302e-02,  4.0768e-03,  ..., -3.5737e-02,\n            2.6939e-02,  4.4217e-03],\n          [ 3.2870e-02, -8.2149e-02, -3.0361e-02,  ...,  1.1724e-02,\n            4.3276e-02,  6.8009e-02],\n          ...,\n          [ 8.2696e-03, -6.5472e-02,  3.0000e-02,  ...,  4.0759e-02,\n            1.6662e-02,  3.8566e-02],\n          [-3.1594e-02, -1.0825e-01,  3.9491e-02,  ...,  1.2462e-03,\n            2.8099e-02,  7.8377e-02],\n          [-9.6568e-02, -4.0488e-02,  1.6921e-03,  ...,  1.1250e-02,\n           -2.5656e-02, -2.9871e-02]],\n\n         [[ 1.5102e-03,  3.8991e-03, -7.7517e-04,  ...,  4.0572e-04,\n            8.0271e-03, -6.1014e-03],\n          [ 1.8081e-01,  2.1108e-01, -8.6902e-02,  ...,  1.1314e-01,\n           -2.1960e-02, -2.7725e-01],\n          [ 4.3475e-02,  8.2225e-04, -1.1295e-01,  ...,  1.5856e-01,\n            4.2436e-01, -9.2775e-02],\n          ...,\n          [ 1.2390e-01, -5.5407e-03,  4.8812e-02,  ..., -1.9440e-01,\n           -1.3007e-01, -1.3517e-01],\n          [ 3.1205e-01,  1.0649e-02,  8.1812e-02,  ..., -5.3279e-01,\n           -1.7880e-01, -9.1527e-02],\n          [ 9.3545e-03, -4.3954e-02,  4.6639e-02,  ...,  8.5805e-02,\n           -6.5857e-02, -2.0760e-01]],\n\n         ...,\n\n         [[-3.4456e-04,  5.5799e-04,  2.7698e-03,  ..., -1.1236e-03,\n            2.5995e-03,  8.5219e-04],\n          [-4.6933e-02, -9.1260e-02, -1.6553e-02,  ..., -3.1954e-02,\n            5.1849e-03, -3.5514e-02],\n          [ 5.2623e-02, -3.4205e-02, -1.2263e-02,  ..., -1.2574e-03,\n           -3.5309e-02, -1.4291e-02],\n          ...,\n          [ 6.5254e-03, -9.0330e-02, -6.9196e-02,  ...,  2.8154e-02,\n           -2.1643e-02, -3.0487e-02],\n          [-1.8610e-02, -1.0050e-01, -5.7811e-02,  ...,  4.1774e-02,\n            7.2571e-03,  4.6638e-03],\n          [-6.5930e-02, -7.6441e-02, -4.3878e-03,  ...,  7.0904e-02,\n           -7.6844e-03,  9.9826e-02]],\n\n         [[-2.2135e-03, -4.5817e-03,  2.9060e-04,  ...,  2.9435e-03,\n            6.1866e-03,  3.4070e-02],\n          [-1.8019e-01, -1.0453e-01, -1.3833e-01,  ...,  6.8960e-02,\n            9.5624e-02, -3.9922e-01],\n          [ 6.6125e-03,  1.0944e-01,  1.1766e-02,  ..., -3.2459e-01,\n            9.1280e-02,  3.3828e-02],\n          ...,\n          [ 1.5072e-01, -1.1191e-01, -4.7532e-01,  ...,  3.0045e-01,\n           -5.8092e-01,  1.2954e-01],\n          [ 1.0011e-01,  6.4213e-02,  3.9986e-01,  ..., -2.3042e-01,\n            4.4859e-02,  3.9932e-01],\n          [-1.2150e-01, -1.9286e-01,  1.0888e-01,  ...,  8.7060e-02,\n           -1.9625e-02,  1.0689e-01]],\n\n         [[-7.7672e-03,  3.3054e-03, -5.0824e-03,  ...,  9.7648e-03,\n            6.4014e-03, -1.5296e-02],\n          [-3.9915e-01,  2.7669e-01, -4.4253e-02,  ...,  6.9821e-02,\n           -1.6403e-02, -2.8970e-01],\n          [-2.6627e-02, -1.8760e-02, -9.2522e-02,  ...,  5.3855e-02,\n           -1.1256e-01, -1.9332e-01],\n          ...,\n          [-1.3172e-01, -7.3418e-02,  1.2111e-01,  ..., -3.6452e-01,\n           -3.1500e-02,  2.0384e-01],\n          [ 9.7390e-02,  2.4340e-01, -1.4968e-01,  ..., -1.2757e-01,\n           -2.1179e-02,  4.1393e-01],\n          [-1.2517e-01, -1.1242e-01, -1.3546e-01,  ..., -4.6589e-01,\n            1.6113e-01,  3.5974e-03]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-5.2442e-01,  6.0131e-01, -2.0030e-01,  ...,  4.7124e-01,\n           -2.5054e-01, -7.3097e-02],\n          [ 1.3463e+00, -1.5652e+00, -1.3724e-01,  ..., -1.2988e+00,\n            7.8416e-01, -5.2989e-02],\n          [ 1.3419e+00, -1.3544e+00, -2.3535e-01,  ..., -1.2622e+00,\n            6.5770e-01, -8.4377e-02],\n          ...,\n          [ 6.1323e-01, -1.0917e+00, -3.2268e-01,  ..., -1.1250e+00,\n            3.6667e-01,  8.8158e-03],\n          [ 6.2011e-01, -1.0730e+00, -6.0070e-01,  ..., -1.0414e+00,\n            1.8267e-02, -2.1617e-02],\n          [ 2.0644e-01, -1.1671e+00, -9.0192e-01,  ..., -1.3996e+00,\n            1.7141e-01, -8.8014e-02]],\n\n         [[ 5.1428e-01,  2.8448e-01, -2.7818e-01,  ...,  3.8113e-01,\n            2.6781e-01, -4.9648e-01],\n          [-9.2211e-01, -7.1362e-01,  6.3773e-01,  ..., -3.4182e-01,\n           -2.4858e-01,  7.3234e-01],\n          [-6.9892e-01, -5.7525e-01,  4.8976e-01,  ..., -2.3095e-01,\n           -1.6524e-01,  5.5622e-01],\n          ...,\n          [-1.5730e-01, -4.8306e-01,  1.5149e-01,  ..., -1.3777e-01,\n           -8.9851e-02,  3.7873e-01],\n          [-2.3652e-01, -5.2114e-01,  2.1433e-01,  ..., -1.7823e-01,\n           -1.2899e-01,  4.3121e-01],\n          [-1.2310e-01, -6.6059e-01,  9.8422e-02,  ..., -1.7187e-01,\n           -1.2095e-01,  5.7391e-01]],\n\n         [[-2.2326e-01, -3.1957e-01,  1.9149e-01,  ..., -2.1345e-02,\n            1.1416e-01,  1.7967e-01],\n          [ 9.0692e-01,  1.1516e+00, -1.2958e+00,  ...,  1.0612e+00,\n           -8.4010e-01, -9.2275e-01],\n          [ 7.8478e-01,  1.0062e+00, -1.3485e+00,  ...,  1.2775e+00,\n           -8.4568e-01, -6.7299e-01],\n          ...,\n          [ 2.9511e-01,  1.0525e+00, -1.6619e+00,  ...,  2.8048e+00,\n           -1.4095e+00,  2.5678e-01],\n          [ 9.2824e-02,  9.1621e-01, -1.8033e+00,  ...,  2.2689e+00,\n           -1.4694e+00,  2.5913e-01],\n          [ 2.0923e-01,  1.2336e+00, -1.6977e+00,  ...,  2.4194e+00,\n           -1.5220e+00,  3.4994e-01]],\n\n         ...,\n\n         [[ 8.4586e-01,  1.6693e-01,  6.0383e-01,  ...,  4.2003e-01,\n            4.7874e-01,  5.0307e-01],\n          [-9.9890e-01, -1.0422e-01, -1.9700e+00,  ..., -1.1771e+00,\n           -1.3026e+00,  1.9837e-01],\n          [-1.3201e+00, -1.7701e-01, -7.1398e-01,  ..., -5.8788e-01,\n           -1.0808e+00,  6.0129e-01],\n          ...,\n          [-1.4236e+00, -1.6728e-01,  5.3863e-01,  ..., -3.8432e-02,\n           -4.5400e-01, -1.5696e-01],\n          [-1.4961e+00, -1.9737e-01,  6.5447e-01,  ..., -5.1434e-02,\n           -3.8194e-01, -1.3210e-01],\n          [-2.1168e+00, -9.5187e-02,  1.2580e+00,  ...,  5.1485e-01,\n           -2.6121e-01,  9.4026e-02]],\n\n         [[-3.1867e-01,  2.7881e-01, -1.0653e-02,  ..., -5.7297e-01,\n           -3.0807e-02, -1.8477e-02],\n          [ 1.4059e+00, -1.9417e+00, -3.0599e-02,  ...,  2.0388e+00,\n           -1.0215e+00, -1.4395e-01],\n          [ 1.1567e+00, -1.3454e+00,  1.7232e-01,  ...,  1.8400e+00,\n           -7.0079e-01, -2.5325e-01],\n          ...,\n          [-2.5498e-01,  5.0550e-01,  2.6952e-02,  ...,  1.5959e+00,\n           -8.3843e-01, -1.4547e-01],\n          [ 4.6490e-01,  4.5633e-01, -6.6298e-02,  ...,  2.2206e+00,\n           -1.1069e+00, -2.0369e-01],\n          [ 2.4664e-01,  1.7680e-01,  5.5649e-02,  ...,  2.2007e+00,\n           -9.1368e-01, -2.2097e-01]],\n\n         [[-3.8110e-01, -1.4502e-01, -3.5062e-01,  ...,  2.5014e-01,\n            1.6930e-02,  3.5777e-01],\n          [ 1.1031e+00,  6.2507e-01,  9.0861e-01,  ..., -1.1495e+00,\n            1.0329e+00, -1.5510e+00],\n          [ 1.3095e+00,  5.9489e-01,  1.4579e+00,  ..., -3.0156e-01,\n            1.3464e+00, -1.1101e+00],\n          ...,\n          [ 1.7581e+00,  5.3365e-01,  2.8388e-01,  ...,  6.0460e-01,\n            1.5685e+00, -4.1763e-01],\n          [ 1.7840e+00,  6.6658e-01, -7.4184e-01,  ...,  1.1222e+00,\n            2.0914e+00,  1.0605e+00],\n          [ 1.6420e+00,  5.8962e-01,  2.6736e-01,  ...,  8.7952e-01,\n            1.6376e+00,  1.0785e-01]]],\n\n\n        [[[-5.2533e-01,  5.9696e-01, -1.9424e-01,  ...,  4.6440e-01,\n           -2.4857e-01, -7.3567e-02],\n          [ 1.3435e+00, -1.5111e+00, -2.9111e-01,  ..., -1.2793e+00,\n            4.7487e-01, -5.4955e-02],\n          [ 1.0781e+00, -1.6426e+00, -3.6515e-01,  ..., -1.6606e+00,\n            7.6989e-01, -4.3754e-02],\n          ...,\n          [ 9.2787e-01, -9.2014e-01, -5.5521e-01,  ..., -8.9762e-01,\n            4.4648e-01, -4.9011e-02],\n          [ 1.1484e+00, -9.3112e-01, -4.8247e-01,  ..., -9.0648e-01,\n            2.7593e-01, -7.8350e-03],\n          [ 1.1822e+00, -1.0103e+00, -6.1822e-01,  ..., -9.7553e-01,\n            4.1846e-01,  5.3182e-03]],\n\n         [[ 5.1107e-01,  2.8146e-01, -2.7592e-01,  ...,  3.7922e-01,\n            2.6629e-01, -4.9287e-01],\n          [-8.4725e-01, -6.4725e-01,  5.9029e-01,  ..., -2.8642e-01,\n           -2.1662e-01,  6.5830e-01],\n          [-8.2339e-01, -8.1061e-01,  5.5095e-01,  ..., -3.3877e-01,\n           -2.6034e-01,  8.5407e-01],\n          ...,\n          [-2.6502e-01, -4.7898e-01,  2.2428e-01,  ..., -1.4590e-01,\n           -9.7838e-02,  3.9580e-01],\n          [-2.1311e-01, -4.5749e-01,  1.9613e-01,  ..., -1.3490e-01,\n           -9.4548e-02,  3.7752e-01],\n          [-3.9431e-01, -5.1674e-01,  3.1507e-01,  ..., -1.9431e-01,\n           -1.3722e-01,  4.5182e-01]],\n\n         [[-2.2106e-01, -3.0628e-01,  1.8815e-01,  ..., -2.7929e-02,\n            1.0514e-01,  1.8283e-01],\n          [ 3.7778e-01,  7.9484e-01, -1.4905e+00,  ...,  2.7942e-01,\n           -8.5752e-01, -5.7656e-01],\n          [ 9.2960e-01,  1.4082e+00, -1.2236e+00,  ...,  8.3512e-01,\n           -8.7970e-01, -8.7329e-01],\n          ...,\n          [ 7.0995e-02,  1.3363e+00, -1.3832e+00,  ...,  2.4392e+00,\n           -1.2050e+00,  5.2226e-01],\n          [ 2.4750e-01,  1.4495e+00, -1.7733e+00,  ...,  2.4066e+00,\n           -1.5702e+00,  5.4152e-01],\n          [ 3.0662e-01,  1.2336e+00, -1.8315e+00,  ...,  2.7671e+00,\n           -1.5179e+00,  3.0152e-01]],\n\n         ...,\n\n         [[ 8.3541e-01,  1.6609e-01,  6.0706e-01,  ...,  4.1979e-01,\n            4.8005e-01,  4.9423e-01],\n          [-8.9278e-01, -3.7045e-02, -1.9404e+00,  ..., -1.0975e+00,\n           -1.1879e+00,  3.8990e-01],\n          [-1.0833e+00, -4.3911e-02, -1.7692e+00,  ..., -1.0278e+00,\n           -1.1707e+00,  1.2903e-01],\n          ...,\n          [-1.2977e+00, -9.1243e-02,  2.3818e-01,  ..., -9.2597e-02,\n           -4.8182e-01,  1.5141e-01],\n          [-1.6070e+00, -1.1806e-01,  5.1308e-01,  ...,  2.8279e-02,\n           -4.0608e-01, -7.4280e-02],\n          [-1.2847e+00, -1.2551e-01, -1.5081e-01,  ..., -3.5607e-01,\n           -7.4541e-01,  3.7867e-01]],\n\n         [[-3.0948e-01,  2.8010e-01, -6.4002e-03,  ..., -5.6041e-01,\n           -2.4692e-02, -2.0910e-02],\n          [ 1.3427e+00, -1.5901e+00, -3.3387e-03,  ...,  1.5485e+00,\n           -5.3993e-01, -1.5272e-01],\n          [ 1.0320e+00, -2.0848e+00, -2.7732e-01,  ...,  1.7913e+00,\n           -5.6742e-01,  1.4009e-02],\n          ...,\n          [-4.3704e-02,  1.5320e-01, -2.8254e-02,  ...,  3.0413e+00,\n           -1.5403e+00, -2.1152e-01],\n          [ 9.7646e-01,  1.7522e-01,  1.8112e-01,  ...,  2.6799e+00,\n           -1.0797e+00, -3.4788e-01],\n          [ 7.5637e-01, -1.0875e-01, -5.0776e-02,  ...,  2.5751e+00,\n           -1.5298e+00, -2.5330e-01]],\n\n         [[-3.6146e-01, -1.5381e-01, -3.4703e-01,  ...,  2.4520e-01,\n            2.9771e-03,  3.4606e-01],\n          [ 1.3179e+00,  6.7309e-01,  1.5526e-01,  ..., -2.9318e-01,\n            1.1383e+00, -1.3209e-01],\n          [ 1.6140e+00,  4.4188e-01,  5.9034e-01,  ..., -9.8789e-01,\n            8.7365e-01, -1.2337e+00],\n          ...,\n          [ 1.6510e+00, -1.3188e-01, -1.4775e-01,  ...,  8.0050e-01,\n            1.1821e+00,  5.5219e-02],\n          [ 1.8318e+00,  3.9865e-01,  2.8536e-01,  ...,  6.5636e-01,\n            1.6207e+00, -5.1577e-01],\n          [ 2.0743e+00,  1.6808e-01, -2.8923e-01,  ...,  9.8232e-01,\n            1.9575e+00, -3.6966e-02]]],\n\n\n        [[[-5.3954e-01,  6.0561e-01, -2.0607e-01,  ...,  4.6480e-01,\n           -2.4752e-01, -7.3239e-02],\n          [ 7.9036e-01, -1.2650e+00, -1.8831e-01,  ..., -1.2139e+00,\n            6.0429e-01, -8.7609e-02],\n          [ 1.6547e+00, -1.4105e+00, -1.0208e-01,  ..., -1.1754e+00,\n            8.5618e-01, -7.1164e-02],\n          ...,\n          [ 6.0993e-01, -1.0769e+00, -5.5834e-01,  ..., -1.2533e+00,\n            3.7456e-01, -2.3608e-03],\n          [ 1.2472e+00, -1.0581e+00, -5.4832e-01,  ..., -8.6165e-01,\n            3.7229e-01, -7.0174e-02],\n          [ 5.6904e-01, -7.2970e-01, -6.3177e-01,  ..., -4.6853e-01,\n            3.2216e-02, -1.3115e-01]],\n\n         [[ 5.1831e-01,  2.8587e-01, -2.8078e-01,  ...,  3.8264e-01,\n            2.6878e-01, -4.9837e-01],\n          [-6.0391e-01, -5.3269e-01,  4.0982e-01,  ..., -1.7849e-01,\n           -1.2353e-01,  5.0829e-01],\n          [-8.3475e-01, -5.8086e-01,  5.7887e-01,  ..., -2.5442e-01,\n           -1.8482e-01,  5.8248e-01],\n          ...,\n          [-2.3966e-01, -5.6501e-01,  1.9244e-01,  ..., -1.8073e-01,\n           -1.1461e-01,  4.7167e-01],\n          [-3.1297e-01, -6.1765e-01,  2.4266e-01,  ..., -2.1955e-01,\n           -1.5566e-01,  5.4989e-01],\n          [-2.3862e-01, -4.2162e-01,  2.1762e-01,  ..., -1.2156e-01,\n           -8.8964e-02,  3.3930e-01]],\n\n         [[-2.2467e-01, -3.1197e-01,  1.9799e-01,  ..., -2.4016e-02,\n            1.1482e-01,  1.8133e-01],\n          [ 7.3086e-01,  1.6549e+00, -1.0916e+00,  ...,  1.0057e+00,\n           -8.6590e-01, -5.1824e-01],\n          [ 7.9006e-01,  1.2341e+00, -1.2931e+00,  ...,  7.7734e-01,\n           -8.4593e-01, -7.5386e-01],\n          ...,\n          [ 4.7118e-01,  1.1940e+00, -1.5597e+00,  ...,  2.9983e+00,\n           -1.3096e+00, -3.5924e-02],\n          [ 3.8218e-01,  1.6430e+00, -1.5186e+00,  ...,  2.1230e+00,\n           -1.4470e+00,  3.6944e-01],\n          [ 2.9725e-01,  1.6190e+00, -1.4047e+00,  ...,  2.1106e+00,\n           -1.4051e+00,  4.9308e-01]],\n\n         ...,\n\n         [[ 8.4937e-01,  1.6862e-01,  6.1422e-01,  ...,  4.2824e-01,\n            4.8544e-01,  5.0794e-01],\n          [-1.1421e+00,  4.4105e-02, -1.0149e+00,  ..., -6.0999e-01,\n           -8.6953e-01,  1.1811e-01],\n          [-7.0708e-01, -3.4455e-03, -1.6289e+00,  ..., -8.8251e-01,\n           -1.0341e+00,  3.9462e-01],\n          ...,\n          [-1.6260e+00, -2.3008e-01,  5.0933e-01,  ..., -9.4125e-02,\n           -5.3984e-01, -1.4154e-01],\n          [-1.6316e+00, -1.1423e-01,  4.5359e-01,  ..., -4.7826e-02,\n           -5.5577e-01,  1.4053e-01],\n          [-1.5353e+00, -1.4672e-01,  4.7400e-01,  ..., -6.5511e-02,\n           -4.9458e-01,  3.5276e-03]],\n\n         [[-3.2035e-01,  2.8379e-01, -9.3891e-03,  ..., -5.7713e-01,\n           -2.4601e-02, -1.8499e-02],\n          [ 1.2171e+00, -1.4160e+00, -5.6460e-03,  ...,  1.6008e+00,\n           -4.8754e-01, -9.0495e-02],\n          [ 1.3004e+00, -1.6063e+00,  3.5956e-02,  ...,  1.8296e+00,\n           -8.8439e-01, -1.9662e-01],\n          ...,\n          [ 8.0422e-01, -1.2008e-01, -1.9686e-01,  ...,  2.3830e+00,\n           -1.2444e+00, -1.3477e-01],\n          [ 8.6312e-01,  3.4604e-01,  1.4506e-01,  ...,  3.0969e+00,\n           -1.3947e+00, -3.6302e-01],\n          [ 8.3284e-01,  3.7966e-01,  3.2666e-01,  ...,  2.8095e+00,\n           -1.3940e+00, -3.9606e-01]],\n\n         [[-3.6930e-01, -1.5774e-01, -3.5244e-01,  ...,  2.6792e-01,\n           -1.3748e-04,  3.6236e-01],\n          [ 1.5609e+00,  2.3627e-01,  9.9657e-01,  ..., -3.8468e-01,\n            8.0817e-01, -1.3060e+00],\n          [ 1.1414e+00,  3.7286e-01,  8.0861e-01,  ..., -6.0054e-01,\n            1.0587e+00, -9.5776e-01],\n          ...,\n          [ 1.8372e+00,  4.7553e-01, -3.1453e-01,  ...,  8.7745e-01,\n            2.0731e+00,  5.6450e-02],\n          [ 2.4263e+00, -4.3629e-01,  1.6437e-01,  ...,  1.2233e+00,\n            1.2292e+00,  3.4587e-01],\n          [ 1.9214e+00,  3.8489e-01,  7.5134e-01,  ...,  7.2235e-01,\n            1.0723e+00, -2.5311e-01]]],\n\n\n        ...,\n\n\n        [[[-5.3579e-01,  6.1459e-01, -2.0869e-01,  ...,  4.7455e-01,\n           -2.5353e-01, -7.2238e-02],\n          [ 9.7114e-01, -1.2002e+00, -5.8254e-02,  ..., -1.1929e+00,\n            6.1962e-01, -6.2434e-02],\n          [ 1.5357e+00, -1.3684e+00,  7.9877e-02,  ..., -1.1381e+00,\n            7.6498e-01, -1.0240e-01],\n          ...,\n          [-3.9666e-01, -1.1186e+00, -9.7865e-01,  ..., -1.4906e+00,\n            1.2760e-01, -5.2379e-02],\n          [ 2.2358e-01, -8.4259e-01, -6.5095e-01,  ..., -9.0877e-01,\n            1.2919e-01, -7.2311e-02],\n          [ 4.1025e-01, -8.5592e-01, -7.7985e-01,  ..., -8.8258e-01,\n            4.0901e-02, -6.9448e-02]],\n\n         [[ 5.1968e-01,  2.9086e-01, -2.8128e-01,  ...,  3.8568e-01,\n            2.7057e-01, -5.0312e-01],\n          [-5.7385e-01, -4.4773e-01,  4.0967e-01,  ..., -1.4611e-01,\n           -1.0582e-01,  4.1237e-01],\n          [-8.0729e-01, -5.7532e-01,  5.5842e-01,  ..., -2.4363e-01,\n           -1.7268e-01,  5.7374e-01],\n          ...,\n          [-2.3051e-01, -5.0945e-01,  2.0274e-01,  ..., -1.5750e-01,\n           -1.1289e-01,  4.3301e-01],\n          [-2.2962e-01, -4.8886e-01,  2.0002e-01,  ..., -1.5000e-01,\n           -1.0353e-01,  3.9701e-01],\n          [-2.4583e-01, -4.9796e-01,  2.1799e-01,  ..., -1.6107e-01,\n           -1.1593e-01,  4.1140e-01]],\n\n         [[-2.2690e-01, -3.2003e-01,  1.9706e-01,  ..., -5.9892e-02,\n            1.1292e-01,  1.8550e-01],\n          [ 6.4250e-01,  1.1384e+00, -1.0844e+00,  ...,  9.0777e-01,\n           -7.2031e-01, -5.2474e-01],\n          [ 8.3469e-01,  6.3068e-01, -1.4504e+00,  ...,  1.0242e+00,\n           -8.4037e-01, -6.6801e-01],\n          ...,\n          [ 2.5053e-01,  1.1686e+00, -1.4888e+00,  ...,  2.8609e+00,\n           -1.3688e+00,  3.3924e-01],\n          [ 2.9967e-01,  1.6703e+00, -1.2637e+00,  ...,  2.0664e+00,\n           -1.3131e+00,  3.3201e-01],\n          [ 1.6662e-01,  1.3359e+00, -1.5858e+00,  ...,  2.2829e+00,\n           -1.3583e+00,  5.1502e-01]],\n\n         ...,\n\n         [[ 8.5831e-01,  1.7085e-01,  6.0564e-01,  ...,  4.2670e-01,\n            4.8672e-01,  5.1629e-01],\n          [-1.1459e+00, -6.2347e-02, -8.9208e-01,  ..., -6.1868e-01,\n           -8.7834e-01,  2.6796e-01],\n          [-1.1321e+00,  1.9261e-02, -1.4377e+00,  ..., -8.6353e-01,\n           -1.0432e+00,  7.3141e-02],\n          ...,\n          [-1.7609e+00, -9.8418e-02,  5.5234e-01,  ...,  9.7874e-02,\n           -4.5130e-01, -1.5266e-02],\n          [-1.3244e+00, -2.1605e-01,  2.8040e-01,  ..., -1.9945e-01,\n           -5.7266e-01,  9.3746e-02],\n          [-1.4626e+00, -3.0695e-01,  5.0006e-01,  ..., -1.7378e-01,\n           -6.1701e-01,  1.4426e-01]],\n\n         [[-3.3015e-01,  2.8881e-01, -1.0123e-02,  ..., -5.8115e-01,\n           -5.9405e-03, -1.6462e-02],\n          [ 9.3389e-01, -1.2242e+00,  2.0832e-01,  ...,  2.0340e+00,\n           -7.0234e-01, -2.3780e-01],\n          [ 5.9721e-01, -2.2570e+00,  1.4407e-01,  ...,  6.5140e-01,\n           -7.4488e-01, -1.6839e-01],\n          ...,\n          [ 1.5934e-01,  3.0459e-01, -9.2881e-02,  ...,  2.2466e+00,\n           -1.0503e+00, -1.0845e-01],\n          [ 2.4077e-01,  4.7246e-01,  7.7998e-02,  ...,  3.0944e+00,\n           -1.2921e+00, -2.9091e-01],\n          [ 8.7768e-01,  8.9474e-01,  2.8313e-01,  ...,  3.0220e+00,\n           -1.0570e+00, -3.8099e-01]],\n\n         [[-3.7137e-01, -1.6289e-01, -3.4141e-01,  ...,  2.6936e-01,\n           -1.7520e-02,  3.6833e-01],\n          [ 1.3187e+00,  5.0184e-01,  1.3639e+00,  ..., -3.7357e-01,\n            8.9817e-01, -8.9440e-01],\n          [ 1.5098e+00,  6.2965e-01,  1.2865e+00,  ..., -1.4263e+00,\n            1.4263e+00, -1.4079e+00],\n          ...,\n          [ 2.0466e+00,  3.1191e-01, -1.0951e-01,  ...,  1.0330e+00,\n            1.6161e+00, -2.3516e-01],\n          [ 2.0933e+00, -7.2674e-02,  3.4550e-01,  ...,  9.5860e-01,\n            8.4432e-01, -1.6504e-01],\n          [ 2.1334e+00,  3.8119e-01,  4.9664e-01,  ...,  1.1075e+00,\n            1.1071e+00, -4.6727e-01]]],\n\n\n        [[[-5.5282e-01,  6.2473e-01, -2.0763e-01,  ...,  4.7889e-01,\n           -2.5286e-01, -7.3721e-02],\n          [ 1.1054e+00, -1.2571e+00,  1.4333e-02,  ..., -1.3136e+00,\n            7.9261e-01, -9.1998e-02],\n          [ 1.5125e+00, -1.6560e+00,  2.0427e-01,  ..., -1.4000e+00,\n            9.2262e-01, -1.0358e-01],\n          ...,\n          [ 4.4247e-01, -6.8878e-01, -6.5206e-01,  ..., -8.0672e-01,\n            2.8354e-01, -5.1965e-02],\n          [-2.5261e-01, -9.5776e-01, -8.7942e-01,  ..., -1.0380e+00,\n           -1.1593e-01, -1.4150e-01],\n          [ 1.9861e-01, -8.4999e-01, -6.3044e-01,  ..., -8.8733e-01,\n           -8.6193e-02, -4.6453e-02]],\n\n         [[ 5.2801e-01,  2.9339e-01, -2.8739e-01,  ...,  3.8854e-01,\n            2.7315e-01, -5.0795e-01],\n          [-6.9047e-01, -6.0120e-01,  4.8197e-01,  ..., -2.3981e-01,\n           -1.6375e-01,  5.8846e-01],\n          [-9.5483e-01, -7.1197e-01,  6.5187e-01,  ..., -3.4341e-01,\n           -2.3757e-01,  7.2856e-01],\n          ...,\n          [-2.5630e-01, -4.8271e-01,  2.1761e-01,  ..., -1.4441e-01,\n           -9.5330e-02,  3.9432e-01],\n          [-1.6194e-01, -4.4762e-01,  1.6584e-01,  ..., -1.2499e-01,\n           -8.4714e-02,  3.4970e-01],\n          [-1.7175e-01, -4.5473e-01,  1.6721e-01,  ..., -1.1993e-01,\n           -8.6829e-02,  3.4813e-01]],\n\n         [[-2.3271e-01, -3.4015e-01,  2.0575e-01,  ..., -3.0573e-02,\n            1.2151e-01,  1.8823e-01],\n          [ 9.2623e-01,  1.0183e+00, -1.2297e+00,  ...,  1.6880e+00,\n           -8.3557e-01, -6.9799e-01],\n          [ 1.1174e+00,  1.0893e+00, -1.4934e+00,  ...,  1.4083e+00,\n           -9.5403e-01, -9.0813e-01],\n          ...,\n          [ 3.3753e-01,  1.2055e+00, -1.3002e+00,  ...,  2.3085e+00,\n           -1.1555e+00,  1.8857e-01],\n          [-5.6925e-02,  1.2219e+00, -1.7996e+00,  ...,  3.0063e+00,\n           -1.5383e+00,  7.9693e-01],\n          [ 6.7482e-02,  9.0181e-01, -1.6821e+00,  ...,  1.9349e+00,\n           -1.3259e+00,  2.4743e-01]],\n\n         ...,\n\n         [[ 8.7271e-01,  1.6947e-01,  6.2891e-01,  ...,  4.3582e-01,\n            4.9346e-01,  5.2275e-01],\n          [-1.3607e+00, -2.7414e-01, -4.2958e-01,  ..., -6.1609e-01,\n           -8.9998e-01, -8.7130e-02],\n          [-1.3127e+00,  3.8281e-02, -1.5485e+00,  ..., -9.6869e-01,\n           -1.0589e+00, -2.8273e-01],\n          ...,\n          [-1.3326e+00, -1.9436e-01, -5.6280e-04,  ..., -2.0182e-01,\n           -5.9508e-01,  8.9161e-02],\n          [-1.7957e+00, -1.0505e-01,  8.5597e-01,  ...,  1.9488e-01,\n           -3.6491e-01, -1.5112e-01],\n          [-1.2988e+00, -1.3687e-01,  4.1221e-01,  ..., -5.8774e-02,\n           -3.7081e-01, -3.3736e-02]],\n\n         [[-3.3730e-01,  3.0609e-01, -7.5251e-03,  ..., -5.9982e-01,\n           -2.2369e-02, -1.9025e-02],\n          [ 1.6327e+00, -1.1325e+00, -3.1568e-02,  ...,  2.2317e+00,\n           -9.7941e-01, -1.2228e-01],\n          [ 1.3503e+00, -2.5068e+00,  5.4337e-02,  ...,  1.2186e+00,\n           -1.1701e+00, -1.6183e-01],\n          ...,\n          [ 8.2955e-01, -1.5254e-02,  6.6770e-02,  ...,  2.6459e+00,\n           -1.4417e+00, -3.0402e-01],\n          [ 5.2171e-01,  2.1965e-01,  1.8947e-01,  ...,  2.2772e+00,\n           -1.2886e+00, -3.1810e-01],\n          [ 4.8771e-01,  1.0645e-01, -7.9628e-02,  ...,  1.6648e+00,\n           -1.0626e+00, -1.6453e-01]],\n\n         [[-3.9079e-01, -1.6378e-01, -3.5557e-01,  ...,  2.6905e-01,\n            6.1875e-03,  3.7731e-01],\n          [ 1.3542e+00,  6.6519e-01,  1.5374e+00,  ..., -1.0254e+00,\n            1.6303e+00, -1.4037e+00],\n          [ 1.2118e+00,  6.3943e-01,  1.5182e+00,  ..., -1.9028e+00,\n            1.8258e+00, -1.9579e+00],\n          ...,\n          [ 1.7055e+00, -2.7415e-01,  7.8864e-02,  ...,  1.2822e+00,\n            1.2592e+00,  1.9439e-01],\n          [ 1.8736e+00,  4.6280e-01,  1.1605e-01,  ...,  1.2561e+00,\n            1.7181e+00, -2.1230e-01],\n          [ 1.4452e+00,  3.0013e-01, -3.0836e-01,  ...,  8.5357e-01,\n            1.4014e+00,  4.5730e-01]]],\n\n\n        [[[-5.0962e-01,  5.5321e-01, -1.9548e-01,  ...,  4.2387e-01,\n           -2.3534e-01, -7.3882e-02],\n          [ 1.0820e+00, -1.4870e+00, -2.7237e-01,  ..., -1.4316e+00,\n            8.1475e-01, -4.5615e-02],\n          [ 1.2661e+00, -1.2996e+00, -8.8871e-02,  ..., -1.0990e+00,\n            6.2171e-01, -7.2579e-02],\n          ...,\n          [ 4.8067e-01, -1.2328e+00, -4.6998e-01,  ..., -1.3257e+00,\n            4.8398e-01, -5.2069e-02],\n          [ 2.3254e-01, -1.2241e+00, -3.5206e-01,  ..., -1.2617e+00,\n            2.9624e-01, -8.7219e-02],\n          [ 2.8989e-01, -1.1649e+00, -5.4059e-01,  ..., -1.4049e+00,\n            5.4086e-01, -4.9303e-03]],\n\n         [[ 4.8598e-01,  2.5730e-01, -2.6031e-01,  ...,  3.6330e-01,\n            2.5509e-01, -4.6507e-01],\n          [-7.1862e-01, -6.0420e-01,  4.9745e-01,  ..., -2.4390e-01,\n           -1.7612e-01,  5.9190e-01],\n          [-7.4909e-01, -5.5606e-01,  5.2409e-01,  ..., -2.2542e-01,\n           -1.6483e-01,  5.5386e-01],\n          ...,\n          [-4.6691e-01, -5.6007e-01,  3.7182e-01,  ..., -2.4221e-01,\n           -1.6971e-01,  5.1170e-01],\n          [-2.6902e-01, -4.8915e-01,  2.2993e-01,  ..., -1.5961e-01,\n           -1.1198e-01,  4.0384e-01],\n          [-3.3682e-01, -5.4057e-01,  2.7156e-01,  ..., -1.9280e-01,\n           -1.3435e-01,  4.6817e-01]],\n\n         [[-1.9783e-01, -2.7655e-01,  1.6087e-01,  ...,  5.8565e-04,\n            8.3618e-02,  1.5964e-01],\n          [ 7.1498e-01,  1.1066e+00, -9.9257e-01,  ...,  7.1596e-01,\n           -6.9327e-01, -7.3037e-01],\n          [ 9.4451e-01,  1.4737e+00, -1.1968e+00,  ...,  7.3466e-01,\n           -8.4200e-01, -8.3369e-01],\n          ...,\n          [ 7.8486e-01,  1.4619e+00, -1.4794e+00,  ...,  2.9299e+00,\n           -1.4093e+00, -1.1805e-01],\n          [ 5.0127e-01,  1.5131e+00, -1.5587e+00,  ...,  2.5806e+00,\n           -1.4995e+00,  2.4974e-01],\n          [ 7.0627e-01,  1.5343e+00, -1.3289e+00,  ...,  2.3790e+00,\n           -1.3428e+00, -3.2821e-02]],\n\n         ...,\n\n         [[ 7.8702e-01,  1.5989e-01,  5.8233e-01,  ...,  4.0025e-01,\n            4.5439e-01,  4.7453e-01],\n          [-8.6388e-01, -2.4574e-02, -1.5280e+00,  ..., -8.8063e-01,\n           -1.0536e+00,  3.4019e-01],\n          [-1.0407e+00, -8.4199e-02, -1.3308e+00,  ..., -7.7213e-01,\n           -1.0631e+00,  5.0711e-01],\n          ...,\n          [-1.5995e+00, -1.3305e-01,  4.0242e-01,  ..., -1.5891e-01,\n           -5.7842e-01, -2.3018e-01],\n          [-2.1956e+00,  1.4878e-01,  1.3172e+00,  ...,  5.1786e-01,\n           -2.2890e-01, -1.0126e-01],\n          [-1.4839e+00, -2.2079e-01,  3.0794e-02,  ..., -3.8001e-01,\n           -7.4647e-01, -1.2630e-01]],\n\n         [[-2.9844e-01,  2.5882e-01, -1.3821e-02,  ..., -5.0366e-01,\n           -3.7879e-02, -1.9666e-02],\n          [ 6.3557e-01, -1.4567e+00, -1.9383e-02,  ...,  1.6362e+00,\n           -5.9358e-01, -1.1478e-01],\n          [ 7.1670e-01, -1.0783e+00,  1.4846e-01,  ...,  2.1316e+00,\n           -8.2112e-02, -1.7445e-01],\n          ...,\n          [ 1.9631e-01,  3.3133e-01, -8.7563e-02,  ...,  2.8898e+00,\n           -1.4740e+00, -1.6867e-01],\n          [-2.3645e-03,  3.7099e-01, -3.9434e-02,  ...,  2.4018e+00,\n           -1.0041e+00, -1.5551e-01],\n          [-1.2990e-01,  5.2443e-01,  9.5663e-02,  ...,  2.4739e+00,\n           -1.1268e+00, -2.4284e-01]],\n\n         [[-3.2656e-01, -1.3885e-01, -3.3208e-01,  ...,  2.2787e-01,\n            2.3864e-02,  3.1901e-01],\n          [ 1.6115e+00, -1.2036e-01,  8.5059e-01,  ...,  1.2491e-02,\n            5.7395e-01, -7.2918e-01],\n          [ 1.3752e+00,  5.2592e-01,  1.5011e+00,  ..., -5.3159e-01,\n            6.4779e-01, -1.3987e+00],\n          ...,\n          [ 1.7770e+00,  2.4043e-01,  3.8346e-01,  ...,  1.1810e-01,\n            1.8746e+00, -3.8117e-01],\n          [ 1.8227e+00,  2.9570e-01,  7.0183e-01,  ...,  7.4947e-01,\n            1.3799e+00, -3.9681e-01],\n          [ 1.6013e+00,  3.4179e-01,  7.1728e-01,  ...,  2.8172e-01,\n            1.1568e+00, -5.0575e-01]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[ 5.9311e-03,  5.8352e-04, -1.1301e-02,  ...,  1.9733e-02,\n           -2.8793e-03, -3.2653e-04],\n          [ 1.2480e-01,  3.4962e-02, -7.7093e-01,  ...,  5.9898e-02,\n           -3.7039e-01, -1.8606e-02],\n          [ 3.6174e-02,  2.7450e-01, -1.0863e-01,  ...,  2.7769e-01,\n            7.4741e-02,  1.0687e-01],\n          ...,\n          [-2.2304e-02,  2.1980e-01,  7.1136e-02,  ...,  7.5249e-02,\n           -2.5234e-01,  2.8453e-02],\n          [-3.4151e-02,  4.3855e-02,  2.6290e-01,  ...,  4.3273e-01,\n           -1.2993e-01,  5.4686e-02],\n          [ 1.6556e-02, -8.7713e-02,  3.0787e-01,  ...,  4.3440e-01,\n            1.3281e-01, -1.5847e-01]],\n\n         [[-1.1704e-03, -1.6712e-03, -1.1722e-03,  ..., -1.0113e-03,\n           -3.1516e-03,  2.5515e-03],\n          [-3.4253e-02,  6.2827e-02, -4.3695e-02,  ..., -4.1693e-02,\n            1.9926e-01,  1.9580e-03],\n          [ 7.3603e-02, -4.4580e-02, -1.3223e-02,  ...,  8.1323e-02,\n            2.9605e-02,  5.1396e-02],\n          ...,\n          [-6.2676e-02,  1.9159e-02, -4.8274e-02,  ..., -3.7006e-02,\n           -1.1691e-01,  8.0403e-02],\n          [-4.3999e-02, -4.7628e-02, -9.8898e-03,  ...,  5.8166e-03,\n           -6.4208e-02,  3.4271e-02],\n          [-1.8315e-02,  3.4448e-03, -1.5377e-02,  ...,  4.0763e-02,\n           -3.5884e-01,  2.2757e-02]],\n\n         [[ 4.8906e-04, -1.5974e-03, -9.5777e-06,  ..., -7.1623e-03,\n            2.1535e-03,  2.4533e-03],\n          [-4.2510e-02,  3.2467e-01, -1.8779e-01,  ..., -2.4970e-01,\n            8.3187e-02,  1.5700e-01],\n          [-2.4971e-01,  5.6778e-01,  1.5770e-01,  ...,  3.3519e-01,\n            1.5665e-01,  4.4623e-02],\n          ...,\n          [-1.6504e-01,  3.6343e-01,  3.4556e-01,  ..., -6.6002e-01,\n            1.1018e-01, -1.5624e-02],\n          [ 1.5268e-01, -1.0724e-01,  6.6533e-02,  ...,  1.4013e-01,\n            2.4052e-01,  5.9086e-02],\n          [-2.5395e-01,  2.4893e-01,  3.0230e-01,  ..., -1.9438e-02,\n            8.3459e-02,  1.8214e-01]],\n\n         ...,\n\n         [[ 3.3869e-03, -2.2447e-03,  1.7876e-03,  ..., -9.1205e-03,\n            2.0257e-03,  5.5732e-03],\n          [-1.0881e-01,  1.5882e-02, -2.9134e-01,  ...,  7.0590e-02,\n           -1.7029e-02, -5.5869e-02],\n          [-7.4423e-02,  4.0504e-02, -1.3250e-01,  ...,  3.7000e-02,\n           -1.5466e-01, -5.5581e-02],\n          ...,\n          [-2.9768e-01, -1.2418e-01,  5.3803e-02,  ..., -6.5532e-02,\n           -1.1796e-01,  2.4277e-02],\n          [-4.4976e-02,  2.1464e-02,  4.9071e-02,  ..., -1.3679e-01,\n            2.3149e-01,  1.1929e-01],\n          [-1.3470e-01,  2.0145e-02,  3.7223e-01,  ...,  2.4849e-01,\n            1.7884e-01, -1.6439e-01]],\n\n         [[-6.0518e-03,  1.1017e-03,  4.4320e-03,  ..., -1.3022e-02,\n           -3.1287e-03, -1.0738e-03],\n          [-7.9187e-02,  2.2113e-01, -1.3952e-01,  ...,  1.2945e-01,\n            3.2943e-01,  2.2287e-01],\n          [ 1.0323e-01,  9.3673e-02, -4.0037e-02,  ...,  2.0505e-01,\n            4.9757e-02,  9.6665e-02],\n          ...,\n          [ 1.9833e-02,  1.4893e-02, -1.7858e-01,  ..., -2.4454e-01,\n           -6.4967e-01,  2.0458e-01],\n          [-1.3779e-01,  1.7145e-01, -2.5653e-01,  ..., -1.1293e-01,\n           -2.4606e-02,  2.2604e-01],\n          [-2.3446e-01,  1.1298e-01,  4.7589e-02,  ...,  9.4655e-03,\n            3.8981e-01, -4.1058e-01]],\n\n         [[-9.0219e-03,  6.0099e-03, -1.3903e-03,  ..., -4.1122e-03,\n           -6.8819e-03, -3.3348e-03],\n          [ 7.5813e-01, -9.4619e-02, -9.5462e-01,  ...,  5.4202e-01,\n           -5.7343e-01,  2.6537e-01],\n          [-8.5746e-03,  4.2573e-01, -7.9779e-02,  ..., -1.7392e-01,\n           -7.4272e-03, -1.9754e-02],\n          ...,\n          [ 3.5417e-01, -1.8981e-01,  8.0504e-02,  ..., -1.5008e-01,\n           -3.8067e-01, -2.0352e-01],\n          [-7.2365e-02, -7.6907e-01,  7.0326e-02,  ...,  1.2459e-01,\n            2.1498e-01, -8.9713e-02],\n          [ 1.0333e-01,  3.2056e-01,  2.5049e-01,  ...,  5.9763e-02,\n           -1.3156e-01, -4.7429e-01]]],\n\n\n        [[[-2.3793e-04, -1.3097e-03, -2.5908e-03,  ...,  1.6766e-02,\n           -4.5151e-03,  2.1755e-03],\n          [-2.9151e-02,  2.9869e-02,  2.3221e-01,  ...,  2.5830e-01,\n           -1.2339e-01,  7.4535e-02],\n          [ 1.1893e-02, -5.5541e-01,  3.0088e-01,  ...,  4.0481e-01,\n            6.8724e-01, -7.8300e-02],\n          ...,\n          [ 1.5183e-03,  3.4592e-01,  4.5567e-01,  ..., -1.0115e-01,\n           -2.1876e-03, -8.7157e-02],\n          [-4.9645e-03,  1.2932e-01, -3.3495e-02,  ...,  3.2347e-01,\n            3.5443e-03,  4.0825e-02],\n          [-1.5193e-02,  4.5408e-01,  9.3410e-03,  ...,  3.8701e-01,\n            6.7939e-03,  1.0028e-01]],\n\n         [[ 1.1996e-04, -1.7706e-03,  3.2822e-04,  ..., -9.3887e-05,\n           -3.2584e-03,  3.7310e-04],\n          [ 2.2763e-02, -5.8676e-03,  3.8379e-02,  ..., -2.7335e-04,\n            1.4229e-02,  6.0663e-02],\n          [ 1.1545e-02,  1.6303e-01,  6.2170e-02,  ..., -9.6581e-02,\n           -9.4451e-02, -9.3873e-03],\n          ...,\n          [ 1.9977e-02,  3.0301e-02,  4.2359e-02,  ...,  6.9709e-02,\n           -2.3828e-02, -9.9104e-02],\n          [ 3.8532e-02, -2.4374e-02,  8.0102e-02,  ...,  2.4201e-02,\n           -8.1029e-02, -5.1454e-02],\n          [-4.4698e-02, -8.1051e-02, -1.0019e-02,  ...,  2.1090e-02,\n            8.6570e-02,  2.8727e-02]],\n\n         [[ 4.2660e-03, -4.6747e-04, -4.2930e-03,  ..., -2.4100e-03,\n            5.8659e-04,  1.8783e-03],\n          [ 6.4973e-02,  2.1233e-01, -6.6083e-02,  ..., -1.6560e-01,\n            1.4684e-01, -9.8707e-02],\n          [-4.5569e-01, -1.6980e-01, -1.4168e-01,  ...,  2.2682e-01,\n           -7.6366e-02,  9.3834e-02],\n          ...,\n          [ 1.8459e-01, -1.7380e-01, -8.5062e-02,  ..., -2.8768e-01,\n           -1.5962e-01,  1.4135e-02],\n          [-2.7141e-01, -2.1015e-01,  1.4814e-01,  ..., -4.3569e-01,\n            1.1659e-01, -2.4534e-02],\n          [-4.1172e-02,  2.4809e-01, -4.6145e-02,  ...,  3.1537e-02,\n            7.9495e-02, -6.2292e-03]],\n\n         ...,\n\n         [[ 1.0034e-02, -1.2708e-03,  5.5354e-03,  ..., -6.5532e-03,\n            1.5293e-03,  3.0498e-03],\n          [ 1.6833e-01,  3.6104e-02, -2.7825e-02,  ..., -4.7654e-03,\n            1.3553e-01,  1.5962e-01],\n          [-8.4447e-02, -7.2507e-02,  1.0392e-01,  ...,  3.4669e-01,\n           -9.8355e-02, -1.6481e-01],\n          ...,\n          [-2.7783e-02,  7.2480e-02,  3.2789e-01,  ..., -8.5472e-02,\n            5.6218e-02, -4.1017e-02],\n          [-3.9916e-02,  8.6833e-02,  8.4361e-02,  ..., -9.8599e-02,\n           -2.9697e-01, -9.2332e-02],\n          [-1.0055e-01,  9.2635e-02,  2.3131e-01,  ..., -3.7687e-01,\n           -2.7278e-01, -1.2432e-02]],\n\n         [[-3.1430e-03,  1.9424e-04, -5.6996e-03,  ..., -1.3098e-03,\n           -3.8647e-03, -8.3881e-03],\n          [-1.1601e-01,  3.0293e-01, -1.6039e-01,  ...,  4.8109e-01,\n           -1.6496e-01,  3.0185e-01],\n          [-9.8772e-02,  2.8802e-02, -1.3314e-02,  ...,  1.6024e-01,\n            5.4187e-01,  5.9013e-02],\n          ...,\n          [-2.2283e-01, -8.0647e-01,  1.3994e-01,  ..., -6.6804e-01,\n           -1.8113e-02,  7.3057e-01],\n          [-2.8819e-01, -1.2472e-01,  1.4719e-01,  ..., -1.8374e-01,\n            3.9236e-02,  2.5689e-01],\n          [-3.1817e-01, -2.4606e-01, -3.3239e-01,  ..., -5.1247e-01,\n           -6.0776e-01,  3.0417e-01]],\n\n         [[-1.4007e-02,  1.4090e-02,  2.4396e-03,  ..., -3.9108e-03,\n           -3.1944e-03,  3.4565e-03],\n          [-2.3520e-02,  1.0242e-01, -2.7482e-01,  ...,  2.9023e-01,\n            4.3798e-01,  3.0490e-01],\n          [ 2.2074e-01, -2.7496e-01,  2.5773e-01,  ...,  3.1673e-01,\n           -1.0036e-01,  9.9111e-02],\n          ...,\n          [ 1.8924e-03,  1.2594e+00,  1.0604e-01,  ...,  2.0116e-01,\n            1.8168e-01,  9.4042e-02],\n          [ 7.8088e-02,  4.0462e-01,  3.6602e-03,  ...,  3.2282e-01,\n            6.1528e-02,  3.5342e-03],\n          [ 2.1304e-01,  1.8603e-01,  2.4735e-01,  ...,  6.1178e-02,\n           -7.8307e-02,  2.3523e-01]]],\n\n\n        [[[ 8.1778e-04,  5.6574e-03,  6.4430e-04,  ...,  2.2319e-02,\n           -2.4597e-03, -2.0112e-03],\n          [ 6.6229e-03,  2.2926e-01,  1.0149e-01,  ..., -1.0719e-01,\n            9.6793e-02, -6.7113e-03],\n          [ 1.1633e-01,  7.5673e-02, -1.3365e-01,  ...,  4.2391e-01,\n           -8.2977e-02,  7.7158e-02],\n          ...,\n          [-9.2190e-02, -2.6119e-01, -7.1763e-02,  ...,  4.9953e-01,\n           -1.5435e-01, -2.4720e-03],\n          [-9.0923e-03, -6.0674e-03, -4.8781e-02,  ...,  3.3954e-01,\n           -8.3775e-03,  1.6275e-02],\n          [ 7.8510e-02,  1.8650e-01,  1.0200e+00,  ...,  2.7543e-01,\n            3.6665e-01,  1.4259e-02]],\n\n         [[ 1.3714e-04, -1.9875e-03, -2.3786e-04,  ...,  3.7384e-04,\n           -5.1508e-03,  1.5081e-03],\n          [ 1.0276e-01,  5.6004e-02,  5.3264e-02,  ...,  3.2981e-02,\n            9.8002e-02,  1.3432e-02],\n          [-6.8565e-05,  4.3027e-02,  6.3404e-03,  ..., -3.1745e-02,\n            3.5488e-01, -5.1539e-02],\n          ...,\n          [-6.3704e-02, -6.8821e-02, -6.4097e-02,  ...,  6.4460e-03,\n           -9.9259e-02,  3.7752e-02],\n          [ 1.4481e-02, -3.4799e-02, -9.8587e-05,  ...,  3.4136e-02,\n           -8.8196e-02, -3.9517e-02],\n          [ 3.3766e-02,  4.1798e-02,  4.0455e-02,  ..., -4.0070e-02,\n            1.9241e-02, -6.0531e-02]],\n\n         [[-9.8738e-04,  1.0910e-02,  3.3054e-04,  ..., -6.9296e-03,\n            2.5849e-03,  4.7929e-03],\n          [-3.9227e-01,  3.1762e-01,  1.0272e-01,  ..., -1.5691e-01,\n            9.9531e-02,  7.3925e-02],\n          [-1.9246e-01,  1.3708e-02, -2.4833e-01,  ..., -2.0844e-01,\n           -1.6437e-01,  6.7568e-02],\n          ...,\n          [ 4.7682e-02, -3.3582e-02, -8.4277e-02,  ..., -2.6390e-01,\n            1.2668e-02,  1.6184e-01],\n          [ 7.1662e-02,  2.9407e-01,  1.1853e-01,  ...,  2.1539e-01,\n            5.5054e-02, -2.4924e-02],\n          [ 1.5865e-01, -1.0625e-01,  2.0969e-01,  ...,  8.7120e-02,\n            1.0004e-01, -5.3690e-01]],\n\n         ...,\n\n         [[ 3.4746e-03, -2.2924e-03,  3.0743e-03,  ..., -4.2592e-03,\n           -7.9132e-04,  2.6126e-03],\n          [-1.4684e-01, -1.3390e-02, -3.9603e-01,  ...,  1.3092e-01,\n           -8.1353e-02,  2.0838e-02],\n          [-4.1462e-02, -7.4650e-02, -2.0725e-01,  ..., -1.4924e-01,\n           -1.8076e-01, -1.4875e-01],\n          ...,\n          [-1.9219e-01,  4.6204e-02,  2.3892e-01,  ...,  1.1982e-01,\n            1.5566e-01,  8.0600e-02],\n          [-4.5321e-01, -6.3706e-02,  9.6230e-03,  ..., -2.8888e-01,\n           -2.8512e-01, -8.8525e-02],\n          [-2.5239e-01, -4.6731e-02, -2.6928e-01,  ..., -1.0831e-01,\n           -3.4621e-01, -1.1676e-01]],\n\n         [[ 6.1725e-04,  6.8243e-03, -6.1911e-04,  ..., -8.1070e-03,\n            1.2095e-02, -6.9302e-03],\n          [ 7.6244e-02,  2.4301e-01,  2.1672e-01,  ...,  3.5023e-01,\n            2.2646e-01,  1.4310e-01],\n          [ 2.9555e-01,  5.3120e-02, -1.6301e-01,  ...,  3.6404e-01,\n           -1.6788e-01, -1.1947e-01],\n          ...,\n          [-3.9296e-01,  4.4589e-01,  1.1242e-01,  ..., -3.3718e-01,\n            2.6137e-01,  2.9397e-01],\n          [-2.9910e-01,  3.8890e-02,  1.1081e-01,  ..., -2.9658e-01,\n           -3.7465e-01,  2.9386e-01],\n          [-2.3002e-01, -4.5590e-01,  2.0880e-01,  ..., -2.4837e-03,\n            5.6558e-01, -4.6165e-01]],\n\n         [[-1.6693e-02,  2.2515e-02,  1.1569e-02,  ..., -6.6505e-03,\n           -3.4433e-04, -3.5654e-04],\n          [-1.2064e-01,  6.9655e-01, -3.3218e-02,  ...,  1.6583e-01,\n            1.0823e-01,  4.8220e-02],\n          [-2.3422e-01,  1.7933e-01, -1.7903e-01,  ..., -1.3208e-01,\n            2.6360e-02, -5.5520e-02],\n          ...,\n          [ 1.5230e-01, -6.9371e-01,  2.5523e-01,  ...,  1.3199e-01,\n           -2.9315e-01,  3.2920e-01],\n          [-2.8740e-01,  6.9113e-01,  6.5579e-01,  ...,  1.8203e-02,\n           -2.1710e-01,  8.2917e-02],\n          [ 4.9886e-02,  2.2467e-01,  1.0670e-01,  ...,  1.7405e-01,\n           -1.4044e-01, -1.7475e-01]]],\n\n\n        ...,\n\n\n        [[[ 1.3320e-03,  1.0778e-02, -8.0029e-03,  ...,  1.5815e-02,\n           -3.8683e-03, -5.6195e-03],\n          [ 1.3664e-01,  6.9424e-02, -2.7381e-02,  ...,  1.3899e-01,\n            1.8976e-01, -1.4447e-01],\n          [-1.5783e-02,  2.8055e-01,  3.1247e-01,  ...,  4.1368e-01,\n           -2.1398e-01,  1.2072e-01],\n          ...,\n          [ 1.2101e-01,  1.4352e-01,  9.8496e-01,  ...,  4.5681e-02,\n            4.9826e-02,  1.1915e-01],\n          [ 3.0261e-02,  7.7437e-02,  2.4323e-01,  ..., -5.7623e-01,\n            8.5402e-03, -9.6034e-02],\n          [ 3.5106e-02,  3.7729e-01, -2.2485e-02,  ..., -5.7685e-01,\n           -6.6528e-02,  1.0810e-01]],\n\n         [[ 1.1937e-04, -2.9501e-03, -6.4974e-04,  ..., -9.2755e-04,\n           -3.8188e-03,  1.0368e-03],\n          [ 6.9528e-02, -3.2804e-03,  7.8520e-03,  ...,  1.6991e-02,\n            2.2550e-03, -5.4981e-02],\n          [ 1.0928e-01,  8.8213e-02,  2.1759e-02,  ...,  4.8529e-02,\n            5.5115e-02, -6.3707e-02],\n          ...,\n          [ 1.1270e-02, -6.3927e-02,  3.2279e-02,  ...,  4.6776e-02,\n           -1.6671e-01, -5.5606e-02],\n          [-5.7836e-03, -3.2164e-02,  9.5575e-03,  ..., -4.0355e-03,\n            1.3051e-02, -7.8615e-02],\n          [-6.6886e-02, -5.5101e-02, -3.8790e-02,  ...,  7.2132e-05,\n            1.3910e-01, -8.4522e-02]],\n\n         [[ 1.0763e-02,  5.5154e-03, -1.5700e-03,  ...,  9.1815e-03,\n            4.7843e-03,  1.8781e-03],\n          [ 2.5507e-01,  3.2082e-02, -1.8723e-01,  ..., -3.5939e-01,\n            2.5897e-01, -1.9750e-01],\n          [ 1.8184e-01, -2.6662e-01, -4.1358e-02,  ...,  2.2081e-01,\n           -1.3691e-02,  5.1362e-02],\n          ...,\n          [-1.8328e-01,  4.3182e-01, -1.1702e-01,  ...,  4.3624e-01,\n           -1.3671e-01,  4.6961e-02],\n          [ 5.4255e-02, -3.2862e-02, -6.7710e-03,  ...,  1.0351e-01,\n           -2.0928e-01,  1.0442e-01],\n          [ 3.7431e-02,  5.1954e-01, -1.5327e-01,  ..., -1.7085e-01,\n           -6.7837e-02,  1.3012e-02]],\n\n         ...,\n\n         [[ 5.1722e-03, -4.6847e-03,  1.8474e-03,  ..., -7.3111e-03,\n           -4.0856e-03, -5.5706e-05],\n          [ 2.4599e-01,  6.5324e-02, -1.6636e-01,  ...,  2.0774e-01,\n            2.1489e-01,  1.5290e-01],\n          [ 2.0671e-01,  1.3404e-02,  4.2020e-02,  ..., -5.6280e-03,\n           -1.2880e-02,  8.7626e-02],\n          ...,\n          [ 2.5829e-02, -5.9136e-02,  5.9752e-02,  ...,  1.4683e-01,\n           -7.1901e-02, -1.2485e-01],\n          [-1.2624e-01, -5.9179e-03, -1.0309e-01,  ..., -2.7041e-01,\n           -2.3986e-01,  8.8825e-02],\n          [ 4.1435e-02, -3.3318e-02, -2.1330e-01,  ...,  1.6987e-01,\n           -1.9361e-01,  6.8222e-02]],\n\n         [[-3.3976e-03, -1.4016e-02, -3.7917e-03,  ...,  4.6811e-03,\n           -2.5234e-03, -9.3919e-04],\n          [-8.1391e-02,  1.6873e-01,  1.8415e-01,  ...,  3.5957e-01,\n           -3.5106e-01,  5.9524e-01],\n          [ 1.0188e-01,  4.3379e-02,  2.3278e-01,  ...,  5.8512e-02,\n            2.5935e-02,  9.8331e-02],\n          ...,\n          [ 1.3360e-01, -2.1312e-01, -1.4242e-01,  ..., -2.4682e-01,\n           -8.5533e-02,  3.3362e-01],\n          [-2.7982e-03, -2.7451e-01, -2.2143e-01,  ..., -3.0105e-01,\n            7.8391e-02,  6.5044e-01],\n          [ 6.7177e-02, -2.1270e-01, -1.4785e-02,  ...,  3.2741e-02,\n           -2.7896e-01,  3.2424e-01]],\n\n         [[-8.4945e-03,  2.1021e-02,  7.2485e-03,  ..., -1.1689e-02,\n           -7.7789e-03,  6.4738e-04],\n          [-3.7948e-01,  2.0104e-01,  2.0720e-02,  ...,  8.9169e-02,\n            5.8569e-01, -8.2567e-02],\n          [ 1.1399e-02, -6.5978e-01,  2.8947e-02,  ..., -9.4991e-02,\n            1.1512e-02,  4.4629e-01],\n          ...,\n          [-7.7518e-02, -2.4437e-01,  3.9280e-03,  ...,  1.7358e-02,\n           -2.9522e-02, -1.0392e-01],\n          [-7.2031e-02,  2.0799e-01,  5.6901e-01,  ..., -8.9935e-02,\n           -1.1525e-01, -5.1730e-01],\n          [-1.3688e-01, -8.2219e-02, -4.2254e-01,  ...,  5.8449e-02,\n           -5.1667e-02, -9.8239e-02]]],\n\n\n        [[[-4.4655e-03,  9.5142e-03, -7.1053e-03,  ...,  2.7343e-02,\n           -3.2324e-03,  4.7344e-03],\n          [-1.6692e-03,  3.6682e-01, -2.4080e-01,  ...,  1.1667e-01,\n            3.8622e-02,  1.4735e-01],\n          [ 5.2787e-02,  1.4678e-01, -3.5098e-01,  ...,  2.1796e-01,\n            1.5116e-01,  1.6168e-01],\n          ...,\n          [ 1.1102e-01,  8.4685e-01,  4.6514e-01,  ..., -2.2010e-01,\n           -1.0828e-01, -1.8775e-01],\n          [-4.8309e-02,  3.3636e-01,  1.4625e-01,  ..., -5.9545e-01,\n            2.5387e-01, -5.1451e-02],\n          [-8.7452e-02,  1.9875e-01,  1.2442e-02,  ...,  2.9267e-01,\n            1.8462e-01, -2.7447e-02]],\n\n         [[-3.7190e-03, -3.6331e-03, -4.4507e-03,  ..., -8.1329e-04,\n           -1.1243e-02, -1.2153e-04],\n          [-3.9809e-02,  8.4760e-03, -3.4983e-02,  ..., -6.8673e-02,\n           -1.2965e-01,  5.4427e-02],\n          [-8.5350e-03,  4.5005e-02, -1.5586e-02,  ..., -2.9093e-02,\n            2.8592e-02, -2.5619e-03],\n          ...,\n          [-9.3189e-02, -1.6203e-02, -1.0550e-01,  ..., -3.0421e-02,\n           -6.8715e-03, -4.3474e-02],\n          [-2.0695e-02, -4.2215e-02, -6.3739e-02,  ...,  4.2644e-02,\n           -6.5672e-02, -1.3323e-02],\n          [-8.6221e-02, -4.2241e-02, -7.5169e-02,  ..., -1.8859e-02,\n            2.4375e-02,  4.4577e-02]],\n\n         [[-3.4597e-03, -5.1756e-03, -1.1762e-02,  ...,  3.0750e-03,\n            1.1601e-03,  5.3023e-03],\n          [-5.3303e-02,  4.9124e-01,  3.3367e-02,  ...,  1.7974e-01,\n           -1.5748e-02,  3.2456e-02],\n          [-6.1427e-02, -9.3182e-02,  2.7412e-02,  ..., -1.4554e-01,\n           -1.6193e-01,  5.2260e-02],\n          ...,\n          [-2.3439e-02, -8.3729e-03, -2.7105e-01,  ..., -8.0170e-02,\n            1.0319e-02,  1.2874e-01],\n          [-1.5015e-01, -5.3712e-02, -8.9916e-02,  ..., -6.6554e-01,\n            9.2098e-02,  3.5624e-02],\n          [-6.9553e-02,  8.9449e-02,  1.6436e-02,  ...,  7.1204e-02,\n            2.4175e-01,  9.6106e-02]],\n\n         ...,\n\n         [[ 8.3781e-03, -5.5893e-03,  1.0153e-02,  ..., -7.7297e-03,\n            4.8306e-03,  6.4666e-03],\n          [ 3.0306e-01, -6.3466e-02, -1.3938e-01,  ...,  3.0418e-01,\n            2.5676e-01,  6.5685e-02],\n          [ 2.1099e-01, -1.0794e-01, -2.5552e-01,  ...,  2.3456e-02,\n           -2.1828e-01, -6.2927e-02],\n          ...,\n          [-2.4499e-02, -5.9277e-02, -1.1351e-01,  ...,  1.7539e-01,\n            1.0494e-02, -6.3236e-02],\n          [-1.7277e-01, -1.7636e-02, -3.9198e-01,  ...,  1.5334e-01,\n           -2.6123e-01,  4.8409e-02],\n          [-4.2466e-02,  9.5969e-02,  5.7747e-02,  ..., -3.6896e-02,\n           -4.7484e-02, -5.2290e-02]],\n\n         [[-5.3028e-03, -1.7826e-02, -6.0743e-03,  ..., -1.7445e-02,\n           -3.0980e-03, -1.2942e-02],\n          [-1.0736e-01,  2.9050e-01,  1.1089e-02,  ..., -3.2224e-02,\n            2.6758e-01,  2.1531e-01],\n          [ 7.5556e-03,  1.5147e-01,  1.4199e-01,  ...,  3.1747e-01,\n           -2.7735e-01,  1.4102e-01],\n          ...,\n          [-2.6210e-01, -1.3435e-01,  2.6307e-01,  ..., -1.6586e-04,\n            3.1692e-02,  2.7481e-01],\n          [-9.8576e-02, -2.0568e-01,  5.0160e-01,  ..., -8.4740e-02,\n            4.7744e-01,  5.4538e-01],\n          [-1.3948e-01,  8.5177e-02, -9.1517e-02,  ...,  1.3629e-02,\n            9.2203e-02, -7.8422e-02]],\n\n         [[-1.1872e-02,  2.4747e-02,  5.6554e-03,  ...,  1.3571e-02,\n            1.1921e-03,  9.4084e-03],\n          [ 2.5852e-02, -1.1261e-01,  2.8180e-01,  ...,  3.8420e-01,\n            2.5310e-02,  9.9279e-03],\n          [ 1.6314e-01,  8.6841e-02,  2.4700e-01,  ...,  4.0769e-02,\n            4.4228e-02,  3.3564e-02],\n          ...,\n          [-1.5949e-01,  8.3033e-01,  1.6230e-01,  ...,  2.0873e-01,\n            3.0106e-01, -3.3733e-02],\n          [-9.4191e-02,  6.1662e-01,  2.2170e-01,  ..., -6.2365e-02,\n            3.7945e-01, -1.5920e-01],\n          [-1.2119e-02,  1.0960e-03,  8.1999e-02,  ...,  1.3713e-01,\n            1.5241e-01,  4.4520e-02]]],\n\n\n        [[[ 2.2613e-03, -3.1469e-03, -1.0983e-02,  ...,  1.3118e-02,\n           -8.9115e-03, -6.0784e-04],\n          [ 1.0665e-01, -2.5780e-01, -5.9263e-02,  ...,  1.2714e-01,\n            2.6378e-01, -1.0528e-01],\n          [ 1.0280e-01,  1.2015e-01, -2.0185e-01,  ...,  2.0557e-01,\n            2.2951e-02, -1.1223e-01],\n          ...,\n          [ 4.9208e-02,  1.3136e-02,  6.1942e-01,  ...,  1.9564e-01,\n            1.7762e-01, -9.4999e-02],\n          [ 6.9037e-02, -8.6868e-02,  3.8645e-01,  ...,  6.3488e-01,\n           -2.6388e-02, -5.9903e-02],\n          [ 1.8714e-02, -1.8850e-02,  1.4584e-01,  ..., -1.5342e-01,\n            1.3350e-01, -3.9566e-02]],\n\n         [[-1.5970e-03, -4.6247e-04, -8.7004e-04,  ..., -2.8798e-03,\n           -4.7654e-03,  4.2729e-03],\n          [ 1.1519e-02,  4.4903e-02,  2.2216e-02,  ..., -1.1002e-02,\n            2.1975e-01, -6.0411e-02],\n          [-1.2788e-02,  1.6777e-02,  8.6631e-03,  ..., -1.9766e-02,\n            1.4592e-01, -6.1249e-02],\n          ...,\n          [-4.2624e-03,  2.2631e-03, -4.7144e-02,  ..., -1.9868e-03,\n            1.1542e-01, -1.1407e-01],\n          [-4.0915e-02,  3.5713e-02, -2.8554e-02,  ..., -3.8730e-02,\n            6.7277e-04, -8.7987e-02],\n          [-3.9572e-02, -2.1762e-02, -9.0869e-02,  ..., -6.4239e-03,\n            1.2094e-01, -6.8877e-02]],\n\n         [[-2.4061e-03,  7.6548e-03, -4.5354e-03,  ..., -4.2352e-03,\n            1.9274e-03,  6.4232e-03],\n          [-1.4253e-01,  9.3156e-02,  1.8982e-01,  ...,  5.6599e-01,\n            2.5163e-01, -7.1876e-02],\n          [-1.8993e-01,  6.1471e-01,  1.3530e-01,  ...,  1.1306e-01,\n            1.2677e-01, -1.2928e-01],\n          ...,\n          [-3.1270e-01, -5.6371e-03,  9.3183e-02,  ...,  2.8999e-01,\n           -7.9225e-03, -1.4512e-01],\n          [-3.8045e-01, -2.1201e-01,  2.4316e-01,  ...,  9.4761e-02,\n            3.1997e-01, -7.8229e-02],\n          [-1.0737e-01, -4.7644e-02,  1.3096e-01,  ...,  9.9645e-02,\n            1.5883e-01,  2.7314e-03]],\n\n         ...,\n\n         [[ 8.2476e-03, -1.7591e-03,  6.0607e-03,  ...,  3.6629e-03,\n            4.1582e-03,  3.9698e-03],\n          [-1.4217e-01, -1.3421e-02,  3.1250e-01,  ...,  4.9009e-02,\n            1.8275e-01, -9.1921e-03],\n          [-1.8103e-01, -9.8818e-02, -1.9190e-01,  ...,  2.0212e-01,\n            2.5020e-02, -4.9550e-02],\n          ...,\n          [-1.2581e-01, -7.7775e-02, -1.4178e-01,  ..., -1.1268e-01,\n            6.1302e-02, -1.1927e-01],\n          [ 1.7494e-01, -1.0469e-01,  1.3087e-01,  ..., -1.2315e-01,\n            2.8732e-01,  1.3291e-01],\n          [ 2.0685e-01,  1.6839e-02,  2.9347e-01,  ...,  9.0163e-02,\n            1.9893e-01,  3.0341e-02]],\n\n         [[-1.1768e-02,  2.4383e-03, -3.0079e-03,  ..., -7.2779e-03,\n            3.4406e-03, -1.2443e-03],\n          [ 8.9010e-02,  2.4998e-01, -6.7884e-02,  ...,  8.6604e-02,\n           -6.6689e-02,  4.3279e-01],\n          [-8.2803e-02, -7.2274e-02, -3.6660e-01,  ...,  2.5013e-01,\n            3.7140e-01,  1.4437e-01],\n          ...,\n          [-7.4779e-02, -1.5154e-01, -1.5854e-02,  ..., -4.7654e-01,\n            9.1109e-03,  1.4522e-01],\n          [ 3.9726e-02, -1.2532e-02, -6.0468e-02,  ..., -1.6824e-01,\n           -1.5467e-01, -1.0065e-01],\n          [-1.4979e-01, -4.0402e-01,  1.3691e-02,  ..., -5.1332e-01,\n           -2.1328e-02, -3.5948e-01]],\n\n         [[-9.9597e-03,  1.3514e-03,  3.5559e-03,  ..., -9.2741e-03,\n           -7.1840e-03, -2.4625e-03],\n          [ 3.1595e-01, -6.9288e-01,  1.2375e-01,  ..., -1.5000e-01,\n           -1.5347e-01,  3.1675e-01],\n          [ 1.2071e-01, -2.5441e-01, -2.1992e-01,  ..., -3.7020e-01,\n           -1.2320e-01,  3.9080e-02],\n          ...,\n          [ 5.5943e-01, -6.5714e-01,  4.5519e-01,  ..., -1.7781e-01,\n           -1.9708e-01, -3.1137e-01],\n          [ 3.3828e-01, -2.2920e-01,  2.9033e-01,  ..., -2.0667e-01,\n            1.9095e-01, -1.3573e-01],\n          [ 2.9788e-01, -3.5161e-01, -1.5876e-01,  ...,  1.8595e-02,\n           -8.7880e-02, -4.7267e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-3.6810e-01,  9.5273e-01, -2.2834e-02,  ...,  1.7442e-02,\n            9.0755e-01, -7.4837e-01],\n          [-1.1457e+00, -6.1723e-02, -1.9198e+00,  ..., -1.6533e+00,\n           -6.7765e-01,  1.7243e+00],\n          [-4.2709e-01,  4.0192e-01, -1.6468e+00,  ..., -1.9093e+00,\n           -5.1757e-01,  9.4043e-01],\n          ...,\n          [-1.2319e+00,  3.6050e-01, -4.2960e-01,  ..., -8.6065e-01,\n            3.1439e+00, -2.6585e+00],\n          [-8.4376e-01,  6.4640e-01,  7.4428e-01,  ...,  8.8319e-03,\n            3.0726e+00, -3.0339e+00],\n          [-8.7835e-01,  3.4906e-01,  1.0126e-02,  ..., -6.5277e-01,\n            2.6653e+00, -2.3860e+00]],\n\n         [[-3.2000e-02,  1.3077e-01, -5.3848e-01,  ...,  5.2912e-01,\n            3.8973e-01,  3.8025e-01],\n          [-3.2085e-02,  1.1842e-01,  1.3291e-01,  ..., -5.3186e-01,\n           -1.3986e-01, -2.9186e-01],\n          [-3.1852e-02,  1.1429e-01,  3.3609e-02,  ..., -4.3341e-01,\n           -5.5048e-02, -2.3751e-01],\n          ...,\n          [-3.2409e-02,  1.2047e-01,  2.8163e-01,  ..., -2.3480e-01,\n           -2.8297e-01, -8.1459e-04],\n          [-3.3641e-02,  1.4887e-01,  3.8301e-01,  ..., -3.7954e-01,\n           -2.8689e-01, -1.1622e-01],\n          [-3.3201e-02,  1.3880e-01,  4.1261e-01,  ..., -3.0433e-01,\n           -3.4119e-01, -3.3252e-02]],\n\n         [[ 3.0237e-01, -6.4402e-01,  1.1026e+00,  ..., -3.1154e-02,\n            3.2774e-01, -2.3269e-01],\n          [-1.6376e-01,  4.8914e-01, -7.1276e-01,  ..., -3.0682e-02,\n           -3.1391e-01,  2.0295e-01],\n          [-1.2452e-01,  4.1488e-01, -6.0573e-01,  ..., -3.2804e-02,\n           -2.6381e-01,  1.6514e-01],\n          ...,\n          [-6.1464e-02,  2.7489e-01, -3.8415e-01,  ..., -3.0084e-02,\n           -1.8088e-01,  1.0252e-01],\n          [-1.2180e-01,  3.7075e-01, -5.1480e-01,  ..., -2.8442e-02,\n           -2.5216e-01,  1.6421e-01],\n          [-8.2188e-02,  2.8137e-01, -3.6905e-01,  ..., -2.9021e-02,\n           -1.9909e-01,  1.2445e-01]],\n\n         ...,\n\n         [[-8.3628e-01,  8.1693e-01, -9.6046e-01,  ..., -6.6184e-02,\n            6.5741e-01, -1.1510e-01],\n          [ 2.5840e-01, -2.3935e+00,  1.2544e+00,  ..., -3.9531e-02,\n           -3.1854e-01, -5.8922e-02],\n          [ 5.3109e-01, -1.2464e+00,  8.3937e-01,  ..., -5.4919e-02,\n           -4.7641e-01, -4.4710e-02],\n          ...,\n          [ 4.0267e-01,  1.8497e+00, -9.3777e-02,  ..., -3.3655e-02,\n           -4.8072e-01,  6.1193e-02],\n          [-8.3586e-01,  1.7823e+00, -3.8215e-01,  ..., -6.8604e-02,\n            2.7174e-01,  4.1787e-02],\n          [-9.2560e-01,  2.2094e+00, -5.2629e-01,  ..., -3.5637e-02,\n            3.5262e-01, -5.9726e-02]],\n\n         [[-6.3172e-01, -4.4354e-01, -3.1725e-01,  ..., -4.1690e-01,\n           -2.7087e-01, -4.2383e-01],\n          [ 2.9991e-01,  3.2550e-01,  6.1821e-01,  ...,  3.0044e-01,\n            5.4515e-01,  3.8919e-01],\n          [ 2.3725e-01,  2.7702e-01,  5.5491e-01,  ...,  2.5665e-01,\n            4.9245e-01,  3.3679e-01],\n          ...,\n          [ 1.3178e-01,  1.8828e-01,  4.4810e-01,  ...,  1.7349e-01,\n            3.9864e-01,  2.4341e-01],\n          [ 1.6336e-01,  2.1116e-01,  4.8029e-01,  ...,  1.9340e-01,\n            4.2418e-01,  2.6871e-01],\n          [ 9.3464e-02,  1.5359e-01,  4.0973e-01,  ...,  1.3992e-01,\n            3.6292e-01,  2.0776e-01]],\n\n         [[ 2.7094e-01,  5.5385e-01,  8.7239e-02,  ..., -3.1074e-01,\n            1.9897e-01,  1.1570e-01],\n          [-5.4456e-01, -1.0473e+00, -5.0866e-02,  ...,  1.5789e+00,\n           -5.2710e-01, -9.6141e-02],\n          [-4.7835e-01, -9.1799e-01, -4.3108e-02,  ...,  1.4290e+00,\n           -4.6862e-01, -8.1761e-02],\n          ...,\n          [-4.1218e-01, -7.6428e-01, -4.6746e-02,  ...,  1.2552e+00,\n           -4.1229e-01, -7.9610e-02],\n          [-4.1268e-01, -7.9419e-01, -3.6282e-02,  ...,  1.2957e+00,\n           -4.0942e-01, -6.5349e-02],\n          [-3.5674e-01, -6.8482e-01, -3.6372e-02,  ...,  1.1664e+00,\n           -3.6018e-01, -5.8583e-02]]],\n\n\n        [[[-3.5695e-01,  9.4973e-01,  4.1544e-04,  ...,  4.0025e-02,\n            9.0372e-01, -7.5460e-01],\n          [-2.9851e-01,  8.1698e-01, -6.7762e-01,  ..., -5.2905e-01,\n           -9.2664e-01,  7.2656e-01],\n          [-8.6798e-01, -1.1589e-01, -2.0273e+00,  ..., -1.5866e+00,\n           -7.7361e-01,  1.6415e+00],\n          ...,\n          [-4.0719e-01,  2.0275e-02, -1.3568e+00,  ..., -1.0180e+00,\n            2.5924e+00, -1.4743e+00],\n          [-3.7756e-01,  1.4902e-01, -9.8330e-01,  ..., -6.2463e-01,\n            2.5439e+00, -2.4299e+00],\n          [-1.0005e+00,  3.8364e-01, -7.9277e-01,  ..., -6.4039e-01,\n            2.6698e+00, -2.1467e+00]],\n\n         [[-3.1991e-02,  1.3077e-01, -5.3784e-01,  ...,  5.2794e-01,\n            3.9015e-01,  3.7904e-01],\n          [-3.2466e-02,  1.2753e-01,  1.1059e-01,  ..., -5.7718e-01,\n           -5.0453e-02, -3.5195e-01],\n          [-3.2843e-02,  1.2288e-01,  2.4763e-01,  ..., -5.0293e-01,\n           -1.9641e-01, -2.4262e-01],\n          ...,\n          [-3.2614e-02,  1.3059e-01,  3.0532e-01,  ..., -3.1989e-01,\n           -2.8622e-01, -7.1430e-02],\n          [-3.2865e-02,  1.3269e-01,  2.9824e-01,  ..., -2.8378e-01,\n           -2.7450e-01, -4.2264e-02],\n          [-3.3223e-02,  1.3698e-01,  2.8702e-01,  ..., -3.0624e-01,\n           -2.5921e-01, -7.2914e-02]],\n\n         [[ 3.0172e-01, -6.4223e-01,  1.0995e+00,  ..., -3.1149e-02,\n            3.2680e-01, -2.3204e-01],\n          [-1.7623e-01,  4.9017e-01, -7.0665e-01,  ..., -2.9846e-02,\n           -3.2394e-01,  2.1792e-01],\n          [-1.8192e-01,  4.8198e-01, -6.6453e-01,  ..., -2.8791e-02,\n           -3.2857e-01,  2.2314e-01],\n          ...,\n          [-1.3391e-01,  4.0072e-01, -5.6221e-01,  ..., -3.0528e-02,\n           -2.6811e-01,  1.7540e-01],\n          [-1.1305e-01,  3.6138e-01, -5.0693e-01,  ..., -3.0988e-02,\n           -2.4193e-01,  1.5579e-01],\n          [-1.3015e-01,  3.7113e-01, -5.0589e-01,  ..., -2.8919e-02,\n           -2.5846e-01,  1.7374e-01]],\n\n         ...,\n\n         [[-8.2660e-01,  8.0707e-01, -9.4695e-01,  ..., -6.6069e-02,\n            6.4819e-01, -1.1594e-01],\n          [ 5.4479e-02, -2.1266e+00,  1.0619e+00,  ..., -7.1937e-02,\n           -1.6451e-01, -3.8597e-02],\n          [ 3.9574e-01, -2.1614e+00,  1.4190e+00,  ..., -3.5934e-02,\n           -4.2736e-01, -6.7313e-02],\n          ...,\n          [-1.6795e-01,  1.2676e+00,  1.3061e-01,  ...,  2.4694e-02,\n           -1.5237e-01,  6.1113e-02],\n          [ 9.8400e-02,  1.5848e+00,  1.5529e-01,  ..., -5.7457e-03,\n           -2.8319e-01,  3.1223e-02],\n          [-3.0507e-01,  8.4601e-01,  1.5312e-01,  ..., -1.5676e-02,\n            1.8404e-02,  1.9131e-02]],\n\n         [[-6.3001e-01, -4.4217e-01, -3.1551e-01,  ..., -4.1564e-01,\n           -2.6939e-01, -4.2236e-01],\n          [ 2.8133e-01,  3.0843e-01,  5.9994e-01,  ...,  2.8373e-01,\n            5.2773e-01,  3.7172e-01],\n          [ 2.3144e-01,  2.6547e-01,  5.4927e-01,  ...,  2.4323e-01,\n            4.8260e-01,  3.2683e-01],\n          ...,\n          [ 1.9965e-01,  2.4244e-01,  5.1676e-01,  ...,  2.2297e-01,\n            4.5695e-01,  3.0139e-01],\n          [ 1.6276e-01,  2.1039e-01,  4.7985e-01,  ...,  1.9236e-01,\n            4.2355e-01,  2.6807e-01],\n          [ 1.4910e-01,  1.9998e-01,  4.6597e-01,  ...,  1.8318e-01,\n            4.1213e-01,  2.5672e-01]],\n\n         [[ 2.6967e-01,  5.5128e-01,  8.7314e-02,  ..., -3.0774e-01,\n            1.9787e-01,  1.1570e-01],\n          [-5.2796e-01, -1.0203e+00, -4.3210e-02,  ...,  1.5469e+00,\n           -5.1147e-01, -8.5398e-02],\n          [-4.8595e-01, -9.4917e-01, -4.1393e-02,  ...,  1.4673e+00,\n           -4.7313e-01, -7.5597e-02],\n          ...,\n          [-4.4297e-01, -8.4009e-01, -4.8079e-02,  ...,  1.3454e+00,\n           -4.3789e-01, -8.0672e-02],\n          [-3.9724e-01, -7.6187e-01, -3.7371e-02,  ...,  1.2566e+00,\n           -3.9655e-01, -6.7924e-02],\n          [-3.9900e-01, -7.6089e-01, -4.1203e-02,  ...,  1.2536e+00,\n           -3.9850e-01, -7.1111e-02]]],\n\n\n        [[[-3.5821e-01,  9.5514e-01, -2.1186e-02,  ...,  2.1379e-02,\n            9.1138e-01, -7.3897e-01],\n          [ 4.4494e-01,  8.1609e-01, -1.7954e+00,  ..., -1.6824e+00,\n           -3.8816e-01,  6.1752e-01],\n          [-1.3376e-01,  2.5281e-01, -1.3795e+00,  ..., -1.0559e+00,\n           -1.7735e-01,  1.0044e-01],\n          ...,\n          [-1.2227e+00,  1.9115e-02, -9.6039e-01,  ..., -8.8428e-01,\n            2.2088e+00, -1.6707e+00],\n          [-3.1385e-01,  3.0877e-01, -7.8907e-01,  ..., -1.1816e+00,\n            2.1258e+00, -1.7453e+00],\n          [ 1.0604e-02,  3.8589e-01, -9.6632e-01,  ..., -9.6858e-01,\n            2.4298e+00, -2.2756e+00]],\n\n         [[-3.2000e-02,  1.3092e-01, -5.3969e-01,  ...,  5.3073e-01,\n            3.9136e-01,  3.8084e-01],\n          [-3.2220e-02,  1.2619e-01,  7.6506e-02,  ..., -4.4491e-01,\n           -9.1986e-02, -2.3480e-01],\n          [-3.1727e-02,  1.2178e-01,  4.8112e-02,  ..., -4.0352e-01,\n           -7.5837e-02, -2.1076e-01],\n          ...,\n          [-3.3498e-02,  1.3143e-01,  3.1310e-01,  ..., -3.6147e-01,\n           -2.6385e-01, -1.0655e-01],\n          [-3.3009e-02,  1.3431e-01,  3.0803e-01,  ..., -3.6160e-01,\n           -2.5052e-01, -1.0621e-01],\n          [-3.3608e-02,  1.4294e-01,  2.2666e-01,  ..., -2.7189e-01,\n           -2.1448e-01, -5.4524e-02]],\n\n         [[ 3.0463e-01, -6.4794e-01,  1.1081e+00,  ..., -3.1135e-02,\n            3.3051e-01, -2.3496e-01],\n          [-1.0292e-01,  3.8323e-01, -5.7086e-01,  ..., -3.2171e-02,\n           -2.3918e-01,  1.4318e-01],\n          [-8.1809e-02,  3.2751e-01, -4.9379e-01,  ..., -3.2884e-02,\n           -2.0671e-01,  1.2096e-01],\n          ...,\n          [-1.3770e-01,  3.7821e-01, -5.0593e-01,  ..., -2.8610e-02,\n           -2.6674e-01,  1.7928e-01],\n          [-1.3497e-01,  4.1858e-01, -5.9725e-01,  ..., -3.0224e-02,\n           -2.7352e-01,  1.7588e-01],\n          [-6.6508e-02,  2.8774e-01, -4.0632e-01,  ..., -2.9793e-02,\n           -1.8646e-01,  1.0949e-01]],\n\n         ...,\n\n         [[-8.3741e-01,  8.0816e-01, -9.5360e-01,  ..., -6.6130e-02,\n            6.5878e-01, -1.1643e-01],\n          [ 1.2789e-01, -1.3559e+00,  8.5425e-01,  ..., -7.3173e-02,\n           -2.3487e-01, -1.1652e-01],\n          [ 9.1185e-01, -2.0052e+00,  1.3471e+00,  ..., -8.0821e-02,\n           -6.7303e-01, -3.6227e-02],\n          ...,\n          [-2.5745e-01,  7.9648e-01,  1.6950e-01,  ..., -3.9575e-02,\n           -1.1294e-01,  5.0029e-02],\n          [ 7.2148e-03,  8.5576e-01,  3.7263e-01,  ..., -4.1905e-02,\n           -2.8224e-01,  5.7974e-02],\n          [-1.1818e-01,  1.2736e+00, -2.2100e-02,  ...,  2.9421e-03,\n           -1.1087e-01,  4.8107e-02]],\n\n         [[-6.3388e-01, -4.4545e-01, -3.1942e-01,  ..., -4.1873e-01,\n           -2.7285e-01, -4.2581e-01],\n          [ 2.3170e-01,  2.6939e-01,  5.4970e-01,  ...,  2.4828e-01,\n            4.8549e-01,  3.2974e-01],\n          [ 2.1841e-01,  2.5743e-01,  5.3620e-01,  ...,  2.3680e-01,\n            4.7311e-01,  3.1742e-01],\n          ...,\n          [ 1.6875e-01,  2.1613e-01,  4.8554e-01,  ...,  1.9822e-01,\n            4.2926e-01,  2.7383e-01],\n          [ 2.0663e-01,  2.4694e-01,  5.2382e-01,  ...,  2.2673e-01,\n            4.6215e-01,  3.0656e-01],\n          [ 1.3653e-01,  1.9022e-01,  4.5352e-01,  ...,  1.7423e-01,\n            4.0158e-01,  2.4620e-01]],\n\n         [[ 2.7315e-01,  5.5759e-01,  8.7901e-02,  ..., -3.1503e-01,\n            2.0100e-01,  1.1663e-01],\n          [-5.0443e-01, -9.5516e-01, -5.0813e-02,  ...,  1.4690e+00,\n           -4.9287e-01, -9.2656e-02],\n          [-4.8451e-01, -9.2826e-01, -3.6983e-02,  ...,  1.4405e+00,\n           -4.7390e-01, -7.7193e-02],\n          ...,\n          [-4.3229e-01, -8.2042e-01, -4.3637e-02,  ...,  1.3223e+00,\n           -4.2818e-01, -7.5361e-02],\n          [-4.6639e-01, -8.8811e-01, -4.9619e-02,  ...,  1.4016e+00,\n           -4.5828e-01, -8.2946e-02],\n          [-3.9998e-01, -7.5691e-01, -4.4055e-02,  ...,  1.2466e+00,\n           -3.9976e-01, -7.2410e-02]]],\n\n\n        ...,\n\n\n        [[[-3.5576e-01,  9.7081e-01, -1.1936e-02,  ...,  3.0077e-02,\n            8.8047e-01, -7.3613e-01],\n          [-8.4125e-03,  3.5849e-01, -1.5423e+00,  ..., -2.0062e+00,\n           -2.1305e-01,  8.8903e-01],\n          [-6.4736e-01, -1.5682e-02, -1.2552e+00,  ..., -1.4288e+00,\n           -7.4919e-01,  6.1206e-01],\n          ...,\n          [-6.4724e-01,  2.1981e-01, -1.0498e+00,  ..., -6.7936e-01,\n            2.3262e+00, -2.0749e+00],\n          [-3.6389e-01,  8.3441e-02, -1.4372e+00,  ..., -1.1238e+00,\n            2.6900e+00, -1.6222e+00],\n          [-1.6399e-02,  7.6670e-01, -7.6409e-01,  ..., -1.1285e+00,\n            3.0371e+00, -2.1584e+00]],\n\n         [[-3.1986e-02,  1.3078e-01, -5.4463e-01,  ...,  5.3448e-01,\n            3.9615e-01,  3.8268e-01],\n          [-3.1730e-02,  1.1388e-01, -9.7937e-03,  ..., -3.9974e-01,\n           -3.6544e-02, -2.1275e-01],\n          [-3.2316e-02,  1.1101e-01, -2.3433e-02,  ..., -4.4487e-01,\n           -5.0160e-02, -2.4927e-01],\n          ...,\n          [-3.3372e-02,  1.4888e-01,  3.0680e-01,  ..., -2.5643e-01,\n           -2.6225e-01, -2.8874e-02],\n          [-3.3055e-02,  1.4080e-01,  2.4228e-01,  ..., -2.8996e-01,\n           -2.2622e-01, -6.7813e-02],\n          [-3.3078e-02,  1.3837e-01,  2.3973e-01,  ..., -2.8126e-01,\n           -2.2080e-01, -6.3717e-02]],\n\n         [[ 3.0517e-01, -6.5016e-01,  1.1124e+00,  ..., -3.1156e-02,\n            3.3144e-01, -2.3533e-01],\n          [-1.0947e-01,  3.5974e-01, -5.1782e-01,  ..., -3.2593e-02,\n           -2.3799e-01,  1.5101e-01],\n          [-1.4077e-01,  4.1487e-01, -5.8976e-01,  ..., -3.2136e-02,\n           -2.7697e-01,  1.8217e-01],\n          ...,\n          [-6.2628e-02,  2.7027e-01, -3.7030e-01,  ..., -2.8996e-02,\n           -1.8094e-01,  1.0680e-01],\n          [-1.2956e-01,  3.9533e-01, -5.4939e-01,  ..., -2.9819e-02,\n           -2.6450e-01,  1.7240e-01],\n          [-1.1125e-01,  3.5976e-01, -4.9765e-01,  ..., -3.0108e-02,\n           -2.4186e-01,  1.5469e-01]],\n\n         ...,\n\n         [[-8.3201e-01,  8.0749e-01, -9.5240e-01,  ..., -6.6450e-02,\n            6.5760e-01, -1.1614e-01],\n          [ 6.2264e-01, -1.5880e+00,  1.0403e+00,  ..., -2.4770e-02,\n           -5.0769e-01, -3.5669e-02],\n          [ 7.4064e-01, -1.6330e+00,  9.4614e-01,  ..., -8.0775e-02,\n           -6.2948e-01, -3.7878e-02],\n          ...,\n          [-1.0680e-01,  1.5052e+00, -8.1891e-03,  ...,  2.9392e-02,\n           -1.5537e-01,  1.8732e-02],\n          [ 1.0057e-01,  1.1003e+00,  2.3842e-01,  ...,  8.9479e-03,\n           -2.9676e-01,  3.3030e-02],\n          [ 1.4781e-01,  1.4255e+00,  1.2952e-01,  ..., -1.8937e-02,\n           -3.2349e-01,  6.0545e-02]],\n\n         [[-6.3712e-01, -4.4820e-01, -3.2267e-01,  ..., -4.2131e-01,\n           -2.7574e-01, -4.2869e-01],\n          [ 1.7767e-01,  2.2430e-01,  4.9512e-01,  ...,  2.0620e-01,\n            4.3771e-01,  2.8218e-01],\n          [ 2.2831e-01,  2.6900e-01,  5.4603e-01,  ...,  2.4897e-01,\n            4.8416e-01,  3.2849e-01],\n          ...,\n          [ 8.4621e-02,  1.4321e-01,  4.0134e-01,  ...,  1.2890e-01,\n            3.5311e-01,  1.9784e-01],\n          [ 1.6940e-01,  2.1626e-01,  4.8646e-01,  ...,  1.9814e-01,\n            4.2958e-01,  2.7410e-01],\n          [ 1.4068e-01,  1.8996e-01,  4.5777e-01,  ...,  1.7260e-01,\n            4.0259e-01,  2.4713e-01]],\n\n         [[ 2.7586e-01,  5.6269e-01,  8.8598e-02,  ..., -3.2108e-01,\n            2.0344e-01,  1.1755e-01],\n          [-4.5476e-01, -8.6692e-01, -3.6792e-02,  ...,  1.3677e+00,\n           -4.4789e-01, -7.4002e-02],\n          [-4.7110e-01, -9.1017e-01, -4.1012e-02,  ...,  1.4206e+00,\n           -4.6176e-01, -8.0535e-02],\n          ...,\n          [-3.6490e-01, -6.9045e-01, -4.0556e-02,  ...,  1.1703e+00,\n           -3.6851e-01, -6.5817e-02],\n          [-4.6227e-01, -8.5544e-01, -5.6862e-02,  ...,  1.3575e+00,\n           -4.5707e-01, -9.3141e-02],\n          [-4.3706e-01, -8.1202e-01, -4.7386e-02,  ...,  1.3073e+00,\n           -4.3378e-01, -8.1465e-02]]],\n\n\n        [[[-3.6169e-01,  9.8311e-01,  3.8201e-04,  ...,  3.6951e-02,\n            9.3218e-01, -7.6198e-01],\n          [-9.2642e-02, -3.7607e-03, -1.0327e+00,  ..., -2.1694e+00,\n            3.0486e-01,  5.8974e-01],\n          [-6.2753e-01, -2.2691e-01, -7.3288e-01,  ..., -1.4846e+00,\n           -5.3083e-01,  4.1872e-01],\n          ...,\n          [-5.1223e-01,  7.6491e-01, -1.0600e+00,  ..., -1.5948e+00,\n            2.7071e+00, -1.5664e+00],\n          [-6.7922e-01,  2.5606e-01, -9.5093e-01,  ..., -2.6711e-01,\n            3.1141e+00, -2.9203e+00],\n          [-8.6385e-01,  6.2671e-01,  5.6243e-01,  ..., -5.5458e-02,\n            2.3964e+00, -2.9081e+00]],\n\n         [[-3.2006e-02,  1.3107e-01, -5.5554e-01,  ...,  5.4521e-01,\n            4.0321e-01,  3.8775e-01],\n          [-3.1814e-02,  1.1304e-01,  5.3573e-02,  ..., -4.6428e-01,\n           -1.0724e-01, -2.5896e-01],\n          [-3.1589e-02,  1.1338e-01,  1.0842e-01,  ..., -4.8642e-01,\n           -1.5056e-01, -2.5391e-01],\n          ...,\n          [-3.2269e-02,  1.2869e-01,  2.9007e-01,  ..., -2.8711e-01,\n           -2.7691e-01, -4.6319e-02],\n          [-3.3091e-02,  1.3668e-01,  3.0860e-01,  ..., -2.3490e-01,\n           -3.1611e-01,  7.4631e-03],\n          [-3.3332e-02,  1.4552e-01,  3.8229e-01,  ..., -3.1218e-01,\n           -2.8168e-01, -5.7317e-02]],\n\n         [[ 3.1115e-01, -6.6538e-01,  1.1367e+00,  ..., -3.1219e-02,\n            3.3974e-01, -2.4086e-01],\n          [-1.3028e-01,  4.2973e-01, -6.4048e-01,  ..., -3.2108e-02,\n           -2.7040e-01,  1.7039e-01],\n          [-1.5077e-01,  4.6089e-01, -6.8096e-01,  ..., -3.1942e-02,\n           -2.9430e-01,  1.9015e-01],\n          ...,\n          [-1.0391e-01,  3.5442e-01, -4.9647e-01,  ..., -2.8923e-02,\n           -2.3416e-01,  1.4613e-01],\n          [-9.5215e-02,  3.2117e-01, -4.4165e-01,  ..., -2.9558e-02,\n           -2.1853e-01,  1.3787e-01],\n          [-8.7065e-02,  3.0123e-01, -4.0733e-01,  ..., -2.8142e-02,\n           -2.0693e-01,  1.2929e-01]],\n\n         ...,\n\n         [[-8.5588e-01,  8.1863e-01, -9.8338e-01,  ..., -6.6619e-02,\n            6.7557e-01, -1.1735e-01],\n          [ 7.0141e-01, -1.4879e+00,  9.9307e-01,  ..., -4.9128e-02,\n           -5.6576e-01, -5.9777e-02],\n          [ 5.9818e-01, -1.3665e+00,  8.4700e-01,  ..., -7.7587e-02,\n           -5.3923e-01, -8.0516e-02],\n          ...,\n          [-2.1213e-01,  1.3871e+00,  3.1143e-02,  ..., -2.6672e-03,\n           -1.0271e-01,  5.1822e-02],\n          [-5.6325e-01,  2.9089e+00, -7.0035e-01,  ..., -2.1501e-02,\n            8.8897e-02,  4.9855e-02],\n          [-9.0636e-01,  2.2966e+00, -5.2742e-01,  ..., -7.3423e-02,\n            2.9107e-01,  9.6108e-03]],\n\n         [[-6.4780e-01, -4.5677e-01, -3.3337e-01,  ..., -4.2923e-01,\n           -2.8490e-01, -4.3782e-01],\n          [ 2.7032e-01,  3.0431e-01,  5.8833e-01,  ...,  2.8189e-01,\n            5.2152e-01,  3.6572e-01],\n          [ 2.9278e-01,  3.2271e-01,  6.1090e-01,  ...,  2.9911e-01,\n            5.4108e-01,  3.8520e-01],\n          ...,\n          [ 1.5580e-01,  2.0286e-01,  4.7278e-01,  ...,  1.8485e-01,\n            4.1609e-01,  2.6063e-01],\n          [ 1.2644e-01,  1.7920e-01,  4.4310e-01,  ...,  1.6297e-01,\n            3.9077e-01,  2.3543e-01],\n          [ 1.3095e-01,  1.8696e-01,  4.4737e-01,  ...,  1.7178e-01,\n            3.9759e-01,  2.4230e-01]],\n\n         [[ 2.8567e-01,  5.8282e-01,  9.0176e-02,  ..., -3.4520e-01,\n            2.1213e-01,  1.2000e-01],\n          [-4.9459e-01, -9.4795e-01, -4.0638e-02,  ...,  1.4655e+00,\n           -4.8311e-01, -8.1455e-02],\n          [-5.1263e-01, -9.9281e-01, -4.5283e-02,  ...,  1.5189e+00,\n           -4.9832e-01, -8.7693e-02],\n          ...,\n          [-4.3910e-01, -8.2796e-01, -4.3906e-02,  ...,  1.3283e+00,\n           -4.3418e-01, -7.5029e-02],\n          [-3.9021e-01, -7.3551e-01, -4.9149e-02,  ...,  1.2245e+00,\n           -3.9169e-01, -7.7871e-02],\n          [-3.7007e-01, -7.0626e-01, -3.1029e-02,  ...,  1.1921e+00,\n           -3.7228e-01, -5.7428e-02]]],\n\n\n        [[[-3.8689e-01,  9.2386e-01, -9.1145e-03,  ...,  1.9981e-02,\n            9.0269e-01, -7.3771e-01],\n          [-5.6076e-01,  3.3688e-01, -1.2116e+00,  ..., -2.0337e+00,\n           -9.0304e-01,  1.5281e+00],\n          [-1.2062e-01,  3.1490e-01, -1.7362e+00,  ..., -2.1330e+00,\n           -8.2056e-01,  1.4119e+00],\n          ...,\n          [-4.1672e-01, -2.0125e-01, -3.7496e-01,  ..., -1.5333e+00,\n            2.4199e+00, -1.5079e+00],\n          [-8.2437e-01, -2.8084e-01, -3.3045e-01,  ..., -6.8315e-01,\n            2.1523e+00, -1.9999e+00],\n          [-7.6137e-01, -7.2888e-02, -3.4232e-01,  ..., -1.6629e+00,\n            2.6295e+00, -1.2625e+00]],\n\n         [[-3.1997e-02,  1.3070e-01, -5.2061e-01,  ...,  5.1147e-01,\n            3.7724e-01,  3.7040e-01],\n          [-3.1636e-02,  1.1194e-01,  1.3667e-01,  ..., -5.3255e-01,\n           -1.2450e-01, -2.9799e-01],\n          [-3.2026e-02,  1.1781e-01,  8.7287e-02,  ..., -5.2415e-01,\n           -8.0071e-02, -2.9700e-01],\n          ...,\n          [-3.3048e-02,  1.3526e-01,  3.6595e-01,  ..., -3.5192e-01,\n           -3.2695e-01, -8.1209e-02],\n          [-3.2810e-02,  1.3026e-01,  3.7398e-01,  ..., -2.4570e-01,\n           -2.9911e-01,  1.5111e-02],\n          [-3.2518e-02,  1.2683e-01,  3.2504e-01,  ..., -3.2008e-01,\n           -2.9747e-01, -6.3353e-02]],\n\n         [[ 2.9372e-01, -6.2435e-01,  1.0719e+00,  ..., -3.1037e-02,\n            3.1604e-01, -2.2451e-01],\n          [-1.5253e-01,  4.4054e-01, -6.3369e-01,  ..., -3.1439e-02,\n           -2.9146e-01,  1.9261e-01],\n          [-1.7985e-01,  5.0411e-01, -7.2560e-01,  ..., -2.9898e-02,\n           -3.2942e-01,  2.1990e-01],\n          ...,\n          [-1.3723e-01,  4.1972e-01, -5.9934e-01,  ..., -2.9138e-02,\n           -2.7538e-01,  1.7739e-01],\n          [-1.0152e-01,  3.3670e-01, -4.6486e-01,  ..., -2.9682e-02,\n           -2.2673e-01,  1.4408e-01],\n          [-1.0967e-01,  3.6727e-01, -5.2485e-01,  ..., -2.9707e-02,\n           -2.3948e-01,  1.4953e-01]],\n\n         ...,\n\n         [[-8.5239e-01,  8.1464e-01, -9.4389e-01,  ..., -6.6081e-02,\n            6.5140e-01, -1.1556e-01],\n          [ 3.6689e-01, -2.1308e+00,  1.1692e+00,  ..., -5.3441e-02,\n           -3.8314e-01, -3.4901e-02],\n          [ 8.1170e-01, -1.7737e+00,  1.2905e+00,  ..., -5.0870e-02,\n           -6.6553e-01, -6.2004e-02],\n          ...,\n          [ 1.6607e-01,  5.4466e-01,  4.5152e-01,  ..., -1.6487e-02,\n           -3.6645e-01,  9.4715e-03],\n          [-1.6764e-01,  1.5792e+00, -8.5490e-02,  ..., -3.6007e-02,\n           -1.2236e-01, -5.2386e-03],\n          [ 2.9909e-01,  8.6930e-01,  3.3779e-01,  ..., -2.6415e-02,\n           -4.3255e-01,  3.1892e-02]],\n\n         [[-6.1634e-01, -4.3096e-01, -3.0180e-01,  ..., -4.0521e-01,\n           -2.5747e-01, -4.1049e-01],\n          [ 1.9875e-01,  2.3913e-01,  5.1615e-01,  ...,  2.1927e-01,\n            4.5424e-01,  2.9863e-01],\n          [ 2.7505e-01,  3.0787e-01,  5.9292e-01,  ...,  2.8541e-01,\n            5.2525e-01,  3.6946e-01],\n          ...,\n          [ 1.6522e-01,  2.1070e-01,  4.8199e-01,  ...,  1.9257e-01,\n            4.2420e-01,  2.6876e-01],\n          [ 7.3162e-02,  1.3124e-01,  3.8951e-01,  ...,  1.1713e-01,\n            3.4109e-01,  1.8590e-01],\n          [ 1.4126e-01,  1.9050e-01,  4.5782e-01,  ...,  1.7359e-01,\n            4.0289e-01,  2.4753e-01]],\n\n         [[ 2.5660e-01,  5.2624e-01,  8.4768e-02,  ..., -2.7828e-01,\n            1.8620e-01,  1.1202e-01],\n          [-4.7789e-01, -9.3480e-01, -3.3540e-02,  ...,  1.4522e+00,\n           -4.6602e-01, -7.0729e-02],\n          [-5.3913e-01, -1.0258e+00, -5.8075e-02,  ...,  1.5531e+00,\n           -5.2381e-01, -1.0213e-01],\n          ...,\n          [-4.4493e-01, -8.6113e-01, -4.6855e-02,  ...,  1.3750e+00,\n           -4.3824e-01, -7.6989e-02],\n          [-3.6862e-01, -7.1807e-01, -2.7769e-02,  ...,  1.2074e+00,\n           -3.6978e-01, -5.3296e-02],\n          [-4.4086e-01, -8.3659e-01, -4.4899e-02,  ...,  1.3424e+00,\n           -4.3609e-01, -7.6906e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-1.1211e-02,  1.0693e-02, -2.4296e-03,  ..., -2.5513e-02,\n           -6.3848e-03, -1.2993e-02],\n          [-1.0472e-01,  8.7357e-02,  3.0602e-01,  ..., -1.0895e-01,\n            1.5502e-01, -2.6175e-02],\n          [ 2.7209e-01, -2.9002e-01, -1.9320e-01,  ..., -3.0075e-01,\n            1.9466e-01, -2.9912e-01],\n          ...,\n          [ 8.0735e-02,  1.4436e-01, -3.7663e-02,  ..., -6.0255e-02,\n           -1.8622e-01,  2.7183e-01],\n          [ 1.1692e-01, -2.4859e-02,  2.2295e-02,  ..., -2.7812e-01,\n           -1.2661e-01,  5.6439e-02],\n          [-2.5633e-01,  1.3906e-01, -2.9036e-01,  ...,  3.7670e-03,\n           -8.7086e-02,  1.4535e-01]],\n\n         [[-8.0029e-04, -1.4369e-02,  2.2305e-03,  ...,  3.5407e-04,\n           -4.4818e-03,  7.7541e-03],\n          [ 2.9789e-02, -3.3900e-01,  7.1132e-04,  ..., -4.8235e-02,\n            1.0533e-01, -8.3939e-02],\n          [-2.6105e-03,  1.4400e-01,  5.1539e-02,  ...,  5.6702e-02,\n            1.1916e-01,  5.1971e-02],\n          ...,\n          [-4.5292e-02, -1.6225e-01, -3.1106e-02,  ...,  6.4766e-02,\n            5.8795e-01, -1.5044e-01],\n          [ 2.4468e-03,  3.2521e-02, -2.4955e-03,  ...,  1.3905e-01,\n           -3.6696e-02, -2.0795e-01],\n          [ 4.1330e-02, -7.4206e-02, -7.3368e-02,  ..., -1.1716e-02,\n           -1.1370e-02, -8.2566e-02]],\n\n         [[-9.4127e-04,  1.4029e-03, -5.6394e-03,  ..., -5.7522e-03,\n            7.1382e-04, -1.8708e-04],\n          [-2.3589e-02,  4.0726e-02,  1.0505e-01,  ..., -1.5166e-02,\n           -5.7989e-02,  7.6335e-03],\n          [-9.3337e-03,  4.2039e-02, -1.6709e-03,  ...,  2.9804e-02,\n           -8.6097e-02,  3.0518e-02],\n          ...,\n          [ 8.8228e-02, -5.1005e-02, -1.0080e-01,  ...,  4.7667e-02,\n            9.8192e-02, -8.1675e-02],\n          [ 8.6559e-02,  1.0339e-02, -1.2124e-02,  ...,  1.5402e-01,\n            1.6905e-02,  8.6308e-03],\n          [ 8.2333e-02,  3.8984e-03, -1.1447e-01,  ...,  8.8902e-02,\n            3.4477e-02,  2.7636e-02]],\n\n         ...,\n\n         [[ 1.4852e-03, -1.0757e-02, -6.1911e-04,  ..., -2.3692e-05,\n           -2.0932e-03, -3.0649e-03],\n          [ 8.6138e-03, -2.6464e-01,  1.9501e-01,  ...,  7.8462e-02,\n            3.6717e-02, -2.4806e-01],\n          [-3.8447e-01,  1.7635e-01,  2.2265e-01,  ..., -1.0380e-01,\n            9.4179e-02, -2.5809e-01],\n          ...,\n          [-7.7070e-02, -2.9839e-01, -8.6853e-02,  ...,  2.0395e-02,\n           -6.7293e-02,  1.9046e-01],\n          [-1.8483e-01, -1.3462e-01, -3.7194e-01,  ..., -1.3902e-02,\n           -8.2068e-02,  4.6163e-02],\n          [ 5.1700e-02, -7.4197e-02, -8.2128e-02,  ..., -7.3874e-02,\n           -4.2295e-03,  4.6900e-02]],\n\n         [[ 1.2937e-03, -1.0279e-03,  6.4847e-03,  ..., -3.9725e-03,\n           -4.5819e-03,  7.5564e-03],\n          [-4.5974e-02, -7.6125e-03,  2.9833e-02,  ..., -1.5288e-02,\n            7.1235e-04, -6.3424e-03],\n          [ 2.1086e-02,  3.7888e-02, -1.6975e-02,  ...,  2.5544e-02,\n           -4.3926e-02, -3.8811e-02],\n          ...,\n          [ 4.6747e-02, -1.0039e-01, -5.9781e-02,  ..., -9.7842e-02,\n           -4.5209e-02, -2.3786e-02],\n          [ 2.2719e-02, -9.5431e-03, -1.6718e-02,  ...,  5.0642e-02,\n            1.4429e-02, -6.3167e-03],\n          [-7.1287e-04,  4.4083e-03, -4.8077e-02,  ...,  2.5998e-02,\n           -4.4066e-02,  2.6127e-02]],\n\n         [[-1.2678e-03, -3.8304e-03, -3.0357e-03,  ...,  1.2288e-03,\n           -2.0843e-03, -3.1873e-03],\n          [-1.2080e-02, -5.8675e-02,  5.1224e-03,  ...,  1.1761e-02,\n            3.1119e-02,  2.8636e-02],\n          [ 3.1814e-02,  1.8995e-02,  7.1857e-03,  ..., -4.0009e-02,\n           -1.3394e-03,  8.6626e-02],\n          ...,\n          [ 7.4633e-02,  9.7411e-02,  4.0392e-02,  ..., -7.2567e-03,\n            2.6813e-02,  4.6559e-02],\n          [-1.5337e-03, -3.9038e-02, -1.0260e-02,  ...,  8.1380e-02,\n            1.4112e-02,  1.0103e-02],\n          [ 7.5556e-02,  3.5336e-02,  1.1928e-02,  ..., -2.8776e-02,\n            3.7671e-02,  2.7183e-02]]],\n\n\n        [[[-6.1966e-03,  1.9051e-02, -1.6427e-02,  ..., -2.6602e-02,\n           -1.2638e-02, -9.4716e-03],\n          [-1.7737e-02, -2.6609e-01, -3.5506e-01,  ..., -7.0619e-02,\n            2.5790e-01, -2.7576e-01],\n          [ 9.8808e-02, -3.8496e-02,  1.9207e-02,  ..., -6.3119e-02,\n            8.0810e-02, -3.9794e-01],\n          ...,\n          [ 7.5925e-02, -2.6465e-01,  6.1286e-01,  ...,  2.5067e-02,\n           -1.9731e-01, -5.9014e-02],\n          [ 4.8906e-02,  1.2644e-01,  4.1226e-01,  ..., -6.4459e-01,\n            6.8830e-03,  6.7037e-02],\n          [-1.9109e-01,  3.3242e-01, -1.5762e-01,  ..., -3.0959e-02,\n           -2.4711e-01,  4.1544e-01]],\n\n         [[-1.6213e-04, -3.6800e-03,  3.4990e-03,  ...,  1.9076e-03,\n           -5.4607e-03,  4.1002e-03],\n          [ 5.3789e-03, -1.3542e-01, -5.0168e-02,  ...,  5.7386e-02,\n           -4.1069e-01, -1.8836e-01],\n          [ 4.6336e-02, -2.3071e-01, -3.2196e-02,  ..., -9.5599e-03,\n           -4.2959e-01,  9.6114e-03],\n          ...,\n          [-4.7251e-02,  3.1218e-01, -7.1318e-02,  ...,  2.0158e-02,\n            2.8752e-01,  2.0461e-01],\n          [-1.4737e-02,  2.7188e-01, -4.1550e-02,  ...,  1.4534e-02,\n            3.2516e-01, -3.5572e-02],\n          [ 6.8662e-02,  6.2906e-01, -1.3573e-02,  ...,  3.1885e-02,\n            1.6276e-01,  3.2393e-02]],\n\n         [[-1.1521e-03,  2.4273e-04, -6.3529e-03,  ..., -5.3968e-03,\n           -2.6977e-04, -1.1172e-03],\n          [-6.1476e-03,  1.8600e-02,  1.5329e-02,  ...,  1.2414e-01,\n           -1.0408e-01,  3.1652e-02],\n          [ 1.1253e-02,  4.6358e-02, -6.1634e-02,  ...,  7.4043e-02,\n           -7.8215e-02,  3.6758e-02],\n          ...,\n          [-6.1371e-04, -1.8397e-03,  1.7964e-01,  ...,  4.8222e-02,\n            1.3248e-01, -3.0051e-02],\n          [ 2.5576e-02, -1.2608e-02,  1.1245e-01,  ...,  4.2772e-02,\n            2.0850e-02, -1.8089e-02],\n          [ 5.7854e-02,  2.6773e-02,  7.7761e-02,  ...,  2.2076e-02,\n           -2.8806e-02, -4.9906e-02]],\n\n         ...,\n\n         [[ 1.6866e-03, -7.8194e-03, -1.0429e-02,  ...,  2.0041e-04,\n           -3.3235e-03,  3.8350e-04],\n          [ 3.4164e-02, -1.0141e-02, -9.0550e-01,  ...,  2.5982e-02,\n            8.0483e-02, -9.2078e-02],\n          [ 7.4208e-02, -8.0664e-02, -1.0486e+00,  ..., -1.4878e-01,\n           -3.7469e-02, -9.4895e-02],\n          ...,\n          [ 2.5623e-01,  3.5301e-02,  1.1613e-01,  ..., -1.2485e-01,\n           -2.5118e-01, -1.9845e-01],\n          [ 4.9364e-02,  8.5104e-02,  6.4549e-01,  ..., -4.0815e-02,\n           -2.4411e-01, -2.0776e-01],\n          [ 1.3169e-01,  6.0576e-02, -1.8218e-01,  ..., -1.0298e-01,\n           -7.0422e-02, -1.8321e-01]],\n\n         [[ 1.4127e-03, -2.8099e-03,  5.2962e-03,  ..., -3.1009e-03,\n           -3.3252e-03,  5.1631e-03],\n          [-2.1973e-02, -2.8998e-02, -1.7706e-03,  ...,  2.9487e-02,\n            4.9031e-02, -3.0393e-02],\n          [ 1.4166e-02,  1.5523e-03, -5.0476e-02,  ...,  3.9286e-02,\n            7.9785e-02, -4.0811e-02],\n          ...,\n          [ 6.5737e-02, -2.4986e-02, -8.3839e-02,  ...,  7.3340e-02,\n            4.8719e-03, -3.9374e-02],\n          [ 4.4358e-02, -3.2475e-02, -6.4721e-02,  ...,  3.4243e-03,\n           -3.2821e-02,  2.8336e-02],\n          [ 7.8578e-02,  9.5732e-03, -4.8835e-02,  ...,  4.2643e-02,\n            3.8696e-03,  3.0491e-02]],\n\n         [[-5.1810e-04, -2.8886e-03, -1.2343e-03,  ...,  2.6604e-03,\n           -2.0963e-03, -2.7032e-03],\n          [-1.5821e-02,  6.5922e-03,  7.6825e-02,  ...,  3.3809e-02,\n            5.1816e-02,  3.2975e-02],\n          [-2.3392e-02, -4.6141e-02,  1.0224e-02,  ...,  6.1172e-02,\n            5.8294e-02, -3.6108e-03],\n          ...,\n          [ 7.4136e-02,  8.4702e-03, -1.0015e-02,  ...,  2.1066e-02,\n            1.9068e-02,  2.3600e-02],\n          [ 3.6161e-02,  3.5151e-03, -1.3821e-02,  ..., -9.0984e-04,\n           -1.4186e-02, -1.4525e-02],\n          [ 6.0735e-02, -2.6181e-03, -4.4476e-02,  ..., -2.3907e-02,\n           -4.6288e-02,  6.1886e-02]]],\n\n\n        [[[-1.1213e-02,  2.1660e-02, -1.2150e-02,  ..., -2.0609e-02,\n           -1.1632e-02, -5.6137e-03],\n          [ 6.3899e-02, -7.0924e-01,  3.8375e-01,  ..., -4.1505e-01,\n            1.3189e-01, -1.6939e-01],\n          [-7.1330e-02,  5.2240e-01, -2.0446e-01,  ...,  2.4542e-01,\n            1.7212e-01,  2.3415e-01],\n          ...,\n          [ 3.7540e-02,  6.4356e-02, -3.0805e-01,  ..., -4.1216e-01,\n           -3.9759e-02, -2.0692e-01],\n          [-1.3249e-01, -9.0280e-02,  1.3823e-01,  ...,  1.0089e-01,\n            1.2362e-01,  3.4685e-01],\n          [-2.4866e-01, -1.4641e-01,  2.0816e-01,  ...,  2.7779e-01,\n            9.0046e-02,  2.4458e-01]],\n\n         [[ 5.6287e-04, -6.8289e-03,  4.2806e-03,  ...,  3.7704e-04,\n           -9.7256e-03,  8.3393e-03],\n          [ 2.7631e-02, -5.5507e-02, -4.2753e-02,  ..., -8.7705e-02,\n           -1.2438e-02,  2.5969e-01],\n          [ 7.0991e-02, -1.1142e-01,  5.3637e-02,  ..., -7.6205e-03,\n           -3.7771e-02,  1.6973e-03],\n          ...,\n          [-5.5879e-02,  1.4829e-01,  7.5974e-02,  ...,  7.4241e-02,\n            2.6001e-01, -1.9313e-01],\n          [ 2.8938e-02,  5.0059e-01, -3.7991e-02,  ...,  2.7318e-02,\n            3.7406e-01,  8.2678e-02],\n          [-7.2613e-02,  1.9466e-01, -4.9464e-02,  ..., -3.7481e-02,\n            9.1159e-02, -6.0000e-02]],\n\n         [[-3.2039e-03,  1.5956e-03, -6.3483e-03,  ..., -7.6663e-03,\n           -3.6218e-05, -1.9586e-03],\n          [-8.5426e-02,  5.3366e-02, -4.3482e-02,  ..., -1.1783e-03,\n           -4.5019e-02, -1.1589e-02],\n          [-2.0408e-02,  1.4550e-02,  5.5370e-02,  ...,  5.3642e-03,\n           -3.7933e-02,  9.9440e-03],\n          ...,\n          [ 6.1299e-04,  5.7866e-02,  6.4064e-02,  ..., -1.0358e-03,\n            9.3837e-02,  5.6279e-02],\n          [ 3.2422e-02,  5.8075e-04,  1.9986e-02,  ...,  1.5636e-02,\n            3.9284e-02, -1.0566e-02],\n          [-3.3101e-02, -4.7851e-02,  7.6589e-03,  ...,  9.7388e-03,\n            4.4891e-02,  1.0056e-02]],\n\n         ...,\n\n         [[ 3.8996e-03, -6.7719e-03, -2.2855e-02,  ...,  2.0950e-03,\n           -1.4180e-03, -3.1220e-03],\n          [-1.8204e-01,  3.0915e-01, -1.6358e-01,  ...,  1.4654e-01,\n            6.3269e-02, -1.4673e-01],\n          [ 1.7405e-01,  1.5425e-02, -5.7450e-01,  ...,  1.1395e-01,\n            7.1398e-02, -2.9878e-01],\n          ...,\n          [-1.7780e-01, -1.9071e-01, -3.1867e-01,  ..., -2.1426e-02,\n           -1.0696e-01, -2.2520e-02],\n          [-1.0733e-01, -5.5500e-02,  1.0270e-01,  ..., -6.9109e-02,\n            9.0061e-03,  2.3887e-01],\n          [-8.1467e-02, -1.9379e-01,  2.6424e-01,  ...,  1.1328e-01,\n           -7.4292e-02, -1.5634e-02]],\n\n         [[ 8.1976e-04, -1.9113e-03,  6.2449e-03,  ..., -3.4039e-03,\n           -2.4797e-03,  6.6411e-03],\n          [ 2.7127e-02, -5.1913e-02,  4.9319e-03,  ..., -4.5022e-02,\n            2.4433e-02, -5.5755e-03],\n          [ 3.3244e-02, -8.0491e-02,  7.9729e-04,  ..., -4.7037e-02,\n            2.2952e-02, -6.3178e-02],\n          ...,\n          [-1.1446e-02, -3.9866e-02, -1.4306e-02,  ..., -5.4170e-02,\n           -4.7168e-02,  7.4412e-02],\n          [ 8.7878e-02,  1.2651e-02,  5.0567e-02,  ...,  5.9275e-04,\n           -1.6332e-03,  7.1767e-02],\n          [ 3.7075e-02, -9.8103e-03, -2.2269e-02,  ...,  5.5196e-03,\n            1.6369e-02, -5.0298e-02]],\n\n         [[-1.2970e-03, -4.1160e-03, -2.8875e-03,  ...,  2.9674e-03,\n           -1.8809e-03, -1.7450e-03],\n          [ 4.4473e-02,  1.1575e-01,  6.9013e-02,  ..., -7.7505e-03,\n            2.2691e-03,  9.2464e-02],\n          [ 4.2034e-02,  9.3125e-02,  6.6148e-02,  ...,  7.5013e-02,\n            1.0525e-01,  4.7915e-02],\n          ...,\n          [ 2.0727e-03, -1.1307e-02, -1.1044e-02,  ...,  1.0470e-02,\n            1.6493e-02,  2.3532e-02],\n          [ 2.1000e-03, -3.2179e-02, -8.3404e-02,  ...,  5.3120e-02,\n            6.6095e-02, -1.8722e-02],\n          [ 4.7796e-02,  2.6417e-02,  1.9600e-02,  ..., -3.5098e-02,\n            4.8629e-02,  1.2814e-03]]],\n\n\n        ...,\n\n\n        [[[-1.1267e-02,  2.2111e-02, -2.3345e-02,  ..., -3.4278e-02,\n           -1.1316e-02, -1.0254e-02],\n          [-4.8248e-03,  2.5688e-01, -1.1270e-01,  ..., -3.2712e-02,\n           -3.1073e-02, -1.6721e-01],\n          [ 1.2900e-01,  1.4346e-01, -7.5498e-02,  ...,  1.2981e-01,\n           -1.3561e-01, -1.7190e-01],\n          ...,\n          [-7.9243e-02,  9.4014e-02,  7.3687e-02,  ...,  5.4102e-02,\n           -1.1062e-01,  1.2468e-01],\n          [-8.5578e-02,  2.0596e-01,  9.8658e-01,  ...,  4.1004e-01,\n           -3.0381e-01,  3.6692e-01],\n          [ 1.7038e-01,  1.9206e-01, -9.5043e-02,  ..., -6.3849e-02,\n           -3.7243e-02,  1.5006e-01]],\n\n         [[-2.6176e-04, -1.7459e-03,  4.3644e-03,  ...,  4.3094e-03,\n           -9.0148e-03, -6.9682e-04],\n          [ 5.0756e-02, -3.0537e-01, -3.7749e-02,  ..., -2.5024e-02,\n            7.5308e-02, -2.9770e-02],\n          [-1.9626e-03,  1.7669e-02, -1.9555e-02,  ..., -1.8934e-02,\n           -1.8350e-01, -8.0799e-02],\n          ...,\n          [ 4.7756e-02,  4.1037e-01,  6.6237e-03,  ...,  4.5520e-02,\n           -9.7046e-02,  1.7203e-01],\n          [-8.4354e-03,  2.3702e-01, -5.5569e-02,  ...,  4.7201e-02,\n            3.9430e-01,  1.9373e-01],\n          [-3.7472e-02,  2.7937e-01,  8.8812e-03,  ...,  1.0371e-01,\n            3.3642e-01,  9.1379e-02]],\n\n         [[-4.2320e-03,  2.5790e-04, -9.8238e-03,  ..., -8.2832e-03,\n           -5.3885e-03, -1.8259e-03],\n          [-5.0858e-02, -1.1484e-01,  4.2621e-02,  ...,  1.6935e-02,\n            2.9594e-02, -6.3076e-02],\n          [-7.5518e-02,  3.7248e-03, -2.4975e-02,  ..., -1.4194e-02,\n            6.4139e-02,  9.3680e-02],\n          ...,\n          [ 1.7201e-02,  2.4201e-02, -7.6075e-02,  ..., -5.3291e-03,\n            3.1387e-02, -3.1226e-02],\n          [ 4.0875e-02, -3.3403e-02,  4.4318e-02,  ..., -2.0571e-02,\n            5.5792e-02, -8.3743e-02],\n          [ 2.8188e-02, -1.9005e-02, -3.2658e-02,  ..., -3.5399e-02,\n            1.1019e-01, -4.0599e-02]],\n\n         ...,\n\n         [[ 2.2192e-03, -7.0674e-03, -2.1127e-02,  ..., -2.1688e-03,\n           -2.7454e-03,  4.8986e-03],\n          [-9.3400e-02, -7.3602e-02,  1.9865e-01,  ...,  4.6858e-02,\n           -1.0446e-02, -2.7028e-01],\n          [-4.7250e-02, -7.8608e-02, -3.2036e-01,  ..., -8.0305e-02,\n           -2.3833e-01,  8.1818e-02],\n          ...,\n          [ 1.5460e-01, -9.5076e-03, -5.1022e-01,  ..., -4.2292e-02,\n           -8.8360e-02,  1.1106e-01],\n          [ 2.0703e-01, -1.2030e-03, -3.9410e-01,  ..., -1.6105e-01,\n           -1.3235e-02,  9.0293e-03],\n          [ 2.6452e-01,  1.6524e-01, -3.7472e-01,  ...,  3.6733e-02,\n           -6.7096e-02,  1.9981e-02]],\n\n         [[ 1.0569e-04, -3.4944e-03,  6.1773e-03,  ..., -3.1193e-03,\n           -2.0700e-03,  4.9985e-03],\n          [-3.8483e-02, -4.8427e-02, -1.8131e-02,  ..., -1.2429e-02,\n            1.1618e-02, -2.1274e-02],\n          [-1.0350e-01,  5.7040e-02, -2.4802e-02,  ...,  2.7416e-02,\n            4.8656e-02, -6.4984e-02],\n          ...,\n          [ 6.2927e-02,  6.6745e-02,  6.0599e-03,  ...,  4.8723e-02,\n            3.7197e-03,  2.9166e-02],\n          [ 3.4528e-02, -3.2236e-02, -3.7643e-02,  ..., -9.4716e-03,\n           -3.6808e-02,  2.2412e-02],\n          [ 3.0524e-02, -7.2114e-03, -2.2669e-02,  ..., -6.6935e-02,\n           -7.9342e-02,  7.1904e-02]],\n\n         [[-2.8680e-03, -3.9611e-03,  3.7315e-04,  ...,  4.2952e-03,\n           -3.5108e-03, -2.7890e-03],\n          [ 4.9250e-02,  4.6780e-03,  1.4526e-01,  ..., -4.2482e-02,\n           -5.5584e-02, -1.7763e-02],\n          [-5.3700e-02,  5.6491e-02,  1.2334e-01,  ..., -1.4473e-01,\n            4.7069e-02,  1.5193e-02],\n          ...,\n          [ 2.5407e-03, -1.6436e-02, -1.0718e-01,  ..., -3.6820e-02,\n            4.4139e-02,  1.2404e-02],\n          [ 9.2362e-02,  2.4772e-02, -3.8368e-02,  ..., -2.4954e-02,\n            6.7685e-02,  4.8471e-02],\n          [-8.6973e-03,  4.3666e-02, -3.4837e-02,  ..., -6.2687e-02,\n            7.2860e-02,  6.0889e-03]]],\n\n\n        [[[-1.0617e-02,  2.1636e-02, -1.0398e-02,  ..., -2.1623e-02,\n           -1.2694e-02, -6.3590e-03],\n          [ 1.5559e-01,  5.2976e-01,  4.9491e-01,  ...,  4.9353e-01,\n           -3.7849e-02,  6.8375e-02],\n          [ 3.0974e-01, -9.1692e-02,  2.0854e-01,  ..., -3.7078e-01,\n            2.0781e-01, -1.1934e-01],\n          ...,\n          [-1.0737e-01,  2.7845e-01,  4.6450e-01,  ...,  2.4936e-01,\n           -4.1846e-01,  8.1374e-01],\n          [-8.4777e-02,  2.6080e-01,  5.1535e-03,  ..., -8.9170e-02,\n           -1.4611e-01,  8.0398e-02],\n          [-4.8745e-02,  2.2463e-01,  2.5858e-01,  ...,  3.8825e-02,\n           -3.4996e-01,  4.6928e-01]],\n\n         [[ 2.9344e-03,  3.7472e-04,  3.9687e-03,  ...,  7.5282e-03,\n           -1.0355e-02, -3.1354e-03],\n          [ 5.5763e-02, -1.7318e-01, -2.7582e-02,  ...,  4.6899e-02,\n            2.0376e-01,  5.7779e-02],\n          [ 5.2913e-02,  2.8767e-02, -9.2113e-03,  ...,  3.3703e-02,\n           -1.3469e-01,  5.8267e-02],\n          ...,\n          [ 1.0706e-01,  3.4749e-01, -5.3512e-02,  ..., -4.0623e-03,\n            4.0837e-01,  1.7200e-01],\n          [ 4.8368e-02,  4.1422e-01, -8.6789e-02,  ..., -2.7146e-02,\n            4.0176e-01,  1.1853e-01],\n          [ 3.6526e-02,  2.3935e-01, -3.7552e-02,  ...,  1.1072e-02,\n           -2.1864e-01,  5.9731e-02]],\n\n         [[-2.3649e-03,  1.9107e-03, -1.0717e-02,  ..., -7.2072e-03,\n           -2.6506e-03, -3.1252e-03],\n          [-3.7076e-02, -6.5753e-02,  2.9012e-02,  ...,  9.1290e-02,\n            6.6012e-02, -3.5019e-02],\n          [-5.9289e-02, -2.6477e-03, -2.6866e-02,  ...,  1.3489e-02,\n           -1.8122e-02,  2.3313e-02],\n          ...,\n          [ 7.0442e-02, -2.0821e-02,  8.9095e-02,  ...,  9.5609e-02,\n            5.1638e-02, -5.5074e-02],\n          [ 8.0684e-02,  3.4228e-02,  1.9415e-02,  ...,  3.0948e-02,\n            7.4276e-02, -5.9641e-02],\n          [ 2.8792e-02, -5.5705e-03, -5.5580e-02,  ...,  9.3196e-02,\n            6.6853e-02,  1.5179e-02]],\n\n         ...,\n\n         [[ 3.8663e-03, -4.4758e-03, -3.0397e-03,  ...,  4.8391e-04,\n           -1.6059e-03, -3.1890e-03],\n          [-1.3460e-01,  5.1756e-01,  1.9061e-01,  ..., -6.6449e-02,\n            1.0033e-01, -4.4723e-01],\n          [-1.5915e-01,  2.3915e-01,  6.1273e-01,  ...,  4.6144e-02,\n           -1.7989e-02, -2.9738e-01],\n          ...,\n          [ 4.1189e-01,  1.2906e-01, -2.9685e-01,  ..., -2.2677e-01,\n           -9.2997e-02, -1.9268e-03],\n          [-3.6623e-02,  2.9561e-01, -1.3352e-01,  ...,  1.1752e-01,\n           -8.0298e-02,  1.3505e-01],\n          [-3.9470e-02,  7.6393e-02, -5.8512e-01,  ..., -7.6543e-02,\n           -8.6522e-02, -1.9047e-02]],\n\n         [[ 1.2606e-03, -1.7956e-03,  5.2812e-03,  ..., -3.0210e-03,\n           -4.6317e-03,  5.6066e-03],\n          [ 6.5257e-02, -5.9635e-02,  1.8268e-02,  ..., -2.4334e-02,\n           -5.3300e-02, -1.5250e-02],\n          [-6.8313e-03,  3.8325e-02,  3.0299e-02,  ..., -8.2929e-03,\n           -7.4325e-03, -2.8718e-02],\n          ...,\n          [ 9.6769e-02, -5.2245e-02,  3.6435e-02,  ..., -6.1939e-04,\n            1.8421e-02,  4.2617e-02],\n          [ 1.1863e-01, -6.0770e-02,  2.5120e-02,  ..., -4.0674e-02,\n           -6.2448e-02,  6.6406e-02],\n          [ 5.1338e-02, -1.7361e-02, -2.2820e-02,  ...,  3.8253e-02,\n            6.5284e-04,  5.9105e-02]],\n\n         [[-1.9719e-03, -4.4517e-03, -2.3506e-03,  ...,  2.8886e-03,\n           -3.0827e-03, -2.3279e-03],\n          [ 1.9267e-02, -1.1157e-02,  6.3626e-02,  ...,  9.5994e-03,\n           -3.1771e-02, -3.5363e-02],\n          [-5.7610e-02, -4.1233e-02,  3.9081e-03,  ..., -3.9740e-02,\n            8.1660e-04, -2.1398e-02],\n          ...,\n          [ 9.3325e-02,  2.8070e-02,  2.0994e-02,  ...,  6.1707e-03,\n            2.1136e-02,  2.7239e-02],\n          [ 7.7002e-02,  3.5685e-02, -9.9125e-03,  ..., -4.9667e-04,\n           -7.6712e-02,  7.5700e-02],\n          [ 4.2135e-02,  2.1895e-02,  1.4480e-02,  ...,  1.2784e-02,\n           -6.6313e-02,  5.5306e-02]]],\n\n\n        [[[-1.4653e-02,  1.4578e-02, -9.4447e-03,  ..., -2.2178e-02,\n           -7.9309e-03, -1.0942e-02],\n          [ 4.0220e-01, -4.8670e-01,  2.1755e-02,  ...,  2.5688e-02,\n            2.4725e-01,  2.0053e-01],\n          [ 9.2860e-03, -2.5941e-01, -2.2913e-01,  ...,  8.7942e-02,\n           -2.7149e-02,  3.1569e-01],\n          ...,\n          [-8.2173e-02,  3.4388e-01,  4.6842e-01,  ...,  3.4328e-01,\n            1.8784e-01,  1.9338e-01],\n          [ 2.4888e-02,  2.2706e-02,  4.3890e-01,  ...,  1.3783e-01,\n           -4.3468e-02, -2.2203e-02],\n          [ 1.3063e-01,  1.5401e-01,  2.9587e-01,  ..., -9.4094e-02,\n           -5.8659e-02,  4.9228e-01]],\n\n         [[-1.0879e-04, -1.0551e-02,  1.5839e-03,  ..., -5.6590e-05,\n           -1.5580e-03,  8.1836e-03],\n          [ 3.2560e-03, -1.9900e-01,  5.2140e-02,  ...,  6.1352e-02,\n            2.5608e-01,  4.9244e-02],\n          [-6.8951e-02, -9.3495e-02,  5.4571e-02,  ...,  3.9217e-02,\n           -8.7136e-02, -1.7652e-02],\n          ...,\n          [-2.1625e-02, -6.1282e-02,  4.5800e-02,  ...,  4.6825e-02,\n            4.6321e-01,  5.0413e-02],\n          [ 2.8813e-03, -9.4040e-02,  2.9021e-02,  ...,  8.8155e-02,\n            4.4773e-01, -6.1440e-02],\n          [-2.0540e-02, -5.7060e-02,  1.7322e-02,  ...,  1.1355e-01,\n            4.6165e-01, -8.5211e-02]],\n\n         [[ 6.4225e-04,  2.8352e-03, -6.3661e-03,  ..., -7.5182e-03,\n           -2.3123e-03, -1.3095e-03],\n          [-1.9857e-02, -1.6455e-02,  6.4907e-02,  ...,  2.0071e-02,\n           -2.8461e-02, -2.3717e-03],\n          [-5.3021e-02, -5.5688e-02,  9.2016e-02,  ..., -1.5067e-02,\n           -4.0497e-02, -4.1838e-02],\n          ...,\n          [ 7.7924e-02, -5.5005e-03,  1.0381e-01,  ...,  2.9459e-02,\n            2.9337e-02, -2.3748e-03],\n          [ 3.9121e-02,  1.1575e-03,  1.0298e-01,  ...,  8.4208e-02,\n            8.5883e-02,  3.2332e-02],\n          [ 6.6356e-02, -1.3395e-03,  6.1592e-02,  ...,  7.0889e-02,\n           -6.8893e-03, -4.2583e-02]],\n\n         ...,\n\n         [[ 3.3163e-03, -9.4454e-03, -1.5183e-02,  ...,  3.3633e-03,\n           -3.0885e-03,  1.5970e-03],\n          [-1.9404e-01, -1.0037e-01, -1.1139e-01,  ..., -9.5853e-03,\n            9.7875e-02, -1.7226e-01],\n          [-2.0341e-01, -7.8882e-02, -2.4299e-01,  ..., -1.0043e-01,\n            1.6284e-01, -1.0946e-01],\n          ...,\n          [-2.2383e-01, -1.6024e-01,  3.0905e-01,  ..., -1.4425e-01,\n            1.8219e-01, -3.6535e-03],\n          [-3.6780e-01, -3.3416e-01,  8.2371e-01,  ...,  1.4570e-01,\n           -9.2942e-03,  3.0967e-02],\n          [-5.8933e-02, -2.8307e-01,  2.7989e-01,  ..., -2.8205e-01,\n            3.5974e-02, -8.2188e-03]],\n\n         [[ 1.4414e-03,  3.8105e-04,  5.2793e-03,  ..., -2.5019e-03,\n           -4.5468e-03,  9.3958e-03],\n          [ 8.1783e-03, -6.4036e-03,  2.8927e-02,  ..., -1.1631e-02,\n            8.6751e-03,  1.4186e-02],\n          [ 8.8681e-03, -3.8707e-02, -2.9474e-02,  ...,  1.0016e-02,\n           -1.7587e-02, -5.9401e-02],\n          ...,\n          [-1.0738e-03,  1.4729e-02, -3.2542e-02,  ..., -1.0079e-02,\n           -3.3003e-02,  2.5396e-02],\n          [-2.6582e-03, -2.1013e-02,  8.9475e-03,  ..., -2.0303e-03,\n            2.3856e-02,  3.3467e-02],\n          [ 5.6663e-02, -9.6774e-03, -4.6171e-02,  ...,  1.8877e-02,\n           -2.3357e-02,  2.2353e-02]],\n\n         [[-6.9739e-04, -5.5982e-03, -5.3985e-03,  ...,  1.0762e-03,\n           -2.6125e-03, -1.6196e-03],\n          [-3.9650e-02, -3.9762e-02,  1.7829e-02,  ...,  4.0005e-02,\n            3.7947e-02, -3.5861e-02],\n          [ 5.0791e-02, -2.6398e-03,  7.8529e-02,  ..., -3.3978e-02,\n            3.3826e-02, -3.8123e-03],\n          ...,\n          [ 2.5971e-02,  3.8307e-02, -3.3652e-03,  ..., -5.9356e-02,\n            5.4869e-02,  1.9850e-02],\n          [-1.4339e-02,  2.4243e-02,  1.5269e-02,  ...,  1.0988e-02,\n            2.4464e-02,  1.5757e-02],\n          [ 5.6121e-02,  4.6843e-02,  1.9863e-02,  ..., -4.2746e-02,\n            5.5631e-03,  3.6206e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[ 2.9236e-01,  3.1014e-01, -1.4112e-01,  ...,  4.9748e-01,\n            2.8265e+00,  3.9914e-01],\n          [-7.7401e-02,  4.2958e-02, -2.3808e-01,  ...,  5.0578e-01,\n           -1.7163e+00,  3.7732e-01],\n          [-5.5044e-02,  4.6847e-01, -1.4311e-01,  ...,  5.7086e-01,\n           -9.9932e-01,  3.7844e-01],\n          ...,\n          [-1.8812e-01,  2.8654e-01, -5.9154e-01,  ...,  4.7014e-01,\n            3.4312e-01,  4.1815e-01],\n          [-1.1426e-01,  5.5510e-01, -4.8369e-01,  ...,  4.8658e-01,\n            3.8221e-01,  3.8483e-01],\n          [-3.6003e-01,  4.3740e-01, -9.5726e-01,  ...,  4.0927e-01,\n            2.3174e-01,  4.5914e-01]],\n\n         [[-9.3674e-01,  1.0492e+00,  1.3240e+00,  ..., -8.1451e-01,\n            9.1472e-01,  7.4128e-01],\n          [-5.3156e-02,  1.2218e-02,  2.2396e-01,  ...,  1.4556e-01,\n           -1.7602e-02, -1.1385e-01],\n          [-7.0845e-02,  3.4532e-02,  2.4819e-01,  ...,  1.2548e-01,\n            1.5480e-03, -9.7054e-02],\n          ...,\n          [-1.2869e-01,  1.0473e-01,  3.2360e-01,  ...,  6.1195e-02,\n            6.3434e-02, -4.1596e-02],\n          [-1.4979e-01,  1.2394e-01,  3.4208e-01,  ...,  4.1368e-02,\n            8.3798e-02, -2.0018e-02],\n          [-1.7397e-01,  1.5155e-01,  3.7111e-01,  ...,  1.5068e-02,\n            1.0921e-01,  3.3869e-03]],\n\n         [[ 1.4939e-01,  6.7541e-01,  4.5004e-01,  ..., -9.9523e-01,\n            5.1137e-01, -4.3167e-01],\n          [ 5.1139e-01,  2.2655e-01, -1.5010e-01,  ..., -6.0780e-02,\n           -1.4823e-02, -4.8084e-01],\n          [ 9.5281e-02,  4.3779e-02, -4.2620e-02,  ...,  2.6883e-03,\n           -9.7941e-02,  1.1887e+00],\n          ...,\n          [ 1.5845e+00,  3.2291e-01, -1.9608e-01,  ..., -1.3927e+00,\n            1.3940e+00,  2.0174e-01],\n          [-2.1300e-02,  5.3356e-01, -1.1322e-01,  ..., -1.4865e+00,\n           -1.4458e+00, -1.1103e+00],\n          [-8.6389e-02, -3.5399e-03, -1.0188e-01,  ..., -1.8631e+00,\n           -1.8854e-01, -1.1394e-01]],\n\n         ...,\n\n         [[-1.5685e+00,  1.3036e+00, -3.1275e-02,  ...,  2.5813e-01,\n           -7.6939e-02, -1.3824e+00],\n          [ 2.9306e-01, -2.8134e+00, -5.6587e-03,  ...,  3.6746e-01,\n            3.1477e-04,  2.2338e+00],\n          [ 1.7899e-01, -2.1014e+00,  4.4846e-02,  ..., -1.3060e-01,\n           -5.5137e-03,  1.6482e+00],\n          ...,\n          [ 3.1009e-01,  2.7394e+00, -6.4294e-02,  ...,  6.1553e-01,\n           -9.0671e-02, -2.3665e+00],\n          [-6.9937e-02,  2.7482e+00, -5.3020e-02,  ...,  1.4420e+00,\n            9.1156e-03, -2.9725e+00],\n          [-3.7501e-01,  3.7142e+00, -1.0177e-01,  ...,  4.8635e-01,\n           -1.3376e-01, -3.2394e+00]],\n\n         [[ 3.2451e-02,  7.7035e-03, -1.0215e+00,  ..., -7.6598e-02,\n           -1.3613e+00, -1.1905e-02],\n          [ 1.3481e-02, -7.5359e-02,  1.5695e-01,  ...,  1.6075e-02,\n            7.0628e-01,  6.5849e-02],\n          [ 2.1780e-02, -5.4485e-02, -2.7323e-01,  ..., -4.7733e-03,\n           -3.1916e-01,  7.6649e-02],\n          ...,\n          [ 1.4234e-02, -3.9897e-02,  1.1329e-02,  ..., -2.6303e-02,\n           -4.1006e-01, -1.1122e-02],\n          [ 5.5788e-02, -1.0020e-01,  3.0866e-01,  ...,  2.5880e-02,\n           -3.4049e-01,  4.1938e-03],\n          [ 3.7428e-02,  3.8973e-02, -1.2471e-01,  ..., -1.1369e-01,\n           -9.0615e-01, -8.7982e-02]],\n\n         [[-5.9457e-01, -6.6813e-01,  5.1520e-01,  ...,  5.1423e-01,\n            5.6381e-01, -6.5756e-01],\n          [ 2.8072e-01,  3.9760e-01, -2.7469e-01,  ..., -3.1763e-01,\n           -3.5270e-01,  3.5591e-01],\n          [ 2.4400e-01,  3.5484e-01, -2.4125e-01,  ..., -2.8257e-01,\n           -3.1447e-01,  3.1447e-01],\n          ...,\n          [ 2.0598e-01,  3.0016e-01, -2.1132e-01,  ..., -2.4878e-01,\n           -2.7254e-01,  2.6373e-01],\n          [ 1.9993e-01,  2.9408e-01, -2.0473e-01,  ..., -2.4242e-01,\n           -2.6670e-01,  2.5804e-01],\n          [ 1.6287e-01,  2.5435e-01, -1.6964e-01,  ..., -2.0643e-01,\n           -2.2874e-01,  2.1846e-01]]],\n\n\n        [[[ 2.9367e-01,  3.1703e-01, -1.3652e-01,  ...,  4.9749e-01,\n            2.8370e+00,  3.9934e-01],\n          [-1.0764e-01, -2.4489e-02, -3.1817e-01,  ...,  5.5073e-01,\n           -1.7342e+00,  3.2613e-01],\n          [-1.7346e-01,  6.9393e-02, -4.4366e-01,  ...,  5.0233e-01,\n           -1.4011e+00,  3.7339e-01],\n          ...,\n          [-1.1733e-01,  7.6838e-01, -3.1556e-01,  ...,  5.5357e-01,\n            2.0231e-01,  3.9667e-01],\n          [-8.4212e-02,  1.2176e+00, -8.2441e-02,  ...,  5.0662e-01,\n            3.6026e-01,  3.6357e-01],\n          [ 4.5698e-02,  1.1497e+00, -3.8840e-02,  ...,  4.6954e-01,\n            9.5456e-02,  4.4409e-01]],\n\n         [[-9.3574e-01,  1.0481e+00,  1.3230e+00,  ..., -8.1348e-01,\n            9.1371e-01,  7.4030e-01],\n          [-1.2024e-01,  8.9844e-02,  3.0607e-01,  ...,  7.2846e-02,\n            5.2910e-02, -4.8813e-02],\n          [-9.2765e-02,  5.9656e-02,  2.7474e-01,  ...,  1.0166e-01,\n            2.4580e-02, -7.5792e-02],\n          ...,\n          [-4.1451e-02,  4.3451e-03,  2.1775e-01,  ...,  1.5542e-01,\n           -2.8148e-02, -1.2624e-01],\n          [-8.5098e-02,  5.1868e-02,  2.6686e-01,  ...,  1.0965e-01,\n            1.6794e-02, -8.3413e-02],\n          [-1.2251e-01,  9.5397e-02,  3.1296e-01,  ...,  6.8992e-02,\n            5.6221e-02, -4.7160e-02]],\n\n         [[ 1.3268e-01,  6.6660e-01,  4.4420e-01,  ..., -9.8416e-01,\n            5.1153e-01, -4.2836e-01],\n          [-2.1782e-01,  6.0997e-01, -8.1277e-01,  ..., -6.9863e-01,\n           -1.4602e+00,  4.9899e-01],\n          [ 8.6896e-01, -7.4028e-01, -8.3258e-01,  ..., -6.9158e-01,\n            5.4069e-01, -4.8417e-01],\n          ...,\n          [-6.7138e-01, -3.0763e-01,  1.9479e-01,  ..., -1.4238e+00,\n            2.7651e-01, -2.4850e-01],\n          [-7.0801e-01, -7.8346e-01, -4.2405e-01,  ..., -1.1585e+00,\n           -1.8144e-02, -3.6343e-01],\n          [ 1.9726e-02, -3.0901e-01,  3.9475e-02,  ..., -2.0928e+00,\n            1.3777e-01, -1.3143e+00]],\n\n         ...,\n\n         [[-1.5713e+00,  1.3068e+00, -2.9823e-02,  ...,  2.5658e-01,\n           -7.7478e-02, -1.3851e+00],\n          [ 3.9535e-01, -1.9791e+00,  8.2083e-04,  ...,  8.2020e-01,\n            2.8587e-02,  8.2838e-01],\n          [ 3.7323e-01, -1.5187e+00, -6.7306e-02,  ...,  2.1369e-01,\n           -1.2136e-01,  1.2345e+00],\n          ...,\n          [-8.2904e-02,  1.3206e+00,  1.2210e-02,  ...,  6.7606e-01,\n           -1.0566e-01, -9.4343e-01],\n          [ 1.4404e-01,  1.8964e+00,  5.5552e-03,  ...,  6.1206e-01,\n           -5.2524e-02, -1.4096e+00],\n          [-2.0148e-01,  1.7212e+00, -2.3487e-02,  ...,  1.0571e+00,\n           -8.1155e-02, -1.5270e+00]],\n\n         [[ 3.2316e-02,  9.5924e-03, -1.0277e+00,  ..., -7.8672e-02,\n           -1.3823e+00, -1.4592e-02],\n          [ 2.0792e-03, -7.9834e-02, -4.0521e-01,  ...,  8.2192e-03,\n           -6.2487e-01, -1.1738e-02],\n          [-3.4572e-02,  7.3334e-03,  7.6504e-02,  ..., -6.5961e-02,\n            4.6777e-01, -5.2850e-02],\n          ...,\n          [ 1.0615e-02, -2.2825e-02,  5.8563e-01,  ..., -4.2614e-02,\n            6.5717e-01, -4.3083e-02],\n          [ 6.0056e-02, -1.5235e-02,  2.1518e-01,  ..., -5.6360e-02,\n            1.5226e-01, -2.1123e-02],\n          [ 7.8823e-02, -1.4837e-02,  6.0765e-01,  ..., -5.8638e-02,\n            6.7136e-01, -1.1580e-02]],\n\n         [[-5.9541e-01, -6.6904e-01,  5.1600e-01,  ...,  5.1505e-01,\n            5.6466e-01, -6.5845e-01],\n          [ 2.3360e-01,  3.4058e-01, -2.3255e-01,  ..., -2.7305e-01,\n           -3.0325e-01,  3.0122e-01],\n          [ 2.2830e-01,  3.3500e-01, -2.2802e-01,  ..., -2.6820e-01,\n           -2.9764e-01,  2.9527e-01],\n          ...,\n          [ 2.5915e-01,  3.6696e-01, -2.5787e-01,  ..., -2.9857e-01,\n           -3.2886e-01,  3.2721e-01],\n          [ 2.4384e-01,  3.4708e-01, -2.4494e-01,  ..., -2.8449e-01,\n           -3.1244e-01,  3.0833e-01],\n          [ 2.1177e-01,  3.0885e-01, -2.1601e-01,  ..., -2.5402e-01,\n           -2.7890e-01,  2.7152e-01]]],\n\n\n        [[[ 2.9200e-01,  3.0889e-01, -1.4382e-01,  ...,  4.9814e-01,\n            2.8341e+00,  3.9860e-01],\n          [-2.0652e-01,  1.6542e-01, -4.0709e-01,  ...,  5.7827e-01,\n           -1.1649e+00,  3.4045e-01],\n          [-7.0598e-02,  3.4865e-01, -8.0688e-02,  ...,  5.3929e-01,\n           -9.2815e-01,  3.8483e-01],\n          ...,\n          [-3.1109e-02,  5.7496e-01, -3.6506e-01,  ...,  4.8157e-01,\n           -7.7595e-02,  4.1674e-01],\n          [-1.5693e-01,  7.0132e-01, -3.3978e-01,  ...,  5.4263e-01,\n           -2.3426e-01,  4.0810e-01],\n          [-1.6174e-01,  8.6890e-01, -2.4030e-01,  ...,  5.1528e-01,\n            2.8996e-01,  4.4486e-01]],\n\n         [[-9.3653e-01,  1.0490e+00,  1.3239e+00,  ..., -8.1432e-01,\n            9.1452e-01,  7.4106e-01],\n          [-1.0118e-01,  7.0996e-02,  2.8716e-01,  ...,  9.2105e-02,\n            3.3838e-02, -6.7846e-02],\n          [-1.2380e-01,  9.0525e-02,  3.0551e-01,  ...,  7.1063e-02,\n            5.5441e-02, -4.4622e-02],\n          ...,\n          [-8.1494e-02,  4.8773e-02,  2.6405e-01,  ...,  1.1307e-01,\n            1.3330e-02, -8.7058e-02],\n          [-7.8794e-02,  4.2057e-02,  2.5563e-01,  ...,  1.1807e-01,\n            9.2605e-03, -8.8917e-02],\n          [-9.3494e-02,  5.8965e-02,  2.7350e-01,  ...,  1.0212e-01,\n            2.4668e-02, -7.4694e-02]],\n\n         [[ 1.4652e-01,  6.6255e-01,  4.4852e-01,  ..., -1.0118e+00,\n            5.1818e-01, -4.3737e-01],\n          [-6.9091e-02,  5.9672e-02, -2.7578e-01,  ..., -1.1278e-01,\n            4.3682e-01,  6.2062e-01],\n          [-9.8667e-01, -2.0822e-01, -8.2219e-01,  ..., -5.2644e-01,\n            4.7496e-03,  3.2606e-02],\n          ...,\n          [ 4.5672e-01, -4.9125e-01, -6.9132e-02,  ..., -1.4477e+00,\n           -4.7223e-01, -1.1643e+00],\n          [-7.7474e-01,  3.7714e-01, -4.0555e-01,  ..., -1.7452e+00,\n           -2.5435e-02, -1.4350e+00],\n          [-1.0900e+00, -8.7604e-02,  4.1120e-01,  ..., -1.5438e+00,\n           -8.9001e-02, -5.4630e-01]],\n\n         ...,\n\n         [[-1.5749e+00,  1.3224e+00, -3.0454e-02,  ...,  2.5278e-01,\n           -7.9000e-02, -1.3924e+00],\n          [ 2.7368e-01, -2.1123e+00,  4.3812e-02,  ..., -4.4320e-01,\n           -3.2820e-02,  1.7334e+00],\n          [ 7.6704e-02, -1.9063e+00, -4.0676e-02,  ..., -5.9954e-02,\n           -1.2760e-01,  1.6266e+00],\n          ...,\n          [ 9.5942e-03,  1.9971e+00, -1.1241e-01,  ...,  7.5145e-01,\n           -1.3433e-01, -1.7368e+00],\n          [ 1.2180e-01,  1.0137e+00, -4.4669e-02,  ...,  1.7098e-01,\n           -1.9576e-01, -7.3431e-01],\n          [ 1.3882e-01,  1.3880e+00,  1.4572e-02,  ...,  4.3748e-01,\n           -1.2079e-01, -9.7459e-01]],\n\n         [[ 3.1638e-02,  9.7654e-03, -1.0265e+00,  ..., -7.8843e-02,\n           -1.3873e+00, -1.5411e-02],\n          [-6.6809e-02, -8.1137e-02, -1.1363e-01,  ...,  3.1502e-02,\n            8.1901e-02,  1.4742e-02],\n          [-3.6228e-02, -6.0348e-02,  8.5568e-02,  ...,  3.4060e-04,\n            2.7133e-01, -1.7825e-03],\n          ...,\n          [ 5.6499e-02, -3.8381e-02,  5.2826e-01,  ..., -2.7431e-02,\n            3.8185e-01,  1.3161e-02],\n          [-1.4618e-02, -5.4714e-02,  6.6085e-01,  ..., -1.2643e-02,\n            1.0863e+00, -1.3860e-02],\n          [-7.3487e-02,  3.8575e-03,  4.9053e-01,  ..., -6.2473e-02,\n            5.4663e-01, -9.2331e-02]],\n\n         [[-5.9537e-01, -6.6907e-01,  5.1593e-01,  ...,  5.1499e-01,\n            5.6465e-01, -6.5848e-01],\n          [ 2.4681e-01,  3.5164e-01, -2.4640e-01,  ..., -2.8656e-01,\n           -3.1612e-01,  3.1321e-01],\n          [ 2.4632e-01,  3.5208e-01, -2.4562e-01,  ..., -2.8597e-01,\n           -3.1578e-01,  3.1323e-01],\n          ...,\n          [ 2.2146e-01,  3.2636e-01, -2.2147e-01,  ..., -2.6152e-01,\n           -2.9056e-01,  2.8742e-01],\n          [ 2.8214e-01,  3.9164e-01, -2.7913e-01,  ..., -3.2062e-01,\n           -3.5258e-01,  3.5215e-01],\n          [ 2.4780e-01,  3.5077e-01, -2.4854e-01,  ..., -2.8825e-01,\n           -3.1650e-01,  3.1239e-01]]],\n\n\n        ...,\n\n\n        [[[ 2.9581e-01,  3.2050e-01, -1.3447e-01,  ...,  4.9601e-01,\n            2.8275e+00,  3.9872e-01],\n          [-1.9020e-01,  2.3333e-01, -2.6079e-01,  ...,  5.6126e-01,\n           -1.1111e+00,  3.3444e-01],\n          [-6.6815e-02,  4.0765e-01, -8.2435e-02,  ...,  5.5457e-01,\n           -8.4060e-01,  4.1639e-01],\n          ...,\n          [-2.1022e-01,  1.0174e+00, -3.8182e-01,  ...,  4.7537e-01,\n            5.0013e-01,  4.0149e-01],\n          [-1.6791e-01,  8.1585e-01, -3.6328e-01,  ...,  5.2684e-01,\n            4.7895e-01,  4.3041e-01],\n          [-1.8116e-01,  9.0458e-01, -3.4553e-01,  ...,  5.2716e-01,\n            2.5612e-01,  3.9668e-01]],\n\n         [[-9.3824e-01,  1.0510e+00,  1.3259e+00,  ..., -8.1616e-01,\n            9.1631e-01,  7.4273e-01],\n          [-1.4915e-01,  1.2007e-01,  3.3672e-01,  ...,  4.3287e-02,\n            8.2264e-02, -2.0184e-02],\n          [-9.2477e-02,  5.9661e-02,  2.7479e-01,  ...,  1.0169e-01,\n            2.4451e-02, -7.6223e-02],\n          ...,\n          [-1.4722e-01,  1.2331e-01,  3.4221e-01,  ...,  4.3028e-02,\n            8.1814e-02, -2.2923e-02],\n          [-7.9460e-02,  4.6127e-02,  2.6108e-01,  ...,  1.1573e-01,\n            1.0998e-02, -8.8871e-02],\n          [-1.4301e-01,  1.1304e-01,  3.2927e-01,  ...,  5.0422e-02,\n            7.5674e-02, -2.6011e-02]],\n\n         [[ 1.2693e-01,  6.6797e-01,  4.2308e-01,  ..., -1.0232e+00,\n            5.2490e-01, -4.3912e-01],\n          [-2.7288e-02,  2.7311e-01,  2.4672e-01,  ..., -5.2555e-01,\n            3.3418e-01,  4.7170e-01],\n          [ 8.1671e-01,  3.1638e-01,  2.5237e-01,  ...,  3.6173e-01,\n            8.6395e-01,  2.2834e-02],\n          ...,\n          [-3.3518e-01,  2.0016e-01, -2.1720e-01,  ..., -2.1956e+00,\n            9.4179e-01, -1.9734e+00],\n          [-1.3202e+00,  1.7118e-01,  4.4919e-01,  ..., -1.7211e+00,\n            2.7052e-01, -8.3078e-01],\n          [-1.1067e+00,  1.5750e-01,  2.6374e-01,  ..., -1.5045e+00,\n           -2.9136e-01, -1.5173e-01]],\n\n         ...,\n\n         [[-1.5684e+00,  1.3081e+00, -2.7768e-02,  ...,  2.4693e-01,\n           -7.8581e-02, -1.3838e+00],\n          [ 1.7250e-01, -1.3172e+00,  1.3232e-01,  ..., -1.9898e-01,\n            8.1132e-03,  1.2101e+00],\n          [ 2.8725e-01, -1.3356e+00, -6.3208e-02,  ..., -3.1020e-01,\n           -1.3142e-01,  1.1007e+00],\n          ...,\n          [ 4.3905e-02,  2.4432e+00, -9.2206e-03,  ...,  5.9035e-01,\n           -9.1136e-02, -1.9599e+00],\n          [-1.2303e-01,  1.6120e+00,  3.1217e-02,  ...,  7.3616e-01,\n           -1.2170e-01, -1.2341e+00],\n          [ 1.4522e-01,  1.9511e+00, -5.0542e-02,  ...,  5.6937e-01,\n           -1.0739e-01, -1.5847e+00]],\n\n         [[ 3.3187e-02,  9.3951e-03, -1.0291e+00,  ..., -7.8523e-02,\n           -1.3908e+00, -1.3973e-02],\n          [-3.3376e-02, -1.8017e-02, -6.5280e-01,  ..., -4.1777e-02,\n           -1.0930e+00, -1.7487e-02],\n          [ 1.2712e-01, -1.6775e-01, -6.2354e-01,  ...,  9.1550e-02,\n           -1.0405e+00,  1.5661e-01],\n          ...,\n          [-7.3420e-02,  3.0377e-02, -2.3040e-01,  ..., -9.3292e-02,\n           -1.0712e+00, -1.4099e-01],\n          [ 5.2078e-03, -4.1449e-02,  5.4961e-01,  ..., -2.2124e-02,\n            7.2620e-01, -1.6656e-02],\n          [-3.7557e-02, -3.6115e-02,  3.6749e-01,  ..., -1.9589e-02,\n            4.6971e-01, -1.2639e-02]],\n\n         [[-5.9782e-01, -6.7185e-01,  5.1821e-01,  ...,  5.1735e-01,\n            5.6717e-01, -6.6118e-01],\n          [ 1.9410e-01,  2.9717e-01, -1.9513e-01,  ..., -2.3465e-01,\n           -2.6276e-01,  2.5869e-01],\n          [ 2.1193e-01,  3.1878e-01, -2.1190e-01,  ..., -2.5196e-01,\n           -2.8124e-01,  2.7889e-01],\n          ...,\n          [ 1.6379e-01,  2.5403e-01, -1.7052e-01,  ..., -2.0732e-01,\n           -2.2963e-01,  2.1891e-01],\n          [ 2.1418e-01,  3.1505e-01, -2.1580e-01,  ..., -2.5507e-01,\n           -2.8247e-01,  2.7734e-01],\n          [ 1.9555e-01,  2.9446e-01, -1.9787e-01,  ..., -2.3676e-01,\n           -2.6351e-01,  2.5756e-01]]],\n\n\n        [[[ 3.0795e-01,  3.1606e-01, -1.2943e-01,  ...,  4.9658e-01,\n            2.8804e+00,  3.9855e-01],\n          [-2.2073e-01,  3.4109e-01, -2.1018e-01,  ...,  6.1915e-01,\n           -9.3136e-01,  3.3690e-01],\n          [-1.4037e-01,  3.0120e-01, -1.6867e-01,  ...,  6.6202e-01,\n           -8.7638e-01,  2.9507e-01],\n          ...,\n          [-8.6369e-02,  7.0475e-01, -3.5664e-01,  ...,  5.2605e-01,\n           -1.4891e-01,  3.8805e-01],\n          [-2.2643e-01,  1.2062e+00, -3.2079e-01,  ...,  5.0916e-01,\n            5.9032e-01,  4.0155e-01],\n          [-2.3891e-01,  6.1170e-01, -5.2561e-01,  ...,  4.7674e-01,\n            4.7097e-01,  4.2614e-01]],\n\n         [[-9.5730e-01,  1.0735e+00,  1.3499e+00,  ..., -8.3690e-01,\n            9.3646e-01,  7.6116e-01],\n          [-1.0927e-01,  7.1400e-02,  2.8446e-01,  ...,  8.7541e-02,\n            3.9585e-02, -5.8454e-02],\n          [-8.9572e-02,  5.0857e-02,  2.6353e-01,  ...,  1.0764e-01,\n            1.9646e-02, -7.8004e-02],\n          ...,\n          [-8.4409e-02,  5.2216e-02,  2.6771e-01,  ...,  1.1009e-01,\n            1.6341e-02, -8.4151e-02],\n          [-7.5628e-02,  4.4121e-02,  2.5983e-01,  ...,  1.1849e-01,\n            7.8066e-03, -9.3114e-02],\n          [-1.6244e-01,  1.3797e-01,  3.5669e-01,  ...,  2.7541e-02,\n            9.7060e-02, -7.7808e-03]],\n\n         [[ 1.2429e-01,  6.8247e-01,  4.4521e-01,  ..., -1.0319e+00,\n            5.1900e-01, -4.3779e-01],\n          [-2.5788e-01, -2.9714e-02,  5.3176e-01,  ...,  2.6894e-01,\n           -9.9318e-02,  3.0407e-01],\n          [ 5.6988e-01, -1.7095e-01,  1.9874e-01,  ...,  9.0666e-01,\n            5.4625e-01,  1.6255e-01],\n          ...,\n          [-7.9340e-01,  5.3039e-01,  2.0420e-01,  ..., -2.0932e+00,\n           -1.8191e-01, -8.3476e-01],\n          [-1.5971e-01,  5.8973e-01,  6.4370e-01,  ..., -1.6222e+00,\n            2.9038e-01, -1.3442e+00],\n          [-2.7136e-01,  1.2321e-01,  3.9041e-02,  ..., -1.6237e+00,\n           -6.6776e-01, -5.6498e-01]],\n\n         ...,\n\n         [[-1.6214e+00,  1.3128e+00, -2.7047e-02,  ...,  2.6208e-01,\n           -7.7474e-02, -1.3951e+00],\n          [ 2.5539e-03, -1.7307e+00,  4.8092e-02,  ...,  1.6006e-01,\n           -1.5906e-03,  1.4714e+00],\n          [ 3.4080e-01, -1.8144e+00, -2.3169e-03,  ..., -1.8497e-01,\n           -3.5728e-02,  1.5088e+00],\n          ...,\n          [ 2.1589e-04,  1.6015e+00,  5.7050e-04,  ...,  6.9652e-01,\n           -7.2331e-02, -1.3471e+00],\n          [ 1.3948e-01,  2.6716e+00,  7.7245e-02,  ...,  5.7136e-01,\n           -2.5068e-02, -2.1646e+00],\n          [-7.9133e-01,  3.5400e+00,  1.6626e-01,  ...,  7.4987e-01,\n            2.2471e-02, -3.2003e+00]],\n\n         [[ 3.2057e-02,  1.0155e-02, -1.0601e+00,  ..., -7.9433e-02,\n           -1.4329e+00, -1.4339e-02],\n          [-4.4170e-02, -2.8090e-03, -9.1783e-02,  ..., -5.8269e-02,\n           -2.6280e-02, -1.8517e-02],\n          [ 2.8013e-02, -7.8139e-02, -4.6181e-01,  ...,  3.7508e-03,\n           -7.4396e-01,  5.9723e-02],\n          ...,\n          [ 7.2945e-03, -3.8033e-02,  5.2953e-01,  ..., -2.8125e-02,\n            5.0982e-01,  8.9186e-04],\n          [ 2.7141e-03, -8.7273e-02, -1.1462e-01,  ...,  2.1919e-02,\n           -8.3981e-01,  8.4916e-03],\n          [ 1.8076e-02, -5.1908e-03,  1.6348e-01,  ..., -6.9202e-02,\n           -1.9481e-01, -7.3533e-02]],\n\n         [[-6.1632e-01, -6.9462e-01,  5.3490e-01,  ...,  5.3495e-01,\n            5.8656e-01, -6.8269e-01],\n          [ 2.6974e-01,  3.8623e-01, -2.6439e-01,  ..., -3.0704e-01,\n           -3.4142e-01,  3.4429e-01],\n          [ 2.6526e-01,  3.8015e-01, -2.6097e-01,  ..., -3.0311e-01,\n           -3.3651e-01,  3.3841e-01],\n          ...,\n          [ 2.2416e-01,  3.2640e-01, -2.2512e-01,  ..., -2.6471e-01,\n           -2.9273e-01,  2.8831e-01],\n          [ 1.9262e-01,  2.9169e-01, -1.9559e-01,  ..., -2.3419e-01,\n           -2.6033e-01,  2.5419e-01],\n          [ 1.7198e-01,  2.6472e-01, -1.7853e-01,  ..., -2.1549e-01,\n           -2.3798e-01,  2.2811e-01]]],\n\n\n        [[[ 2.8675e-01,  3.0418e-01, -1.5187e-01,  ...,  4.9742e-01,\n            2.7940e+00,  3.9831e-01],\n          [-7.2333e-02,  5.4773e-02, -3.2805e-01,  ...,  5.2638e-01,\n           -1.3558e+00,  4.1343e-01],\n          [-1.3059e-01,  2.4173e-01, -2.7442e-01,  ...,  6.0208e-01,\n           -1.4832e+00,  3.5909e-01],\n          ...,\n          [-2.0020e-01,  5.4899e-01, -5.5910e-01,  ...,  4.9693e-01,\n            1.3892e-01,  4.4697e-01],\n          [-2.7020e-01,  7.6697e-01, -5.6111e-01,  ...,  5.2751e-01,\n            6.0964e-01,  4.3342e-01],\n          [-1.4129e-01,  6.5909e-01, -4.5632e-01,  ...,  4.3955e-01,\n           -9.8352e-02,  4.6360e-01]],\n\n         [[-9.2834e-01,  1.0395e+00,  1.3138e+00,  ..., -8.0544e-01,\n            9.0591e-01,  7.3313e-01],\n          [-1.5446e-01,  1.2541e-01,  3.4205e-01,  ...,  3.7951e-02,\n            8.7636e-02, -1.4860e-02],\n          [-7.1789e-02,  3.0725e-02,  2.4244e-01,  ...,  1.2692e-01,\n            9.8315e-04, -9.5218e-02],\n          ...,\n          [-7.9057e-02,  3.8560e-02,  2.5051e-01,  ...,  1.1963e-01,\n            8.3696e-03, -8.7994e-02],\n          [-1.4156e-01,  1.1374e-01,  3.3093e-01,  ...,  5.0300e-02,\n            7.5071e-02, -2.7994e-02],\n          [-1.2476e-01,  9.2499e-02,  3.0787e-01,  ...,  6.9313e-02,\n            5.6865e-02, -4.3978e-02]],\n\n         [[ 1.8262e-01,  6.8153e-01,  4.4553e-01,  ..., -9.8722e-01,\n            5.2111e-01, -4.6439e-01],\n          [ 3.9824e-01,  3.4120e-01, -3.1461e-01,  ..., -6.6433e-01,\n            3.2230e-01,  2.1075e-01],\n          [ 6.8796e-01,  2.8302e-01, -5.9740e-01,  ..., -2.8880e-01,\n            3.9087e-01,  3.6852e-01],\n          ...,\n          [ 1.7171e-01,  3.6389e-02,  1.7086e-01,  ..., -1.5483e+00,\n            1.2004e-01, -1.0513e+00],\n          [ 7.0366e-01, -2.3810e-01,  3.3345e-01,  ..., -9.6193e-01,\n            9.4487e-01, -5.7712e-01],\n          [-1.6101e-01, -3.0903e-01,  1.4120e-01,  ..., -1.2941e+00,\n            2.8574e-01, -7.0870e-01]],\n\n         ...,\n\n         [[-1.5602e+00,  1.3027e+00, -3.1953e-02,  ...,  2.5920e-01,\n           -7.7569e-02, -1.3832e+00],\n          [ 2.5288e-03, -1.8723e+00,  2.4743e-02,  ..., -1.8720e-01,\n           -1.2191e-01,  1.5653e+00],\n          [ 4.9310e-01, -2.2787e+00,  3.8412e-02,  ..., -1.7756e-01,\n           -3.6393e-02,  1.9887e+00],\n          ...,\n          [-1.0703e-01,  1.6344e+00,  1.2620e-02,  ...,  9.4904e-01,\n           -1.0541e-01, -1.2083e+00],\n          [ 6.8146e-02,  2.7764e+00,  3.3976e-02,  ...,  3.8028e-01,\n           -1.2113e-01, -1.9945e+00],\n          [-2.9048e-02,  1.3782e+00, -3.5577e-02,  ...,  7.6826e-01,\n           -6.8452e-02, -9.9647e-01]],\n\n         [[ 3.2893e-02,  7.3708e-03, -1.0195e+00,  ..., -7.6559e-02,\n           -1.3678e+00, -1.3594e-02],\n          [-5.8149e-02, -4.9329e-02,  2.5235e-01,  ..., -1.0762e-02,\n            5.8061e-01, -2.1769e-02],\n          [-1.7488e-02, -2.8774e-02,  2.0772e-01,  ..., -3.2724e-02,\n            8.1838e-01, -9.9345e-03],\n          ...,\n          [-2.1865e-03, -6.7275e-02,  5.8033e-01,  ...,  4.0102e-03,\n            5.3460e-01,  6.0054e-03],\n          [ 9.2958e-03, -3.2042e-02,  3.0628e-01,  ..., -3.9663e-02,\n            1.6520e-01, -1.4881e-02],\n          [ 1.9230e-02, -2.8161e-02,  8.4630e-01,  ..., -4.0447e-02,\n            1.2590e+00, -9.7114e-03]],\n\n         [[-5.8642e-01, -6.5843e-01,  5.0782e-01,  ...,  5.0648e-01,\n            5.5529e-01, -6.4822e-01],\n          [ 2.4016e-01,  3.4583e-01, -2.3932e-01,  ..., -2.7973e-01,\n           -3.0962e-01,  3.0706e-01],\n          [ 2.7766e-01,  3.9280e-01, -2.7291e-01,  ..., -3.1525e-01,\n           -3.4906e-01,  3.5125e-01],\n          ...,\n          [ 2.6127e-01,  3.7209e-01, -2.5826e-01,  ..., -2.9977e-01,\n           -3.3177e-01,  3.3180e-01],\n          [ 2.0292e-01,  3.0103e-01, -2.0640e-01,  ..., -2.4476e-01,\n           -2.7038e-01,  2.6365e-01],\n          [ 2.0958e-01,  3.1186e-01, -2.1149e-01,  ..., -2.5065e-01,\n           -2.7784e-01,  2.7313e-01]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-8.3572e-03,  8.7772e-03,  7.3313e-03,  ..., -9.0132e-04,\n           -9.2461e-04, -4.1441e-03],\n          [-3.2705e-02,  6.0896e-02,  2.0447e-01,  ...,  9.9325e-02,\n           -8.2438e-02, -1.5768e-01],\n          [-3.7453e-01, -2.9815e-01,  1.8704e-01,  ...,  9.1396e-02,\n           -1.2742e-01,  6.2683e-02],\n          ...,\n          [ 5.2696e-02, -3.3501e-02, -9.0531e-02,  ...,  3.3878e-01,\n           -2.5024e-02,  1.8779e-01],\n          [ 8.3320e-01, -3.0187e-02, -8.5914e-03,  ..., -3.9786e-02,\n           -2.6481e-01,  5.0709e-02],\n          [ 7.2010e-01, -3.6474e-01,  5.5638e-02,  ..., -5.9023e-01,\n            2.0216e-01, -7.5752e-01]],\n\n         [[-2.5614e-03,  5.6983e-03,  3.8289e-03,  ...,  7.4066e-03,\n            4.9901e-03,  3.4033e-03],\n          [ 5.2725e-02, -3.4381e-02, -8.1667e-02,  ..., -4.2640e-02,\n            3.0357e-02, -1.3893e-02],\n          [-2.4816e-02, -1.3788e-01, -1.2096e-01,  ...,  1.4693e-02,\n            1.3338e-02,  4.5384e-02],\n          ...,\n          [ 3.1684e-02, -1.5668e-02, -7.7177e-02,  ..., -2.4130e-02,\n            1.5914e-02,  7.1821e-02],\n          [ 7.6404e-02,  1.0909e-01, -4.9908e-03,  ..., -6.4482e-02,\n            1.0782e-02, -1.4264e-02],\n          [ 3.4907e-03,  8.1872e-02,  7.5988e-02,  ..., -8.9065e-03,\n           -2.4649e-02,  6.6554e-04]],\n\n         [[-8.6377e-03, -1.6607e-02, -1.0488e-02,  ...,  7.0547e-04,\n            2.3829e-02,  2.0419e-03],\n          [-3.7217e-02,  7.5796e-01, -1.8851e-01,  ..., -4.4664e-01,\n           -5.9630e-01,  4.7847e-01],\n          [-1.0312e-01,  1.2741e-01,  1.5063e-01,  ..., -5.1857e-01,\n           -1.5890e-01,  2.1275e-01],\n          ...,\n          [-7.2437e-02,  8.4400e-02, -1.2914e-01,  ...,  9.3800e-02,\n           -7.7629e-02,  5.5676e-02],\n          [ 9.2386e-02,  4.6691e-01, -2.8913e-01,  ...,  7.4606e-01,\n           -6.2098e-01, -9.5627e-01],\n          [ 2.3599e-03, -2.7322e-01,  8.0503e-01,  ...,  6.1301e-01,\n            3.9875e-01, -2.6252e-01]],\n\n         ...,\n\n         [[ 1.9470e-02, -1.7901e-02,  5.5854e-03,  ...,  9.0509e-03,\n            9.8247e-03,  2.3858e-02],\n          [ 3.3798e-01, -2.0990e-01, -4.8863e-02,  ...,  3.8561e-01,\n            2.6165e-01, -3.1955e-01],\n          [ 4.4996e-01,  3.4841e-01, -4.9094e-01,  ...,  3.6431e-01,\n           -1.4942e-01,  3.0505e-01],\n          ...,\n          [-5.4820e-01, -2.5729e-01,  3.4461e-02,  ...,  5.4094e-01,\n            4.1578e-01,  5.6215e-01],\n          [ 4.5094e-01,  7.8649e-02,  1.2808e-01,  ..., -2.2998e-02,\n            2.9352e-01,  1.5922e-02],\n          [ 1.4907e-01,  7.8503e-01, -2.2226e-01,  ..., -4.4803e-01,\n            2.4384e-01, -4.0006e-01]],\n\n         [[-6.7835e-03,  2.2898e-02,  1.8126e-02,  ...,  1.0658e-02,\n            1.0150e-02, -2.6584e-03],\n          [-1.4312e-01,  2.1996e-01, -2.9765e-02,  ...,  2.4008e-02,\n            1.4928e-01, -9.6744e-02],\n          [ 4.9968e-02, -8.1927e-01,  2.6899e-01,  ..., -6.6493e-02,\n           -7.1908e-02, -4.8520e-02],\n          ...,\n          [-1.9395e-01,  5.1242e-02, -2.6954e-02,  ..., -2.0179e-01,\n            6.7920e-03, -7.1082e-04],\n          [-7.5649e-02,  4.5451e-01,  3.2656e-02,  ..., -1.2944e-01,\n            8.0874e-03,  5.3690e-02],\n          [-9.4080e-02,  8.3082e-02, -1.1519e-01,  ..., -1.4785e-01,\n            7.4344e-02, -6.0062e-02]],\n\n         [[ 7.2397e-03, -1.7192e-03, -1.7627e-03,  ...,  1.1686e-02,\n           -7.4305e-03, -1.2544e-03],\n          [-3.5998e-02,  6.7684e-02, -8.5316e-02,  ..., -8.8413e-02,\n           -1.0602e-01, -2.8265e-02],\n          [-1.2794e-02,  7.7713e-02, -2.6814e-02,  ..., -3.3411e-02,\n            3.4991e-02,  1.2011e-01],\n          ...,\n          [-2.3939e-02,  2.7018e-02,  9.2516e-02,  ..., -2.1054e-02,\n            8.3742e-02, -2.6468e-02],\n          [ 1.4362e-02, -1.2781e-03,  4.8512e-02,  ..., -5.2361e-02,\n            1.5851e-01,  9.0462e-02],\n          [ 8.3549e-02, -1.5441e-02, -2.4522e-02,  ...,  2.5422e-04,\n            9.3856e-02,  2.1095e-02]]],\n\n\n        [[[-1.2490e-02,  5.4856e-03,  2.6433e-03,  ...,  5.0811e-03,\n           -2.1909e-03, -1.5894e-03],\n          [ 3.6051e-01, -1.1335e-01, -8.6707e-02,  ...,  1.2071e-01,\n           -2.2319e-01, -3.3644e-01],\n          [-8.2214e-02, -4.3147e-01,  9.1400e-02,  ..., -5.7579e-02,\n            4.4088e-01, -3.7472e-01],\n          ...,\n          [ 1.7441e-01,  1.1296e-01, -1.6861e-01,  ...,  3.6129e-01,\n            2.7292e-01, -2.9402e-01],\n          [-3.8483e-01, -1.2674e-02,  2.3340e-02,  ...,  5.6084e-01,\n            2.5744e-01, -8.5951e-03],\n          [-6.0695e-02, -1.5604e-01, -1.1530e-01,  ...,  3.5598e-01,\n           -1.5612e-01, -1.2145e-01]],\n\n         [[-5.9828e-03,  4.7778e-03,  5.3642e-04,  ...,  1.0065e-02,\n            1.7313e-03,  3.5952e-03],\n          [ 4.3207e-02,  5.5371e-02,  7.3715e-03,  ...,  7.8924e-03,\n            8.7449e-02, -4.1875e-02],\n          [ 4.3429e-02,  1.3584e-02,  3.0572e-02,  ..., -1.1084e-02,\n            4.7452e-03,  6.8089e-02],\n          ...,\n          [ 7.8792e-02,  8.7002e-03,  2.1353e-03,  ...,  3.4506e-02,\n           -1.9020e-01, -1.2100e-01],\n          [ 3.2632e-02,  4.5242e-02,  2.9817e-02,  ...,  3.8203e-02,\n           -1.8261e-01, -4.1211e-02],\n          [-4.6642e-02,  1.2770e-01,  4.9524e-02,  ...,  1.1091e-01,\n           -8.3363e-02, -3.9269e-02]],\n\n         [[-4.8820e-03, -1.8801e-02, -4.7739e-03,  ...,  8.6303e-03,\n            2.5093e-02, -9.9822e-03],\n          [ 4.1471e-02,  1.6923e-01,  2.2291e-01,  ...,  5.9682e-01,\n            2.5756e-01, -5.2336e-01],\n          [ 8.0376e-02,  1.8437e-01,  1.7915e-01,  ...,  9.8134e-01,\n            3.7580e-01,  3.7255e-01],\n          ...,\n          [-4.0766e-03, -1.6574e-01,  7.4200e-01,  ...,  8.0427e-02,\n           -2.0233e-01, -2.8939e-01],\n          [-2.0674e-02,  3.1430e-01, -6.8161e-02,  ...,  5.2214e-01,\n           -5.4361e-01, -4.6134e-01],\n          [-1.0294e-01, -1.6778e-01, -2.3542e-01,  ...,  2.5327e-01,\n           -5.4999e-02, -5.5170e-01]],\n\n         ...,\n\n         [[ 6.6297e-03, -2.6737e-02,  8.1985e-03,  ...,  9.9429e-03,\n            1.5309e-02,  1.5521e-02],\n          [-3.7281e-01,  3.0957e-01, -1.0739e-01,  ..., -1.5309e-01,\n            4.4118e-01, -4.2091e-02],\n          [ 1.9935e-01, -2.4557e-01, -2.2244e-01,  ..., -1.1133e-01,\n           -2.9327e-01,  9.8165e-02],\n          ...,\n          [-2.6535e-01,  2.1584e-01, -2.4108e-01,  ..., -3.2291e-01,\n            1.8492e-01, -4.3896e-01],\n          [ 1.2955e-01, -5.4856e-01,  1.1763e-01,  ...,  1.9472e-02,\n            6.2695e-01,  7.4149e-02],\n          [ 1.8369e-01, -3.6943e-01,  1.9003e-02,  ...,  7.2221e-02,\n            6.0894e-01, -5.6103e-02]],\n\n         [[-7.8871e-03,  2.6313e-02,  1.5367e-02,  ...,  1.1099e-02,\n            9.6509e-03, -2.9980e-03],\n          [ 1.3760e-01, -4.7961e-02, -8.7473e-02,  ..., -8.9546e-02,\n           -1.1195e-01,  1.3700e-01],\n          [ 1.4143e-01, -2.1272e-01,  5.6227e-02,  ...,  3.0823e-01,\n            5.9016e-02,  2.9476e-03],\n          ...,\n          [ 6.5387e-02,  1.4215e-01, -2.4101e-02,  ...,  1.6269e-01,\n            2.1460e-01,  1.2601e-02],\n          [-1.0142e-01, -5.2034e-01, -2.6931e-02,  ...,  1.9441e-01,\n           -2.2545e-05, -9.1704e-02],\n          [-4.8188e-02, -1.1164e+00,  4.5174e-02,  ...,  2.4334e-03,\n            1.0575e-01,  1.6943e-01]],\n\n         [[ 7.7663e-03, -4.4315e-03,  6.8776e-04,  ...,  1.2915e-02,\n           -8.3839e-03, -4.7893e-03],\n          [ 3.0626e-02,  3.2825e-02,  1.1196e-02,  ...,  1.5653e-04,\n           -1.4904e-02,  3.5014e-02],\n          [ 9.7210e-02,  1.5665e-02, -6.7940e-02,  ..., -5.2923e-02,\n            1.4341e-02, -2.2503e-02],\n          ...,\n          [-2.2812e-02, -5.7419e-02,  4.1284e-03,  ..., -2.6905e-02,\n            2.0257e-02,  1.5550e-02],\n          [ 4.4570e-02, -8.8500e-03, -6.5192e-02,  ...,  1.0833e-01,\n            5.7484e-03,  5.6224e-02],\n          [ 1.0561e-01, -1.7498e-02, -8.5291e-02,  ...,  5.7393e-02,\n            5.2543e-02, -8.1876e-02]]],\n\n\n        [[[-1.1478e-02,  4.2140e-03,  1.6084e-02,  ...,  1.4063e-03,\n            1.7063e-03, -3.9521e-03],\n          [-2.7321e-01, -1.8497e-01,  4.6058e-01,  ..., -8.8768e-02,\n            3.6806e-02, -6.1015e-02],\n          [-6.4171e-02,  1.1133e-01, -1.6393e-01,  ...,  1.0886e-01,\n           -1.3203e-01, -3.9966e-01],\n          ...,\n          [-2.5874e-02, -2.0474e-01,  3.5093e-01,  ..., -1.8042e-01,\n            2.2946e-01,  8.0790e-02],\n          [ 5.6661e-01,  1.9284e-01,  3.7906e-01,  ...,  8.4686e-02,\n           -1.3690e-01, -2.5709e-02],\n          [ 2.0695e-01,  4.5746e-02,  9.1620e-02,  ..., -1.9087e-01,\n           -2.3083e-02, -1.8377e-02]],\n\n         [[-5.6902e-03,  3.6833e-03,  2.5753e-03,  ...,  8.2455e-03,\n            1.5260e-03,  5.3077e-03],\n          [ 1.0025e-01, -7.2659e-02, -1.0388e-01,  ..., -4.1184e-02,\n           -8.8245e-03,  2.3281e-02],\n          [ 8.1617e-02, -3.7093e-02, -5.7784e-02,  ..., -3.4616e-02,\n            2.1241e-02,  6.2764e-02],\n          ...,\n          [-3.3775e-02,  4.3956e-02,  4.2431e-02,  ...,  2.1749e-02,\n            9.2388e-02, -5.7772e-02],\n          [ 1.4986e-01, -1.1430e-01, -5.9939e-02,  ..., -1.1785e-01,\n            6.0296e-02, -6.6856e-03],\n          [-3.6045e-02, -5.7884e-02,  5.3192e-02,  ..., -5.6608e-02,\n            1.2596e-01, -1.9563e-02]],\n\n         [[-5.4496e-03, -2.2185e-02, -6.8321e-03,  ...,  1.8011e-02,\n            2.6766e-02, -4.2901e-03],\n          [-1.2948e-01, -4.6546e-01,  5.5769e-01,  ..., -2.1823e-02,\n            1.3804e-01, -4.3079e-02],\n          [-3.6574e-01, -8.3152e-02, -2.0374e-01,  ...,  3.9150e-01,\n           -8.5803e-02, -1.2546e-01],\n          ...,\n          [ 3.1576e-01,  5.4307e-01, -7.2988e-01,  ...,  9.7210e-01,\n           -6.4938e-01, -4.7595e-01],\n          [-3.7887e-01, -8.2276e-01, -1.9503e-01,  ...,  7.3396e-01,\n            5.3495e-01, -8.6034e-01],\n          [-1.6134e-01, -4.4516e-01,  5.0783e-02,  ...,  7.5172e-01,\n            7.4334e-01, -4.8832e-01]],\n\n         ...,\n\n         [[ 6.9946e-03, -2.7006e-03,  2.0142e-03,  ...,  7.8185e-03,\n            1.8486e-02,  4.7279e-03],\n          [-4.6481e-01,  2.4716e-01,  3.4136e-02,  ...,  1.5461e-01,\n            3.3158e-01,  1.3004e-01],\n          [ 1.8731e-01, -3.8182e-01, -4.9148e-03,  ..., -4.3864e-03,\n            2.3508e-01,  1.3862e-01],\n          ...,\n          [ 4.4954e-01, -1.3496e-01,  4.3891e-01,  ...,  1.2120e-01,\n            1.1600e-01, -1.4273e-01],\n          [-3.4730e-02, -4.5033e-01,  2.1852e-01,  ...,  1.5204e-01,\n           -5.2920e-02, -1.9099e-01],\n          [-7.8598e-02, -6.6093e-01, -3.0121e-01,  ..., -2.8868e-01,\n           -1.4237e-01, -6.7493e-01]],\n\n         [[-7.8352e-03,  1.9240e-02,  1.5146e-02,  ...,  1.7016e-02,\n            1.1613e-02, -5.6165e-03],\n          [ 7.0236e-04,  2.0298e-01, -8.0109e-02,  ..., -8.0854e-02,\n            2.2976e-02, -2.6789e-02],\n          [-1.2170e-01, -2.4635e-01,  2.7468e-02,  ...,  2.4159e-01,\n            1.4168e-01,  4.4644e-02],\n          ...,\n          [-2.5359e-02, -4.8030e-01,  7.3209e-02,  ..., -1.6438e-01,\n           -3.3524e-02, -5.9793e-02],\n          [-3.1381e-02,  4.7170e-01,  1.4822e-01,  ...,  9.5593e-02,\n            1.1303e-02,  1.3741e-01],\n          [-1.6623e-01, -1.5687e-01,  9.5577e-02,  ...,  5.0319e-02,\n            6.5746e-02,  1.7876e-01]],\n\n         [[ 8.2587e-03, -5.1981e-03,  1.0439e-03,  ...,  1.2722e-02,\n           -8.8819e-03, -2.4979e-03],\n          [-1.8509e-02, -3.2465e-02,  1.5242e-02,  ..., -8.9919e-02,\n            5.2100e-03,  5.2116e-02],\n          [-2.4596e-02,  2.1846e-02, -6.7451e-02,  ..., -1.1337e-01,\n           -1.1957e-01,  1.8605e-02],\n          ...,\n          [ 7.7692e-02,  2.1979e-02, -5.2922e-03,  ..., -3.2809e-02,\n           -1.0230e-01, -4.3187e-03],\n          [-9.8085e-02,  4.8899e-02, -6.8251e-02,  ..., -8.3869e-02,\n           -1.0130e-01, -1.6087e-02],\n          [-8.1504e-02, -1.8900e-03, -5.2652e-03,  ..., -4.1044e-02,\n           -3.6494e-02, -3.5584e-03]]],\n\n\n        ...,\n\n\n        [[[-2.7844e-03,  1.2712e-03,  3.9653e-03,  ..., -2.2131e-03,\n            1.8079e-03,  1.0931e-02],\n          [ 4.5800e-02, -9.7327e-02, -2.7871e-01,  ..., -7.5050e-02,\n            1.0759e-02, -8.8927e-02],\n          [-3.1598e-02,  2.0352e-01, -4.1221e-02,  ...,  1.0751e-01,\n           -1.8861e-01, -2.7944e-01],\n          ...,\n          [ 7.8579e-01,  3.8940e-01,  1.9867e-02,  ..., -3.0869e-01,\n            1.0976e-01,  1.8173e-01],\n          [ 1.0545e-01, -1.7637e-02,  7.1109e-02,  ...,  1.0732e-01,\n            1.2426e-01, -1.4458e-01],\n          [-4.4668e-01,  3.9476e-02,  1.1966e-01,  ...,  7.3663e-02,\n            1.2834e-01,  1.6979e-01]],\n\n         [[-9.4980e-03,  9.5068e-04, -2.2029e-03,  ...,  1.4284e-02,\n            5.9646e-04,  6.3744e-03],\n          [ 3.3584e-02, -4.5946e-02, -8.1592e-02,  ...,  7.5466e-04,\n            8.6020e-02,  2.9419e-02],\n          [-1.0318e-01, -4.6562e-02, -5.4649e-02,  ...,  1.1449e-01,\n            2.4811e-01,  3.3481e-02],\n          ...,\n          [ 7.4818e-02,  4.4499e-02,  1.4815e-02,  ..., -4.9799e-02,\n           -1.4072e-01, -3.1613e-02],\n          [ 1.6548e-02,  6.3486e-03,  8.3354e-02,  ...,  3.7766e-02,\n           -6.2979e-02, -3.0187e-02],\n          [ 9.7723e-02,  4.9222e-02,  3.5661e-02,  ...,  5.7191e-02,\n           -2.0508e-02, -8.4039e-02]],\n\n         [[ 6.2456e-03, -1.0213e-02, -3.1820e-02,  ...,  1.1007e-02,\n            3.1030e-02,  6.3586e-04],\n          [ 7.0158e-02,  4.3515e-01, -1.1601e-01,  ...,  3.7445e-01,\n           -7.6046e-02,  3.2795e-01],\n          [-2.0930e-01, -3.3559e-01, -2.3839e-01,  ...,  2.3345e-01,\n           -2.1658e-01,  1.3624e-01],\n          ...,\n          [ 3.0379e-01, -2.1552e-01,  7.4903e-01,  ...,  5.5296e-01,\n            3.9837e-01, -2.6925e-01],\n          [ 2.9523e-02, -4.0345e-01,  7.5790e-01,  ...,  3.4472e-01,\n            3.4358e-01, -1.7737e-01],\n          [-1.7293e-01,  2.5151e-02,  6.3670e-01,  ...,  2.9531e-01,\n            5.9623e-01,  1.2362e-02]],\n\n         ...,\n\n         [[ 1.4807e-02, -3.2165e-02,  3.0283e-03,  ...,  1.3263e-02,\n            2.1439e-02,  1.1783e-02],\n          [-4.5393e-01,  1.2328e-01, -4.3325e-01,  ..., -2.5172e-01,\n            2.3418e-01,  1.0841e-01],\n          [ 6.3867e-02, -1.4822e-02, -2.3760e-01,  ..., -1.2848e-01,\n            1.5800e-01, -1.9397e-01],\n          ...,\n          [-2.7759e-01,  3.5540e-01, -3.7412e-01,  ...,  9.9240e-02,\n            5.7786e-01, -2.5444e-01],\n          [-9.0381e-02,  5.8477e-02, -4.1125e-01,  ..., -1.9737e-01,\n           -4.9507e-02,  1.5000e-01],\n          [-1.0431e-01, -8.2084e-01,  3.9064e-01,  ...,  1.0150e-01,\n            1.0919e-01, -1.2496e-01]],\n\n         [[-1.0491e-02,  2.2584e-02,  1.1200e-02,  ...,  1.9683e-02,\n            1.0351e-02, -5.4991e-03],\n          [ 6.3771e-02,  1.2113e+00, -2.5047e-02,  ...,  7.4779e-02,\n           -9.1334e-02, -7.8086e-02],\n          [ 1.3180e-01,  3.6365e-01,  7.5743e-02,  ...,  1.7018e-01,\n            9.2484e-02, -9.1366e-02],\n          ...,\n          [ 1.8985e-01,  5.6394e-01,  6.9536e-02,  ..., -3.9092e-02,\n            4.4623e-02,  8.2250e-02],\n          [ 2.3579e-01, -1.7253e-01,  3.2576e-02,  ...,  7.8520e-02,\n            5.6449e-02, -1.7279e-02],\n          [ 6.0650e-02, -2.1210e-02, -7.2723e-02,  ...,  1.8832e-01,\n            2.3227e-03, -6.1076e-02]],\n\n         [[ 1.0102e-02, -2.7769e-03,  1.3947e-03,  ...,  1.5689e-02,\n           -9.9842e-03, -3.8806e-03],\n          [-8.8567e-02, -6.4064e-03,  2.4736e-02,  ...,  6.4501e-02,\n           -9.4306e-02, -4.7508e-02],\n          [-4.5687e-02,  3.5451e-02,  8.9112e-02,  ...,  8.9320e-02,\n           -4.8448e-02, -9.7075e-02],\n          ...,\n          [-1.0791e-01, -4.1692e-02,  4.7802e-05,  ..., -7.1118e-02,\n            2.5641e-02, -1.0802e-02],\n          [ 9.8131e-03, -3.3091e-02,  5.0300e-03,  ...,  3.6235e-02,\n           -6.5224e-02, -1.5391e-02],\n          [-5.5952e-02, -9.1286e-03, -9.4308e-02,  ...,  3.4120e-02,\n           -1.3334e-01, -2.1067e-02]]],\n\n\n        [[[-1.7779e-02, -1.7576e-03,  3.7820e-04,  ..., -3.1869e-03,\n            2.6342e-03,  1.4757e-02],\n          [-1.6696e-01,  3.9880e-02, -2.9631e-02,  ...,  8.1066e-03,\n            1.5095e-02, -9.4224e-02],\n          [-1.7416e-01,  8.8079e-02, -3.3149e-02,  ...,  3.6138e-01,\n           -1.6289e-01, -1.8423e-01],\n          ...,\n          [ 1.1739e-02, -2.5675e-01, -9.0760e-02,  ..., -4.4762e-02,\n            5.3337e-01,  1.4731e-01],\n          [-1.5099e-02,  1.5181e-02,  2.4588e-01,  ..., -5.1440e-01,\n            3.9022e-01,  1.5428e-01],\n          [ 1.6641e-01,  1.8669e-01,  1.6441e-01,  ..., -2.8699e-01,\n           -8.5755e-02, -6.6323e-02]],\n\n         [[-8.4915e-03,  1.2889e-03, -3.6595e-03,  ...,  1.5555e-02,\n           -1.4797e-03,  4.8673e-03],\n          [-4.1883e-02, -1.2509e-03, -1.2050e-01,  ...,  1.2059e-01,\n            2.3063e-02, -9.7548e-02],\n          [-7.6836e-02, -1.0315e-02, -4.2172e-02,  ...,  1.0141e-01,\n            4.8792e-02, -4.1112e-02],\n          ...,\n          [ 3.1148e-02,  8.8284e-02,  4.8492e-02,  ...,  4.1800e-02,\n           -6.1777e-02, -5.3625e-02],\n          [-1.2907e-01, -1.0740e-02, -4.2314e-02,  ...,  1.3334e-01,\n           -6.3923e-02, -5.1305e-02],\n          [-7.1306e-02,  8.1377e-02,  1.0030e-01,  ...,  9.5871e-04,\n            2.6272e-02, -6.0776e-02]],\n\n         [[-5.1497e-04, -1.1658e-02, -3.1820e-03,  ...,  1.4546e-02,\n            3.2118e-02, -3.2306e-03],\n          [-1.4764e-01, -3.4818e-01,  3.4221e-02,  ...,  1.4438e-01,\n            8.2880e-02,  3.0085e-01],\n          [-2.8448e-01, -1.6994e-01, -2.3056e-01,  ..., -3.2604e-01,\n           -3.0981e-01,  1.2340e-01],\n          ...,\n          [-1.0682e-01, -3.6834e-01,  7.2816e-02,  ...,  1.1117e-01,\n            3.6483e-01, -1.2343e-01],\n          [-2.1382e-02, -2.1181e-01,  2.3587e-01,  ..., -3.2526e-01,\n            5.0248e-01, -2.1765e-01],\n          [-1.4716e-01, -2.4524e-01,  1.4821e-01,  ..., -8.2869e-02,\n            3.5369e-01, -4.2641e-01]],\n\n         ...,\n\n         [[ 2.0996e-02, -2.2252e-02,  1.3282e-02,  ...,  2.1456e-02,\n            1.2398e-02,  3.0451e-02],\n          [-1.5283e-01,  1.3545e-02, -1.9215e-01,  ..., -1.6769e-01,\n           -1.3526e-01,  1.9924e-01],\n          [ 3.3962e-02, -1.0488e-01, -4.2496e-01,  ..., -1.3283e-02,\n            1.2244e-01,  2.4402e-01],\n          ...,\n          [ 1.2951e-03, -2.7061e-01, -2.0607e-01,  ..., -5.4190e-02,\n            1.3428e-01,  1.7408e-02],\n          [ 1.7165e-01, -8.3207e-01,  8.4392e-02,  ..., -1.2467e-01,\n            2.0864e-01, -2.3596e-01],\n          [ 8.5001e-01,  2.8523e-01,  1.9031e-01,  ...,  2.8324e-01,\n            1.1445e-01, -1.0923e-01]],\n\n         [[-1.0063e-02,  3.6864e-02,  1.1215e-02,  ...,  1.9012e-02,\n            7.5709e-03,  5.5775e-04],\n          [-2.3646e-02, -5.5436e-01,  5.6985e-02,  ...,  2.0215e-02,\n            2.1428e-02,  5.6225e-02],\n          [ 6.2345e-02, -5.4281e-01,  8.4368e-02,  ..., -2.9449e-02,\n            4.7119e-02,  1.0700e-02],\n          ...,\n          [-4.3240e-02,  8.9012e-02,  9.0177e-02,  ..., -9.5389e-03,\n            1.7331e-01,  5.0403e-02],\n          [-6.4329e-02,  1.8040e-01,  3.4788e-03,  ...,  1.4955e-01,\n           -1.8959e-02,  2.4546e-02],\n          [-8.5158e-02,  7.1183e-02, -9.4420e-02,  ..., -2.4908e-01,\n            1.8572e-02, -1.4767e-02]],\n\n         [[ 1.0783e-02, -3.1059e-05, -4.0899e-03,  ...,  1.3964e-02,\n           -1.1293e-02, -3.2754e-03],\n          [-3.3282e-02,  7.6668e-02, -7.3698e-02,  ...,  3.8194e-02,\n           -2.8151e-02, -4.4303e-02],\n          [-4.4520e-02,  7.2298e-02, -1.0252e-01,  ...,  7.8505e-02,\n           -1.1569e-01, -1.6798e-03],\n          ...,\n          [ 9.2948e-02,  1.9453e-02, -6.5521e-02,  ...,  5.9502e-02,\n            1.0406e-03, -1.4633e-03],\n          [ 1.4248e-02, -5.7475e-02,  6.4961e-02,  ...,  1.2969e-01,\n           -8.3730e-03, -2.3405e-02],\n          [ 3.0736e-02, -4.3260e-02,  2.2133e-03,  ...,  7.3579e-02,\n           -7.2725e-03, -3.1251e-03]]],\n\n\n        [[[ 5.5073e-03,  1.2701e-02,  1.3628e-02,  ..., -1.3232e-03,\n           -3.4898e-03, -2.4135e-03],\n          [ 4.3087e-01, -1.4547e-01, -6.2482e-03,  ...,  1.6854e-01,\n           -1.7049e-01, -1.4197e-01],\n          [ 2.3894e-01, -1.3880e-01, -2.8842e-02,  ..., -3.2284e-02,\n           -4.7938e-02,  1.7765e-02],\n          ...,\n          [ 5.0237e-01, -5.1689e-02,  3.1669e-02,  ..., -8.2405e-02,\n            3.0153e-01,  7.4332e-03],\n          [ 1.2040e+00,  8.8264e-02, -1.8086e-01,  ...,  2.9841e-01,\n           -2.2751e-01,  2.0509e-01],\n          [ 1.5770e-01, -4.0683e-01, -5.5277e-01,  ...,  1.5169e-01,\n           -1.1309e-01, -1.3057e-01]],\n\n         [[-2.6354e-04,  4.9050e-03,  3.6843e-03,  ...,  2.7455e-03,\n            6.2177e-03,  1.4939e-03],\n          [ 9.0244e-02, -1.3394e-01, -1.3044e-01,  ..., -1.1857e-01,\n            1.1653e-01, -5.7802e-03],\n          [ 1.4790e-02, -1.4402e-01, -1.0358e-01,  ..., -1.1143e-01,\n            1.6968e-01,  3.0003e-02],\n          ...,\n          [ 1.3124e-01, -1.1379e-03,  4.9588e-02,  ..., -1.8035e-01,\n            1.1183e-01, -2.3794e-02],\n          [ 1.2331e-01,  2.0043e-02,  1.1696e-02,  ..., -9.5788e-02,\n            1.1407e-02, -2.8335e-02],\n          [ 1.3699e-01,  4.8848e-02,  2.3074e-04,  ..., -7.1736e-02,\n            6.9768e-02, -1.1891e-01]],\n\n         [[-1.1398e-02, -1.5713e-02, -1.1012e-02,  ...,  1.8858e-03,\n            2.0373e-02, -3.8493e-03],\n          [-3.7197e-01,  3.0692e-01, -3.3003e-01,  ..., -2.6507e-01,\n           -1.0768e-01, -4.5778e-01],\n          [-5.2225e-01,  3.7883e-01, -4.3078e-01,  ...,  1.0244e-01,\n            1.0604e-01, -2.0794e-01],\n          ...,\n          [-2.7237e-03,  1.6899e-01, -2.0002e-01,  ...,  1.0757e+00,\n            3.5637e-01,  9.8644e-02],\n          [ 1.3944e-01, -9.2345e-02, -2.4628e-01,  ...,  8.6220e-01,\n            1.1772e-01,  2.8879e-01],\n          [-2.3582e-01,  9.6812e-02, -3.8383e-01,  ...,  5.8307e-01,\n            1.0916e-01,  5.0390e-01]],\n\n         ...,\n\n         [[ 1.9452e-02, -2.1652e-02, -6.3928e-03,  ...,  9.0625e-03,\n            1.1480e-02,  8.6802e-03],\n          [ 5.5239e-02, -4.7092e-02, -3.5189e-01,  ...,  2.2414e-01,\n           -1.2623e-03, -4.9783e-02],\n          [-3.5179e-01, -1.7987e-01, -7.9333e-03,  ..., -8.5854e-02,\n           -6.0467e-02,  2.9949e-01],\n          ...,\n          [ 9.4977e-04, -2.2161e-01,  5.7333e-01,  ..., -4.4085e-02,\n            3.4864e-01, -3.0815e-02],\n          [-4.6587e-01, -1.1613e+00,  7.0900e-01,  ..., -9.7523e-02,\n            1.6470e-01, -3.4225e-01],\n          [-3.4473e-01, -2.3912e-01,  1.5353e-01,  ...,  2.7043e-01,\n            3.0980e-01,  1.8237e-01]],\n\n         [[-2.6979e-03,  2.1014e-02,  1.5833e-02,  ...,  7.1024e-03,\n            9.0220e-03, -6.5634e-03],\n          [ 1.8327e-01, -6.1511e-01,  1.1325e-01,  ...,  1.2651e-01,\n           -2.6083e-03, -6.7659e-02],\n          [ 3.5352e-02, -3.7466e-01,  1.3400e-01,  ..., -8.8418e-03,\n           -1.5387e-01, -4.8754e-02],\n          ...,\n          [-1.5378e-02, -6.2611e-02,  1.6139e-01,  ...,  9.9023e-02,\n           -7.3435e-02,  4.1976e-02],\n          [-9.4348e-03,  2.4605e-01,  1.3114e-01,  ..., -1.4254e-02,\n           -8.4942e-02,  1.1460e-01],\n          [-2.4177e-02, -1.6079e-01,  1.2261e-01,  ...,  8.5383e-02,\n           -7.8386e-02,  1.4053e-01]],\n\n         [[ 5.7278e-03, -5.0452e-04, -9.4092e-04,  ...,  1.1174e-02,\n           -8.5051e-03, -2.2561e-03],\n          [ 2.2479e-02,  1.1367e-01,  5.6312e-02,  ..., -1.7478e-01,\n           -1.3485e-02, -3.0077e-02],\n          [ 7.6479e-03,  1.1865e-01, -7.4325e-03,  ..., -1.8199e-01,\n           -7.8261e-02, -4.3288e-02],\n          ...,\n          [-7.3896e-02,  5.0268e-02,  6.6179e-03,  ..., -6.6987e-02,\n            2.8923e-02, -3.3891e-03],\n          [-6.4767e-02, -1.8210e-02, -3.3039e-02,  ..., -1.8461e-02,\n            8.6039e-02, -2.6281e-02],\n          [-4.0677e-02,  1.5081e-02,  9.1487e-03,  ..., -6.4359e-02,\n            6.2208e-02, -1.3460e-01]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-3.3599e-01, -7.0936e-01,  3.3078e-01,  ..., -1.6289e+00,\n           -5.1449e-01,  3.8020e-01],\n          [-7.8448e-01, -3.7674e-01,  2.2572e-01,  ..., -5.4392e-03,\n           -5.1063e-01,  1.1118e-01],\n          [-7.6527e-01, -3.8504e-01,  2.2386e-01,  ...,  8.6619e-02,\n           -4.7556e-01,  1.0325e-01],\n          ...,\n          [-3.1278e-01, -4.4585e-01,  2.6715e-01,  ..., -7.9220e-01,\n           -4.8294e-01,  1.4020e-01],\n          [-1.9391e-01, -5.2579e-01,  3.0015e-01,  ..., -1.6987e+00,\n           -4.6519e-01,  2.2135e-01],\n          [-2.9705e-01, -4.4284e-01,  2.6890e-01,  ..., -1.0849e+00,\n           -4.9698e-01,  1.5212e-01]],\n\n         [[-2.8281e-01, -7.5192e-01,  5.7363e-01,  ..., -1.6826e-01,\n           -6.3332e-01, -7.8236e-02],\n          [-3.5839e-01, -7.2092e-01,  6.6777e-01,  ..., -8.4099e-02,\n           -4.1735e-01, -1.2703e-01],\n          [-4.9068e-01, -6.2406e-01,  5.0688e-01,  ...,  1.9808e-02,\n           -1.0579e+00, -1.1247e-01],\n          ...,\n          [-6.3646e-01, -1.6727e+00, -2.4072e-01,  ..., -6.5867e-02,\n            1.9280e+00,  6.4439e-02],\n          [-1.0268e+00, -8.8246e-01, -4.2464e-01,  ...,  1.3174e-02,\n            9.4071e-01, -3.9054e-01],\n          [-8.3206e-01,  8.6790e-03, -6.3119e-01,  ..., -4.7073e-02,\n            8.0145e-01, -1.3451e-01]],\n\n         [[-8.0352e-02,  3.4045e-01, -3.4254e-01,  ...,  2.0948e-01,\n           -3.7584e-02, -2.5826e-01],\n          [-2.5593e-01, -1.4053e+00, -4.6984e-01,  ...,  7.9811e-01,\n            4.5498e-01, -2.2852e-01],\n          [-1.7208e-01, -4.9344e-01, -3.5251e-01,  ...,  4.8865e-01,\n            3.6296e-01, -2.9383e-01],\n          ...,\n          [-2.4776e-01,  7.7732e-01, -3.2593e-01,  ...,  4.3981e-01,\n           -1.0630e-01, -6.1679e-01],\n          [-3.7557e-01, -9.3821e-01, -5.6508e-01,  ...,  8.5797e-01,\n           -3.3641e-02, -5.5922e-01],\n          [-4.3202e-01, -3.2573e-01, -4.9969e-01,  ...,  7.7670e-01,\n            2.9327e-01, -3.3434e-01]],\n\n         ...,\n\n         [[-1.6681e-02,  1.7863e-01,  5.3308e-01,  ..., -5.3833e-01,\n           -1.2712e-01, -2.7022e-01],\n          [-6.5818e-02,  1.7335e-01, -7.7499e-02,  ...,  7.1989e-01,\n           -1.8552e-01, -2.2458e-01],\n          [-2.1091e-02,  1.8460e-01, -4.0495e-02,  ...,  4.0886e-01,\n           -1.2306e-01, -2.7481e-01],\n          ...,\n          [-7.5856e-02,  2.2051e-01, -5.8691e-02,  ...,  2.4247e-01,\n           -1.6241e-01, -2.3786e-01],\n          [-4.7674e-03,  2.0908e-01, -4.8368e-02,  ...,  3.6157e-01,\n           -1.2273e-01, -2.6087e-01],\n          [-3.2362e-02,  1.5076e-01,  1.7709e-01,  ..., -6.9992e-02,\n           -7.4783e-02, -2.9906e-01]],\n\n         [[-2.4879e-01,  9.3863e-01, -1.4741e-01,  ..., -5.8402e-01,\n            4.5283e-01, -1.7370e-01],\n          [ 9.9977e-03,  1.1101e-01, -1.5965e-01,  ...,  3.1750e-01,\n           -2.8254e-01,  7.2464e-01],\n          [ 1.1416e-01, -6.7043e-01, -7.5966e-02,  ...,  2.6582e-01,\n           -4.7081e-03,  5.8999e-01],\n          ...,\n          [ 7.9571e-02, -1.1488e-02, -7.5977e-02,  ...,  2.7329e-01,\n           -1.6177e+00,  4.0380e-01],\n          [-6.8099e-02,  1.0928e+00, -4.7279e-02,  ...,  7.4222e-01,\n           -2.3427e+00,  2.2664e-01],\n          [-5.4765e-02, -2.6440e-01, -1.0177e-01,  ...,  3.4976e-01,\n           -8.4801e-01, -1.9221e-01]],\n\n         [[ 6.8482e-02,  1.1439e+00, -1.0569e+00,  ..., -9.2623e-01,\n           -5.3461e-02, -5.2838e-02],\n          [ 1.8648e-01,  8.5939e-01, -1.9612e-01,  ...,  9.5801e-01,\n           -1.1233e-02, -4.0428e-01],\n          [-3.3214e-01,  8.2114e-01, -5.9324e-02,  ...,  4.5127e-01,\n           -5.6140e-02,  1.5673e-01],\n          ...,\n          [-5.9758e-02, -1.0052e+00,  1.4896e+00,  ..., -2.1499e+00,\n            5.1493e-02,  4.7864e-01],\n          [ 3.5775e-01, -2.6200e+00,  1.1163e-01,  ..., -1.2542e+00,\n           -3.1122e-02,  1.8041e+00],\n          [ 5.5487e-01, -2.2051e+00,  5.0413e-01,  ..., -1.0236e+00,\n           -2.5512e-02,  5.1023e-01]]],\n\n\n        [[[-3.3538e-01, -7.0940e-01,  3.3084e-01,  ..., -1.6270e+00,\n           -5.1443e-01,  3.8016e-01],\n          [-6.5255e-01, -4.8294e-01,  2.6434e-01,  ..., -6.0631e-01,\n           -4.5903e-01,  1.9966e-01],\n          [-7.4065e-01, -3.9418e-01,  2.2771e-01,  ..., -1.0641e-01,\n           -4.7925e-01,  1.2782e-01],\n          ...,\n          [-4.1731e-01, -3.8338e-01,  2.3503e-01,  ..., -6.3492e-01,\n           -4.7823e-01,  9.1956e-02],\n          [-3.6701e-01, -4.2045e-01,  2.6127e-01,  ..., -6.7116e-01,\n           -4.8888e-01,  1.2377e-01],\n          [-3.9024e-01, -4.2388e-01,  2.5775e-01,  ..., -8.6054e-01,\n           -4.7647e-01,  1.3700e-01]],\n\n         [[-2.7734e-01, -7.3494e-01,  5.6093e-01,  ..., -1.6836e-01,\n           -6.5178e-01, -7.4897e-02],\n          [-7.3231e-01, -4.9178e-01,  7.9753e-01,  ...,  6.2374e-02,\n           -1.7400e+00, -2.5698e-01],\n          [-3.9371e-01,  1.8972e-01,  4.4036e-01,  ..., -2.6275e-01,\n           -9.2381e-01, -9.1532e-02],\n          ...,\n          [-2.5722e-01,  4.4814e-03, -6.8641e-01,  ..., -1.1829e-01,\n            5.8391e-02,  5.9517e-03],\n          [-2.7258e-01,  1.7680e-01, -8.0259e-01,  ..., -8.5623e-02,\n            2.4409e-01,  1.7619e-02],\n          [-4.4615e-01, -2.2972e-01, -3.1434e-01,  ..., -9.3660e-02,\n            7.1855e-01, -4.2795e-02]],\n\n         [[-7.8438e-02,  3.3809e-01, -3.3910e-01,  ...,  2.0389e-01,\n           -3.3651e-02, -2.6169e-01],\n          [-1.8076e-01, -1.3806e+00, -3.9701e-01,  ...,  2.6793e-01,\n            2.0749e-01, -5.7353e-01],\n          [-3.0642e-01, -1.1650e+00, -3.6356e-01,  ...,  8.6851e-01,\n            5.3353e-01, -1.6086e-01],\n          ...,\n          [-3.0069e-01, -3.7279e-01, -4.7367e-01,  ...,  1.0100e+00,\n            1.4335e-01, -9.7009e-01],\n          [-4.2426e-01, -4.0834e-01, -4.1901e-01,  ...,  7.5170e-01,\n            1.1282e-01, -5.3888e-01],\n          [-4.4932e-01, -5.4090e-01, -4.6652e-01,  ...,  9.3079e-01,\n            1.1704e-02, -5.3992e-01]],\n\n         ...,\n\n         [[-1.6691e-02,  1.7799e-01,  5.3611e-01,  ..., -5.4356e-01,\n           -1.2698e-01, -2.7071e-01],\n          [-2.2290e-02,  1.7681e-01, -6.2470e-02,  ...,  5.9366e-01,\n           -1.6374e-01, -2.3283e-01],\n          [ 1.4762e-03,  1.7085e-01,  2.8186e-04,  ...,  4.7963e-01,\n           -1.4369e-01, -2.5967e-01],\n          ...,\n          [-3.3857e-02,  2.1367e-01,  1.8645e-01,  ...,  1.6885e-01,\n           -1.2056e-01, -2.6059e-01],\n          [-3.7092e-02,  2.1569e-01,  6.1509e-02,  ...,  1.9629e-01,\n           -1.1143e-01, -2.6641e-01],\n          [-2.5051e-02,  1.9242e-01,  1.1147e-01,  ...,  2.5242e-01,\n           -1.1360e-01, -2.7158e-01]],\n\n         [[-2.4829e-01,  9.2694e-01, -1.4604e-01,  ..., -5.9366e-01,\n            4.6730e-01, -1.6951e-01],\n          [-4.9666e-02,  8.9602e-01, -1.2829e-01,  ...,  6.0669e-01,\n           -2.0345e-01,  3.8467e-02],\n          [-4.0626e-01, -8.8300e-01, -3.7352e-01,  ..., -6.5441e-01,\n           -1.4456e-01,  1.1782e+00],\n          ...,\n          [-7.7181e-02, -1.5686e+00,  2.5048e-02,  ..., -3.7463e-01,\n           -1.6411e+00,  9.1743e-01],\n          [ 1.3953e-01, -1.3659e+00,  1.9364e-03,  ..., -1.4199e-01,\n           -1.5880e+00,  7.8119e-01],\n          [ 3.0179e-03, -9.0922e-01, -3.8057e-02,  ..., -4.5099e-01,\n           -2.3111e+00,  7.2143e-01]],\n\n         [[ 7.0228e-02,  1.1460e+00, -1.0573e+00,  ..., -9.2917e-01,\n           -5.5470e-02, -5.8228e-02],\n          [ 2.8086e-01,  9.4791e-02, -4.0980e-01,  ...,  7.2262e-01,\n           -6.5447e-02,  6.7666e-01],\n          [ 4.3865e-01,  2.4457e-01,  3.7961e-01,  ...,  5.1771e-01,\n           -1.4463e-01, -2.2549e-01],\n          ...,\n          [ 5.9854e-01, -1.9820e+00,  8.8852e-01,  ..., -1.7947e+00,\n           -1.3579e-01,  3.2918e-01],\n          [ 4.4857e-01, -2.6521e+00,  7.7849e-01,  ..., -1.7353e+00,\n           -6.9003e-02,  5.3214e-01],\n          [ 4.6570e-01, -2.2562e+00,  8.5496e-01,  ..., -1.6912e+00,\n           -1.3410e-01,  4.7747e-01]]],\n\n\n        [[[-3.3540e-01, -7.0965e-01,  3.3087e-01,  ..., -1.6266e+00,\n           -5.1452e-01,  3.8030e-01],\n          [-7.4329e-01, -4.1904e-01,  2.4401e-01,  ...,  5.4157e-02,\n           -4.9777e-01,  1.3465e-01],\n          [-7.9333e-01, -3.7999e-01,  2.2715e-01,  ...,  9.1273e-04,\n           -5.1688e-01,  1.0459e-01],\n          ...,\n          [-3.8765e-01, -3.8587e-01,  2.4896e-01,  ..., -8.1230e-01,\n           -4.9309e-01,  1.1044e-01],\n          [-4.4363e-01, -4.1147e-01,  2.5218e-01,  ..., -5.8948e-01,\n           -5.0427e-01,  1.2532e-01],\n          [-4.2438e-01, -4.0620e-01,  2.5138e-01,  ..., -5.0802e-01,\n           -4.8425e-01,  1.1678e-01]],\n\n         [[-2.8343e-01, -7.3641e-01,  5.6967e-01,  ..., -1.6950e-01,\n           -6.4954e-01, -7.1809e-02],\n          [-5.4182e-01,  3.5597e-02,  4.6747e-01,  ..., -2.3098e-02,\n           -1.7088e+00, -5.1981e-02],\n          [-4.7892e-01, -1.1409e-01,  7.5264e-01,  ..., -4.3621e-02,\n           -1.4036e+00, -1.1598e-01],\n          ...,\n          [-6.1072e-01, -5.2523e-01, -7.1586e-01,  ..., -4.3341e-02,\n            1.5429e+00, -2.0037e-01],\n          [-5.9135e-01, -1.5447e-01, -3.8437e-01,  ..., -9.6698e-02,\n            2.0899e-01, -8.0734e-02],\n          [-7.1626e-01, -2.5473e-01, -4.4927e-01,  ..., -1.2433e-01,\n           -1.8826e-01, -6.9461e-02]],\n\n         [[-8.1272e-02,  3.4724e-01, -3.4119e-01,  ...,  2.1300e-01,\n           -2.9174e-02, -2.5562e-01],\n          [-1.0577e-01, -9.6097e-01, -2.4269e-01,  ..., -3.0056e-01,\n            1.1548e-01, -7.4431e-01],\n          [-1.1530e-01, -1.1572e+00, -3.1652e-01,  ...,  3.8613e-01,\n            3.6821e-01, -9.9576e-01],\n          ...,\n          [-5.7410e-01, -6.1361e-01, -5.5489e-01,  ...,  8.4124e-01,\n            1.4408e-01, -1.8091e-01],\n          [-4.2421e-01, -1.1681e+00, -4.4280e-01,  ...,  7.2205e-01,\n            4.2657e-02, -4.9198e-01],\n          [-4.8192e-01, -8.5377e-01, -3.3839e-01,  ...,  8.3920e-01,\n            1.0249e-01, -6.8722e-01]],\n\n         ...,\n\n         [[-1.7414e-02,  1.7824e-01,  5.3546e-01,  ..., -5.4308e-01,\n           -1.2649e-01, -2.7065e-01],\n          [-1.2341e-01,  2.0360e-01,  7.3580e-02,  ...,  3.8183e-01,\n           -1.5650e-01, -2.1929e-01],\n          [-1.1954e-01,  1.9849e-01,  1.1050e-01,  ...,  4.9319e-01,\n           -1.7564e-01, -2.1455e-01],\n          ...,\n          [-6.1597e-02,  2.2897e-01,  1.3520e-01,  ...,  1.6055e-01,\n           -1.6603e-01, -2.1975e-01],\n          [-1.4691e-01,  2.2003e-01,  2.0275e-01,  ...,  1.5981e-01,\n           -1.9524e-01, -1.8937e-01],\n          [-1.7538e-01,  1.8604e-01,  2.3431e-01,  ..., -3.8936e-02,\n           -1.5467e-01, -2.2450e-01]],\n\n         [[-2.4849e-01,  9.3672e-01, -1.4794e-01,  ..., -5.9237e-01,\n            4.8129e-01, -1.7297e-01],\n          [ 1.9690e-02, -1.1507e+00, -8.9197e-02,  ..., -1.8364e-01,\n            5.5903e-01,  7.1915e-01],\n          [-2.1738e-02,  3.2709e-02, -9.3404e-02,  ..., -2.3411e-01,\n           -3.9492e-01,  5.6584e-01],\n          ...,\n          [-2.1670e-01,  9.6394e-02, -1.2173e-01,  ..., -1.0167e-01,\n           -1.9634e+00,  9.6864e-01],\n          [-1.3788e-01, -8.0740e-01,  1.4830e-02,  ..., -4.9006e-01,\n           -9.1750e-01,  9.8422e-01],\n          [-4.4146e-02, -1.2700e+00, -2.9127e-02,  ..., -4.2576e-01,\n           -1.0871e+00,  9.3140e-01]],\n\n         [[ 6.9934e-02,  1.1487e+00, -1.0507e+00,  ..., -9.2682e-01,\n           -5.6569e-02, -7.4680e-02],\n          [ 5.2256e-02,  4.3784e-01, -4.6725e-01,  ...,  5.0862e-01,\n           -5.4948e-02, -9.1174e-03],\n          [-7.8570e-02,  2.7912e-01, -2.4931e-01,  ...,  5.3124e-01,\n           -9.7385e-03, -1.4741e-01],\n          ...,\n          [ 4.4583e-01, -1.7876e+00,  7.3948e-01,  ..., -9.4302e-01,\n           -4.3292e-03,  3.9752e-01],\n          [ 4.9326e-01, -1.7504e+00,  2.6139e-01,  ..., -1.0910e+00,\n           -9.0898e-02,  4.7996e-01],\n          [ 4.2287e-01, -1.8041e+00,  4.4170e-01,  ..., -1.5178e+00,\n            3.2934e-02,  3.4232e-01]]],\n\n\n        ...,\n\n\n        [[[-3.3564e-01, -7.1062e-01,  3.3123e-01,  ..., -1.6302e+00,\n           -5.1468e-01,  3.8127e-01],\n          [-8.0544e-01, -3.6467e-01,  2.1599e-01,  ...,  1.8547e-01,\n           -4.6762e-01,  9.2307e-02],\n          [-7.9751e-01, -3.5964e-01,  2.1148e-01,  ...,  1.1920e-01,\n           -4.6751e-01,  8.0111e-02],\n          ...,\n          [-3.0487e-01, -4.3997e-01,  2.6145e-01,  ..., -8.7060e-01,\n           -5.0057e-01,  1.3614e-01],\n          [-3.9532e-01, -3.8604e-01,  2.4018e-01,  ..., -6.1403e-01,\n           -4.9317e-01,  9.0683e-02],\n          [-3.6613e-01, -4.3257e-01,  2.6793e-01,  ..., -5.6864e-01,\n           -4.9444e-01,  1.3720e-01]],\n\n         [[-2.7949e-01, -7.2486e-01,  5.7029e-01,  ..., -1.6711e-01,\n           -6.7801e-01, -7.6275e-02],\n          [-5.6254e-02, -5.6301e-01,  8.0715e-01,  ..., -6.8711e-02,\n           -1.4176e+00,  1.8334e-02],\n          [-1.9781e-01, -1.1145e+00,  5.8955e-01,  ...,  1.6110e-02,\n           -1.5245e-01, -1.0332e-01],\n          ...,\n          [-3.1483e-01, -2.2145e-01, -3.8331e-01,  ...,  1.5273e-02,\n            7.7811e-01, -8.5025e-02],\n          [-3.8174e-02, -2.9729e-01, -6.9921e-01,  ..., -1.2207e-02,\n            1.7885e-01, -8.7178e-02],\n          [-3.9727e-01, -1.0582e-01, -8.7666e-01,  ...,  7.4417e-03,\n           -2.7724e-01, -3.3930e-02]],\n\n         [[-7.9514e-02,  3.3458e-01, -3.4221e-01,  ...,  2.1794e-01,\n           -2.8685e-02, -2.5329e-01],\n          [-1.7476e-01, -3.1335e-01, -2.7697e-01,  ...,  6.4215e-01,\n            4.8379e-01, -4.7131e-01],\n          [-2.3602e-01, -1.4450e-01, -3.7528e-01,  ...,  4.1724e-01,\n            3.3809e-01, -4.4415e-01],\n          ...,\n          [-3.5885e-01,  1.0915e+00, -4.2364e-01,  ...,  6.0979e-01,\n           -2.9796e-02, -1.0474e+00],\n          [-4.1450e-01, -5.3775e-01, -4.5187e-01,  ...,  7.5175e-01,\n            3.9779e-02, -1.0298e+00],\n          [-4.5229e-01, -6.9128e-01, -4.6500e-01,  ...,  6.6823e-01,\n            7.0640e-02, -8.1981e-01]],\n\n         ...,\n\n         [[-1.7120e-02,  1.7818e-01,  5.3937e-01,  ..., -5.4720e-01,\n           -1.2814e-01, -2.6989e-01],\n          [-8.5892e-02,  1.8089e-01,  1.5953e-01,  ...,  2.0326e-01,\n           -1.8263e-01, -2.2467e-01],\n          [-7.3361e-02,  1.8505e-01,  7.5797e-02,  ...,  2.0842e-01,\n           -1.8569e-01, -2.1438e-01],\n          ...,\n          [-6.2608e-02,  1.8446e-01,  1.1101e-01,  ...,  4.6576e-02,\n           -1.2228e-01, -2.5632e-01],\n          [-8.6771e-02,  2.0145e-01,  1.3824e-01,  ...,  1.1655e-01,\n           -1.2272e-01, -2.5401e-01],\n          [-2.2603e-01,  2.1729e-01,  1.5771e-01,  ...,  1.1282e-01,\n           -1.5007e-01, -2.1469e-01]],\n\n         [[-2.4906e-01,  9.3003e-01, -1.4629e-01,  ..., -6.0998e-01,\n            4.8094e-01, -1.7646e-01],\n          [-8.9945e-02, -3.2515e-01, -1.1894e-01,  ..., -3.7521e-01,\n            5.7358e-01,  2.1933e-02],\n          [-3.0251e-02, -2.0226e-01, -6.2898e-02,  ..., -5.4146e-02,\n           -9.3821e-02,  2.9026e-01],\n          ...,\n          [-6.1719e-02, -8.6475e-01,  5.4645e-02,  ..., -8.7481e-01,\n           -1.5008e+00,  9.5665e-01],\n          [ 3.3366e-03, -1.4754e+00,  3.7254e-02,  ..., -7.0204e-01,\n           -1.4499e+00,  1.1168e+00],\n          [-8.6652e-03, -1.3244e+00, -1.0664e-02,  ..., -3.8506e-01,\n           -1.0987e+00,  8.8470e-01]],\n\n         [[ 6.3235e-02,  1.1564e+00, -1.0679e+00,  ..., -9.2375e-01,\n           -5.8569e-02, -6.9057e-02],\n          [-9.9926e-02, -2.2687e-01, -3.0173e-01,  ...,  7.0227e-01,\n            4.0767e-02,  2.5715e-02],\n          [-7.6814e-02,  1.4429e+00, -3.2664e-01,  ...,  1.0909e-01,\n           -1.2302e-01,  3.6537e-02],\n          ...,\n          [ 1.4351e-01, -1.2816e+00,  8.8317e-01,  ..., -2.3298e+00,\n           -2.0456e-01,  3.7937e-01],\n          [ 3.5639e-01, -1.5213e+00,  8.4560e-01,  ..., -2.0518e+00,\n           -1.9041e-01, -5.9970e-02],\n          [ 2.6060e-01, -1.9512e+00,  7.4093e-01,  ..., -1.7282e+00,\n           -8.4838e-02,  2.4239e-01]]],\n\n\n        [[[-3.2883e-01, -7.1813e-01,  3.3344e-01,  ..., -1.6553e+00,\n           -5.1544e-01,  3.8778e-01],\n          [-7.4351e-01, -3.8353e-01,  2.3636e-01,  ...,  9.8261e-02,\n           -4.8882e-01,  1.0679e-01],\n          [-7.9016e-01, -3.7059e-01,  2.2926e-01,  ...,  1.0167e-01,\n           -4.8703e-01,  9.9382e-02],\n          ...,\n          [-4.2250e-01, -3.9495e-01,  2.4751e-01,  ..., -6.8785e-01,\n           -4.9872e-01,  1.0925e-01],\n          [-2.9433e-01, -4.1487e-01,  2.5165e-01,  ..., -8.3233e-01,\n           -4.7720e-01,  1.0971e-01],\n          [-2.4571e-01, -4.9936e-01,  2.8455e-01,  ..., -1.5523e+00,\n           -4.7452e-01,  1.9319e-01]],\n\n         [[-2.7991e-01, -7.4309e-01,  5.8022e-01,  ..., -1.7028e-01,\n           -6.8396e-01, -7.2338e-02],\n          [-1.7740e-01, -5.9440e-01,  4.7987e-01,  ...,  2.0402e-02,\n           -9.6638e-01, -1.9705e-01],\n          [-4.0573e-01, -9.3956e-01,  5.8296e-01,  ...,  7.4839e-02,\n            1.9997e-01, -2.8904e-01],\n          ...,\n          [-3.5708e-01, -9.0602e-01,  1.2526e-01,  ...,  4.1884e-02,\n           -5.3902e-01, -1.9831e-01],\n          [-3.9555e-01, -1.1343e+00, -2.2399e-01,  ...,  3.7929e-03,\n            3.6015e-01, -1.3031e-01],\n          [-9.1834e-01, -6.6592e-01, -4.2162e-01,  ..., -8.7530e-02,\n            2.2639e-01, -2.2339e-01]],\n\n         [[-7.1743e-02,  3.6109e-01, -3.3908e-01,  ...,  2.0218e-01,\n           -3.5459e-02, -2.5505e-01],\n          [-3.0053e-01, -1.0024e+00, -3.0947e-01,  ...,  5.9963e-01,\n            3.7183e-01, -4.2955e-01],\n          [-1.9040e-01, -5.0160e-01, -2.6607e-01,  ...,  5.5653e-01,\n            3.7399e-01, -9.0747e-02],\n          ...,\n          [-4.7315e-01, -1.2076e+00, -5.1042e-01,  ...,  7.5869e-01,\n            2.3837e-02, -6.6447e-01],\n          [-4.0536e-01, -1.0681e-01, -4.8328e-01,  ...,  3.5730e-01,\n           -7.2632e-02, -9.1289e-01],\n          [-5.1143e-01, -5.2894e-01, -5.7502e-01,  ...,  7.3225e-01,\n            1.5346e-01, -4.4762e-01]],\n\n         ...,\n\n         [[-1.7980e-02,  1.7873e-01,  5.5295e-01,  ..., -5.6292e-01,\n           -1.2773e-01, -2.6988e-01],\n          [-1.5059e-01,  1.6099e-01,  1.5984e-01,  ...,  2.4770e-01,\n           -1.7766e-01, -2.2269e-01],\n          [-3.1719e-02,  1.9583e-01, -2.6557e-02,  ...,  3.9875e-01,\n           -1.8491e-01, -2.2351e-01],\n          ...,\n          [-1.7583e-01,  2.1865e-01,  2.0141e-01,  ...,  1.1405e-01,\n           -1.9672e-01, -1.9263e-01],\n          [-3.8261e-02,  2.4476e-01, -1.0103e-01,  ...,  2.7131e-01,\n           -1.2633e-01, -2.4706e-01],\n          [ 4.3970e-02,  1.7048e-01, -1.8195e-02,  ...,  2.6044e-01,\n           -4.1380e-02, -3.1953e-01]],\n\n         [[-2.5544e-01,  9.3379e-01, -1.4672e-01,  ..., -6.2015e-01,\n            4.8334e-01, -1.7964e-01],\n          [-1.2097e-02, -6.7119e-01, -7.1754e-02,  ...,  1.2549e-02,\n           -3.1342e-02,  2.7255e-01],\n          [-2.3465e-02, -1.3968e-01, -1.6056e-01,  ...,  2.5529e-01,\n           -2.9679e-01,  2.3342e-01],\n          ...,\n          [-7.5018e-02, -8.8849e-01, -2.8273e-02,  ..., -3.9721e-01,\n           -1.2862e+00,  5.4935e-01],\n          [ 6.1236e-02, -1.2279e+00, -2.4586e-02,  ..., -2.9313e-01,\n           -2.0166e+00,  6.3622e-01],\n          [-1.3715e-01,  4.4181e-01, -1.9240e-01,  ...,  3.0606e-01,\n           -2.0750e+00,  3.8067e-01]],\n\n         [[ 7.1375e-02,  1.1848e+00, -1.0745e+00,  ..., -9.4099e-01,\n           -5.4292e-02, -8.9384e-02],\n          [-4.3286e-03, -2.8553e-01,  3.4739e-01,  ...,  6.9601e-01,\n           -1.4082e-02, -7.2917e-02],\n          [ 7.9510e-02,  8.3527e-01,  5.3327e-01,  ...,  6.6216e-01,\n           -8.9915e-02, -2.2841e-02],\n          ...,\n          [ 5.9858e-01, -1.6740e+00,  7.3694e-01,  ..., -1.4906e+00,\n           -1.8425e-01,  2.3700e-01],\n          [ 7.2888e-01, -2.7118e+00,  9.6349e-01,  ..., -2.4010e+00,\n           -1.1813e-01,  8.0558e-01],\n          [ 5.6856e-01, -1.9043e+00,  4.7888e-01,  ..., -1.3614e+00,\n           -7.8687e-02,  7.0432e-01]]],\n\n\n        [[[-3.3671e-01, -7.0586e-01,  3.3023e-01,  ..., -1.6189e+00,\n           -5.1446e-01,  3.7798e-01],\n          [-7.9210e-01, -3.8418e-01,  2.3094e-01,  ..., -6.8405e-03,\n           -4.8259e-01,  1.1736e-01],\n          [-7.9776e-01, -3.7414e-01,  2.2941e-01,  ...,  1.4845e-01,\n           -4.9125e-01,  1.0416e-01],\n          ...,\n          [-4.0866e-01, -3.6328e-01,  2.3874e-01,  ..., -6.5309e-01,\n           -4.7744e-01,  8.6097e-02],\n          [-4.5189e-01, -3.5585e-01,  2.3265e-01,  ..., -5.8745e-01,\n           -4.8532e-01,  7.8311e-02],\n          [-4.7644e-01, -3.5905e-01,  2.3749e-01,  ..., -5.3402e-01,\n           -4.9042e-01,  8.6117e-02]],\n\n         [[-2.8407e-01, -7.3628e-01,  5.5823e-01,  ..., -1.6894e-01,\n           -5.9030e-01, -8.0759e-02],\n          [-5.5385e-01, -1.3421e-01,  4.1811e-01,  ..., -5.9789e-02,\n           -1.2808e+00, -1.4636e-01],\n          [-3.1889e-01, -2.1153e-01,  3.8215e-01,  ..., -3.7046e-02,\n           -7.0077e-01,  1.0667e-02],\n          ...,\n          [-3.5892e-01, -7.0021e-01, -5.7706e-01,  ..., -3.3438e-02,\n            8.9156e-01, -1.5446e-01],\n          [-2.7642e-01, -4.5254e-01, -2.4108e-01,  ..., -3.9820e-02,\n            1.1114e+00, -1.6109e-01],\n          [-5.3237e-01, -1.0244e+00, -1.9564e-01,  ..., -2.4128e-02,\n            7.0958e-01, -1.6105e-01]],\n\n         [[-8.3314e-02,  3.4022e-01, -3.4568e-01,  ...,  2.2133e-01,\n           -3.6990e-02, -2.5584e-01],\n          [-2.2765e-01, -1.0345e+00, -4.6050e-01,  ...,  7.9423e-01,\n            1.7250e-01, -1.3526e-01],\n          [-2.2996e-01, -6.7964e-01, -3.4410e-01,  ...,  9.4474e-01,\n            4.9372e-01,  2.2857e-02],\n          ...,\n          [-5.1911e-01, -8.2201e-01, -5.4002e-01,  ...,  1.0898e+00,\n            8.1868e-02, -5.4628e-01],\n          [-4.8063e-01, -3.0658e-01, -4.9616e-01,  ...,  8.7809e-01,\n           -3.4349e-02, -2.1597e-01],\n          [-3.9490e-01, -8.4743e-01, -4.1281e-01,  ...,  9.5370e-01,\n           -6.9520e-02, -1.5570e-01]],\n\n         ...,\n\n         [[-1.6417e-02,  1.7867e-01,  5.2964e-01,  ..., -5.2727e-01,\n           -1.2731e-01, -2.7011e-01],\n          [-1.1704e-01,  2.0414e-01,  1.4434e-01,  ...,  3.7144e-01,\n           -2.2741e-01, -1.7932e-01],\n          [-1.1822e-01,  1.9250e-01,  1.6323e-01,  ...,  2.6847e-01,\n           -1.9900e-01, -2.0873e-01],\n          ...,\n          [-1.3219e-01,  1.9326e-01,  1.5656e-01,  ...,  1.4834e-01,\n           -1.6454e-01, -2.1836e-01],\n          [-1.5209e-01,  2.0105e-01,  1.5143e-01,  ...,  4.7786e-02,\n           -1.9204e-01, -2.0096e-01],\n          [-1.6689e-01,  1.6991e-01,  2.2768e-01,  ...,  1.1693e-02,\n           -1.9723e-01, -2.0892e-01]],\n\n         [[-2.5551e-01,  9.5848e-01, -1.4941e-01,  ..., -5.8899e-01,\n            4.3275e-01, -1.7957e-01],\n          [-1.9442e-01,  1.2531e-01, -1.4656e-01,  ..., -4.5036e-01,\n            4.0668e-01,  3.2587e-01],\n          [-9.7402e-02, -1.6977e-01, -2.3191e-01,  ..., -1.7665e-01,\n            6.0082e-01,  4.0905e-01],\n          ...,\n          [-6.4866e-02,  4.7701e-02, -1.1669e-01,  ..., -5.4227e-02,\n           -1.6270e+00,  4.6289e-01],\n          [ 8.7316e-02, -1.5742e-01, -8.1197e-02,  ..., -1.6476e-01,\n           -1.0369e+00, -1.3564e-01],\n          [-8.2756e-03, -2.3915e-01, -1.0883e-01,  ...,  6.2416e-02,\n           -1.3180e+00,  2.0559e-01]],\n\n         [[ 8.8523e-02,  1.0992e+00, -1.0236e+00,  ..., -9.1310e-01,\n           -5.8396e-02, -5.6075e-02],\n          [-2.2068e-01,  4.5820e-01, -1.1005e-01,  ...,  9.4872e-01,\n           -9.8724e-02, -2.4860e-01],\n          [-2.7261e-01,  1.8799e-02, -1.1881e-01,  ...,  9.5004e-01,\n            3.6051e-02, -7.5842e-02],\n          ...,\n          [-2.1762e-02, -1.9199e+00,  5.8325e-01,  ..., -1.3845e+00,\n           -4.3968e-02, -1.1356e-01],\n          [ 2.4500e-01, -1.8028e+00,  7.5488e-01,  ..., -1.0910e+00,\n           -9.5760e-02, -6.5500e-02],\n          [ 9.4625e-02, -2.2294e+00,  6.9777e-01,  ..., -8.9249e-01,\n           -4.3502e-02, -1.1356e-03]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[-2.1439e-03, -5.5914e-04, -2.1274e-03,  ...,  3.1147e-03,\n            8.6172e-03,  2.2785e-03],\n          [ 6.2197e-02,  6.5266e-02, -7.7604e-02,  ...,  1.5363e-01,\n            6.2168e-02,  1.1366e-02],\n          [ 4.0407e-03, -9.7246e-02, -1.4253e-01,  ..., -8.2385e-02,\n           -3.1254e-02,  3.5707e-02],\n          ...,\n          [-5.7113e-02, -3.8309e-01,  5.4195e-02,  ...,  3.3675e-02,\n            9.3928e-03,  1.4511e-02],\n          [-2.3267e-01, -2.8227e-01,  1.6418e-01,  ..., -2.3110e-01,\n           -5.8598e-02, -2.7748e-02],\n          [-1.8836e-01,  2.1391e-01,  2.3785e-01,  ..., -4.5674e-01,\n           -9.1355e-02, -1.2167e-01]],\n\n         [[ 5.2292e-03,  1.3358e-02, -1.0107e-02,  ...,  1.5417e-02,\n            1.5633e-02, -3.1351e-03],\n          [ 1.1078e-01, -3.1568e-01,  1.6005e-01,  ...,  2.1315e-01,\n            1.0829e-01,  9.7122e-02],\n          [ 1.0978e-01, -6.6183e-01,  4.1571e-01,  ...,  1.3703e-02,\n            1.1994e-01, -1.9134e-01],\n          ...,\n          [ 5.9571e-02, -5.3989e-01,  4.0925e-01,  ..., -4.7650e-02,\n           -6.7599e-02,  8.5383e-02],\n          [ 2.5744e-01,  9.6139e-02, -1.1840e-01,  ...,  3.2070e-01,\n           -6.7054e-02, -2.5794e-01],\n          [ 5.6483e-01,  4.7597e-01, -1.3123e-01,  ...,  2.6420e-01,\n           -3.9466e-01, -2.4167e-01]],\n\n         [[ 3.8105e-03,  1.4735e-02,  7.3746e-03,  ...,  1.1680e-02,\n            1.5635e-03,  4.4056e-04],\n          [ 1.9272e-01, -4.2289e-01,  1.1067e-02,  ..., -1.2224e-01,\n            1.3172e-01, -2.6911e-02],\n          [ 1.3165e-01, -3.7715e-01,  1.2166e-01,  ..., -3.3554e-02,\n           -1.8766e-02,  4.9695e-02],\n          ...,\n          [ 5.5727e-01, -4.9510e-01, -3.8607e-02,  ..., -1.4490e-01,\n            6.3470e-02,  1.9599e-01],\n          [-1.0659e-01,  5.1536e-02,  6.4284e-02,  ..., -2.4662e-02,\n           -1.5930e-01, -2.0356e-01],\n          [-4.5406e-01,  2.7258e-01,  6.4688e-02,  ..., -5.3773e-02,\n           -1.9547e-02, -1.3887e-01]],\n\n         ...,\n\n         [[-3.6236e-03,  1.0879e-03, -3.8817e-03,  ...,  5.2002e-03,\n           -5.4375e-04, -9.2088e-03],\n          [ 4.5706e-01, -7.2927e-03, -3.6880e-01,  ..., -4.8575e-01,\n            1.3150e-01, -1.5646e-01],\n          [ 1.1462e-01, -4.6642e-02, -1.2884e-01,  ...,  3.5003e-01,\n            1.6690e-01, -1.7019e-01],\n          ...,\n          [ 3.0852e-01, -9.1584e-02, -6.3244e-03,  ..., -3.5006e-01,\n           -1.9842e-01, -5.8390e-02],\n          [-2.8731e-03, -3.6123e-02,  3.7974e-01,  ..., -3.1924e-02,\n           -4.3944e-03,  4.8170e-02],\n          [ 3.1086e-01,  5.4083e-02,  1.6653e-01,  ..., -4.4194e-01,\n            1.8925e-01,  1.1245e-01]],\n\n         [[-7.4614e-04, -2.5395e-03,  6.0192e-03,  ...,  1.3779e-02,\n           -1.4429e-02,  1.2163e-02],\n          [ 3.6836e-02, -2.7740e-01,  1.4934e-01,  ..., -1.9405e-03,\n           -6.3149e-01,  7.6956e-02],\n          [ 3.8361e-02,  2.3225e-01, -1.9124e-02,  ..., -3.2962e-01,\n           -1.9327e-01, -8.4314e-02],\n          ...,\n          [ 8.8172e-02,  1.1736e-02,  7.2894e-01,  ...,  1.6020e-03,\n           -3.1838e-01, -1.6946e-01],\n          [ 3.3774e-01,  2.1051e-01,  2.7312e-01,  ..., -3.1619e-01,\n           -4.8380e-01,  4.0269e-01],\n          [ 1.3651e-01,  2.6327e-01, -3.8162e-01,  ..., -2.3186e-01,\n            1.6418e-01,  1.2690e-01]],\n\n         [[-7.6272e-03,  3.2134e-02, -2.5948e-03,  ...,  5.6250e-03,\n            9.0716e-03, -6.1454e-03],\n          [-2.4990e-01,  1.4962e+00,  3.2113e-01,  ..., -2.9328e-01,\n           -1.9672e-01, -3.3637e-02],\n          [ 5.2070e-02,  5.9392e-01,  5.3320e-02,  ...,  8.5869e-02,\n            2.1880e-01, -2.2996e-01],\n          ...,\n          [ 6.2036e-01,  7.8892e-01,  3.0836e-01,  ..., -2.6149e-01,\n            4.3635e-02, -1.0577e-01],\n          [ 2.1455e-01,  1.0262e+00,  1.4066e-01,  ..., -4.5923e-01,\n            5.0683e-01, -3.1891e-01],\n          [-1.1888e-01,  5.6234e-01,  2.1417e-03,  ...,  1.6003e-02,\n            2.7337e-01, -5.6862e-02]]],\n\n\n        [[[ 4.4276e-04, -2.5888e-03,  1.0100e-03,  ...,  2.1085e-03,\n            1.0966e-02,  4.0614e-03],\n          [ 8.7270e-02, -2.0231e-01,  1.6970e-01,  ..., -1.7194e-01,\n            4.2159e-02,  1.3418e-02],\n          [ 5.7260e-03,  2.5200e-01,  1.9814e-01,  ...,  8.2540e-02,\n           -9.9714e-03, -4.8595e-02],\n          ...,\n          [-1.3448e-02,  3.6688e-02,  1.0995e-01,  ...,  2.9214e-01,\n           -5.5682e-02, -1.6306e-02],\n          [-8.6900e-02, -2.2254e-01, -7.8773e-02,  ...,  3.1296e-01,\n           -9.5957e-02,  1.6010e-02],\n          [-3.0296e-02, -1.1776e-01,  9.8977e-02,  ...,  1.7124e-01,\n           -1.1828e-01,  2.0173e-02]],\n\n         [[ 7.4739e-03,  2.3769e-02, -6.5222e-03,  ...,  1.3592e-02,\n            9.9993e-03, -1.4363e-03],\n          [-3.3974e-02,  4.8614e-01,  3.3067e-02,  ...,  1.1183e-01,\n           -2.1195e-02, -3.0983e-01],\n          [ 3.1118e-01, -5.5111e-01, -2.1990e-01,  ...,  8.5364e-01,\n           -4.1766e-01, -3.8307e-01],\n          ...,\n          [ 3.3255e-01, -5.2529e-01,  1.0551e-01,  ...,  3.7533e-01,\n           -6.3743e-01,  2.2577e-01],\n          [-2.5523e-01, -7.2946e-01, -2.4718e-01,  ...,  3.7441e-01,\n           -5.1212e-01, -2.6203e-01],\n          [-2.8758e-01, -1.0904e+00, -5.8444e-02,  ...,  3.7238e-01,\n            2.4277e-01, -8.0277e-01]],\n\n         [[ 3.9390e-03,  1.3032e-02,  8.0780e-03,  ...,  9.5470e-03,\n           -9.2634e-04, -5.0397e-04],\n          [-2.4699e-01, -1.7320e-01,  6.3693e-02,  ..., -1.3548e-01,\n           -7.7238e-02, -2.4315e-01],\n          [-3.7294e-01,  1.9689e-01, -1.8151e-01,  ..., -2.2389e-01,\n            1.3600e-02, -7.2004e-02],\n          ...,\n          [-6.3466e-01,  2.6026e-01, -2.7873e-01,  ..., -8.1802e-03,\n            9.2774e-03, -2.5554e-02],\n          [ 1.6046e-01, -1.7306e-02, -1.0916e-01,  ..., -1.7603e-01,\n           -3.4570e-02, -1.0408e-01],\n          [-6.4898e-02, -1.5857e-01, -6.1656e-02,  ..., -1.1865e-02,\n           -1.7938e-01, -1.3339e-01]],\n\n         ...,\n\n         [[-6.5136e-03,  2.3014e-03,  3.3767e-03,  ...,  4.1490e-03,\n           -3.5522e-03, -5.7516e-03],\n          [ 1.0113e-01, -1.1169e-02,  1.0901e-01,  ...,  2.0590e-01,\n           -2.0785e-01, -4.0699e-02],\n          [ 1.6411e-01, -1.2145e-01,  2.0635e-01,  ...,  3.1943e-01,\n           -1.8927e-01, -1.6854e-01],\n          ...,\n          [ 8.2054e-02, -1.5891e-02,  2.4096e-01,  ..., -3.7013e-01,\n            3.3299e-02,  3.6837e-02],\n          [-1.7031e-01, -1.1659e-01,  6.9816e-02,  ...,  8.6431e-02,\n           -2.4668e-01, -7.7542e-02],\n          [-2.1765e-01, -1.1267e-01,  1.0146e-02,  ...,  3.2612e-01,\n           -3.6097e-01, -2.4950e-01]],\n\n         [[ 1.0521e-04, -3.8998e-03,  4.7144e-03,  ...,  1.8480e-02,\n           -2.2271e-03,  2.9042e-03],\n          [ 4.5227e-01,  4.2141e-01, -3.1630e-01,  ...,  1.8698e-02,\n           -3.2498e-01, -5.8748e-03],\n          [-1.3894e-01,  4.2039e-01, -2.4960e-02,  ...,  2.9901e-01,\n            3.1807e-01,  3.3329e-03],\n          ...,\n          [-3.6570e-01,  4.8515e-02,  1.4614e-01,  ...,  5.1998e-02,\n           -3.7394e-01,  1.3949e-01],\n          [-3.7306e-01,  2.5540e-01,  3.9660e-01,  ...,  1.2789e-01,\n            1.1143e-01, -1.3238e-01],\n          [-1.0084e-01,  4.2591e-02,  1.9316e-01,  ..., -9.2473e-02,\n           -3.5591e-01, -1.0140e-01]],\n\n         [[-5.6180e-03,  1.1172e-02, -1.1971e-03,  ..., -6.5563e-03,\n            1.1659e-02, -3.7967e-03],\n          [-2.7131e-01, -3.7176e-01,  3.1329e-01,  ..., -3.0226e-01,\n            3.9986e-01, -4.5047e-01],\n          [-1.8035e-01, -1.8840e-01, -1.7852e-01,  ...,  3.1317e-01,\n           -2.5325e-01,  4.9291e-02],\n          ...,\n          [-3.0367e-01,  1.2086e-03, -2.1724e-01,  ...,  2.0534e-01,\n            2.1208e-01,  1.4077e-01],\n          [ 1.9156e-01,  1.3698e-01,  2.7824e-01,  ..., -1.3760e-01,\n            4.9487e-01, -1.7574e-01],\n          [-2.2435e-02, -5.8954e-02, -1.1626e-02,  ..., -1.7359e-01,\n            4.4597e-01, -6.5431e-02]]],\n\n\n        [[[-2.6414e-03, -1.3416e-03,  2.2046e-03,  ...,  6.8259e-03,\n            1.1729e-02,  3.1918e-03],\n          [ 7.4674e-02,  1.0796e-01,  6.6743e-02,  ...,  3.0253e-01,\n            6.7491e-02, -1.4065e-02],\n          [ 9.1129e-02,  1.5879e-01,  6.3600e-02,  ...,  3.9041e-01,\n            9.9325e-02,  4.9465e-02],\n          ...,\n          [-1.5049e-01, -4.7131e-02,  1.8164e-01,  ...,  1.3901e-02,\n           -3.6482e-02, -1.6089e-02],\n          [ 2.9630e-03,  1.7442e-01,  3.6663e-01,  ...,  8.4030e-02,\n           -1.7643e-02, -3.4570e-02],\n          [ 7.3934e-05,  1.1826e-01,  2.9715e-01,  ...,  1.0810e-01,\n           -7.3370e-02, -6.3895e-02]],\n\n         [[ 6.1827e-03,  2.3275e-02,  3.2787e-03,  ...,  1.8966e-02,\n            7.6158e-03, -3.9020e-04],\n          [-1.1822e-01,  7.9022e-01, -1.1456e-01,  ...,  2.4197e-01,\n            1.4396e-01, -4.4334e-01],\n          [ 2.8665e-02,  6.6124e-01,  5.1462e-01,  ...,  1.1530e-01,\n            2.5601e-02,  7.2100e-02],\n          ...,\n          [ 1.5849e-01, -5.2971e-01, -6.4074e-03,  ...,  5.4593e-01,\n           -2.1780e-01,  3.0207e-02],\n          [-1.3399e-01,  1.4470e-01,  1.2853e-02,  ...,  3.5371e-01,\n            8.3183e-02, -7.2830e-03],\n          [-3.9974e-01,  5.1567e-01,  2.6848e-01,  ..., -2.1086e-01,\n            2.6450e-01,  3.1499e-01]],\n\n         [[ 1.2915e-02,  9.3862e-03,  2.2489e-03,  ...,  9.4919e-03,\n           -2.1735e-03,  4.6819e-03],\n          [ 5.5088e-01, -1.1732e-01,  1.0238e-01,  ..., -6.6287e-02,\n           -3.5274e-02, -6.3841e-02],\n          [-8.0762e-02, -3.5204e-01, -9.8361e-02,  ..., -1.8695e-01,\n           -5.4131e-02, -1.6923e-01],\n          ...,\n          [-1.1993e-01, -3.1806e-02, -1.4273e-01,  ..., -4.6809e-02,\n            1.9902e-03,  2.7439e-03],\n          [ 1.7896e-01,  5.0074e-02, -1.8120e-02,  ..., -2.8845e-02,\n           -8.7619e-02,  9.0707e-02],\n          [-5.5568e-03,  6.5445e-02,  4.6677e-02,  ..., -1.3351e-02,\n            1.5730e-02, -6.6801e-03]],\n\n         ...,\n\n         [[-3.6473e-03, -8.7930e-04,  1.4068e-03,  ...,  1.6268e-02,\n            7.4589e-04, -7.0861e-03],\n          [ 1.2650e-01, -1.4352e-01, -2.8100e-01,  ..., -1.5243e-01,\n           -2.4103e-01, -1.5618e-02],\n          [ 3.6294e-01, -1.2752e-01, -3.4813e-01,  ..., -6.9936e-02,\n           -2.5118e-01,  7.5358e-02],\n          ...,\n          [-1.4426e-01,  2.9624e-02, -5.7633e-02,  ..., -3.3418e-01,\n            2.5449e-01,  2.4940e-01],\n          [-2.4874e-01, -1.1223e-01, -5.2324e-03,  ..., -2.6207e-01,\n            1.5537e-01,  2.6867e-01],\n          [-3.8061e-01, -1.3197e-01,  1.5676e-02,  ..., -5.7261e-01,\n           -5.2457e-02,  3.8923e-01]],\n\n         [[-6.8016e-04, -2.7073e-03,  8.3164e-03,  ...,  1.5615e-02,\n           -1.4738e-03,  5.0437e-03],\n          [-1.2140e-01, -2.9350e-02, -3.0524e-01,  ...,  1.5200e-01,\n            9.0102e-02, -1.9419e-02],\n          [ 2.3161e-01,  1.3793e-02, -1.3485e-02,  ..., -2.2236e-02,\n           -4.2066e-01,  4.7823e-02],\n          ...,\n          [ 2.6549e-02,  3.2137e-01,  5.9065e-01,  ..., -4.3392e-01,\n            2.6864e-01, -2.0372e-01],\n          [ 2.0156e-01,  7.7265e-02, -2.3904e-02,  ..., -1.1876e-02,\n           -3.0438e-02,  3.7915e-02],\n          [-3.0766e-02,  1.2467e-01, -1.9762e-01,  ..., -9.5102e-02,\n            8.6555e-02,  9.3867e-02]],\n\n         [[ 2.7364e-03,  1.0175e-02,  6.5319e-04,  ..., -4.7730e-03,\n            1.1333e-02, -5.3602e-03],\n          [ 9.1656e-02, -7.7501e-01,  1.2995e-01,  ..., -4.7967e-01,\n            3.2122e-01, -1.7253e-01],\n          [-2.0234e-02,  2.1118e-01, -1.4166e-01,  ..., -1.9333e-01,\n           -1.7492e-01,  4.1999e-01],\n          ...,\n          [-4.5816e-01,  2.7300e-01,  7.8113e-02,  ...,  2.6208e-01,\n            1.5865e-01,  1.9592e-01],\n          [ 3.2991e-01,  1.7313e-01,  1.1765e-01,  ...,  2.7887e-01,\n            1.5288e-01, -5.1722e-02],\n          [ 5.3294e-01, -3.6406e-01,  4.4984e-02,  ...,  1.9036e-01,\n            1.6797e-01,  4.3704e-02]]],\n\n\n        ...,\n\n\n        [[[-9.1887e-04,  5.9610e-03,  3.4356e-03,  ...,  2.9842e-03,\n            1.2438e-02,  2.9676e-03],\n          [ 1.2320e-01,  7.0629e-02,  6.6237e-02,  ...,  2.7083e-01,\n           -5.6546e-03,  6.1437e-02],\n          [ 4.2429e-02,  2.3353e-01, -9.3145e-02,  ...,  1.3175e-02,\n           -9.6687e-02, -7.1207e-03],\n          ...,\n          [-1.6336e-02,  1.0169e-01,  3.7545e-01,  ..., -2.3800e-01,\n           -1.4539e-01, -3.6706e-02],\n          [-2.3842e-02,  7.0971e-02,  4.2318e-01,  ...,  2.8161e-02,\n           -8.3832e-02, -6.9767e-02],\n          [-4.4776e-03,  5.5965e-03,  3.5742e-01,  ..., -2.2036e-02,\n           -4.3042e-02, -9.2371e-02]],\n\n         [[ 7.9782e-04,  3.6425e-02, -5.3956e-03,  ...,  5.7136e-03,\n            9.4624e-03, -6.0400e-03],\n          [ 4.9467e-03, -5.8736e-01,  1.8337e-01,  ..., -7.9067e-01,\n            3.7390e-01, -7.7921e-02],\n          [ 4.2309e-01, -1.0333e+00,  2.1602e-01,  ..., -1.4004e+00,\n            3.0901e-01,  5.0890e-01],\n          ...,\n          [-4.3662e-01,  3.8123e-01, -3.7219e-02,  ..., -7.6318e-01,\n            5.2521e-01, -4.1759e-01],\n          [-3.9542e-02,  4.2253e-01, -8.3507e-02,  ..., -4.1836e-01,\n           -1.8752e-01, -1.8756e-01],\n          [ 2.3381e-01,  4.3730e-01, -3.0482e-01,  ...,  1.5852e-01,\n            1.5036e-01, -2.3008e-01]],\n\n         [[ 5.6939e-03,  1.4556e-02,  6.3100e-03,  ...,  1.3173e-02,\n            1.6083e-03,  1.5435e-03],\n          [ 1.1409e-01, -8.4113e-02,  5.9216e-02,  ...,  6.6120e-03,\n            2.0628e-01,  3.1324e-02],\n          [-5.5855e-02, -9.4540e-02,  3.1837e-03,  ..., -8.2548e-03,\n            1.6640e-01,  4.8846e-02],\n          ...,\n          [-8.8291e-01,  3.2902e-01, -1.4828e-02,  ..., -1.0144e-01,\n           -1.2046e-01,  6.5577e-02],\n          [-1.1709e-01, -1.7209e-01, -1.0432e-01,  ...,  5.1716e-02,\n           -4.8984e-03, -2.7594e-02],\n          [ 8.1157e-02, -1.9799e-01, -1.7582e-02,  ...,  7.5732e-02,\n            3.8027e-02, -1.7374e-01]],\n\n         ...,\n\n         [[-1.2253e-02,  2.1905e-03,  8.6864e-03,  ...,  1.7423e-02,\n            6.9386e-03, -1.1949e-02],\n          [-2.6936e-02, -2.4883e-02, -3.3261e-01,  ..., -7.1238e-01,\n           -2.7358e-01, -3.2569e-01],\n          [-2.2633e-02,  7.3879e-02, -2.8186e-01,  ..., -3.3577e-01,\n            7.7409e-02, -2.0007e-01],\n          ...,\n          [-4.7835e-01, -1.3039e-01,  1.2477e-01,  ..., -5.2077e-01,\n            1.2729e-01,  3.7609e-01],\n          [-9.3791e-02, -1.8457e-01, -4.6177e-02,  ..., -4.5631e-01,\n           -7.3163e-02,  5.6553e-03],\n          [-2.3584e-01, -1.5384e-01, -9.1096e-02,  ...,  6.2632e-03,\n           -3.2594e-01,  1.8022e-01]],\n\n         [[-2.6973e-04, -3.5975e-03,  8.5783e-04,  ...,  1.5526e-02,\n           -1.5832e-02, -1.8388e-03],\n          [ 3.7221e-01,  3.7515e-01, -4.7106e-01,  ...,  1.7962e-01,\n           -1.4393e-01, -1.9195e-01],\n          [ 5.2718e-01,  5.6354e-03,  5.4579e-01,  ..., -7.5184e-02,\n           -4.5652e-01,  3.6827e-01],\n          ...,\n          [-2.8900e-01, -2.6092e-02,  2.0437e-01,  ..., -1.1770e-01,\n           -4.3803e-01,  3.7134e-01],\n          [-4.5827e-02,  9.1578e-02,  1.5695e-01,  ...,  1.4473e-02,\n            1.3759e-01,  1.0166e-02],\n          [-1.9880e-01,  8.0815e-03,  2.4291e-01,  ...,  1.7778e-01,\n           -8.5072e-02,  2.1823e-01]],\n\n         [[-5.6554e-03,  1.3665e-02, -7.9512e-03,  ..., -5.5845e-03,\n            1.0951e-02, -5.7516e-03],\n          [-1.6232e-01, -3.7706e-01, -1.0213e-01,  ...,  2.5899e-02,\n            2.0351e-01,  2.8826e-01],\n          [-2.1933e-01, -3.2565e-01,  2.8237e-01,  ...,  1.1509e-01,\n            3.3821e-01,  2.7462e-03],\n          ...,\n          [ 3.0341e-01, -4.1254e-01,  2.2307e-03,  ...,  6.9828e-02,\n           -1.1931e-01, -3.9091e-02],\n          [ 1.1544e-01, -3.4819e-01, -7.5037e-02,  ...,  1.3821e-01,\n            1.4647e-01,  6.0195e-02],\n          [ 1.6901e-01, -2.1010e-01, -4.4355e-02,  ..., -5.3710e-01,\n           -1.1834e-02, -2.0039e-01]]],\n\n\n        [[[-4.9288e-04, -1.6051e-04,  3.9549e-03,  ...,  3.0177e-03,\n            1.3336e-02,  7.3476e-03],\n          [ 8.6966e-02,  3.8776e-02,  6.7592e-02,  ...,  3.4290e-01,\n           -4.6669e-03, -2.3889e-02],\n          [ 1.6165e-01,  3.2941e-02, -2.8162e-01,  ...,  1.7560e-01,\n           -6.6171e-02, -5.4859e-02],\n          ...,\n          [-3.2778e-02,  3.0677e-02,  2.2019e-01,  ...,  2.7154e-01,\n           -5.0092e-02,  1.7938e-02],\n          [-5.8369e-02, -1.3219e-01, -5.7646e-02,  ...,  8.5980e-02,\n           -1.5957e-01, -3.5535e-03],\n          [-3.2005e-01, -8.4593e-02, -3.6372e-01,  ..., -4.4054e-01,\n           -9.2445e-02, -9.5143e-02]],\n\n         [[ 1.3906e-02,  4.1949e-02,  6.8685e-03,  ...,  8.1589e-03,\n            5.4102e-03, -8.1206e-04],\n          [ 4.2426e-01, -6.8585e-01,  3.4290e-01,  ..., -1.5779e-01,\n            1.0969e-01, -2.1689e-01],\n          [ 4.2168e-02, -1.5424e+00,  3.8297e-01,  ..., -5.0033e-01,\n            6.8565e-01, -1.7717e-01],\n          ...,\n          [ 4.4508e-01, -2.1806e-02,  4.0641e-02,  ...,  4.9711e-01,\n           -7.2170e-02, -6.1285e-01],\n          [ 1.8206e-01, -5.6575e-02,  5.7298e-02,  ..., -3.2549e-01,\n            5.2821e-01, -5.7238e-01],\n          [ 1.0665e-01,  1.3604e-01,  2.0609e-01,  ...,  3.3759e-01,\n            4.7631e-03, -3.6574e-01]],\n\n         [[ 8.7238e-03,  1.0014e-02,  7.4854e-03,  ...,  1.1162e-02,\n           -2.6439e-03, -2.5325e-03],\n          [ 1.8617e-01, -2.8199e-01,  1.7674e-01,  ..., -1.3753e-01,\n           -7.3578e-02, -1.7705e-01],\n          [ 6.0951e-02, -3.2927e-01,  1.2449e-01,  ..., -2.1428e-01,\n           -5.2616e-02, -6.8601e-02],\n          ...,\n          [-1.2334e-01, -1.6972e-01,  3.9938e-02,  ...,  9.2223e-02,\n           -1.3410e-01, -2.1660e-01],\n          [ 8.6380e-02,  1.1440e-01,  1.3169e-01,  ...,  7.3434e-02,\n            1.1442e-01,  1.4517e-02],\n          [-2.7617e-01,  4.4582e-02, -1.4951e-01,  ...,  1.3545e-02,\n           -4.0418e-02,  6.7399e-02]],\n\n         ...,\n\n         [[-1.2593e-03,  4.3698e-03,  2.6882e-03,  ...,  1.0409e-02,\n           -1.5263e-03, -5.6996e-03],\n          [ 2.8546e-01, -1.0837e-01, -4.9383e-01,  ..., -2.5639e-01,\n           -4.6968e-02, -2.9347e-01],\n          [ 4.6506e-01, -9.4585e-02, -1.0693e-01,  ..., -1.6728e-01,\n            4.2265e-02, -3.2363e-01],\n          ...,\n          [ 1.1855e-01, -1.6694e-01, -1.4543e-01,  ..., -3.2996e-01,\n            3.1694e-03, -3.6924e-02],\n          [-2.6619e-01, -1.1595e-01,  1.6685e-02,  ..., -3.7171e-01,\n            1.7115e-01,  1.2354e-01],\n          [-1.9721e-01,  1.5406e-02,  8.6034e-02,  ..., -6.3526e-03,\n            5.0332e-03,  3.8467e-02]],\n\n         [[ 6.8176e-03,  4.7736e-03, -4.0170e-03,  ...,  1.7337e-02,\n           -2.1410e-02,  3.4194e-03],\n          [ 3.3266e-01,  3.2316e-01, -4.7243e-01,  ...,  6.6638e-02,\n           -4.3917e-01, -4.4010e-01],\n          [ 3.3657e-01,  2.4771e-01, -5.6785e-01,  ..., -2.6902e-02,\n           -4.4564e-01, -2.8382e-01],\n          ...,\n          [-2.3545e-01, -1.6106e-02,  4.6196e-01,  ..., -2.4465e-02,\n           -2.3292e-01, -1.6450e-01],\n          [-4.2650e-01,  1.0614e-03,  5.2683e-01,  ...,  1.1221e-01,\n           -6.8554e-02, -1.1465e-01],\n          [-2.6625e-01,  3.1247e-01,  1.0735e-01,  ..., -4.5260e-02,\n            1.6894e-01,  3.5520e-01]],\n\n         [[-4.9860e-03,  3.4963e-02, -4.4362e-03,  ..., -1.6590e-02,\n            1.8122e-02, -1.0724e-02],\n          [ 6.4464e-02,  1.9793e-01, -6.7906e-02,  ..., -1.7688e-01,\n            3.4739e-02, -6.3248e-02],\n          [ 1.9227e-01,  7.4874e-02, -5.3583e-02,  ...,  2.3147e-01,\n           -2.2231e-01, -7.7235e-02],\n          ...,\n          [-4.0457e-01,  1.7881e-01, -1.4472e-01,  ...,  1.5215e-01,\n            3.3098e-02, -1.0402e-01],\n          [-1.5454e-01, -2.2454e-02, -7.7064e-03,  ...,  1.8748e-01,\n            1.8271e-01, -2.1117e-02],\n          [-3.9685e-02,  2.0446e-01,  5.2010e-02,  ...,  1.6460e-01,\n            3.2652e-01,  2.8547e-01]]],\n\n\n        [[[ 1.1825e-03, -6.3252e-04,  3.4120e-04,  ...,  2.1523e-04,\n            7.8058e-03, -1.1582e-03],\n          [ 2.0186e-01,  2.1221e-01,  9.6513e-02,  ..., -4.8159e-02,\n           -8.2303e-03,  4.0503e-04],\n          [ 3.5313e-02,  7.4040e-02, -2.2497e-02,  ...,  1.3872e-01,\n            7.8809e-02,  6.1374e-03],\n          ...,\n          [ 4.5509e-02,  2.5431e-01,  3.7264e-02,  ..., -1.4842e-01,\n            2.1907e-02, -4.4453e-02],\n          [ 7.8891e-03,  2.7757e-01,  5.6974e-02,  ..., -1.6039e-01,\n           -5.3021e-02, -8.8227e-02],\n          [-4.8997e-02,  1.6832e-01, -4.8171e-02,  ...,  1.6624e-01,\n           -3.9742e-02, -5.5657e-02]],\n\n         [[ 9.1068e-03,  1.3246e-02, -1.2845e-02,  ...,  1.6375e-02,\n            1.4728e-02, -1.2636e-02],\n          [ 2.6322e-01, -4.8118e-01,  2.1339e-01,  ...,  2.5182e-01,\n           -2.1972e-02, -1.7342e-01],\n          [-2.5796e-01, -7.9089e-01,  3.7729e-02,  ..., -9.2116e-02,\n            5.7835e-01, -2.0966e-01],\n          ...,\n          [ 2.7134e-01,  1.8300e-01,  1.7880e-02,  ...,  3.0698e-01,\n            9.6801e-02, -1.5706e-01],\n          [-1.2604e-02, -7.2592e-01,  1.7081e-01,  ...,  3.7435e-01,\n            1.1834e-01, -1.3327e-01],\n          [-1.4056e-01, -6.2082e-01,  5.0982e-01,  ...,  9.9673e-02,\n            9.0805e-02,  2.2967e-02]],\n\n         [[ 2.5903e-03,  8.4916e-03,  9.0814e-03,  ...,  1.2409e-02,\n            7.1609e-05,  3.4463e-03],\n          [ 3.6834e-01, -2.9546e-01,  3.4943e-02,  ...,  3.5992e-02,\n           -6.5390e-02,  2.8437e-02],\n          [ 1.8952e-01, -3.6786e-01, -2.4276e-02,  ..., -2.0264e-01,\n           -2.4866e-03, -2.9206e-02],\n          ...,\n          [-1.0135e-01,  1.0857e-01, -1.2179e-01,  ..., -2.9167e-02,\n            3.4248e-02, -3.2310e-02],\n          [ 1.5286e-01,  4.2168e-02,  7.1741e-02,  ...,  1.0206e-03,\n            1.4916e-02, -7.8563e-02],\n          [-2.3473e-01,  9.9291e-02, -8.7834e-02,  ..., -1.2515e-01,\n            4.1428e-04, -1.6965e-01]],\n\n         ...,\n\n         [[-1.0695e-02,  1.3194e-03,  4.5708e-03,  ...,  2.0884e-02,\n           -3.6547e-03, -7.2375e-03],\n          [-1.3590e-01, -1.0090e-01, -3.1210e-01,  ...,  3.7306e-02,\n           -3.1372e-01, -4.4448e-01],\n          [-1.0013e-01, -1.2066e-01, -1.0942e-01,  ..., -1.0761e-01,\n           -1.9377e-02, -2.3381e-01],\n          ...,\n          [-3.3107e-01,  2.4443e-02,  5.1306e-02,  ..., -2.1377e-01,\n            1.2825e-01, -1.0630e-01],\n          [-4.4143e-01, -9.9397e-04, -9.5390e-02,  ..., -2.2604e-01,\n            4.6294e-02, -2.6835e-01],\n          [-4.0498e-01, -5.2207e-03, -1.0966e-01,  ..., -3.9360e-01,\n           -1.1366e-01, -8.4348e-03]],\n\n         [[ 5.8603e-03, -1.5884e-03,  6.7160e-03,  ...,  1.6735e-02,\n           -8.5367e-03,  4.8575e-03],\n          [-9.0484e-02,  3.0722e-01,  1.2989e-01,  ..., -1.3023e-01,\n            2.3264e-01, -2.3468e-01],\n          [ 1.3892e-01,  2.6590e-01,  2.5506e-02,  ...,  1.3733e-01,\n            2.2026e-02, -9.3753e-02],\n          ...,\n          [-2.4856e-01, -1.5604e-01,  5.6709e-01,  ...,  8.1300e-02,\n           -1.0149e-01,  2.6131e-01],\n          [-3.5383e-02, -1.8877e-01,  7.9645e-01,  ..., -1.8370e-01,\n            2.4985e-03, -1.4895e-01],\n          [-3.4272e-01, -4.5894e-02,  8.7667e-01,  ...,  1.3558e-01,\n           -3.9949e-01,  7.8721e-02]],\n\n         [[-7.2658e-03,  2.0310e-02,  1.0364e-03,  ...,  1.6179e-03,\n            5.3121e-03, -3.7165e-03],\n          [ 3.5117e-01,  5.7338e-01, -9.5224e-02,  ..., -1.9822e-01,\n            1.9591e-01,  2.7091e-02],\n          [ 4.1842e-01,  4.9469e-01, -5.5823e-02,  ..., -1.6860e-01,\n           -1.1991e-01, -2.5629e-01],\n          ...,\n          [-3.4172e-02, -1.6228e-01, -1.8940e-01,  ..., -7.5973e-02,\n           -3.7028e-02,  1.4915e-01],\n          [-4.3624e-02,  1.8638e-01, -1.9648e-01,  ...,  1.1219e-01,\n           -3.1885e-01, -1.0107e-01],\n          [ 1.7487e-01,  7.5069e-02, -4.9608e-02,  ..., -9.8837e-02,\n           -3.0297e-01,  2.9985e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-6.1734e-01, -2.2655e-01,  6.4064e-01,  ...,  7.5916e-02,\n            3.1733e-01, -4.3967e-01],\n          [ 1.0365e-01,  4.9641e-01, -3.9477e-01,  ..., -4.3101e-01,\n           -3.6637e-01,  2.7624e-01],\n          [ 8.0328e-02,  4.7304e-01, -3.6253e-01,  ..., -4.1108e-01,\n           -3.4380e-01,  2.5301e-01],\n          ...,\n          [ 8.5219e-03,  4.0109e-01, -2.7205e-01,  ..., -3.5344e-01,\n           -2.7434e-01,  1.8152e-01],\n          [ 5.2752e-02,  4.4533e-01, -3.1556e-01,  ..., -3.9694e-01,\n           -3.1862e-01,  2.2576e-01],\n          [ 5.6626e-03,  3.9817e-01, -2.5794e-01,  ..., -3.5953e-01,\n           -2.7303e-01,  1.7887e-01]],\n\n         [[ 6.3978e-01, -1.0496e+00, -2.8582e-01,  ...,  6.2090e-01,\n           -2.8250e-01,  2.0674e-01],\n          [-1.0619e+00,  1.0462e+00, -2.9073e-01,  ..., -4.4597e-01,\n           -2.9193e-01,  1.7757e-01],\n          [-8.3527e-01,  9.6801e-01, -3.1862e-01,  ..., -2.6866e-02,\n           -2.8290e-01,  2.2345e-01],\n          ...,\n          [-5.4617e-01,  9.9317e-01, -2.7476e-01,  ...,  7.0359e-01,\n           -2.7475e-01,  1.7975e-01],\n          [-9.0455e-01,  1.0035e+00, -2.8501e-01,  ..., -6.2884e-02,\n           -2.8649e-01,  1.8250e-01],\n          [-9.7616e-01,  1.0463e+00, -2.6761e-01,  ..., -1.5129e-01,\n           -2.9223e-01,  1.6789e-01]],\n\n         [[-1.0617e+00,  1.5272e-01,  5.3499e-01,  ...,  5.1621e-01,\n            5.6954e-01, -1.0031e+00],\n          [ 1.2478e+00, -1.8082e-01, -6.8412e-02,  ..., -8.9534e-01,\n           -5.3438e-01,  2.0184e+00],\n          [ 5.4299e-01, -7.5689e-01,  1.1371e-01,  ..., -1.1711e-01,\n           -7.1818e-01,  1.9159e+00],\n          ...,\n          [-2.1857e+00,  8.3355e-01,  8.2533e-01,  ..., -1.0266e+00,\n           -9.6751e-01,  2.8521e+00],\n          [ 8.5123e-02, -1.2915e-01,  5.6576e-01,  ..., -1.2858e+00,\n           -7.7026e-01,  1.7478e+00],\n          [-9.1217e-01, -2.7903e-01,  2.0205e-01,  ..., -1.5810e+00,\n           -6.7489e-01,  5.3520e-01]],\n\n         ...,\n\n         [[ 4.9195e-01, -1.8472e-01,  3.1206e-01,  ..., -2.6733e-01,\n           -9.3663e-01,  3.3601e-01],\n          [-1.0531e+00, -2.7271e-01,  2.9124e-01,  ..., -3.2836e-01,\n            5.1605e-01,  3.3532e-01],\n          [-9.3178e-01, -1.5445e-01,  1.6280e-01,  ..., -2.1791e-01,\n            6.1497e-01,  4.4145e-01],\n          ...,\n          [-7.4127e-01, -1.1301e-01,  2.3409e-01,  ..., -2.1478e-01,\n           -3.2982e-01,  4.3124e-01],\n          [-6.8330e-01, -1.0274e-01,  2.3323e-01,  ..., -7.1080e-02,\n            2.3867e-01,  4.1989e-01],\n          [-6.9149e-01, -6.4428e-02,  1.6123e-01,  ..., -4.1942e-02,\n            6.5367e-03,  4.2157e-01]],\n\n         [[-1.1490e-01,  8.2181e-01,  2.8168e-01,  ...,  5.1580e-01,\n            4.1409e-01,  5.0758e-01],\n          [ 1.1724e+00,  9.9827e-01,  3.2143e-01,  ...,  4.0177e-01,\n           -2.1716e-01,  1.2841e+00],\n          [ 8.4084e-01,  1.4285e+00,  3.3125e-01,  ...,  2.9933e-01,\n           -5.0952e-01,  1.1770e+00],\n          ...,\n          [ 1.2460e+00,  1.1746e+00,  4.4656e-01,  ...,  4.5649e-01,\n           -1.9277e+00,  9.3682e-01],\n          [ 1.7772e+00,  1.1038e+00,  4.4003e-01,  ...,  6.0134e-01,\n           -1.4365e+00,  1.3692e+00],\n          [ 2.2684e+00,  9.1169e-01,  5.0199e-01,  ...,  6.4038e-01,\n           -7.6597e-01,  8.4332e-01]],\n\n         [[ 2.9311e-01, -8.5210e-01,  1.3341e+00,  ..., -1.6041e+00,\n           -1.2972e+00,  7.4901e-02],\n          [ 1.6781e-01,  1.7047e-01, -4.0570e-01,  ...,  4.5931e-01,\n            2.1990e-01,  1.0983e-01],\n          [ 1.7589e-01,  1.3178e-01, -1.9738e-01,  ...,  2.3271e-01,\n            1.5799e-01,  1.0539e-01],\n          ...,\n          [ 1.5934e-01,  8.1882e-02, -7.4355e-03,  ...,  2.3365e-01,\n            7.0419e-02,  1.1562e-01],\n          [ 1.4688e-01,  9.6527e-02, -1.6813e-01,  ...,  3.7053e-01,\n            8.7828e-02,  1.1887e-01],\n          [ 1.4327e-01,  8.2285e-02, -1.2438e-01,  ...,  2.8865e-01,\n            6.6753e-02,  1.2354e-01]]],\n\n\n        [[[-6.1807e-01, -2.2728e-01,  6.4162e-01,  ...,  7.6475e-02,\n            3.1803e-01, -4.4040e-01],\n          [ 7.7127e-02,  4.6983e-01, -3.5368e-01,  ..., -4.1164e-01,\n           -3.4128e-01,  2.4991e-01],\n          [ 6.0353e-02,  4.5309e-01, -3.4659e-01,  ..., -3.8995e-01,\n           -3.2350e-01,  2.3300e-01],\n          ...,\n          [ 2.1840e-02,  4.1446e-01, -2.9250e-01,  ..., -3.6186e-01,\n           -2.8686e-01,  1.9474e-01],\n          [ 1.3078e-02,  4.0569e-01, -2.8393e-01,  ..., -3.5160e-01,\n           -2.7793e-01,  1.8595e-01],\n          [ 2.0467e-04,  3.9279e-01, -2.6471e-01,  ..., -3.4299e-01,\n           -2.6579e-01,  1.7318e-01]],\n\n         [[ 6.3813e-01, -1.0490e+00, -2.8503e-01,  ...,  6.1898e-01,\n           -2.8303e-01,  2.0620e-01],\n          [-1.0941e+00,  9.6994e-01, -2.8916e-01,  ..., -5.9383e-01,\n           -3.1007e-01,  1.6709e-01],\n          [-1.0575e+00,  1.0444e+00, -2.6326e-01,  ..., -4.0889e-01,\n           -3.0114e-01,  1.5359e-01],\n          ...,\n          [-1.0038e+00,  1.1607e+00, -2.8013e-01,  ..., -1.3929e-02,\n           -2.8731e-01,  1.8332e-01],\n          [-7.7464e-01,  1.1072e+00, -2.7249e-01,  ...,  4.3422e-01,\n           -2.8399e-01,  1.8840e-01],\n          [-9.1327e-01,  1.1061e+00, -2.6562e-01,  ...,  1.2182e-01,\n           -2.9982e-01,  1.7091e-01]],\n\n         [[-1.0853e+00,  1.7061e-01,  5.4748e-01,  ...,  4.9247e-01,\n            5.8617e-01, -1.0113e+00],\n          [ 2.1606e+00, -3.3999e-01, -6.3167e-02,  ..., -1.6785e+00,\n           -7.2672e-01,  1.0581e+00],\n          [ 9.1111e-01, -7.9258e-01,  3.4807e-01,  ..., -3.6993e-01,\n            2.4830e-01,  5.2703e-02],\n          ...,\n          [-2.2276e+00,  2.0744e-01,  1.7764e-02,  ..., -1.6862e+00,\n           -3.8657e-01,  3.5989e+00],\n          [-2.4612e+00,  4.2841e-01,  3.7218e-01,  ..., -1.4523e+00,\n           -1.0109e+00,  2.0433e+00],\n          [-6.1122e-01,  3.5160e-02, -6.0026e-02,  ..., -1.7295e+00,\n           -6.8951e-01,  2.1354e+00]],\n\n         ...,\n\n         [[ 4.9469e-01, -1.8626e-01,  3.1572e-01,  ..., -2.6831e-01,\n           -9.2923e-01,  3.3253e-01],\n          [-6.3605e-01, -2.7645e-01,  3.0322e-01,  ..., -2.3974e-01,\n            1.6888e-01,  3.6088e-01],\n          [-9.8528e-01, -3.1168e-01,  3.6863e-01,  ..., -3.2639e-01,\n            6.2002e-01,  2.3804e-01],\n          ...,\n          [-1.0054e+00, -8.9978e-02,  2.2094e-01,  ..., -1.0492e-01,\n            7.3354e-01,  4.0901e-01],\n          [-9.1173e-01, -1.0223e-01,  2.0484e-01,  ..., -1.2060e-01,\n            2.3868e-01,  4.2875e-01],\n          [-1.1034e+00, -8.6221e-02,  2.5029e-01,  ..., -1.3652e-01,\n            5.7649e-01,  3.5870e-01]],\n\n         [[-1.3072e-01,  8.2756e-01,  2.8225e-01,  ...,  5.1609e-01,\n            4.1615e-01,  5.0261e-01],\n          [ 3.5084e-01,  1.2487e+00,  3.1503e-01,  ...,  5.1197e-01,\n           -4.6966e-02,  7.9958e-01],\n          [ 1.4703e+00,  1.1980e+00,  3.6771e-01,  ...,  5.4511e-01,\n            1.7327e-01,  9.9949e-01],\n          ...,\n          [ 9.4009e-01,  1.6974e+00,  3.9612e-01,  ...,  5.9671e-01,\n           -2.0817e+00,  1.4122e+00],\n          [ 1.1883e+00,  1.5137e+00,  4.6421e-01,  ...,  5.0439e-01,\n           -2.7168e+00,  1.1412e+00],\n          [ 1.9222e+00,  1.3207e+00,  5.0162e-01,  ...,  5.0194e-01,\n           -1.7717e+00,  1.4221e+00]],\n\n         [[ 2.9380e-01, -8.5331e-01,  1.3391e+00,  ..., -1.6085e+00,\n           -1.2989e+00,  7.4628e-02],\n          [ 1.8214e-01,  9.5118e-02, -2.1002e-01,  ...,  2.2041e-01,\n            9.5939e-02,  1.0046e-01],\n          [ 1.8199e-01,  1.1286e-01, -3.1859e-01,  ...,  3.5524e-01,\n            1.3772e-01,  1.0366e-01],\n          ...,\n          [ 1.6754e-01,  1.4132e-01, -1.0212e-01,  ...,  3.3983e-01,\n            1.6328e-01,  1.1588e-01],\n          [ 1.7010e-01,  1.2979e-01, -1.3212e-01,  ...,  3.6646e-01,\n            1.5309e-01,  1.1699e-01],\n          [ 1.7396e-01,  1.0350e-01, -1.9997e-01,  ...,  3.9889e-01,\n            1.1796e-01,  1.1713e-01]]],\n\n\n        [[[-6.1852e-01, -2.2773e-01,  6.4216e-01,  ...,  7.6889e-02,\n            3.1847e-01, -4.4085e-01],\n          [ 5.0310e-02,  4.4292e-01, -3.1467e-01,  ..., -3.9229e-01,\n           -3.1588e-01,  2.2330e-01],\n          [ 3.1859e-02,  4.2444e-01, -2.9370e-01,  ..., -3.7893e-01,\n           -2.9804e-01,  2.0493e-01],\n          ...,\n          [ 4.4201e-02,  4.3684e-01, -3.1750e-01,  ..., -3.8235e-01,\n           -3.0891e-01,  2.1705e-01],\n          [ 3.6918e-02,  4.2957e-01, -3.1075e-01,  ..., -3.7395e-01,\n           -3.0146e-01,  2.0976e-01],\n          [ 8.2429e-03,  4.0082e-01, -2.7282e-01,  ..., -3.5424e-01,\n           -2.7417e-01,  1.8126e-01]],\n\n         [[ 6.3857e-01, -1.0509e+00, -2.8455e-01,  ...,  6.2033e-01,\n           -2.8281e-01,  2.0518e-01],\n          [-1.1195e+00,  9.3730e-01, -2.6599e-01,  ..., -6.5589e-01,\n           -3.0438e-01,  1.5223e-01],\n          [-1.1241e+00,  9.5808e-01, -2.5971e-01,  ..., -6.5297e-01,\n           -3.0408e-01,  1.4664e-01],\n          ...,\n          [-8.9077e-01,  1.1369e+00, -2.6474e-01,  ...,  2.3252e-01,\n           -2.9153e-01,  1.7652e-01],\n          [-9.6811e-01,  1.0975e+00, -2.4607e-01,  ..., -4.6872e-02,\n           -2.9957e-01,  1.3452e-01],\n          [-1.0486e+00,  1.1224e+00, -2.4498e-01,  ..., -1.4399e-01,\n           -3.1617e-01,  1.0850e-01]],\n\n         [[-1.0991e+00,  1.5863e-01,  5.4259e-01,  ...,  5.0348e-01,\n            6.0274e-01, -1.0131e+00],\n          [ 2.1023e-01, -5.3819e-01,  4.4282e-01,  ..., -6.3262e-01,\n           -5.2520e-01,  1.4887e+00],\n          [ 1.8487e+00,  2.5992e-01,  1.2470e+00,  ...,  4.4422e-01,\n            1.6969e-01,  7.8223e-01],\n          ...,\n          [-1.1750e-01, -7.2160e-01, -3.7033e-01,  ..., -1.0892e+00,\n           -7.2958e-02,  1.1275e+00],\n          [-5.9764e-01, -2.7600e-01, -6.3827e-01,  ..., -3.2990e-01,\n            2.8824e-02,  2.2893e+00],\n          [-2.1169e+00,  8.8306e-02,  1.7488e-01,  ..., -2.0778e-01,\n           -1.2167e-01,  1.9182e+00]],\n\n         ...,\n\n         [[ 4.9524e-01, -1.8803e-01,  3.1638e-01,  ..., -2.7014e-01,\n           -9.5224e-01,  3.3164e-01],\n          [-8.8417e-01, -1.7260e-01,  1.7043e-01,  ..., -2.0460e-01,\n            1.6878e-01,  4.2469e-01],\n          [-7.3338e-01, -2.8052e-01,  2.7971e-01,  ..., -2.7006e-01,\n            3.9753e-02,  3.6342e-01],\n          ...,\n          [-9.7011e-01, -8.2554e-02,  2.0537e-01,  ..., -1.2098e-01,\n            7.5279e-01,  4.0388e-01],\n          [-8.7668e-01, -8.1184e-02,  1.8163e-01,  ..., -1.8530e-01,\n            4.5048e-01,  4.3954e-01],\n          [-8.1983e-01, -1.5211e-01,  2.5104e-01,  ..., -1.9729e-01,\n           -2.8715e-01,  3.7255e-01]],\n\n         [[-1.1472e-01,  8.2208e-01,  2.8426e-01,  ...,  5.1553e-01,\n            4.2564e-01,  5.0079e-01],\n          [-4.9973e-01,  1.4534e+00,  4.2282e-01,  ...,  5.6957e-01,\n           -5.5679e-01,  3.5349e-01],\n          [ 4.7494e-01,  1.0520e+00,  4.9192e-01,  ...,  5.6713e-01,\n           -3.7515e-01,  8.0546e-01],\n          ...,\n          [ 2.2275e+00,  1.1848e+00,  4.1033e-01,  ...,  4.9602e-01,\n           -9.7119e-01,  1.0224e+00],\n          [ 1.2811e+00,  1.4278e+00,  4.1242e-01,  ...,  5.1798e-01,\n           -1.4193e+00,  1.0561e+00],\n          [ 8.7343e-01,  1.6230e+00,  4.5848e-01,  ...,  4.7514e-01,\n           -2.3536e+00,  9.6304e-01]],\n\n         [[ 2.9385e-01, -8.5344e-01,  1.3387e+00,  ..., -1.6092e+00,\n           -1.2991e+00,  7.4630e-02],\n          [ 1.8177e-01,  8.1485e-02,  4.9040e-03,  ...,  2.1618e-02,\n            6.7532e-02,  1.0415e-01],\n          [ 1.9064e-01,  5.9534e-02, -5.9000e-02,  ...,  7.7318e-02,\n            4.7168e-02,  9.9175e-02],\n          ...,\n          [ 1.6465e-01,  1.2038e-01, -2.3265e-01,  ...,  4.3977e-01,\n            1.4303e-01,  1.1836e-01],\n          [ 1.6943e-01,  1.1381e-01, -1.6814e-01,  ...,  3.3145e-01,\n            1.2602e-01,  1.1316e-01],\n          [ 1.6812e-01,  1.0077e-01, -5.3871e-02,  ...,  2.4658e-01,\n            1.0195e-01,  1.1248e-01]]],\n\n\n        ...,\n\n\n        [[[-6.2058e-01, -2.2980e-01,  6.4512e-01,  ...,  7.8184e-02,\n            3.2041e-01, -4.4289e-01],\n          [ 6.1776e-03,  3.9868e-01, -2.5804e-01,  ..., -3.6216e-01,\n           -2.7376e-01,  1.7942e-01],\n          [ 5.8848e-02,  4.5149e-01, -3.3064e-01,  ..., -4.0077e-01,\n           -3.2402e-01,  2.3177e-01],\n          ...,\n          [-2.4823e-02,  3.6770e-01, -2.3080e-01,  ..., -3.2675e-01,\n           -2.4211e-01,  1.4833e-01],\n          [-3.2751e-02,  3.5974e-01, -2.1937e-01,  ..., -3.2181e-01,\n           -2.3464e-01,  1.4046e-01],\n          [-1.7525e-02,  3.7500e-01, -2.4313e-01,  ..., -3.3230e-01,\n           -2.4892e-01,  1.5555e-01]],\n\n         [[ 6.4248e-01, -1.0566e+00, -2.8397e-01,  ...,  6.2156e-01,\n           -2.8291e-01,  2.0522e-01],\n          [-8.8340e-01,  9.1747e-01, -2.8932e-01,  ..., -1.4114e-01,\n           -2.9833e-01,  1.9665e-01],\n          [-9.2948e-01,  1.0406e+00, -3.0206e-01,  ..., -1.8885e-01,\n           -2.9631e-01,  1.9433e-01],\n          ...,\n          [-8.5304e-01,  1.0260e+00, -2.6581e-01,  ...,  1.2845e-01,\n           -2.9206e-01,  1.6424e-01],\n          [-9.9406e-01,  1.0228e+00, -2.6347e-01,  ..., -1.4903e-01,\n           -3.1224e-01,  1.4497e-01],\n          [-8.7461e-01,  1.0451e+00, -2.5797e-01,  ...,  1.7506e-01,\n           -3.0153e-01,  1.5560e-01]],\n\n         [[-1.0884e+00,  1.5971e-01,  5.4346e-01,  ...,  5.2738e-01,\n            6.2737e-01, -1.0334e+00],\n          [ 2.3620e-02, -1.3237e-02,  1.2263e+00,  ...,  8.5994e-01,\n            7.3632e-01,  1.6687e+00],\n          [ 1.7236e+00, -6.8831e-01,  5.1519e-01,  ..., -3.1984e-01,\n           -9.0406e-01,  1.0107e+00],\n          ...,\n          [-1.3600e-01, -1.2648e-02,  4.5617e-01,  ..., -8.8613e-01,\n           -4.8496e-01,  1.1244e+00],\n          [-1.4291e+00,  7.4808e-01,  9.8714e-01,  ...,  3.7458e-01,\n           -1.6659e-01,  3.1771e+00],\n          [-1.4252e+00,  1.0257e+00,  1.1628e+00,  ..., -8.1934e-01,\n           -4.5300e-01,  2.4967e+00]],\n\n         ...,\n\n         [[ 4.9958e-01, -1.8890e-01,  3.1848e-01,  ..., -2.7306e-01,\n           -9.4923e-01,  3.2985e-01],\n          [-7.3072e-01, -2.5810e-01,  2.2493e-01,  ..., -3.1587e-01,\n           -1.7341e-01,  3.9652e-01],\n          [-8.8234e-01, -1.8170e-01,  1.4220e-01,  ..., -2.2344e-01,\n            2.8370e-02,  4.7491e-01],\n          ...,\n          [-8.2704e-01, -1.7031e-01,  2.8624e-01,  ..., -2.2600e-01,\n            1.3240e-01,  3.6861e-01],\n          [-8.6091e-01, -1.0817e-01,  1.7211e-01,  ..., -1.3732e-01,\n            2.3426e-02,  4.7322e-01],\n          [-8.0087e-01, -1.2130e-01,  1.5896e-01,  ..., -1.5480e-01,\n           -2.4070e-01,  4.7204e-01]],\n\n         [[-1.2626e-01,  8.2834e-01,  2.8443e-01,  ...,  5.1595e-01,\n            4.3060e-01,  4.9418e-01],\n          [ 1.0779e+00,  1.1451e+00,  3.5807e-01,  ...,  4.9114e-01,\n           -4.6183e-01,  6.7312e-01],\n          [ 6.3020e-01,  1.2602e+00,  3.9547e-01,  ...,  5.0762e-01,\n           -4.1641e-01,  7.8125e-01],\n          ...,\n          [ 8.7925e-01,  1.6191e+00,  3.9154e-01,  ...,  5.1624e-01,\n           -1.3930e+00,  3.1924e-01],\n          [ 7.2587e-01,  1.7065e+00,  3.9096e-01,  ...,  5.8525e-01,\n           -1.8500e+00,  1.0688e+00],\n          [ 8.6119e-01,  1.4543e+00,  3.9243e-01,  ...,  3.9635e-01,\n           -2.3342e+00,  1.1540e+00]],\n\n         [[ 2.9447e-01, -8.5608e-01,  1.3403e+00,  ..., -1.6142e+00,\n           -1.3027e+00,  7.4515e-02],\n          [ 1.8704e-01,  6.1306e-02, -8.1978e-02,  ...,  9.8456e-02,\n            4.9695e-02,  9.6321e-02],\n          [ 1.6482e-01,  1.3481e-01, -2.1730e-01,  ...,  2.7130e-01,\n            1.5738e-01,  1.0786e-01],\n          ...,\n          [ 1.7262e-01,  5.4462e-02, -1.3183e-02,  ...,  2.0849e-01,\n            3.6965e-02,  1.1340e-01],\n          [ 1.7765e-01,  6.4968e-02,  6.9161e-02,  ...,  1.4415e-01,\n            4.5512e-02,  1.1089e-01],\n          [ 1.7831e-01,  8.6326e-02,  2.9305e-02,  ...,  1.7520e-01,\n            8.0689e-02,  1.0704e-01]]],\n\n\n        [[[-6.3614e-01, -2.4541e-01,  6.6768e-01,  ...,  8.8996e-02,\n            3.3514e-01, -4.5834e-01],\n          [ 4.8069e-02,  4.4072e-01, -3.2335e-01,  ..., -3.8841e-01,\n           -3.1284e-01,  2.2093e-01],\n          [ 7.4972e-02,  4.6766e-01, -3.5332e-01,  ..., -4.1056e-01,\n           -3.3917e-01,  2.4776e-01],\n          ...,\n          [-4.2950e-03,  3.8824e-01, -2.5184e-01,  ..., -3.4460e-01,\n           -2.6232e-01,  1.6882e-01],\n          [-1.5587e-03,  3.9095e-01, -2.5239e-01,  ..., -3.4935e-01,\n           -2.6538e-01,  1.7160e-01],\n          [ 2.5752e-02,  4.1827e-01, -2.7640e-01,  ..., -3.7811e-01,\n           -2.9310e-01,  1.9896e-01]],\n\n         [[ 6.7180e-01, -1.1038e+00, -2.8399e-01,  ...,  6.2032e-01,\n           -2.8289e-01,  2.0405e-01],\n          [-9.1195e-01,  1.0391e+00, -2.7839e-01,  ..., -7.7530e-02,\n           -3.2693e-01,  1.5556e-01],\n          [-8.8524e-01,  1.1409e+00, -2.9005e-01,  ...,  1.0516e-02,\n           -2.9761e-01,  1.6920e-01],\n          ...,\n          [-9.9502e-01,  1.0718e+00, -2.5242e-01,  ..., -6.2279e-02,\n           -3.1104e-01,  1.4511e-01],\n          [-6.5313e-01,  9.9355e-01, -2.7308e-01,  ...,  5.8581e-01,\n           -2.9547e-01,  1.6848e-01],\n          [-1.0596e+00,  1.0110e+00, -2.7845e-01,  ..., -3.1759e-01,\n           -2.9683e-01,  1.8997e-01]],\n\n         [[-1.1155e+00,  1.8189e-01,  5.4261e-01,  ...,  5.6544e-01,\n            6.4654e-01, -1.0777e+00],\n          [-2.5198e-01,  4.6789e-02,  1.1073e+00,  ...,  4.9175e-01,\n           -2.8913e-01,  2.0227e+00],\n          [ 5.8806e-01, -3.5286e-01,  3.3725e-01,  ..., -9.1360e-01,\n           -6.5176e-01,  1.8765e+00],\n          ...,\n          [-1.8333e+00,  8.4124e-01,  2.7841e-01,  ..., -6.1328e-01,\n            4.4957e-01,  4.1834e+00],\n          [-2.6227e+00,  9.5522e-01,  4.3520e-01,  ..., -1.0898e+00,\n           -9.9958e-01,  4.4178e+00],\n          [ 5.6147e-02, -1.8307e-01, -8.4774e-02,  ..., -2.4460e+00,\n           -8.2314e-01,  7.9213e-01]],\n\n         ...,\n\n         [[ 5.3608e-01, -1.9266e-01,  3.2126e-01,  ..., -2.7437e-01,\n           -9.6276e-01,  3.2862e-01],\n          [-9.2572e-01, -2.3876e-01,  2.6183e-01,  ..., -2.8269e-01,\n            2.4053e-01,  3.6074e-01],\n          [-9.4703e-01, -2.1957e-01,  2.2441e-01,  ..., -2.6275e-01,\n            1.5824e-01,  4.1221e-01],\n          ...,\n          [-8.8051e-01, -1.2142e-01,  2.1222e-01,  ..., -2.0858e-01,\n            7.3250e-01,  3.8525e-01],\n          [-7.2714e-01, -7.2538e-03,  1.1353e-01,  ..., -8.1131e-02,\n           -1.0106e-01,  5.2126e-01],\n          [-6.5151e-01, -3.0458e-02,  1.6456e-01,  ..., -4.7187e-02,\n            5.6292e-01,  3.8318e-01]],\n\n         [[-1.3277e-01,  8.2085e-01,  2.8107e-01,  ...,  5.1553e-01,\n            4.6069e-01,  4.8132e-01],\n          [ 1.4771e+00,  9.0457e-01,  4.3317e-01,  ...,  3.1383e-01,\n           -3.1154e-01,  9.0462e-01],\n          [ 1.6754e+00,  1.0064e+00,  5.1033e-01,  ...,  4.5032e-01,\n           -2.3170e-01,  7.5826e-01],\n          ...,\n          [ 8.3205e-01,  1.4190e+00,  3.6293e-01,  ...,  3.9693e-01,\n           -2.1276e+00,  1.3754e+00],\n          [ 9.5257e-02,  1.6135e+00,  2.9415e-01,  ...,  6.9924e-01,\n           -3.2656e+00,  7.2361e-01],\n          [ 2.0309e+00,  1.0637e+00,  5.0441e-01,  ...,  6.8210e-01,\n           -7.1669e-01,  1.1905e+00]],\n\n         [[ 2.9756e-01, -8.7926e-01,  1.3771e+00,  ..., -1.6578e+00,\n           -1.3369e+00,  7.3426e-02],\n          [ 1.7122e-01,  1.3853e-01, -1.6100e-01,  ...,  2.5326e-01,\n            1.6222e-01,  1.0467e-01],\n          [ 1.5476e-01,  1.6784e-01, -3.2494e-01,  ...,  4.2101e-01,\n            2.0750e-01,  1.1480e-01],\n          ...,\n          [ 1.8119e-01,  8.0452e-02, -6.6715e-02,  ...,  2.4637e-01,\n            7.7611e-02,  1.0934e-01],\n          [ 1.8114e-01,  6.2429e-02,  4.4031e-02,  ...,  1.8303e-01,\n            4.5247e-02,  1.1184e-01],\n          [ 1.5808e-01,  6.4743e-02, -8.5951e-02,  ...,  2.4425e-01,\n            4.1074e-02,  1.1747e-01]]],\n\n\n        [[[-6.1106e-01, -2.2025e-01,  6.3163e-01,  ...,  7.1581e-02,\n            3.1138e-01, -4.3344e-01],\n          [ 1.8895e-02,  4.1145e-01, -2.8050e-01,  ..., -3.6911e-01,\n           -2.8534e-01,  1.9197e-01],\n          [ 6.0589e-02,  4.5329e-01, -3.4519e-01,  ..., -3.9326e-01,\n           -3.2412e-01,  2.3327e-01],\n          ...,\n          [ 4.8557e-02,  4.4122e-01, -3.2908e-01,  ..., -3.8274e-01,\n           -3.1244e-01,  2.2129e-01],\n          [ 3.2041e-03,  3.9575e-01, -2.6176e-01,  ..., -3.5208e-01,\n           -2.6970e-01,  1.7631e-01],\n          [ 1.9007e-02,  4.1159e-01, -2.8479e-01,  ..., -3.6438e-01,\n           -2.8475e-01,  1.9200e-01]],\n\n         [[ 6.2430e-01, -1.0290e+00, -2.8516e-01,  ...,  6.1251e-01,\n           -2.8327e-01,  2.0557e-01],\n          [-1.1136e+00,  9.7724e-01, -2.6580e-01,  ..., -5.8318e-01,\n           -3.0389e-01,  1.3798e-01],\n          [-9.2479e-01,  1.0580e+00, -2.7519e-01,  ..., -8.3209e-02,\n           -2.8633e-01,  1.5189e-01],\n          ...,\n          [-9.4696e-01,  1.1071e+00, -2.6359e-01,  ...,  1.4737e-03,\n           -2.9744e-01,  1.6219e-01],\n          [-7.5215e-01,  1.0209e+00, -2.7682e-01,  ...,  3.3266e-01,\n           -2.8076e-01,  1.9793e-01],\n          [-9.0073e-01,  1.0764e+00, -2.6800e-01,  ...,  6.7296e-02,\n           -2.9135e-01,  1.9505e-01]],\n\n         [[-1.0391e+00,  1.7448e-01,  5.3520e-01,  ...,  4.6666e-01,\n            5.5726e-01, -9.6434e-01],\n          [ 1.4533e+00, -2.4478e-01,  2.5633e-01,  ..., -4.5494e-01,\n            5.7733e-01,  1.3533e+00],\n          [ 3.7296e-01, -2.2830e-02,  1.4498e+00,  ..., -1.4442e-01,\n            5.8076e-01,  9.8952e-01],\n          ...,\n          [-2.3698e-01,  4.8706e-01,  5.2005e-01,  ..., -1.0376e+00,\n            5.8375e-02,  1.6646e+00],\n          [-7.2057e-01,  2.2876e-01,  5.3389e-01,  ..., -1.0623e+00,\n           -5.5524e-01,  1.1401e+00],\n          [-9.4820e-01,  5.4827e-01,  5.2086e-01,  ..., -9.0298e-01,\n           -3.4228e-01,  1.8969e+00]],\n\n         ...,\n\n         [[ 4.8285e-01, -1.8447e-01,  3.1152e-01,  ..., -2.6627e-01,\n           -9.3507e-01,  3.3624e-01],\n          [-8.4704e-01, -1.7457e-01,  1.6801e-01,  ..., -2.3115e-01,\n            3.8685e-01,  4.3290e-01],\n          [-9.2768e-01, -1.9345e-01,  1.6886e-01,  ..., -2.7034e-01,\n            2.2140e-01,  4.3188e-01],\n          ...,\n          [-1.1115e+00, -5.1487e-02,  1.0467e-01,  ..., -1.8712e-01,\n            5.6146e-01,  4.8975e-01],\n          [-9.9147e-01, -1.2138e-01,  1.6508e-01,  ..., -1.8409e-01,\n           -1.4496e-01,  4.5995e-01],\n          [-1.0498e+00, -1.6496e-01,  2.4553e-01,  ..., -2.4683e-01,\n            6.1818e-01,  3.8493e-01]],\n\n         [[-8.2520e-02,  8.1958e-01,  2.8179e-01,  ...,  5.1781e-01,\n            4.2731e-01,  5.0054e-01],\n          [ 1.6279e+00,  1.2071e+00,  4.8369e-01,  ...,  4.7182e-01,\n            2.6320e-01,  9.6453e-01],\n          [ 1.3455e+00,  1.0566e+00,  3.9167e-01,  ...,  4.8781e-01,\n            7.4364e-02,  8.5465e-01],\n          ...,\n          [ 2.0036e+00,  1.5042e+00,  5.4211e-01,  ...,  4.5837e-01,\n           -1.2246e+00,  1.2771e+00],\n          [ 1.6794e+00,  1.6001e+00,  5.4148e-01,  ...,  5.1174e-01,\n           -1.0547e+00,  8.9216e-01],\n          [ 2.0380e+00,  1.4180e+00,  4.6544e-01,  ...,  4.6993e-01,\n           -1.1408e+00,  1.5041e+00]],\n\n         [[ 2.9168e-01, -8.4376e-01,  1.3170e+00,  ..., -1.5801e+00,\n           -1.2848e+00,  7.5474e-02],\n          [ 1.7559e-01,  6.3803e-02, -1.9888e-01,  ...,  2.2600e-01,\n            6.3458e-02,  1.0818e-01],\n          [ 1.6544e-01,  1.3780e-01, -2.8840e-01,  ...,  3.2814e-01,\n            1.6524e-01,  1.1290e-01],\n          ...,\n          [ 1.5002e-01,  1.6421e-01, -1.9826e-01,  ...,  4.4301e-01,\n            1.9277e-01,  1.1970e-01],\n          [ 1.4120e-01,  1.1456e-01, -1.1827e-01,  ...,  3.3156e-01,\n            1.1364e-01,  1.2649e-01],\n          [ 1.4255e-01,  1.4828e-01, -2.2986e-01,  ...,  4.4192e-01,\n            1.6896e-01,  1.2789e-01]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[ 9.1729e-04,  4.6281e-03, -1.7935e-03,  ..., -1.0229e-02,\n           -1.0252e-02, -8.8332e-03],\n          [ 9.4049e-02,  1.0712e-02,  6.1437e-02,  ..., -1.4131e-02,\n            8.2948e-02, -5.7284e-02],\n          [ 4.5824e-02,  6.6514e-02,  8.1019e-02,  ...,  2.8060e-02,\n            4.2651e-02, -7.7277e-02],\n          ...,\n          [-5.9278e-02, -5.0575e-02, -4.5646e-02,  ..., -2.9355e-02,\n           -5.9794e-02,  4.7113e-02],\n          [-2.6537e-02,  5.0939e-02,  4.8075e-02,  ..., -8.0897e-02,\n            1.1133e-02,  6.1894e-02],\n          [ 4.1286e-02,  2.9683e-02, -2.8461e-02,  ..., -4.8700e-02,\n            2.0239e-02,  2.2984e-02]],\n\n         [[-6.6360e-03,  3.6086e-03, -1.5440e-03,  ...,  1.3370e-02,\n            1.8285e-02,  6.0684e-03],\n          [-7.2320e-02, -2.0663e-01,  9.7962e-02,  ...,  4.2375e-02,\n           -2.0558e-01, -3.4586e-02],\n          [-1.2589e-01, -4.2768e-02, -1.6339e-01,  ..., -4.2135e-02,\n            6.3070e-02,  1.2037e-02],\n          ...,\n          [ 2.1006e-03, -1.3452e-01,  1.6821e-01,  ..., -1.0722e-01,\n            1.7648e-01, -9.0845e-02],\n          [-3.2390e-03, -5.4568e-02,  3.6357e-01,  ...,  2.1167e-02,\n            3.7623e-01,  1.0594e-01],\n          [ 3.9772e-03, -1.5534e-01,  1.8631e-02,  ..., -6.0047e-02,\n            2.7843e-01, -1.1074e-01]],\n\n         [[ 6.6171e-03,  1.4218e-02, -1.3520e-02,  ...,  1.8239e-02,\n            8.0588e-03, -1.6087e-02],\n          [-1.9622e-01, -3.8490e-01,  7.0789e-01,  ..., -7.5257e-01,\n            2.0857e-01,  2.0918e-01],\n          [ 2.8036e-01, -9.6011e-02, -2.9995e-01,  ...,  2.7594e-01,\n           -1.8184e-01,  1.0311e-01],\n          ...,\n          [ 6.2397e-01, -1.5356e-01,  5.4392e-01,  ..., -2.9236e-01,\n           -1.2288e-01,  2.8703e-01],\n          [-2.9190e-01, -5.3125e-01,  4.3235e-01,  ..., -2.4625e-01,\n           -2.7187e-02,  1.7019e-01],\n          [-3.1233e-01, -1.7600e-01, -1.0659e-01,  ...,  1.6499e-01,\n            1.8582e-01,  4.9791e-01]],\n\n         ...,\n\n         [[-7.6869e-04, -1.4566e-02,  1.0443e-02,  ..., -4.9612e-03,\n           -7.8372e-03,  5.9125e-03],\n          [ 6.2965e-02, -6.7051e-02, -8.4122e-02,  ..., -8.1375e-02,\n           -1.8473e-02, -1.7481e-02],\n          [-8.4016e-02,  2.6419e-02,  2.3229e-01,  ..., -6.2643e-02,\n            4.6651e-02, -1.7292e-01],\n          ...,\n          [ 1.3082e-02, -3.7483e-02,  9.4455e-02,  ...,  2.3744e-02,\n           -2.6985e-02, -1.6104e-01],\n          [ 1.9349e-01,  7.6523e-03,  3.9297e-02,  ...,  1.0172e-02,\n           -1.1036e-02, -3.9253e-02],\n          [ 1.3237e-01, -2.8874e-01,  1.0986e-01,  ...,  3.3078e-03,\n           -2.7851e-02,  2.0129e-01]],\n\n         [[ 1.0932e-02, -7.7027e-03,  1.0114e-02,  ...,  3.7075e-03,\n           -1.5079e-02,  5.7426e-05],\n          [ 5.4022e-01,  7.4126e-02,  4.1322e-01,  ...,  1.7209e-01,\n           -2.8677e-01, -5.2341e-01],\n          [ 2.8056e-02,  4.3189e-01, -1.3390e-01,  ...,  5.5155e-01,\n            8.1003e-01, -3.3827e-01],\n          ...,\n          [ 3.9780e-01, -3.2599e-01,  7.6047e-02,  ...,  1.7191e-02,\n            5.4655e-01,  1.4588e-01],\n          [ 9.2087e-02, -1.2182e-01,  2.6224e-01,  ...,  1.2581e-01,\n            1.0672e-01,  3.6779e-02],\n          [-2.5105e-01, -2.1562e-01,  2.6562e-03,  ..., -8.8198e-02,\n           -1.9687e-01,  4.8055e-02]],\n\n         [[ 6.5182e-03,  5.9723e-03,  3.6811e-03,  ...,  9.0805e-03,\n           -5.8592e-03,  2.8265e-04],\n          [-6.5397e-03, -1.0640e-01, -8.4746e-02,  ..., -3.3591e-02,\n           -4.0068e-02, -1.5432e-02],\n          [ 2.1633e-02,  5.3909e-02, -2.9601e-02,  ..., -6.6045e-02,\n            7.2200e-02, -7.1482e-02],\n          ...,\n          [-2.9401e-02,  7.4176e-02, -4.3468e-02,  ...,  1.7547e-02,\n            8.1995e-02, -9.2265e-02],\n          [ 7.5438e-02,  4.5620e-02, -7.5376e-02,  ...,  3.4831e-03,\n            1.0888e-01, -1.2419e-01],\n          [-2.7965e-03,  5.0708e-02,  1.6625e-02,  ...,  1.5672e-02,\n           -4.7460e-02,  5.8895e-02]]],\n\n\n        [[[-1.0797e-03,  2.8138e-03, -3.3314e-03,  ..., -9.1669e-03,\n           -1.1742e-02, -6.6732e-03],\n          [-2.8655e-02,  5.7713e-02,  4.3430e-02,  ..., -3.6377e-02,\n           -3.3156e-02,  1.9459e-03],\n          [ 4.3057e-02, -2.3101e-02, -6.2865e-02,  ...,  8.8980e-02,\n            5.3677e-02,  2.1770e-02],\n          ...,\n          [-3.7233e-02, -7.1266e-02, -1.6796e-02,  ..., -3.2358e-02,\n            3.0363e-02,  1.8772e-02],\n          [-8.9011e-02, -1.3738e-02, -4.0891e-02,  ..., -3.7465e-02,\n            5.1189e-02,  4.3607e-02],\n          [-1.2727e-01, -8.9491e-03, -1.5099e-02,  ...,  8.2508e-04,\n           -3.7487e-03,  7.0005e-02]],\n\n         [[-4.1360e-03,  3.5829e-03, -4.2506e-03,  ...,  9.3706e-03,\n            1.6201e-02,  1.8274e-03],\n          [-1.8349e-01, -4.1426e-02, -7.7293e-02,  ...,  3.9569e-02,\n            3.0075e-01,  7.9862e-02],\n          [-5.7406e-02,  1.0943e-01, -5.4548e-02,  ...,  6.9786e-02,\n           -1.0282e-01, -1.2868e-01],\n          ...,\n          [-1.0889e-01,  1.5410e-01,  1.8529e-02,  ..., -9.7283e-02,\n           -1.0278e-01, -1.3745e-02],\n          [-2.6039e-02,  7.3395e-02, -1.3251e-01,  ..., -8.8302e-02,\n           -2.9693e-01, -4.9383e-02],\n          [-5.7994e-02,  4.1966e-04,  3.3525e-01,  ...,  5.2021e-03,\n            1.5355e-01, -1.6574e-02]],\n\n         [[-3.6499e-03,  2.1557e-02,  8.0033e-03,  ...,  8.4313e-03,\n            1.7495e-02, -2.2238e-02],\n          [-2.4822e-01, -2.0706e-01,  2.5536e-02,  ...,  4.0670e-02,\n            4.8932e-01, -3.2671e-01],\n          [-3.1391e-01, -3.2496e-01,  1.8384e-01,  ..., -2.5596e-02,\n            7.6095e-01,  5.1601e-01],\n          ...,\n          [ 1.6455e-01, -4.0928e-01, -7.0626e-01,  ..., -1.7037e-02,\n            8.3012e-02, -1.9493e-01],\n          [-2.8909e-01,  8.0324e-02, -2.8311e-01,  ..., -6.2960e-02,\n           -5.3267e-01,  2.4731e-01],\n          [-3.5117e-01,  4.5106e-02, -5.1921e-01,  ...,  1.9985e-01,\n           -4.9638e-01,  1.1355e-01]],\n\n         ...,\n\n         [[-6.1675e-04, -5.0323e-03,  5.6485e-03,  ..., -1.3850e-03,\n           -6.2755e-03,  6.8432e-03],\n          [ 1.4029e-01, -1.5330e-01,  2.6000e-01,  ...,  2.1494e-02,\n           -3.1604e-03, -1.9985e-01],\n          [-8.9034e-02, -3.7196e-01,  2.2664e-01,  ...,  4.5100e-02,\n           -9.5967e-02, -2.0394e-01],\n          ...,\n          [ 2.3704e-01, -1.6855e-02, -6.8931e-02,  ..., -3.4012e-02,\n           -1.2528e-02,  1.1746e-01],\n          [ 1.0905e-01,  2.2005e-01, -5.1339e-02,  ..., -3.6798e-02,\n            1.4365e-01, -2.2871e-01],\n          [ 1.2095e-01,  1.4306e-02, -2.4917e-01,  ...,  4.6252e-02,\n            9.4829e-02, -2.7047e-01]],\n\n         [[ 6.8684e-04, -8.6156e-03,  1.9237e-02,  ..., -1.1699e-03,\n           -1.0415e-02,  1.0328e-03],\n          [-5.0448e-01, -1.5642e-01, -1.3746e-02,  ..., -3.6966e-01,\n            2.0639e-01,  2.5620e-02],\n          [ 4.3195e-01,  1.6802e-01,  1.3517e-01,  ..., -3.1284e-01,\n           -1.1894e-01, -5.4592e-02],\n          ...,\n          [ 9.0112e-01, -2.0702e-01, -5.4274e-01,  ...,  6.0724e-01,\n           -5.0072e-01, -7.5713e-02],\n          [ 9.2417e-01, -3.5647e-01, -4.2032e-01,  ...,  6.4282e-01,\n           -1.1618e-01, -8.3169e-02],\n          [ 7.0578e-01, -3.3583e-01, -4.4374e-02,  ...,  5.9548e-01,\n           -3.7349e-02, -1.5532e-01]],\n\n         [[ 5.8057e-03,  3.5469e-03,  4.9679e-03,  ...,  5.4425e-03,\n           -3.6143e-03, -3.7760e-04],\n          [ 1.7833e-02, -5.8680e-02, -7.2014e-02,  ..., -2.9175e-02,\n           -1.1131e-02, -1.1153e-01],\n          [-7.1477e-02, -1.0412e-01, -1.1386e-01,  ..., -6.7574e-02,\n           -2.4319e-02, -6.4137e-02],\n          ...,\n          [-5.2713e-03,  7.7288e-02,  2.2258e-02,  ..., -3.9559e-02,\n            1.0395e-02,  4.1431e-02],\n          [ 4.9023e-03,  1.4594e-01,  1.2994e-02,  ...,  9.3824e-03,\n            1.1497e-02,  5.9792e-02],\n          [ 9.4960e-03,  8.6806e-02,  1.8386e-03,  ...,  4.0377e-02,\n            3.4887e-03, -1.3586e-03]]],\n\n\n        [[[-2.9451e-03,  1.0927e-03, -1.7175e-03,  ..., -9.4891e-03,\n           -8.7360e-03, -8.9465e-03],\n          [ 7.6660e-02,  5.9562e-02, -1.3403e-02,  ..., -4.2134e-02,\n            8.3892e-02, -2.1345e-02],\n          [-3.7410e-02, -2.4476e-02, -4.8010e-02,  ...,  5.5364e-02,\n            3.8971e-02, -3.2366e-02],\n          ...,\n          [-9.9463e-02,  3.1839e-02, -4.6333e-04,  ...,  3.3809e-02,\n           -9.3263e-03,  1.7638e-04],\n          [-2.5693e-02,  2.4686e-03,  1.9405e-02,  ..., -5.7410e-02,\n            6.9239e-02, -5.8620e-02],\n          [ 2.0944e-02,  8.7198e-03, -3.9972e-02,  ..., -1.0844e-02,\n            6.2951e-02, -1.7264e-01]],\n\n         [[-3.0622e-03,  2.0083e-03,  1.0507e-03,  ...,  1.4379e-02,\n            1.1172e-02,  3.0350e-03],\n          [ 4.7033e-02,  7.1351e-02, -2.1504e-01,  ...,  7.5388e-02,\n            1.0522e-01,  1.9773e-01],\n          [ 2.3832e-02,  8.7062e-02, -6.7360e-02,  ...,  4.3810e-02,\n            2.4832e-02,  2.2353e-01],\n          ...,\n          [-1.1813e-01, -5.6155e-02,  3.3382e-01,  ...,  7.9226e-02,\n            4.3544e-01,  5.1439e-02],\n          [-7.2445e-02,  1.0965e-01,  1.7977e-01,  ...,  1.3254e-01,\n            7.9630e-02, -1.0954e-02],\n          [-7.3850e-02,  1.5209e-01,  3.7612e-02,  ..., -2.2512e-02,\n            6.2101e-01,  9.2086e-02]],\n\n         [[ 1.3185e-02,  1.2329e-02,  1.6595e-03,  ..., -9.7258e-03,\n            4.0440e-03, -3.5216e-02],\n          [-1.3325e-01, -1.6050e-01,  5.3454e-02,  ..., -3.5692e-01,\n           -3.8273e-01, -3.1767e-01],\n          [ 1.9860e-01, -1.8844e-01, -1.6387e-01,  ..., -2.5862e-01,\n           -7.7880e-02, -4.5695e-01],\n          ...,\n          [ 3.0274e-01,  3.2469e-01, -1.7156e-01,  ...,  1.0631e-01,\n            2.7929e-01, -9.0036e-02],\n          [-5.7512e-01,  9.2917e-01,  5.3664e-01,  ..., -1.2767e+00,\n           -4.1243e-02, -9.5903e-02],\n          [ 1.5448e-01,  9.4318e-01, -2.0159e-01,  ..., -5.9708e-01,\n           -5.2408e-01,  6.9879e-01]],\n\n         ...,\n\n         [[ 2.5122e-03, -1.0263e-02,  2.2052e-03,  ..., -3.6513e-03,\n           -6.7246e-03,  1.2602e-02],\n          [-9.0955e-03, -6.8051e-02,  1.6752e-01,  ...,  6.7797e-02,\n           -5.2737e-02, -3.2865e-01],\n          [ 1.2088e-01,  3.6821e-01,  3.6515e-01,  ...,  9.5479e-02,\n            2.0624e-01,  4.6545e-02],\n          ...,\n          [ 2.7833e-01,  1.1531e-01,  4.2665e-02,  ..., -9.2720e-02,\n            6.7518e-02,  9.0842e-03],\n          [ 6.6449e-02, -1.0742e-01,  1.2539e-01,  ..., -5.1468e-02,\n           -1.1843e-01,  6.3816e-02],\n          [-9.5892e-02, -5.8650e-02,  1.3276e-01,  ...,  3.7146e-02,\n           -1.3054e-01,  1.9920e-02]],\n\n         [[-6.1081e-04, -7.3253e-03,  1.5451e-02,  ..., -1.0520e-02,\n           -9.2689e-03, -1.2797e-03],\n          [-9.2337e-01,  5.6056e-02, -2.1513e-01,  ..., -6.5147e-01,\n            3.1062e-03, -1.1168e-01],\n          [ 8.3409e-03,  7.8745e-02, -1.4051e-01,  ..., -2.9790e-01,\n           -2.8397e-01, -2.4411e-01],\n          ...,\n          [ 6.6267e-03, -4.1720e-01,  3.7119e-01,  ...,  2.7633e-01,\n            5.8072e-02, -9.6333e-02],\n          [-8.4918e-02, -2.9165e-01,  1.7258e-02,  ..., -6.8007e-02,\n           -2.4032e-01, -6.9962e-02],\n          [ 2.1609e-01, -4.7461e-01,  4.4101e-01,  ..., -7.7623e-01,\n            1.0486e-01,  7.2484e-03]],\n\n         [[ 5.2475e-03,  1.6704e-03,  3.6839e-03,  ...,  2.7860e-03,\n           -3.7045e-03, -9.9724e-04],\n          [ 1.4330e-02, -6.6967e-02, -4.6438e-02,  ..., -9.7922e-04,\n           -3.0531e-02,  2.1759e-02],\n          [ 4.8856e-03, -1.1833e-01, -8.2106e-02,  ..., -1.9288e-01,\n            1.4094e-02, -1.1605e-01],\n          ...,\n          [ 8.6811e-03,  5.8269e-02, -3.6851e-02,  ...,  7.5888e-02,\n            5.5612e-02,  2.8441e-02],\n          [-3.4085e-02,  7.5187e-02,  1.2818e-02,  ...,  1.3300e-01,\n            3.7873e-02,  2.3441e-02],\n          [ 9.8100e-02,  8.6456e-03,  8.3603e-02,  ...,  9.0046e-02,\n            4.2294e-02,  9.9668e-02]]],\n\n\n        ...,\n\n\n        [[[-1.1004e-03,  1.3470e-03, -2.9116e-04,  ..., -9.3921e-03,\n           -1.1889e-02, -3.8048e-03],\n          [ 1.7945e-02, -2.3853e-02,  5.5122e-02,  ...,  4.7621e-03,\n            1.2285e-02, -2.0447e-02],\n          [ 6.5531e-03,  7.3279e-02,  1.0465e-01,  ..., -1.2368e-02,\n           -1.7896e-02,  8.1107e-02],\n          ...,\n          [-4.3909e-02, -1.7197e-02,  1.0368e-01,  ..., -9.1027e-02,\n            5.3178e-02, -1.7773e-01],\n          [-2.7685e-04,  2.1737e-02,  8.6017e-02,  ..., -6.5808e-02,\n            7.4325e-03, -1.6043e-01],\n          [-5.7395e-02, -3.8018e-02,  1.1842e-02,  ...,  1.7463e-02,\n            5.5881e-02, -6.5074e-02]],\n\n         [[-2.0279e-03, -1.4192e-04,  6.9266e-03,  ...,  1.1614e-02,\n            1.3148e-02, -3.4460e-03],\n          [-4.4661e-02, -7.3244e-02,  7.1118e-02,  ..., -3.9577e-02,\n           -1.8523e-01, -6.3158e-02],\n          [-1.0627e-01,  8.0536e-02, -1.7327e-01,  ...,  1.3345e-01,\n           -4.8299e-01,  8.8104e-02],\n          ...,\n          [ 1.3277e-02, -1.7222e-01,  6.9233e-02,  ...,  1.4876e-01,\n            4.3835e-01, -1.2350e-02],\n          [-2.9643e-04, -1.1228e-01,  1.3419e-02,  ...,  3.9063e-04,\n           -2.1940e-01, -1.1649e-01],\n          [ 6.5943e-02, -6.5192e-02,  3.6587e-01,  ..., -3.9867e-02,\n            3.5436e-01, -1.3197e-01]],\n\n         [[ 2.7505e-02,  3.0663e-02,  1.9819e-03,  ...,  1.4001e-02,\n           -5.8033e-03, -1.9666e-02],\n          [ 1.8599e-01,  5.4259e-01,  3.8833e-01,  ...,  9.4125e-01,\n            1.5858e-01,  1.2499e-02],\n          [-1.5762e-01,  3.8899e-01, -1.8474e-01,  ...,  6.0417e-01,\n            1.6291e-01, -7.1828e-02],\n          ...,\n          [-5.7859e-01,  1.6720e-01,  9.9481e-02,  ...,  9.0738e-02,\n            5.2027e-01,  7.8578e-01],\n          [ 7.4437e-01, -1.2977e-01,  4.5031e-01,  ..., -8.4629e-01,\n           -5.3666e-01, -3.2963e-01],\n          [ 2.1762e-01,  1.4665e-01,  4.8005e-01,  ..., -4.4412e-02,\n            4.0530e-01,  2.1801e-01]],\n\n         ...,\n\n         [[-1.9234e-03, -5.3026e-03, -1.0056e-03,  ..., -1.8501e-04,\n           -2.5461e-03,  5.3369e-03],\n          [ 1.8589e-01,  4.6541e-02,  1.0579e-01,  ..., -9.9257e-02,\n            8.9756e-02,  1.5220e-01],\n          [-1.3065e-01, -9.8007e-02, -7.7110e-02,  ..., -1.8447e-01,\n            1.0060e-01, -8.2542e-02],\n          ...,\n          [ 1.0586e-01, -1.8447e-01, -2.1826e-01,  ...,  1.1813e-01,\n            2.7642e-02,  9.6656e-02],\n          [ 1.3543e-01,  2.0921e-01, -4.5170e-02,  ..., -7.3373e-02,\n            1.2190e-01,  2.9022e-01],\n          [-2.6349e-02,  6.0584e-02,  2.2480e-01,  ..., -8.0556e-02,\n            4.3182e-02, -3.0702e-01]],\n\n         [[ 4.1524e-03, -1.4360e-02,  2.7534e-02,  ..., -1.3403e-03,\n           -1.0387e-02,  7.3054e-03],\n          [ 4.9901e-01,  8.7725e-02, -1.1138e-01,  ..., -2.1515e-02,\n            3.9810e-01, -1.6750e-01],\n          [ 5.5951e-01,  2.7052e-01, -8.3974e-01,  ...,  6.5351e-01,\n            3.7885e-01,  5.0762e-02],\n          ...,\n          [ 1.9746e-01, -2.8743e-01, -1.5531e-01,  ...,  4.5688e-01,\n           -6.6949e-01,  2.0343e-01],\n          [ 3.1276e-01, -2.5108e-01, -8.7388e-02,  ...,  2.8024e-01,\n           -6.8903e-01,  1.5953e-01],\n          [ 1.3923e-01, -3.7139e-01, -8.9424e-02,  ..., -3.9262e-01,\n            1.4410e-01,  6.6272e-02]],\n\n         [[ 7.3724e-03,  1.0826e-03,  6.1202e-03,  ...,  1.4884e-03,\n            4.0766e-03, -1.3582e-03],\n          [-1.9754e-02,  1.2565e-01,  1.2382e-02,  ..., -4.2666e-04,\n            6.8165e-02,  2.4727e-02],\n          [ 9.2192e-04,  8.7182e-02, -3.0715e-02,  ...,  6.6621e-02,\n            6.0170e-02, -4.2617e-02],\n          ...,\n          [ 1.0445e-01,  1.8483e-02,  6.8546e-02,  ...,  1.0070e-01,\n           -6.1839e-02, -2.5164e-03],\n          [ 5.3400e-02,  3.4972e-02,  5.4393e-02,  ...,  4.1514e-02,\n           -5.6230e-02,  3.1753e-02],\n          [-3.1432e-02,  2.5242e-02,  2.2310e-02,  ...,  3.4018e-02,\n            1.5620e-02, -5.3537e-02]]],\n\n\n        [[[ 7.5895e-04, -8.0591e-04,  1.2639e-04,  ..., -1.0551e-02,\n           -8.2634e-03, -4.3127e-03],\n          [ 2.2346e-02,  1.0779e-01, -7.7186e-03,  ..., -4.4355e-02,\n           -6.2076e-02, -5.5847e-02],\n          [ 2.1628e-02,  1.4213e-01,  5.2400e-02,  ..., -2.1537e-02,\n           -9.4610e-02,  1.7617e-02],\n          ...,\n          [-9.1437e-03, -3.6002e-02,  4.5965e-02,  ..., -5.2296e-02,\n            4.8730e-02,  1.4353e-02],\n          [-6.8753e-02,  5.7309e-04, -7.5437e-03,  ..., -1.0080e-02,\n           -4.7401e-02,  8.1920e-02],\n          [-2.1242e-03,  1.0472e-02,  8.2420e-03,  ..., -2.7521e-02,\n            1.2265e-01,  1.1711e-01]],\n\n         [[-1.8536e-03,  1.3460e-03,  8.3156e-03,  ...,  1.8049e-02,\n            9.2816e-03, -5.9402e-03],\n          [-1.9414e-01, -5.1005e-02,  1.1501e-01,  ...,  3.5670e-02,\n           -4.0317e-01, -9.3594e-02],\n          [-7.8094e-02, -1.1056e-01,  1.4850e-01,  ..., -3.2491e-02,\n           -5.5031e-01, -4.7581e-02],\n          ...,\n          [-6.5067e-02, -1.7171e-01,  1.7496e-01,  ..., -5.1762e-02,\n            2.0906e-01, -1.5124e-01],\n          [ 6.5993e-04, -1.8034e-01,  2.8384e-01,  ..., -6.1998e-03,\n            3.2526e-01,  7.2884e-02],\n          [ 3.7814e-02, -1.4609e-01,  2.3988e-02,  ..., -4.8444e-02,\n            3.7599e-01,  4.5855e-02]],\n\n         [[ 9.4980e-05,  1.7966e-02, -5.3293e-03,  ...,  1.3930e-03,\n            3.0255e-02, -3.2563e-02],\n          [-3.8567e-01,  3.7798e-01,  3.0996e-01,  ..., -3.2736e-01,\n           -1.6500e-01, -4.8438e-01],\n          [-3.5527e-01,  3.2211e-01, -2.7769e-01,  ..., -3.2855e-01,\n            4.7894e-01,  3.2281e-02],\n          ...,\n          [ 5.4675e-01,  1.3951e-02, -5.0804e-01,  ...,  3.9720e-01,\n           -8.3486e-02,  6.6328e-01],\n          [-4.4949e-01,  3.2705e-01, -2.3865e-01,  ...,  2.1286e-01,\n           -3.7902e-01,  5.2304e-01],\n          [-2.8043e-01, -5.0309e-01, -3.4394e-01,  ...,  8.7894e-02,\n           -1.0313e-01,  4.5171e-01]],\n\n         ...,\n\n         [[-5.3751e-03,  2.3507e-04, -5.1949e-03,  ..., -5.1251e-03,\n           -1.8306e-03,  8.8268e-03],\n          [-1.0381e-02, -1.1037e-01,  1.8192e-01,  ..., -1.3335e-01,\n            9.5262e-02, -1.6024e-03],\n          [-1.3309e-01, -2.0280e-02,  2.0849e-01,  ..., -1.4995e-01,\n            1.0736e-01, -1.7703e-01],\n          ...,\n          [ 1.2616e-01, -7.2101e-02,  2.0151e-01,  ..., -1.2760e-02,\n            1.0019e-01, -1.6770e-01],\n          [-6.0161e-02, -6.2131e-02, -2.4855e-01,  ..., -1.4186e-01,\n            1.2619e-01, -1.3200e-01],\n          [-4.3003e-02, -4.6648e-02, -4.4556e-01,  ...,  1.2094e-01,\n           -2.4949e-02, -6.5680e-02]],\n\n         [[-5.1558e-03, -7.9876e-03,  2.2857e-02,  ..., -4.2506e-03,\n           -1.1705e-02,  1.0632e-02],\n          [ 4.2911e-01, -5.0829e-02,  9.9730e-02,  ...,  1.9139e-01,\n            5.6582e-01, -2.5036e-01],\n          [ 3.2745e-01,  1.6712e-01,  4.1075e-02,  ...,  6.4766e-01,\n            5.4739e-01, -4.2455e-01],\n          ...,\n          [ 4.0752e-01,  8.6477e-02, -4.5401e-01,  ..., -3.3019e-01,\n           -2.4923e-01,  2.9515e-02],\n          [ 1.4749e-02, -8.3375e-02, -2.3385e-01,  ..., -3.3503e-01,\n           -4.8643e-02,  1.8184e-01],\n          [ 1.6365e-01, -1.8112e-01,  1.1476e-01,  ..., -2.6439e-01,\n           -9.6515e-02, -1.6914e-02]],\n\n         [[ 5.0850e-03,  7.3640e-03,  6.1663e-03,  ...,  8.6112e-04,\n            1.9408e-03, -3.2689e-03],\n          [ 2.0954e-02,  8.3511e-02,  2.8794e-02,  ..., -7.0244e-02,\n            2.0327e-02, -3.8749e-02],\n          [ 5.4317e-02,  9.8321e-02, -5.0161e-02,  ...,  2.0555e-02,\n           -3.0564e-02, -2.0975e-02],\n          ...,\n          [ 1.7833e-02, -2.9093e-02, -5.7625e-02,  ...,  1.6875e-02,\n            6.6941e-03, -5.2086e-02],\n          [ 1.1936e-01,  6.4370e-02,  4.1323e-02,  ...,  1.1247e-01,\n            7.0505e-02,  2.3851e-02],\n          [ 9.7372e-03,  2.1558e-02,  3.1140e-02,  ...,  4.4662e-02,\n           -2.8584e-02,  9.5975e-02]]],\n\n\n        [[[ 4.4304e-04,  4.0498e-03, -1.4642e-03,  ..., -9.6124e-03,\n           -1.1912e-02, -6.5324e-03],\n          [-1.1122e-01,  8.1293e-02, -2.1440e-02,  ...,  9.7751e-02,\n           -1.5828e-01, -1.5053e-02],\n          [-3.9033e-02,  1.2606e-01,  2.3342e-02,  ...,  1.9341e-02,\n           -1.1564e-01, -1.2334e-01],\n          ...,\n          [ 3.5253e-02,  7.4787e-02,  1.1502e-01,  ..., -4.2409e-02,\n            2.4502e-02, -1.3151e-01],\n          [-7.6224e-02,  3.4497e-02,  2.6929e-02,  ...,  2.2575e-02,\n           -1.5008e-01, -1.0662e-01],\n          [-1.5518e-02,  9.2889e-03,  6.6658e-02,  ...,  4.4768e-02,\n           -4.3101e-02, -6.7347e-02]],\n\n         [[-8.2246e-03, -1.2549e-03, -6.3177e-04,  ...,  1.1213e-02,\n            1.3743e-02, -1.2036e-04],\n          [-5.0323e-02, -4.5139e-02,  1.3090e-02,  ..., -3.2682e-02,\n            1.4064e-01,  4.4278e-02],\n          [-8.6544e-02, -3.1447e-02, -1.1022e-01,  ..., -1.7837e-01,\n           -1.8624e-01, -5.9432e-02],\n          ...,\n          [-9.3399e-02, -1.3363e-01,  5.9831e-02,  ..., -1.7760e-02,\n            2.5487e-01, -2.0147e-02],\n          [-3.0410e-02, -5.1399e-02,  8.7495e-02,  ..., -2.0387e-01,\n            3.5966e-01, -6.6020e-02],\n          [ 5.3965e-02, -1.0022e-01,  2.6393e-01,  ..., -9.2818e-02,\n            5.4163e-01, -1.7177e-03]],\n\n         [[-7.0620e-03,  1.4944e-02, -4.7332e-03,  ..., -5.1596e-03,\n            4.5748e-03, -2.1413e-02],\n          [ 1.3224e+00, -1.4017e-01, -4.7988e-01,  ..., -4.4620e-01,\n           -5.5145e-01, -4.4926e-01],\n          [ 4.0444e-01, -3.4407e-01, -1.7214e-01,  ...,  5.4550e-01,\n           -9.4210e-01, -1.0331e+00],\n          ...,\n          [-2.0559e-01,  3.8871e-01, -7.8334e-01,  ...,  2.0852e-01,\n           -4.3260e-01,  1.7510e+00],\n          [ 3.5077e-01, -1.8626e-01, -1.2774e-01,  ...,  8.0317e-02,\n           -3.5195e-01,  1.0453e+00],\n          [-5.8166e-01, -5.4282e-02, -2.8338e-01,  ...,  1.0122e-01,\n           -3.2446e-01,  1.9275e+00]],\n\n         ...,\n\n         [[ 4.7252e-04, -1.6384e-02,  1.1759e-02,  ..., -5.5384e-03,\n           -4.1605e-03,  1.1635e-02],\n          [ 8.6477e-02,  4.4977e-02,  3.6548e-02,  ...,  5.2225e-03,\n           -4.9467e-02,  2.4419e-02],\n          [ 7.2765e-02,  2.7119e-02,  2.5753e-01,  ...,  2.3548e-02,\n            7.0923e-03,  4.2378e-02],\n          ...,\n          [ 1.9924e-02, -6.6116e-02,  2.2365e-01,  ...,  1.9864e-01,\n           -2.3095e-02, -4.6782e-02],\n          [ 3.2655e-02, -2.7448e-01, -2.7472e-02,  ...,  1.5257e-01,\n            9.6176e-02,  8.3514e-02],\n          [-8.3536e-02, -7.7594e-02,  1.6105e-01,  ...,  5.6917e-02,\n           -3.2283e-02,  8.0781e-03]],\n\n         [[ 2.1886e-02, -4.6003e-03,  2.3247e-02,  ...,  1.7014e-03,\n           -2.2660e-02,  2.3320e-03],\n          [ 2.7114e-01, -8.8055e-02,  3.4542e-01,  ..., -1.1355e-01,\n           -1.8775e-01,  1.2812e-01],\n          [ 7.8987e-01,  2.2401e-01,  1.5250e-01,  ...,  3.2683e-01,\n            2.8878e-01, -2.6615e-02],\n          ...,\n          [-2.6531e-01, -2.2607e-01,  3.8150e-01,  ..., -4.9765e-01,\n            8.2023e-02,  2.9743e-01],\n          [-2.5501e-01,  3.5662e-02,  2.4314e-02,  ...,  1.9666e-02,\n           -1.4888e-02,  3.2250e-01],\n          [ 1.1493e-01,  2.9259e-02,  2.0720e-01,  ..., -4.5437e-01,\n           -3.3389e-01,  1.1400e-01]],\n\n         [[ 6.1202e-03,  6.1161e-03,  2.8392e-03,  ...,  9.6664e-03,\n           -6.1220e-03,  3.9458e-04],\n          [ 1.6154e-02,  2.5098e-02, -5.4607e-02,  ...,  4.1514e-02,\n           -4.4471e-02, -2.1845e-02],\n          [ 5.9269e-02,  4.1166e-02, -2.8204e-02,  ..., -1.5380e-02,\n           -1.0281e-02, -7.3791e-02],\n          ...,\n          [-7.9806e-03,  2.2864e-02, -3.6994e-02,  ...,  1.4909e-01,\n            4.2976e-02,  1.7424e-02],\n          [ 9.0579e-02,  1.2558e-01,  9.3110e-02,  ...,  1.3509e-01,\n           -5.9059e-03,  6.7982e-02],\n          [ 1.9191e-02,  3.4537e-02,  6.4414e-02,  ...,  1.6057e-01,\n           -3.1739e-02,  3.7927e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[-8.6723e-01,  1.1392e-02, -4.0018e-01,  ..., -2.3547e-01,\n           -3.9650e-01,  3.2682e-03],\n          [-5.5617e-02, -6.8698e-01, -1.2188e+00,  ...,  5.1063e-01,\n           -1.2167e+00, -6.8970e-01],\n          [-2.8951e-02, -7.1349e-01, -1.2455e+00,  ...,  5.3720e-01,\n           -1.2434e+00, -7.1619e-01],\n          ...,\n          [-7.0729e-02, -6.7687e-01, -1.2034e+00,  ...,  4.9840e-01,\n           -1.2013e+00, -6.7983e-01],\n          [-7.6136e-02, -6.6996e-01, -1.1981e+00,  ...,  4.9217e-01,\n           -1.1960e+00, -6.7285e-01],\n          [-9.4673e-02, -6.5348e-01, -1.1794e+00,  ...,  4.7485e-01,\n           -1.1773e+00, -6.5645e-01]],\n\n         [[ 1.2085e-01, -2.4571e-02, -6.7766e-01,  ..., -8.9603e-02,\n            1.6772e-01,  3.1264e-01],\n          [-8.6054e-01,  7.1611e-01, -9.6982e-02,  ...,  7.7451e-01,\n            1.1576e+00, -8.0740e-01],\n          [-9.0410e-01,  7.5655e-01, -5.9012e-02,  ...,  8.1660e-01,\n            1.2013e+00, -8.5273e-01],\n          ...,\n          [-7.9475e-01,  6.6777e-01, -1.3343e-01,  ...,  7.1716e-01,\n            1.0922e+00, -7.3281e-01],\n          [-7.9116e-01,  6.6157e-01, -1.4210e-01,  ...,  7.1247e-01,\n            1.0871e+00, -7.2876e-01],\n          [-7.8015e-01,  6.5282e-01, -1.5004e-01,  ...,  7.0266e-01,\n            1.0769e+00, -7.1709e-01]],\n\n         [[-1.9233e-01, -3.0821e-01,  7.6750e-01,  ...,  2.0984e-01,\n           -2.2150e-01, -3.8371e-01],\n          [ 6.1321e-01,  5.4156e-01,  1.6946e-02,  ..., -6.1968e-01,\n            5.2530e-01,  4.5132e-01],\n          [ 6.2600e-01,  5.5455e-01,  4.3900e-03,  ..., -6.3258e-01,\n            5.3784e-01,  4.6423e-01],\n          ...,\n          [ 5.6507e-01,  4.9106e-01,  6.2071e-02,  ..., -5.7025e-01,\n            4.8041e-01,  4.0156e-01],\n          [ 5.5847e-01,  4.8450e-01,  6.8680e-02,  ..., -5.6367e-01,\n            4.7381e-01,  3.9498e-01],\n          [ 5.5289e-01,  4.7820e-01,  7.3440e-02,  ..., -5.5771e-01,\n            4.6908e-01,  3.8894e-01]],\n\n         ...,\n\n         [[ 2.7618e-01,  6.8182e-01,  7.8458e-01,  ..., -1.1797e-01,\n           -4.1521e-01, -3.2264e-01],\n          [-4.8314e-01,  4.8735e-02,  2.9640e-02,  ..., -9.3483e-01,\n            3.7044e-01,  5.0844e-01],\n          [-4.9893e-01,  3.2448e-02,  1.3816e-02,  ..., -9.5043e-01,\n            3.8614e-01,  5.2400e-01],\n          ...,\n          [-4.5015e-01,  7.5538e-02,  6.2406e-02,  ..., -8.9911e-01,\n            3.3616e-01,  4.7205e-01],\n          [-4.4477e-01,  8.0817e-02,  6.7787e-02,  ..., -8.9366e-01,\n            3.3075e-01,  4.6658e-01],\n          [-4.1967e-01,  1.0233e-01,  9.2774e-02,  ..., -8.6695e-01,\n            3.0492e-01,  4.3948e-01]],\n\n         [[-4.7187e-01, -9.5862e-01, -6.8326e-02,  ..., -9.4518e-01,\n            4.2062e-01, -1.8579e+00],\n          [-2.9678e-01,  5.8131e-01, -4.3358e-01,  ...,  1.4780e+00,\n           -1.8342e+00,  9.8098e-01],\n          [-3.0780e-01,  6.2577e-01, -4.0797e-01,  ...,  1.6397e+00,\n           -1.9677e+00,  9.7995e-01],\n          ...,\n          [-4.1996e-01, -2.1261e-01, -2.3884e-01,  ..., -3.0793e-01,\n           -2.4037e-01,  1.2810e+00],\n          [-4.1901e-01, -1.9193e-01, -2.4023e-01,  ..., -1.7249e-01,\n           -3.4633e-01,  1.1767e+00],\n          [-4.2020e-01, -1.2896e-01, -2.3504e-01,  ...,  3.7549e-02,\n           -5.1335e-01,  1.0384e+00]],\n\n         [[-4.1956e-01,  1.6109e+00,  4.5483e-01,  ...,  9.2694e-02,\n            4.5513e-01, -1.0385e+00],\n          [ 4.2056e-01,  7.6103e-01, -2.9318e-01,  ..., -7.2575e-01,\n            1.2031e+00, -3.0680e-01],\n          [ 4.4878e-01,  7.3279e-01, -3.2110e-01,  ..., -7.5390e-01,\n            1.2310e+00, -2.7894e-01],\n          ...,\n          [ 4.0466e-01,  7.7751e-01, -2.8222e-01,  ..., -7.1102e-01,\n            1.1921e+00, -3.1693e-01],\n          [ 4.0594e-01,  7.7618e-01, -2.8288e-01,  ..., -7.1217e-01,\n            1.1927e+00, -3.1639e-01],\n          [ 3.7737e-01,  8.0497e-01, -2.5669e-01,  ..., -6.8416e-01,\n            1.1666e+00, -3.4214e-01]]],\n\n\n        [[[-8.6740e-01,  1.1531e-02, -4.0001e-01,  ..., -2.3562e-01,\n           -3.9633e-01,  3.4055e-03],\n          [-8.3775e-02, -6.5867e-01, -1.1906e+00,  ...,  4.8238e-01,\n           -1.1886e+00, -6.6139e-01],\n          [-1.0040e-01, -6.4557e-01, -1.1738e+00,  ...,  4.6777e-01,\n           -1.1717e+00, -6.4847e-01],\n          ...,\n          [-2.6229e-02, -7.1562e-01, -1.2483e+00,  ...,  5.3960e-01,\n           -1.2462e+00, -7.1830e-01],\n          [-4.4151e-02, -6.9769e-01, -1.2303e+00,  ...,  5.2164e-01,\n           -1.2283e+00, -7.0038e-01],\n          [-6.3583e-02, -6.8065e-01, -1.2107e+00,  ...,  5.0361e-01,\n           -1.2087e+00, -6.8344e-01]],\n\n         [[ 1.2225e-01, -2.5793e-02, -6.7873e-01,  ..., -9.0919e-02,\n            1.6632e-01,  3.1412e-01],\n          [-8.2663e-01,  6.8659e-01, -1.2319e-01,  ...,  7.4267e-01,\n            1.1229e+00, -7.7060e-01],\n          [-7.7774e-01,  6.4581e-01, -1.5735e-01,  ...,  6.9752e-01,\n            1.0748e+00, -7.1849e-01],\n          ...,\n          [-8.3144e-01,  6.9361e-01, -1.1480e-01,  ...,  7.4856e-01,\n            1.1290e+00, -7.7550e-01],\n          [-8.4004e-01,  6.9963e-01, -1.1081e-01,  ...,  7.5598e-01,\n            1.1374e+00, -7.8508e-01],\n          [-8.1593e-01,  6.8131e-01, -1.2483e-01,  ...,  7.3464e-01,\n            1.1128e+00, -7.5746e-01]],\n\n         [[-1.9311e-01, -3.0899e-01,  7.6827e-01,  ...,  2.1062e-01,\n           -2.2227e-01, -3.8449e-01],\n          [ 6.0315e-01,  5.3228e-01,  2.7857e-02,  ..., -6.1002e-01,\n            5.1433e-01,  4.4175e-01],\n          [ 5.6851e-01,  4.9549e-01,  5.9888e-02,  ..., -5.7424e-01,\n            4.8248e-01,  4.0569e-01],\n          ...,\n          [ 5.9687e-01,  5.2477e-01,  3.2709e-02,  ..., -6.0310e-01,\n            5.0961e-01,  4.3463e-01],\n          [ 6.1231e-01,  5.4034e-01,  1.7430e-02,  ..., -6.1861e-01,\n            5.2487e-01,  4.5017e-01],\n          [ 5.8142e-01,  5.0849e-01,  4.7112e-02,  ..., -5.8719e-01,\n            4.9528e-01,  4.1864e-01]],\n\n         ...,\n\n         [[ 2.7719e-01,  6.8278e-01,  7.8558e-01,  ..., -1.1695e-01,\n           -4.1622e-01, -3.2366e-01],\n          [-4.5501e-01,  7.4386e-02,  5.7684e-02,  ..., -9.0581e-01,\n            3.4188e-01,  4.7922e-01],\n          [-4.2278e-01,  1.0140e-01,  8.9755e-02,  ..., -8.7121e-01,\n            3.0856e-01,  4.4405e-01],\n          ...,\n          [-4.6994e-01,  6.1316e-02,  4.2817e-02,  ..., -9.2147e-01,\n            3.5713e-01,  4.9507e-01],\n          [-4.6470e-01,  6.7513e-02,  4.8075e-02,  ..., -9.1663e-01,\n            3.5208e-01,  4.9032e-01],\n          [-4.4411e-01,  8.3946e-02,  6.8541e-02,  ..., -8.9422e-01,\n            3.3064e-01,  4.6745e-01]],\n\n         [[-4.7288e-01, -9.6560e-01, -6.7163e-02,  ..., -9.6152e-01,\n            4.3442e-01, -1.8502e+00],\n          [-3.1691e-01,  5.7365e-01, -3.9735e-01,  ...,  1.5779e+00,\n           -1.8925e+00,  9.1140e-01],\n          [-3.2672e-01,  4.1147e-01, -3.8685e-01,  ...,  1.1745e+00,\n           -1.5425e+00,  1.0629e+00],\n          ...,\n          [-4.2586e-01, -4.1912e-01, -2.1929e-01,  ..., -8.8750e-01,\n            2.7873e-01,  1.7758e+00],\n          [-4.1702e-01, -2.3507e-01, -2.4571e-01,  ..., -4.7523e-01,\n           -9.3332e-02,  1.6455e+00],\n          [-4.1414e-01, -2.5722e-01, -2.4028e-01,  ..., -3.8616e-01,\n           -1.3673e-01,  1.4405e+00]],\n\n         [[-4.2002e-01,  1.6113e+00,  4.5529e-01,  ...,  9.3152e-02,\n            4.5467e-01, -1.0390e+00],\n          [ 4.2536e-01,  7.5628e-01, -2.9822e-01,  ..., -7.3061e-01,\n            1.2081e+00, -3.0172e-01],\n          [ 3.8906e-01,  7.9285e-01, -2.6458e-01,  ..., -6.9492e-01,\n            1.1745e+00, -3.3488e-01],\n          ...,\n          [ 4.2201e-01,  7.5975e-01, -2.9569e-01,  ..., -7.2745e-01,\n            1.2055e+00, -3.0414e-01],\n          [ 4.1801e-01,  7.6370e-01, -2.9102e-01,  ..., -7.2327e-01,\n            1.2009e+00, -3.0891e-01],\n          [ 3.9907e-01,  7.8291e-01, -2.7491e-01,  ..., -7.0501e-01,\n            1.1848e+00, -3.2453e-01]]],\n\n\n        [[[-8.6951e-01,  1.3360e-02, -3.9788e-01,  ..., -2.3757e-01,\n           -3.9420e-01,  5.2216e-03],\n          [-7.5005e-02, -6.7179e-01, -1.1992e+00,  ...,  4.9368e-01,\n           -1.1970e+00, -6.7471e-01],\n          [-1.1332e-01, -6.3898e-01, -1.1605e+00,  ...,  4.5853e-01,\n           -1.1583e+00, -6.4216e-01],\n          ...,\n          [-7.4838e-02, -6.6995e-01, -1.1995e+00,  ...,  4.9269e-01,\n           -1.1974e+00, -6.7278e-01],\n          [-7.2172e-02, -6.7419e-01, -1.2020e+00,  ...,  4.9627e-01,\n           -1.1999e+00, -6.7710e-01],\n          [-6.1641e-02, -6.8447e-01, -1.2126e+00,  ...,  5.0663e-01,\n           -1.2105e+00, -6.8736e-01]],\n\n         [[ 1.2422e-01, -2.7237e-02, -6.7981e-01,  ..., -9.2641e-02,\n            1.6443e-01,  3.1629e-01],\n          [-8.3522e-01,  6.9831e-01, -1.1062e-01,  ...,  7.5303e-01,\n            1.1316e+00, -7.7691e-01],\n          [-7.6702e-01,  6.4698e-01, -1.4928e-01,  ...,  6.9272e-01,\n            1.0644e+00, -7.0137e-01],\n          ...,\n          [-7.8017e-01,  6.5132e-01, -1.5089e-01,  ...,  7.0161e-01,\n            1.0775e+00, -7.1923e-01],\n          [-7.8717e-01,  6.5846e-01, -1.4363e-01,  ...,  7.0872e-01,\n            1.0833e+00, -7.2491e-01],\n          [-7.8492e-01,  6.5592e-01, -1.4635e-01,  ...,  7.0630e-01,\n            1.0823e+00, -7.2409e-01]],\n\n         [[-1.9520e-01, -3.1118e-01,  7.7023e-01,  ...,  2.1277e-01,\n           -2.2422e-01, -3.8664e-01],\n          [ 5.7909e-01,  5.0601e-01,  4.9266e-02,  ..., -5.8479e-01,\n            4.9309e-01,  4.1623e-01],\n          [ 5.2262e-01,  4.4681e-01,  1.0226e-01,  ..., -5.2683e-01,\n            4.4036e-01,  3.5793e-01],\n          ...,\n          [ 5.7187e-01,  4.9860e-01,  5.6216e-02,  ..., -5.7746e-01,\n            4.8620e-01,  4.0887e-01],\n          [ 5.5377e-01,  4.7984e-01,  7.3479e-02,  ..., -5.5899e-01,\n            4.6900e-01,  3.9032e-01],\n          [ 5.6454e-01,  4.9112e-01,  6.3337e-02,  ..., -5.7005e-01,\n            4.7909e-01,  4.0143e-01]],\n\n         ...,\n\n         [[ 2.7861e-01,  6.8390e-01,  7.8700e-01,  ..., -1.1539e-01,\n           -4.1771e-01, -3.2526e-01],\n          [-4.5589e-01,  7.1166e-02,  5.6715e-02,  ..., -9.0543e-01,\n            3.4220e-01,  4.7853e-01],\n          [-3.9554e-01,  1.2242e-01,  1.1677e-01,  ..., -8.4112e-01,\n            2.8002e-01,  4.1324e-01],\n          ...,\n          [-4.3986e-01,  8.7107e-02,  7.2747e-02,  ..., -8.8934e-01,\n            3.2612e-01,  4.6239e-01],\n          [-4.3921e-01,  8.6924e-02,  7.3360e-02,  ..., -8.8827e-01,\n            3.2528e-01,  4.6122e-01],\n          [-4.3230e-01,  9.4421e-02,  8.0293e-02,  ..., -8.8171e-01,\n            3.1852e-01,  4.5476e-01]],\n\n         [[-4.7270e-01, -9.6477e-01, -6.7133e-02,  ..., -9.5796e-01,\n            4.3344e-01, -1.8605e+00],\n          [-3.5966e-01,  2.4243e-01, -3.3037e-01,  ...,  8.4043e-01,\n           -1.2634e+00,  1.1567e+00],\n          [-3.3774e-01,  3.1090e-01, -3.6379e-01,  ...,  9.4890e-01,\n           -1.3573e+00,  1.0537e+00],\n          ...,\n          [-4.0856e-01, -4.3349e-02, -2.6445e-01,  ...,  1.6582e-01,\n           -6.3481e-01,  1.1223e+00],\n          [-4.1439e-01, -1.5516e-01, -2.4522e-01,  ..., -1.2663e-01,\n           -3.8648e-01,  1.2825e+00],\n          [-4.4049e-01, -3.8048e-01, -2.0161e-01,  ..., -7.3242e-01,\n            1.4540e-01,  1.5048e+00]],\n\n         [[-4.2194e-01,  1.6133e+00,  4.5699e-01,  ...,  9.5021e-02,\n            4.5297e-01, -1.0407e+00],\n          [ 4.1111e-01,  7.7079e-01, -2.8629e-01,  ..., -7.1692e-01,\n            1.1962e+00, -3.1327e-01],\n          [ 3.4674e-01,  8.3572e-01, -2.2757e-01,  ..., -6.5386e-01,\n            1.1374e+00, -3.7097e-01],\n          ...,\n          [ 3.8809e-01,  7.9392e-01, -2.6432e-01,  ..., -6.9413e-01,\n            1.1742e+00, -3.3505e-01],\n          [ 3.7211e-01,  8.1002e-01, -2.4937e-01,  ..., -6.7838e-01,\n            1.1592e+00, -3.4983e-01],\n          [ 3.7902e-01,  8.0302e-01, -2.5543e-01,  ..., -6.8511e-01,\n            1.1653e+00, -3.4392e-01]]],\n\n\n        ...,\n\n\n        [[[-8.6933e-01,  1.3182e-02, -3.9807e-01,  ..., -2.3739e-01,\n           -3.9439e-01,  5.0447e-03],\n          [-6.5869e-02, -6.8277e-01, -1.2082e+00,  ...,  5.0387e-01,\n           -1.2060e+00, -6.8577e-01],\n          [-1.8157e-02, -7.2157e-01, -1.2564e+00,  ...,  5.4642e-01,\n           -1.2544e+00, -7.2416e-01],\n          ...,\n          [-9.1078e-02, -6.5653e-01, -1.1830e+00,  ...,  4.7806e-01,\n           -1.1809e+00, -6.5949e-01],\n          [-1.2015e-01, -6.3060e-01, -1.1538e+00,  ...,  4.5076e-01,\n           -1.1516e+00, -6.3372e-01],\n          [-1.0149e-01, -6.4844e-01, -1.1725e+00,  ...,  4.6895e-01,\n           -1.1703e+00, -6.5150e-01]],\n\n         [[ 1.2463e-01, -2.7658e-02, -6.8021e-01,  ..., -9.3060e-02,\n            1.6402e-01,  3.1668e-01],\n          [-8.4122e-01,  7.0829e-01, -9.7081e-02,  ...,  7.6074e-01,\n            1.1382e+00, -7.8195e-01],\n          [-9.1802e-01,  7.6473e-01, -5.4828e-02,  ...,  8.2777e-01,\n            1.2144e+00, -8.6889e-01],\n          ...,\n          [-7.6999e-01,  6.4329e-01, -1.5754e-01,  ...,  6.9255e-01,\n            1.0669e+00, -7.0730e-01],\n          [-7.4971e-01,  6.2820e-01, -1.6904e-01,  ...,  6.7471e-01,\n            1.0473e+00, -6.8503e-01],\n          [-7.4577e-01,  6.2509e-01, -1.7091e-01,  ...,  6.7105e-01,\n            1.0439e+00, -6.8150e-01]],\n\n         [[-1.9574e-01, -3.1171e-01,  7.7077e-01,  ...,  2.1330e-01,\n           -2.2476e-01, -3.8718e-01],\n          [ 5.6690e-01,  4.9301e-01,  6.0430e-02,  ..., -5.7215e-01,\n            4.8201e-01,  4.0349e-01],\n          [ 6.4160e-01,  5.7181e-01, -9.1834e-03,  ..., -6.4906e-01,\n            5.5127e-01,  4.8092e-01],\n          ...,\n          [ 5.4817e-01,  4.7440e-01,  7.9290e-02,  ..., -5.5349e-01,\n            4.6317e-01,  3.8483e-01],\n          [ 5.2699e-01,  4.5207e-01,  9.9066e-02,  ..., -5.3169e-01,\n            4.4349e-01,  3.6288e-01],\n          [ 5.3132e-01,  4.5652e-01,  9.4852e-02,  ..., -5.3608e-01,\n            4.4770e-01,  3.6728e-01]],\n\n         ...,\n\n         [[ 2.7933e-01,  6.8460e-01,  7.8771e-01,  ..., -1.1467e-01,\n           -4.1843e-01, -3.2597e-01],\n          [-4.4885e-01,  7.5627e-02,  6.3668e-02,  ..., -8.9734e-01,\n            3.3466e-01,  4.7017e-01],\n          [-5.0374e-01,  3.0146e-02,  9.0985e-03,  ..., -9.5651e-01,\n            3.9151e-01,  5.3039e-01],\n          ...,\n          [-4.2255e-01,  1.0250e-01,  8.9999e-02,  ..., -8.7129e-01,\n            3.0845e-01,  4.4418e-01],\n          [-4.1213e-01,  1.1148e-01,  1.0036e-01,  ..., -8.6017e-01,\n            2.9772e-01,  4.3290e-01],\n          [-4.0584e-01,  1.1698e-01,  1.0663e-01,  ..., -8.5360e-01,\n            2.9130e-01,  4.2626e-01]],\n\n         [[-4.7257e-01, -9.6226e-01, -6.7258e-02,  ..., -9.5389e-01,\n            4.3042e-01, -1.8596e+00],\n          [-3.1913e-01,  3.6274e-01, -3.8194e-01,  ...,  1.0255e+00,\n           -1.4356e+00,  1.1766e+00],\n          [-3.1665e-01,  6.5801e-01, -4.0933e-01,  ...,  1.7040e+00,\n           -2.0672e+00,  1.0694e+00],\n          ...,\n          [-4.4111e-01, -3.0107e-01, -2.0649e-01,  ..., -4.3245e-01,\n           -1.0728e-01,  1.3049e+00],\n          [-4.3516e-01, -5.4893e-01, -2.0858e-01,  ..., -1.1366e+00,\n            5.0305e-01,  1.6257e+00],\n          [-4.1411e-01, -5.8348e-01, -2.2806e-01,  ..., -1.2210e+00,\n            5.9369e-01,  1.6530e+00]],\n\n         [[-4.2253e-01,  1.6139e+00,  4.5758e-01,  ...,  9.5619e-02,\n            4.5238e-01, -1.0412e+00],\n          [ 3.9901e-01,  7.8304e-01, -2.7600e-01,  ..., -7.0523e-01,\n            1.1859e+00, -3.2323e-01],\n          [ 4.7015e-01,  7.1117e-01, -3.4003e-01,  ..., -7.7470e-01,\n            1.2499e+00, -2.6044e-01],\n          ...,\n          [ 3.8381e-01,  7.9826e-01, -2.6033e-01,  ..., -6.8993e-01,\n            1.1702e+00, -3.3900e-01],\n          [ 3.5963e-01,  8.2266e-01, -2.3837e-01,  ..., -6.6626e-01,\n            1.1482e+00, -3.6055e-01],\n          [ 3.6345e-01,  8.1885e-01, -2.4227e-01,  ..., -6.7009e-01,\n            1.1521e+00, -3.5665e-01]]],\n\n\n        [[[-8.8988e-01,  3.0936e-02, -3.7734e-01,  ..., -2.5632e-01,\n           -3.7362e-01,  2.2664e-02],\n          [-3.3306e-02, -7.0875e-01, -1.2412e+00,  ...,  5.3263e-01,\n           -1.2391e+00, -7.1143e-01],\n          [-5.5001e-03, -7.3337e-01, -1.2691e+00,  ...,  5.5861e-01,\n           -1.2671e+00, -7.3591e-01],\n          ...,\n          [-9.5743e-02, -6.5468e-01, -1.1782e+00,  ...,  4.7503e-01,\n           -1.1761e+00, -6.5778e-01],\n          [-6.0124e-02, -6.8592e-01, -1.2141e+00,  ...,  5.0809e-01,\n           -1.2120e+00, -6.8881e-01],\n          [-8.0803e-02, -6.6712e-01, -1.1933e+00,  ...,  4.8857e-01,\n           -1.1912e+00, -6.7010e-01]],\n\n         [[ 1.4787e-01, -4.5034e-02, -6.9374e-01,  ..., -1.1343e-01,\n            1.4044e-01,  3.4345e-01],\n          [-8.9917e-01,  7.5226e-01, -6.2878e-02,  ...,  8.1197e-01,\n            1.1962e+00, -8.4732e-01],\n          [-9.3818e-01,  7.8158e-01, -4.0912e-02,  ...,  8.4647e-01,\n            1.2341e+00, -8.9003e-01],\n          ...,\n          [-7.4982e-01,  6.2840e-01, -1.6850e-01,  ...,  6.7485e-01,\n            1.0468e+00, -6.8450e-01],\n          [-8.0560e-01,  6.7160e-01, -1.3424e-01,  ...,  7.2462e-01,\n            1.1021e+00, -7.4646e-01],\n          [-7.9187e-01,  6.6264e-01, -1.4125e-01,  ...,  7.1341e-01,\n            1.0892e+00, -7.3045e-01]],\n\n         [[-2.1428e-01, -3.3132e-01,  7.8799e-01,  ...,  2.3242e-01,\n           -2.4189e-01, -4.0643e-01],\n          [ 6.3193e-01,  5.6088e-01, -9.8667e-04,  ..., -6.3873e-01,\n            5.4319e-01,  4.7042e-01],\n          [ 6.5929e-01,  5.8948e-01, -2.6767e-02,  ..., -6.6675e-01,\n            5.6886e-01,  4.9861e-01],\n          ...,\n          [ 5.2235e-01,  4.4649e-01,  1.0243e-01,  ..., -5.2653e-01,\n            4.4021e-01,  3.5761e-01],\n          [ 5.6988e-01,  4.9638e-01,  5.7836e-02,  ..., -5.7534e-01,\n            4.8461e-01,  4.0670e-01],\n          [ 5.5317e-01,  4.7884e-01,  7.3588e-02,  ..., -5.5820e-01,\n            4.6889e-01,  3.8946e-01]],\n\n         ...,\n\n         [[ 2.9766e-01,  6.9986e-01,  8.0594e-01,  ..., -9.4931e-02,\n           -4.3740e-01, -3.4606e-01],\n          [-4.9176e-01,  4.0619e-02,  2.1030e-02,  ..., -9.4378e-01,\n            3.7919e-01,  5.1749e-01],\n          [-5.1786e-01,  1.7488e-02, -4.9777e-03,  ..., -9.7118e-01,\n            4.0589e-01,  5.4520e-01],\n          ...,\n          [-4.1992e-01,  1.0323e-01,  9.2552e-02,  ..., -8.6765e-01,\n            3.0539e-01,  4.4028e-01],\n          [-4.6530e-01,  6.4036e-02,  4.7381e-02,  ..., -9.1590e-01,\n            3.5206e-01,  4.8924e-01],\n          [-4.3116e-01,  9.3137e-02,  8.1355e-02,  ..., -8.7942e-01,\n            3.1686e-01,  4.5218e-01]],\n\n         [[-4.7419e-01, -9.8576e-01, -6.2134e-02,  ..., -9.8448e-01,\n            4.5643e-01, -1.9370e+00],\n          [-3.0260e-01,  3.6186e-01, -4.1596e-01,  ...,  9.4222e-01,\n           -1.4041e+00,  1.3355e+00],\n          [-3.1448e-01,  6.3515e-01, -4.1782e-01,  ...,  1.6472e+00,\n           -2.0457e+00,  1.0810e+00],\n          ...,\n          [-4.1733e-01, -3.8732e-01, -2.2553e-01,  ..., -7.4897e-01,\n            1.6949e-01,  1.4744e+00],\n          [-4.3476e-01, -3.3059e-01, -2.1811e-01,  ..., -6.0726e-01,\n            1.0739e-02,  1.4083e+00],\n          [-4.2322e-01, -1.8459e-01, -2.4367e-01,  ..., -1.3777e-01,\n           -3.6673e-01,  1.2075e+00]],\n\n         [[-4.4384e-01,  1.6354e+00,  4.7660e-01,  ...,  1.1639e-01,\n            4.3336e-01, -1.0599e+00],\n          [ 4.4061e-01,  7.4096e-01, -3.1299e-01,  ..., -7.4574e-01,\n            1.2228e+00, -2.8705e-01],\n          [ 4.7998e-01,  7.0132e-01, -3.4963e-01,  ..., -7.8447e-01,\n            1.2595e+00, -2.5090e-01],\n          ...,\n          [ 3.5908e-01,  8.2339e-01, -2.3957e-01,  ..., -6.6613e-01,\n            1.1494e+00, -3.5906e-01],\n          [ 4.1589e-01,  7.6609e-01, -2.9157e-01,  ..., -7.2181e-01,\n            1.2014e+00, -3.0792e-01],\n          [ 3.9838e-01,  7.8374e-01, -2.7541e-01,  ..., -7.0466e-01,\n            1.1853e+00, -3.2387e-01]]],\n\n\n        [[[-8.6037e-01,  5.4821e-03, -4.0710e-01,  ..., -2.2916e-01,\n           -4.0343e-01, -2.5967e-03],\n          [-1.0722e-01, -6.4480e-01, -1.1666e+00,  ...,  4.6448e-01,\n           -1.1644e+00, -6.4796e-01],\n          [-1.3303e-02, -7.2734e-01, -1.2612e+00,  ...,  5.5185e-01,\n           -1.2592e+00, -7.2997e-01],\n          ...,\n          [-4.5373e-02, -6.9897e-01, -1.2289e+00,  ...,  5.2195e-01,\n           -1.2269e+00, -7.0177e-01],\n          [-7.2187e-02, -6.7470e-01, -1.2020e+00,  ...,  4.9660e-01,\n           -1.1999e+00, -6.7763e-01],\n          [-6.1922e-02, -6.8447e-01, -1.2123e+00,  ...,  5.0659e-01,\n           -1.2102e+00, -6.8737e-01]],\n\n         [[ 1.1272e-01, -1.8443e-02, -6.7291e-01,  ..., -8.2436e-02,\n            1.7574e-01,  3.0356e-01],\n          [-7.8610e-01,  6.6207e-01, -1.3716e-01,  ...,  7.0991e-01,\n            1.0830e+00, -7.2203e-01],\n          [-8.8652e-01,  7.3898e-01, -7.6241e-02,  ...,  7.9898e-01,\n            1.1830e+00, -8.3458e-01],\n          ...,\n          [-8.0887e-01,  6.7479e-01, -1.3189e-01,  ...,  7.2798e-01,\n            1.1062e+00, -7.5025e-01],\n          [-7.9359e-01,  6.6585e-01, -1.3706e-01,  ...,  7.1585e-01,\n            1.0903e+00, -7.3079e-01],\n          [-7.8924e-01,  6.6130e-01, -1.4102e-01,  ...,  7.1126e-01,\n            1.0865e+00, -7.2747e-01]],\n\n         [[-1.8547e-01, -3.0098e-01,  7.6110e-01,  ...,  2.0278e-01,\n           -2.1513e-01, -3.7660e-01],\n          [ 5.3080e-01,  4.5518e-01,  9.4384e-02,  ..., -5.3512e-01,\n            4.4821e-01,  3.6625e-01],\n          [ 6.2160e-01,  5.5034e-01,  9.0752e-03,  ..., -6.2828e-01,\n            5.3315e-01,  4.5995e-01],\n          ...,\n          [ 5.8608e-01,  5.1328e-01,  4.2763e-02,  ..., -5.9195e-01,\n            4.9960e-01,  4.2341e-01],\n          [ 5.6720e-01,  4.9334e-01,  6.0252e-02,  ..., -5.7248e-01,\n            4.8221e-01,  4.0381e-01],\n          [ 5.6111e-01,  4.8711e-01,  6.6146e-02,  ..., -5.6632e-01,\n            4.7633e-01,  3.9763e-01]],\n\n         ...,\n\n         [[ 2.7033e-01,  6.7698e-01,  7.7876e-01,  ..., -1.2428e-01,\n           -4.0915e-01, -3.1621e-01],\n          [-4.0620e-01,  1.1271e-01,  1.0614e-01,  ..., -8.5215e-01,\n            2.9086e-01,  4.2436e-01],\n          [-4.9638e-01,  3.6177e-02,  1.6418e-02,  ..., -9.4842e-01,\n            3.8382e-01,  5.2211e-01],\n          ...,\n          [-4.5599e-01,  7.3677e-02,  5.6705e-02,  ..., -9.0664e-01,\n            3.4279e-01,  4.7997e-01],\n          [-4.3263e-01,  9.3315e-02,  7.9939e-02,  ..., -8.8157e-01,\n            3.1865e-01,  4.5449e-01],\n          [-4.3451e-01,  9.1892e-02,  7.8072e-02,  ..., -8.8370e-01,\n            3.2064e-01,  4.5668e-01]],\n\n         [[-4.7126e-01, -9.5125e-01, -7.0598e-02,  ..., -9.3466e-01,\n            4.0802e-01, -1.8319e+00],\n          [-3.4403e-01,  3.6789e-01, -3.5605e-01,  ...,  1.1378e+00,\n           -1.5157e+00,  9.3520e-01],\n          [-3.1062e-01,  5.4212e-01, -4.1236e-01,  ...,  1.3313e+00,\n           -1.7084e+00,  1.1970e+00],\n          ...,\n          [-4.3186e-01, -2.8185e-01, -2.2460e-01,  ..., -4.7803e-01,\n           -8.8754e-02,  1.4723e+00],\n          [-4.2642e-01, -2.0255e-01, -2.2858e-01,  ..., -2.3669e-01,\n           -2.8070e-01,  1.2169e+00],\n          [-4.1469e-01, -2.6399e-01, -2.4372e-01,  ..., -4.1135e-01,\n           -1.3436e-01,  1.3765e+00]],\n\n         [[-4.1226e-01,  1.6035e+00,  4.4830e-01,  ...,  8.5577e-02,\n            4.6165e-01, -1.0321e+00],\n          [ 3.5228e-01,  8.3021e-01, -2.3348e-01,  ..., -6.5949e-01,\n            1.1434e+00, -3.6499e-01],\n          [ 4.4553e-01,  7.3607e-01, -3.1807e-01,  ..., -7.5070e-01,\n            1.2279e+00, -2.8195e-01],\n          ...,\n          [ 4.0636e-01,  7.7550e-01, -2.8098e-01,  ..., -7.1205e-01,\n            1.1908e+00, -3.1871e-01],\n          [ 3.8863e-01,  7.9348e-01, -2.6568e-01,  ..., -6.9488e-01,\n            1.1755e+00, -3.3358e-01],\n          [ 3.8145e-01,  8.0070e-01, -2.5891e-01,  ..., -6.8779e-01,\n            1.1688e+00, -3.4027e-01]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>), tensor([[[[ 3.3632e-03,  2.8864e-03,  8.4375e-03,  ..., -9.8342e-05,\n            6.2948e-03,  1.0630e-03],\n          [-1.1266e-02,  4.6630e-02, -2.3812e-02,  ...,  5.2737e-03,\n           -2.1314e-02, -2.0161e-02],\n          [ 2.2460e-02, -3.8529e-02,  3.6383e-02,  ...,  1.3934e-02,\n            2.9880e-02,  6.8753e-02],\n          ...,\n          [ 6.0069e-02, -3.9035e-02, -6.1179e-02,  ..., -7.7186e-02,\n            5.2994e-02,  1.0312e-01],\n          [-8.2839e-03, -3.9707e-02,  4.0155e-03,  ..., -4.3428e-02,\n            7.5311e-02,  6.1150e-04],\n          [-6.2400e-03, -6.0740e-02,  3.1130e-02,  ...,  2.5640e-03,\n            8.4632e-03, -3.0579e-02]],\n\n         [[ 8.0066e-03, -2.0996e-03, -1.7658e-03,  ...,  1.5093e-04,\n            7.4658e-03,  3.6835e-03],\n          [-7.7456e-03, -3.0720e-02,  6.0757e-02,  ...,  1.1276e-02,\n           -3.3685e-02, -6.9015e-03],\n          [ 2.0728e-02,  1.5513e-02, -1.3548e-03,  ..., -4.5578e-02,\n           -7.6521e-03, -5.9662e-02],\n          ...,\n          [ 5.7854e-02,  4.3729e-02, -7.7733e-03,  ..., -1.5130e-02,\n           -1.8374e-02, -7.2529e-02],\n          [ 4.6019e-02,  1.9027e-03,  4.6505e-02,  ..., -5.6043e-02,\n           -1.0633e-02, -4.3616e-02],\n          [-2.4311e-02, -8.5693e-03,  4.7051e-02,  ..., -1.7557e-02,\n           -5.8335e-02, -2.1492e-02]],\n\n         [[-3.3931e-03, -1.0869e-04,  7.0892e-04,  ...,  2.9550e-03,\n           -3.3838e-03,  2.4479e-03],\n          [-1.0708e-02, -1.5741e-02, -8.4572e-02,  ..., -3.5507e-02,\n            4.9714e-02, -5.5478e-02],\n          [-2.5359e-02,  4.7675e-03, -4.1684e-02,  ..., -2.0871e-02,\n            3.0756e-02,  1.6557e-02],\n          ...,\n          [-1.0500e-01,  1.5648e-02,  4.9177e-02,  ..., -2.1644e-02,\n            2.6041e-02,  7.6070e-02],\n          [-4.4042e-02, -3.4263e-02,  3.1863e-02,  ...,  9.1488e-02,\n            4.2987e-02,  9.3135e-02],\n          [ 2.2869e-02,  1.9296e-02, -7.4450e-03,  ...,  7.5716e-03,\n            3.0236e-02,  1.5496e-02]],\n\n         ...,\n\n         [[-3.9514e-03,  5.9469e-04, -3.0829e-03,  ...,  2.8159e-03,\n           -7.6862e-03,  7.0052e-03],\n          [-2.9325e-02, -4.9062e-02, -2.4668e-02,  ...,  4.4814e-02,\n           -2.0906e-02,  4.8891e-02],\n          [-4.2665e-03,  3.4663e-02, -4.5656e-02,  ...,  3.9724e-02,\n           -4.7545e-02,  3.4700e-02],\n          ...,\n          [-1.0346e-03,  7.8381e-02,  4.7989e-02,  ..., -1.7365e-02,\n           -7.9318e-02,  3.2954e-02],\n          [ 5.7721e-03,  1.1569e-01,  3.7590e-02,  ..., -3.8221e-02,\n            2.6142e-02,  2.3375e-02],\n          [ 7.7712e-03,  5.6483e-02,  8.4661e-02,  ...,  1.1692e-02,\n            2.3521e-02, -2.9307e-02]],\n\n         [[-4.1836e-03, -3.0035e-04, -5.1192e-03,  ...,  5.7703e-03,\n            2.3439e-03, -2.1605e-03],\n          [ 1.5010e-01, -1.6030e-01, -1.1558e-01,  ..., -7.7066e-02,\n            3.4229e-02, -3.5869e-02],\n          [ 1.1556e-01, -1.5789e-02, -7.5938e-02,  ...,  6.3440e-02,\n           -8.6056e-02, -1.1425e-01],\n          ...,\n          [ 4.7942e-02, -2.2501e-01,  5.1005e-02,  ...,  9.0525e-02,\n           -2.5069e-02,  2.2084e-02],\n          [ 7.6861e-02, -3.0378e-01,  2.3396e-02,  ...,  9.6995e-02,\n           -4.7719e-02,  1.5637e-01],\n          [ 9.9474e-02, -1.3342e-01,  5.3394e-02,  ...,  8.0066e-02,\n            3.0324e-02,  4.8963e-03]],\n\n         [[ 4.2959e-03, -2.5069e-04,  4.4567e-03,  ..., -2.1758e-03,\n            2.4185e-03,  3.4245e-03],\n          [-3.8523e-03, -8.8029e-03, -4.4086e-02,  ..., -5.0733e-02,\n           -8.6635e-03, -1.3337e-02],\n          [-3.6414e-02,  2.0276e-02, -6.8755e-03,  ..., -5.7558e-02,\n           -1.5285e-02, -3.5182e-03],\n          ...,\n          [-3.5405e-02,  4.8853e-02, -3.7605e-02,  ..., -7.9286e-03,\n           -6.5576e-02, -5.7921e-02],\n          [ 4.7708e-03,  5.0083e-02,  2.0783e-02,  ...,  2.7945e-02,\n           -1.2193e-02,  1.4477e-02],\n          [ 3.3578e-02, -1.3485e-02, -3.2055e-02,  ..., -2.0411e-02,\n           -5.1617e-03, -3.6220e-02]]],\n\n\n        [[[ 2.6455e-03,  5.1160e-03,  7.8697e-03,  ..., -8.4265e-04,\n            5.5840e-03,  9.4546e-04],\n          [-5.5102e-02, -7.7867e-02,  4.2447e-02,  ...,  5.4859e-02,\n           -2.5140e-02, -7.1313e-02],\n          [-2.4260e-02, -3.6704e-02, -2.2271e-02,  ...,  3.3862e-02,\n            1.5615e-02, -3.0002e-02],\n          ...,\n          [ 2.8798e-02, -1.3416e-01,  7.7507e-03,  ..., -4.1444e-03,\n            1.7638e-02, -5.0538e-02],\n          [ 3.1233e-02, -3.6103e-02, -4.6768e-02,  ...,  2.9554e-03,\n            7.8486e-03,  8.6374e-03],\n          [ 4.4366e-02, -2.1087e-03, -7.9126e-02,  ...,  1.8164e-03,\n            8.8345e-03, -5.1469e-02]],\n\n         [[ 6.4045e-03, -2.2840e-03, -2.3431e-03,  ...,  1.0142e-03,\n            9.9901e-03,  5.7543e-03],\n          [ 5.6675e-04, -7.3456e-02,  3.0681e-02,  ..., -1.3465e-01,\n            3.1583e-02, -1.6925e-01],\n          [-5.5561e-02, -6.8743e-02,  5.0599e-03,  ..., -3.4501e-02,\n            6.4001e-02, -6.9612e-02],\n          ...,\n          [-9.8284e-02, -5.3429e-02, -5.5516e-02,  ...,  7.1924e-03,\n            1.7773e-02, -3.9292e-02],\n          [-4.1114e-02, -5.0577e-02,  1.3185e-02,  ...,  3.7010e-02,\n           -3.8781e-02, -5.4851e-02],\n          [-5.1809e-02, -5.1194e-02,  1.0757e-02,  ...,  2.5131e-02,\n            5.5734e-03, -7.2231e-02]],\n\n         [[-3.0927e-03, -3.5527e-03,  1.5772e-03,  ...,  2.5089e-03,\n           -4.3933e-03,  2.4069e-03],\n          [ 7.3327e-02,  3.8730e-02, -8.2059e-02,  ...,  7.3944e-02,\n            5.7197e-02, -5.8466e-02],\n          [ 6.7782e-02,  1.9293e-02, -8.3410e-02,  ...,  5.8397e-03,\n            2.5661e-02, -7.6953e-02],\n          ...,\n          [ 1.2149e-02, -1.8163e-02,  1.0273e-01,  ...,  1.5843e-02,\n            5.8290e-02,  8.0162e-04],\n          [-1.9676e-02, -9.9158e-03,  2.2270e-03,  ...,  3.5079e-02,\n            2.6632e-03, -5.0169e-03],\n          [-5.8376e-03, -1.9479e-02,  1.5171e-02,  ...,  8.0397e-02,\n            3.6133e-02, -4.6227e-02]],\n\n         ...,\n\n         [[-4.2316e-03, -4.7858e-04, -1.3381e-03,  ...,  2.1143e-03,\n           -8.0378e-03,  9.3706e-03],\n          [ 5.4068e-02,  4.8446e-02,  2.3502e-03,  ...,  1.0894e-02,\n            9.8614e-02,  2.3607e-03],\n          [ 1.3016e-03,  1.1738e-02,  1.1432e-02,  ...,  7.5371e-03,\n            1.3357e-01, -1.8235e-02],\n          ...,\n          [ 8.9689e-02, -1.2257e-02,  1.4015e-01,  ...,  2.7534e-02,\n            9.5600e-02, -2.9945e-02],\n          [-8.5070e-03,  2.3153e-02,  1.2757e-01,  ...,  5.6123e-02,\n            7.5470e-02, -4.4940e-02],\n          [ 1.7917e-02, -2.3591e-02,  1.3559e-01,  ...,  3.8657e-02,\n            5.1370e-02,  8.4702e-03]],\n\n         [[-3.2149e-03, -4.0709e-03, -6.3057e-03,  ...,  4.2888e-03,\n           -1.2891e-03,  6.1102e-04],\n          [ 8.9419e-02, -1.1991e-01, -1.1922e-01,  ..., -6.8595e-02,\n           -2.3202e-02,  8.3410e-02],\n          [ 1.1749e-01, -2.7352e-01, -6.4118e-02,  ...,  2.8545e-02,\n           -5.3512e-02, -6.4085e-02],\n          ...,\n          [-1.0878e-01,  2.9840e-01, -9.6930e-04,  ...,  1.4119e-01,\n            4.4074e-02,  1.0265e-01],\n          [-5.7987e-02,  3.3006e-01, -6.9090e-02,  ..., -6.5084e-02,\n           -4.6258e-02,  6.5449e-02],\n          [-1.9935e-02, -2.5619e-02,  3.7400e-02,  ...,  1.3639e-01,\n           -8.2110e-02,  1.0891e-01]],\n\n         [[ 4.6842e-03,  4.8103e-04,  3.8097e-03,  ..., -2.1989e-03,\n            2.1343e-03,  2.6536e-03],\n          [-4.8373e-02, -1.1663e-02, -3.8947e-03,  ...,  5.3812e-02,\n            4.9251e-02,  6.2889e-02],\n          [-4.4052e-02, -2.9747e-02, -8.8718e-02,  ...,  3.1850e-02,\n            2.8794e-02,  5.9071e-02],\n          ...,\n          [ 5.9858e-02,  1.1647e-02,  9.6198e-03,  ..., -5.5037e-02,\n            3.6053e-02, -3.9649e-02],\n          [ 3.3931e-02,  4.2947e-02,  2.0014e-02,  ..., -6.3603e-02,\n           -4.1382e-02, -3.2362e-02],\n          [ 6.2577e-02,  6.8216e-02, -2.3643e-02,  ..., -6.0132e-02,\n           -2.7947e-02,  1.3577e-02]]],\n\n\n        [[[ 4.7429e-04,  4.1826e-03,  8.3502e-03,  ...,  2.1871e-03,\n            4.6715e-03, -9.0566e-05],\n          [-4.0668e-02,  6.1576e-03, -1.1597e-02,  ...,  5.6583e-02,\n           -7.9199e-02, -3.0938e-03],\n          [-9.0266e-02, -4.8659e-02,  2.5680e-03,  ...,  6.2404e-02,\n           -5.2213e-02, -2.9955e-02],\n          ...,\n          [-1.4568e-02, -2.0664e-02,  2.2147e-03,  ..., -4.3136e-02,\n            2.5108e-04, -9.4869e-03],\n          [-4.2695e-03, -5.0149e-02,  1.6064e-02,  ..., -1.9997e-02,\n            2.7478e-02, -8.1424e-03],\n          [ 4.2589e-02,  4.7071e-02, -4.3374e-02,  ..., -8.1514e-02,\n            1.4052e-02,  3.8242e-02]],\n\n         [[ 4.9954e-03, -1.2934e-03, -2.0090e-03,  ...,  2.4002e-03,\n            8.3834e-03,  8.9376e-03],\n          [-2.1269e-02, -2.3293e-02,  6.9846e-03,  ..., -1.6583e-02,\n           -2.5596e-02, -3.9359e-02],\n          [-9.6478e-02, -2.3434e-02, -1.7434e-02,  ...,  2.4245e-02,\n           -1.2622e-02,  5.5737e-02],\n          ...,\n          [-8.7231e-02, -6.3980e-02,  6.8738e-02,  ...,  4.2879e-02,\n           -4.4572e-02, -2.9028e-02],\n          [-9.8776e-02, -4.3976e-02,  7.0796e-02,  ...,  1.3339e-02,\n           -4.1428e-02,  1.4969e-02],\n          [-8.1535e-02,  1.8641e-03,  4.4540e-02,  ...,  6.8266e-02,\n           -5.6986e-02,  9.9796e-02]],\n\n         [[-3.6306e-03, -2.2525e-03,  1.7499e-03,  ...,  4.2495e-03,\n           -5.4843e-03,  2.4843e-03],\n          [ 4.2705e-02,  4.1200e-02, -9.0712e-02,  ...,  3.3788e-02,\n           -1.1847e-02, -1.2272e-03],\n          [ 5.2699e-02,  9.6443e-03, -2.4425e-02,  ..., -4.3710e-02,\n            1.6089e-02, -2.0511e-02],\n          ...,\n          [-2.8955e-02, -1.8502e-02, -7.0943e-02,  ..., -2.8167e-02,\n            8.3523e-02, -4.3418e-02],\n          [-3.0071e-02, -9.2431e-03, -2.1304e-02,  ...,  2.6317e-02,\n            6.1475e-02,  9.0779e-03],\n          [-1.1842e-02, -7.5576e-02,  1.9397e-02,  ..., -1.0493e-01,\n           -2.8009e-02, -7.7107e-04]],\n\n         ...,\n\n         [[-5.6828e-03,  2.3988e-04, -9.5586e-04,  ...,  3.5969e-03,\n           -6.8660e-03,  8.1164e-03],\n          [-1.2192e-02,  5.8342e-02,  1.1602e-02,  ...,  1.0098e-01,\n            8.4766e-02, -3.3229e-02],\n          [-3.4432e-02, -5.5432e-02,  1.6504e-02,  ...,  3.7376e-02,\n            2.8447e-02, -9.7050e-02],\n          ...,\n          [-3.1045e-02, -4.9498e-02,  2.9772e-02,  ...,  2.9520e-02,\n           -3.3132e-02,  5.3147e-02],\n          [-3.6021e-02, -2.2148e-02,  2.4671e-02,  ...,  6.0348e-02,\n            2.7433e-02,  7.0863e-02],\n          [-4.0867e-02, -4.1710e-02,  6.2930e-02,  ..., -2.9447e-02,\n            4.0371e-03, -9.8397e-03]],\n\n         [[-5.2801e-03, -9.3518e-03, -5.4764e-03,  ...,  2.2632e-03,\n           -3.9117e-03,  4.9235e-04],\n          [-3.4853e-02, -1.7796e-01, -1.5715e-01,  ...,  1.7340e-03,\n            1.0287e-01, -4.5111e-02],\n          [ 1.7285e-01, -2.2508e-01, -1.0657e-01,  ...,  3.2566e-02,\n           -8.7721e-02, -2.3792e-03],\n          ...,\n          [-3.9347e-02,  9.4903e-02, -2.6288e-02,  ...,  1.1081e-01,\n            8.8447e-02, -2.2933e-02],\n          [-4.6603e-02,  2.0844e-01, -1.3563e-02,  ...,  1.4600e-01,\n            7.1287e-02, -4.1109e-02],\n          [-6.0275e-02,  2.7926e-01,  1.4734e-02,  ...,  1.4058e-01,\n            1.8101e-01,  5.2741e-02]],\n\n         [[ 6.2338e-03, -1.9606e-03,  4.7299e-03,  ..., -1.8225e-03,\n            2.8928e-03,  2.6744e-03],\n          [-1.5045e-02, -4.2768e-03,  1.2464e-03,  ..., -4.3899e-02,\n           -3.1794e-02,  8.7931e-03],\n          [-7.7382e-03, -4.7445e-02, -7.4259e-02,  ...,  2.4022e-02,\n           -2.5453e-02, -1.4696e-02],\n          ...,\n          [ 1.6851e-02,  2.2110e-02, -6.5920e-02,  ...,  1.9388e-02,\n           -1.8303e-02, -4.7590e-02],\n          [ 6.1592e-02,  4.3007e-03, -2.4483e-02,  ...,  2.6278e-04,\n            2.6719e-02, -1.2701e-02],\n          [ 1.5700e-02,  2.5638e-02, -1.0232e-01,  ..., -5.5173e-02,\n           -5.5596e-02, -9.4661e-02]]],\n\n\n        ...,\n\n\n        [[[-4.8488e-04,  6.6051e-03,  7.3890e-03,  ...,  1.6032e-04,\n            2.6190e-03,  2.6268e-04],\n          [ 8.1988e-03, -1.2575e-02,  2.5139e-02,  ..., -1.4150e-02,\n           -6.1477e-02,  7.0484e-02],\n          [ 1.6368e-02, -1.0513e-02, -3.2780e-02,  ...,  1.8208e-02,\n           -1.5396e-02,  6.5380e-02],\n          ...,\n          [ 2.5616e-03,  5.3041e-02, -3.2889e-02,  ..., -2.7136e-02,\n            1.7410e-02, -7.5208e-02],\n          [ 4.5659e-03,  1.5655e-02, -4.9750e-02,  ..., -9.0646e-03,\n            2.1503e-03, -7.3693e-02],\n          [ 4.4892e-02,  2.7441e-02, -5.9699e-02,  ...,  4.3542e-03,\n            4.6220e-02, -4.8175e-02]],\n\n         [[ 3.5121e-03,  1.8058e-03, -1.3096e-03,  ...,  2.9368e-03,\n            8.1703e-03,  1.1790e-02],\n          [-2.5494e-02,  3.9225e-02, -3.8720e-02,  ...,  1.3650e-02,\n           -1.3972e-02, -1.0425e-02],\n          [ 2.6047e-03, -2.0323e-02, -3.4513e-02,  ..., -1.2827e-02,\n           -2.9786e-02, -9.1036e-02],\n          ...,\n          [-2.6269e-02,  3.2405e-02,  7.7157e-02,  ...,  1.5960e-02,\n           -2.3626e-02,  8.7774e-02],\n          [-2.0732e-02,  1.4753e-02,  3.4231e-02,  ...,  3.7139e-02,\n           -2.6055e-02,  8.1815e-02],\n          [-2.0777e-02, -1.0024e-02, -7.9266e-02,  ...,  4.3387e-02,\n            7.9860e-02,  9.4152e-02]],\n\n         [[-4.3852e-04, -3.5074e-03,  5.2888e-03,  ...,  3.3605e-03,\n           -5.9052e-03, -3.6427e-04],\n          [ 7.5071e-02,  3.9794e-02, -5.9424e-02,  ..., -4.3066e-02,\n            3.8707e-03, -5.6407e-02],\n          [ 4.6230e-02,  1.0013e-01, -1.5582e-01,  ..., -4.4294e-02,\n            6.1437e-03, -4.5250e-02],\n          ...,\n          [ 7.4650e-03, -5.0181e-02,  2.8029e-02,  ...,  1.6558e-02,\n            1.2168e-02, -6.9948e-02],\n          [-2.7496e-02, -9.4614e-02,  5.3535e-03,  ...,  1.5763e-02,\n            3.1771e-02, -9.2160e-02],\n          [-7.5658e-02, -8.7584e-02,  1.1863e-02,  ..., -3.7857e-02,\n            1.1070e-02, -5.4013e-02]],\n\n         ...,\n\n         [[-3.1832e-03, -2.4372e-03,  9.2472e-04,  ...,  3.9169e-03,\n           -4.1242e-03,  7.6242e-03],\n          [ 2.5513e-02, -2.1517e-02, -3.3274e-02,  ..., -2.4433e-02,\n            1.5245e-02, -4.9868e-02],\n          [ 5.5689e-02,  2.0539e-02, -3.9367e-03,  ...,  3.8013e-02,\n            3.7226e-02, -5.4786e-03],\n          ...,\n          [-3.8393e-02, -7.1065e-02, -2.3664e-02,  ..., -7.8741e-02,\n           -1.8165e-02,  2.7869e-02],\n          [-6.9321e-03, -9.9312e-02,  2.0232e-02,  ..., -3.3895e-02,\n           -9.2321e-03,  2.3261e-02],\n          [ 1.3066e-02, -1.0497e-01,  7.1893e-02,  ...,  7.8089e-03,\n           -1.3858e-02,  8.7131e-03]],\n\n         [[-8.2316e-03,  7.9152e-03, -7.7121e-03,  ...,  1.8171e-04,\n           -1.9026e-03,  2.6725e-03],\n          [ 1.0557e-01,  1.1373e-01, -6.1581e-02,  ..., -2.0288e-02,\n            7.1590e-02,  1.1138e-02],\n          [-7.1803e-02,  3.2417e-01, -2.0066e-01,  ..., -1.7973e-01,\n            1.1425e-01,  3.7878e-02],\n          ...,\n          [ 3.3962e-03,  2.4849e-01,  3.0827e-02,  ...,  5.8249e-02,\n            2.4560e-02,  7.6437e-02],\n          [-1.0509e-01,  2.5134e-01,  4.0693e-02,  ..., -1.1104e-01,\n           -7.5764e-03,  2.3017e-01],\n          [ 1.3525e-02, -1.5154e-01,  8.9616e-02,  ...,  2.3530e-01,\n            1.1604e-01,  5.5338e-02]],\n\n         [[ 6.1916e-03, -1.5423e-03,  5.0506e-03,  ..., -1.8143e-03,\n            2.4774e-03,  2.7548e-03],\n          [-6.5858e-02,  3.4079e-02, -5.4226e-02,  ..., -2.8974e-02,\n            4.4860e-02,  4.2130e-03],\n          [-9.3040e-02,  1.4746e-02, -1.3037e-02,  ...,  1.1038e-02,\n            6.3692e-04,  2.1250e-03],\n          ...,\n          [ 6.3575e-03, -1.6493e-02, -6.9364e-02,  ...,  8.2718e-02,\n            3.8606e-02,  3.5597e-02],\n          [ 1.5736e-02, -1.3999e-02, -7.7066e-02,  ...,  1.0179e-02,\n            6.8227e-02, -8.8561e-03],\n          [ 4.8620e-02, -2.3291e-03, -7.2164e-02,  ...,  2.1432e-03,\n            3.2003e-02, -2.3222e-02]]],\n\n\n        [[[ 3.0411e-03,  5.7699e-03,  7.7526e-03,  ...,  3.2474e-03,\n            2.9890e-03,  2.5259e-03],\n          [ 5.3025e-03,  5.8846e-03, -2.0909e-02,  ...,  1.2889e-02,\n           -8.2749e-02,  9.0810e-02],\n          [-6.2645e-03, -3.7096e-02,  1.7386e-02,  ...,  3.0166e-03,\n           -4.0186e-02,  7.8691e-02],\n          ...,\n          [ 1.1173e-02, -7.1112e-02,  3.2160e-03,  ...,  1.4826e-03,\n           -4.0871e-02,  1.7099e-02],\n          [ 9.3183e-03, -4.7554e-02, -5.0403e-03,  ...,  3.2939e-02,\n           -9.8309e-03,  1.0866e-02],\n          [-3.6481e-02,  4.9850e-04,  6.3274e-03,  ...,  2.2966e-02,\n           -2.9890e-02, -4.3004e-02]],\n\n         [[ 2.8344e-03,  2.9770e-03, -1.9144e-03,  ...,  4.6648e-03,\n            9.7242e-03,  1.4022e-02],\n          [ 2.2094e-02,  6.0952e-02, -4.6248e-02,  ..., -2.4470e-02,\n            9.9555e-03, -6.4637e-02],\n          [ 3.2220e-02, -3.9087e-03,  7.7116e-03,  ..., -6.1158e-02,\n           -5.4617e-02, -1.3891e-01],\n          ...,\n          [-2.5548e-02,  7.1792e-02, -4.3117e-02,  ...,  6.2600e-02,\n           -4.0832e-02,  2.1822e-02],\n          [-4.0417e-02, -1.4363e-02, -3.2348e-02,  ...,  6.5689e-03,\n           -2.7626e-02, -6.6122e-02],\n          [-4.2707e-02,  6.0022e-03,  2.8326e-02,  ..., -1.2351e-03,\n           -4.3596e-02,  1.9502e-02]],\n\n         [[-2.9843e-03, -4.4106e-03,  8.3043e-03,  ...,  2.1334e-03,\n           -5.9458e-03,  5.2967e-04],\n          [ 5.1597e-03, -5.3327e-03,  2.7612e-02,  ..., -2.6787e-02,\n           -3.9876e-02, -4.8553e-03],\n          [ 4.4113e-03,  5.6320e-02, -8.3828e-02,  ..., -3.4054e-02,\n            2.3250e-02, -1.2115e-02],\n          ...,\n          [-3.7840e-02,  3.0435e-02,  4.8002e-02,  ..., -4.2520e-02,\n           -1.9506e-02,  2.1363e-02],\n          [ 1.6135e-02,  2.3586e-02, -5.5097e-03,  ..., -3.8961e-02,\n           -2.3788e-03,  4.0564e-02],\n          [-5.1459e-02, -1.4659e-02, -1.6069e-02,  ...,  8.5124e-03,\n            1.1989e-02,  3.8973e-02]],\n\n         ...,\n\n         [[-3.9415e-03, -3.7837e-03, -1.0179e-04,  ...,  3.2781e-03,\n           -8.3530e-03,  8.6270e-03],\n          [ 2.6394e-02,  3.3072e-02,  2.1350e-02,  ..., -1.5030e-02,\n            2.4853e-03, -2.4309e-02],\n          [ 2.8672e-02,  1.0653e-02, -1.9758e-02,  ...,  6.5485e-02,\n           -2.5133e-02, -1.1553e-03],\n          ...,\n          [ 3.7844e-02,  6.6398e-03,  5.7684e-02,  ..., -6.4936e-03,\n            3.5469e-02, -3.2955e-02],\n          [ 1.0869e-02,  6.5887e-02,  8.9520e-02,  ...,  1.2789e-03,\n            8.9142e-02, -9.3480e-03],\n          [-8.2533e-03,  3.7655e-02,  1.0034e-01,  ...,  2.3460e-02,\n            7.1473e-02, -3.5425e-03]],\n\n         [[-1.8623e-03, -1.4773e-03, -5.5956e-03,  ...,  5.5035e-03,\n            9.3511e-04,  1.7700e-03],\n          [ 2.2962e-01,  1.1228e-01, -1.9664e-02,  ..., -8.5464e-03,\n            5.9641e-02, -1.0826e-01],\n          [ 1.3115e-01,  1.7259e-01, -1.0165e-01,  ..., -5.5103e-02,\n            5.1820e-02, -5.1620e-02],\n          ...,\n          [-1.0070e-01,  2.0524e-01,  1.5149e-01,  ...,  3.1590e-01,\n           -1.8474e-02, -2.0516e-02],\n          [-9.1699e-02, -8.4528e-02,  3.2446e-02,  ...,  9.3566e-03,\n            9.4709e-02,  7.8093e-02],\n          [-3.7874e-02, -2.1834e-01, -3.7033e-02,  ..., -1.4223e-02,\n           -2.4178e-02,  4.7481e-02]],\n\n         [[ 8.1082e-03, -3.9472e-03,  3.2305e-03,  ..., -4.0258e-03,\n            3.4173e-03,  2.3419e-03],\n          [-4.3566e-02,  7.5794e-03, -3.6267e-02,  ..., -1.0246e-01,\n           -3.3833e-02, -9.6189e-02],\n          [-5.1257e-02,  8.0464e-03,  2.8295e-02,  ..., -2.9348e-02,\n           -6.0765e-03, -1.9014e-02],\n          ...,\n          [ 1.3815e-02,  5.2758e-03, -8.3187e-02,  ..., -2.9306e-02,\n            4.4721e-02, -4.1384e-03],\n          [-3.5232e-02, -6.6743e-03, -5.9538e-02,  ...,  1.3002e-02,\n            3.5940e-02, -3.8297e-04],\n          [ 2.9889e-02, -2.7368e-02, -2.1515e-02,  ...,  3.4143e-02,\n           -1.6917e-02,  3.0183e-02]]],\n\n\n        [[[ 3.5796e-03,  2.4947e-03,  8.9467e-03,  ..., -5.7025e-04,\n            7.1207e-03, -1.7705e-03],\n          [-3.5575e-02, -3.8708e-02,  3.8982e-02,  ...,  1.4823e-02,\n           -5.6198e-02, -2.4170e-02],\n          [-2.7347e-02, -4.7303e-02,  2.8746e-02,  ..., -5.0932e-02,\n           -8.5914e-02,  3.4880e-02],\n          ...,\n          [-8.3180e-02, -5.5109e-02,  6.0643e-03,  ..., -5.7978e-03,\n           -6.9814e-02, -8.4688e-02],\n          [-2.4292e-02, -6.4872e-02,  1.4165e-02,  ..., -7.3159e-03,\n           -1.6722e-02, -8.7978e-02],\n          [-5.8600e-02, -4.2183e-02, -1.7662e-02,  ..., -3.9003e-03,\n           -8.0696e-02, -5.7562e-02]],\n\n         [[ 7.3487e-03, -4.7574e-03, -2.6589e-03,  ...,  2.1361e-04,\n            8.6717e-03,  7.5384e-04],\n          [-7.5886e-02, -4.0042e-02,  3.8769e-02,  ..., -4.4430e-03,\n           -3.4238e-02, -1.4854e-02],\n          [-6.5820e-02, -7.2883e-02, -4.8486e-02,  ...,  2.5517e-02,\n           -2.9675e-02, -7.9238e-02],\n          ...,\n          [-9.2367e-02, -3.2233e-02, -1.8676e-02,  ...,  4.6257e-02,\n           -1.2666e-02,  7.4060e-02],\n          [-4.2820e-02, -5.6851e-02, -1.8953e-02,  ...,  1.0556e-02,\n           -2.2884e-02, -5.3207e-02],\n          [-9.3182e-02, -3.2837e-02, -5.9402e-02,  ...,  5.0628e-02,\n           -9.9679e-03,  2.6984e-02]],\n\n         [[-8.8195e-04, -5.1124e-04, -2.5661e-03,  ...,  3.6047e-03,\n           -3.1039e-03,  1.7182e-04],\n          [ 6.8338e-02,  3.5110e-02, -4.3565e-02,  ..., -9.8986e-04,\n            6.0343e-02, -1.3808e-02],\n          [-5.1855e-03,  1.6052e-02, -4.7414e-02,  ..., -1.0399e-01,\n            5.5306e-02,  2.8132e-02],\n          ...,\n          [-1.6971e-03,  2.5073e-02,  2.6328e-02,  ..., -2.7846e-02,\n            5.1152e-02,  7.0009e-03],\n          [ 1.0981e-02,  8.0463e-04, -2.7473e-02,  ...,  7.5948e-03,\n            7.0039e-02, -9.8815e-03],\n          [-2.2481e-02, -1.1305e-02,  1.2686e-02,  ..., -5.9937e-02,\n            4.1172e-02,  1.1969e-02]],\n\n         ...,\n\n         [[-1.9469e-03, -7.8590e-04, -2.2111e-03,  ...,  1.7589e-03,\n           -7.6032e-03,  9.5458e-03],\n          [-6.5730e-04, -4.9279e-02, -3.7917e-03,  ...,  1.9239e-02,\n           -1.3493e-02, -4.4985e-03],\n          [ 3.9774e-02, -4.0390e-02,  6.9341e-02,  ...,  7.1774e-02,\n            1.3768e-02, -4.7837e-02],\n          ...,\n          [ 1.9473e-02, -5.2244e-02,  8.3558e-02,  ...,  3.3703e-02,\n            2.5970e-02, -4.3264e-02],\n          [ 4.1110e-02, -1.9218e-02,  5.6278e-02,  ...,  3.2228e-02,\n            3.9979e-02,  3.5345e-03],\n          [ 2.7541e-02, -4.6998e-02,  5.2801e-02,  ...,  1.6975e-02,\n            5.0250e-02, -4.4104e-02]],\n\n         [[-3.6953e-03,  2.9426e-03, -8.0733e-03,  ...,  3.8522e-03,\n            3.9171e-03,  2.0712e-03],\n          [ 5.2397e-02,  2.9235e-02,  7.9064e-02,  ...,  1.6597e-01,\n            1.8458e-02,  3.7178e-03],\n          [ 2.2674e-02,  2.1083e-01,  5.1485e-02,  ..., -6.3919e-02,\n           -6.8707e-02, -1.3028e-01],\n          ...,\n          [-5.3074e-02,  3.1751e-01,  6.1428e-02,  ...,  1.3346e-01,\n            1.0574e-01,  6.2364e-02],\n          [-5.0529e-02,  1.3053e-01,  3.4387e-02,  ...,  3.3176e-02,\n            7.1810e-02,  9.4851e-02],\n          [-6.1783e-02,  3.1205e-01,  1.1191e-01,  ...,  1.0515e-01,\n            1.7889e-02, -6.0415e-02]],\n\n         [[ 4.9166e-03,  5.6642e-04,  3.1530e-03,  ..., -9.7834e-04,\n            4.4121e-03,  4.7009e-03],\n          [-2.3528e-02, -1.9456e-02, -3.8360e-03,  ..., -2.2225e-03,\n           -2.0198e-02, -5.9659e-02],\n          [ 2.0214e-02,  2.0431e-02, -8.7879e-03,  ..., -7.7737e-02,\n           -7.4588e-02, -1.4870e-01],\n          ...,\n          [ 4.1588e-02, -1.6800e-02, -5.4090e-02,  ...,  4.5229e-02,\n           -4.8153e-02, -2.6206e-02],\n          [ 2.0910e-02,  4.2224e-02, -2.5984e-02,  ...,  4.0191e-02,\n           -9.8526e-03, -4.1944e-03],\n          [-1.9341e-02,  8.2647e-03, -8.4056e-02,  ...,  7.5430e-02,\n           -4.1423e-02, -5.0166e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>)), (tensor([[[[1.4890e+00, 1.6985e+00, 3.6899e-01,  ..., -1.6904e-01, 1.5792e-01, 1.9374e-01],\n          [6.7411e-01, 7.4564e-01, 1.2274e+00,  ..., 7.4631e-01, -7.4065e-01, -6.5749e-01],\n          [6.6308e-01, 7.3152e-01, 1.2394e+00,  ..., 7.5959e-01, -7.5353e-01, -6.6932e-01],\n          ...,\n          [6.1583e-01, 6.7929e-01, 1.2883e+00,  ..., 8.1047e-01, -8.0387e-01, -7.1792e-01],\n          [6.2094e-01, 6.8718e-01, 1.2823e+00,  ..., 8.0334e-01, -7.9709e-01, -7.1209e-01],\n          [6.1480e-01, 6.8077e-01, 1.2885e+00,  ..., 8.0969e-01, -8.0341e-01, -7.1830e-01]],\n\n         [[4.4449e-01, -1.6483e-01, -1.2195e+00,  ..., 3.4685e-02, -6.0196e-02, -1.0735e+00],\n          [1.1970e+00, -8.6216e-01, -5.5589e-01,  ..., 1.3035e+00, -6.4928e-01, -3.1969e-01],\n          [1.2175e+00, -8.8125e-01, -5.3768e-01,  ..., 1.3390e+00, -6.6560e-01, -2.9910e-01],\n          ...,\n          [1.2782e+00, -9.3941e-01, -4.8106e-01,  ..., 1.4171e+00, -7.1853e-01, -2.3846e-01],\n          [1.2719e+00, -9.3376e-01, -4.8629e-01,  ..., 1.4025e+00, -7.1420e-01, -2.4486e-01],\n          [1.2759e+00, -9.3735e-01, -4.8295e-01,  ..., 1.4104e+00, -7.1702e-01, -2.4084e-01]],\n\n         [[6.3673e-02, 1.6179e+00, -5.8001e-01,  ..., -1.2565e+00, -7.2014e-02, -8.4150e-02],\n          [8.9617e-01, 7.7819e-01, -1.1567e+00,  ..., -2.3863e-01, 9.7932e-01, 7.2301e-01],\n          [9.1174e-01, 7.6255e-01, -1.1703e+00,  ..., -2.2080e-01, 9.9762e-01, 7.3833e-01],\n          ...,\n          [9.6217e-01, 7.1168e-01, -1.2048e+00,  ..., -1.6084e-01, 1.0592e+00, 7.8733e-01],\n          [9.4071e-01, 7.3333e-01, -1.1908e+00,  ..., -1.8755e-01, 1.0316e+00, 7.6661e-01],\n          [9.4784e-01, 7.2616e-01, -1.1959e+00,  ..., -1.7956e-01, 1.0397e+00, 7.7359e-01]],\n\n         ...,\n\n         [[1.5615e+00, 2.8368e-01, -3.4091e-02,  ..., 2.1744e-01, 1.8886e-02, 2.2510e-01],\n          [8.0416e-01, 1.2439e+00, -8.6741e-01,  ..., 1.2645e+00, -7.7486e-01, 1.2639e+00],\n          [8.0265e-01, 1.2480e+00, -8.6976e-01,  ..., 1.2700e+00, -7.7676e-01, 1.2693e+00],\n          ...,\n          [7.6413e-01, 1.2979e+00, -9.1277e-01,  ..., 1.3241e+00, -8.1751e-01, 1.3230e+00],\n          [7.7884e-01, 1.2799e+00, -8.9683e-01,  ..., 1.3046e+00, -8.0222e-01, 1.3037e+00],\n          [7.7270e-01, 1.2862e+00, -9.0305e-01,  ..., 1.3110e+00, -8.0839e-01, 1.3101e+00]],\n\n         [[3.2005e-01, -1.0829e+00, 5.4474e-02,  ..., 3.6405e-02, -2.5220e-01, 3.4876e-01],\n          [-6.5403e-01, -2.8600e-01, 1.3103e+00,  ..., 8.3470e-01, -1.2532e+00, 1.1896e+00],\n          [-6.6487e-01, -2.7707e-01, 1.3258e+00,  ..., 8.4365e-01, -1.2644e+00, 1.1990e+00],\n          ...,\n          [-7.1009e-01, -2.4182e-01, 1.3815e+00,  ..., 8.7903e-01, -1.3108e+00, 1.2369e+00],\n          [-6.9746e-01, -2.5098e-01, 1.3626e+00,  ..., 8.6986e-01, -1.2976e+00, 1.2269e+00],\n          [-6.9289e-01, -2.5545e-01, 1.3587e+00,  ..., 8.6538e-01, -1.2931e+00, 1.2224e+00]],\n\n         [[2.6532e-01, 3.3320e-01, -2.3979e-01,  ..., 4.8500e-01, -3.1935e-01, -3.4577e-01],\n          [1.2198e+00, 1.2000e+00, -1.2206e+00,  ..., 1.1579e+00, -1.1999e+00, -1.1969e+00],\n          [1.2468e+00, 1.2240e+00, -1.2485e+00,  ..., 1.1755e+00, -1.2244e+00, -1.2203e+00],\n          ...,\n          [1.3284e+00, 1.2994e+00, -1.3319e+00,  ..., 1.2355e+00, -1.3007e+00, -1.2946e+00],\n          [1.3049e+00, 1.2784e+00, -1.3078e+00,  ..., 1.2198e+00, -1.2793e+00, -1.2740e+00],\n          [1.2930e+00, 1.2667e+00, -1.2958e+00,  ..., 1.2090e+00, -1.2676e+00, -1.2624e+00]]],\n\n\n        [[[1.4989e+00, 1.7098e+00, 3.5866e-01,  ..., -1.7998e-01, 1.6868e-01, 2.0399e-01],\n          [6.7245e-01, 7.4245e-01, 1.2295e+00,  ..., 7.4906e-01, -7.4321e-01, -6.5954e-01],\n          [7.4361e-01, 8.2219e-01, 1.1556e+00,  ..., 6.7162e-01, -6.6679e-01, -5.8609e-01],\n          ...,\n          [7.2952e-01, 8.0830e-01, 1.1696e+00,  ..., 6.8551e-01, -6.8072e-01, -6.0010e-01],\n          [6.8580e-01, 7.5651e-01, 1.2159e+00,  ..., 7.3517e-01, -7.2942e-01, -6.4599e-01],\n          [7.0573e-01, 7.8161e-01, 1.1944e+00,  ..., 7.1146e-01, -7.0633e-01, -6.2469e-01]],\n\n         [[4.3517e-01, -1.5610e-01, -1.2279e+00,  ..., 2.0275e-02, -5.2645e-02, -1.0828e+00],\n          [1.2325e+00, -8.9587e-01, -5.2328e-01,  ..., 1.3532e+00, -6.7938e-01, -2.8417e-01],\n          [1.1713e+00, -8.3802e-01, -5.7907e-01,  ..., 1.2627e+00, -6.2819e-01, -3.4545e-01],\n          ...,\n          [1.1825e+00, -8.4773e-01, -5.7032e-01,  ..., 1.2864e+00, -6.3482e-01, -3.3413e-01],\n          [1.2224e+00, -8.8563e-01, -5.3366e-01,  ..., 1.3417e+00, -6.6853e-01, -2.9424e-01],\n          [1.2174e+00, -8.8120e-01, -5.3775e-01,  ..., 1.3296e+00, -6.6518e-01, -2.9927e-01]],\n\n         [[5.3667e-02, 1.6280e+00, -5.7260e-01,  ..., -1.2682e+00, -8.4087e-02, -9.3909e-02],\n          [8.9295e-01, 7.8143e-01, -1.1565e+00,  ..., -2.4266e-01, 9.7524e-01, 7.1999e-01],\n          [8.3281e-01, 8.4201e-01, -1.1100e+00,  ..., -3.1260e-01, 9.0352e-01, 6.6118e-01],\n          ...,\n          [8.5932e-01, 8.1535e-01, -1.1296e+00,  ..., -2.8159e-01, 9.3528e-01, 6.8704e-01],\n          [9.0047e-01, 7.7380e-01, -1.1567e+00,  ..., -2.3154e-01, 9.8688e-01, 7.2691e-01],\n          [8.6700e-01, 8.0749e-01, -1.1319e+00,  ..., -2.7053e-01, 9.4694e-01, 6.9423e-01]],\n\n         ...,\n\n         [[1.5698e+00, 2.7313e-01, -2.4893e-02,  ..., 2.0598e-01, 2.7659e-02, 2.1373e-01],\n          [8.0737e-01, 1.2418e+00, -8.6456e-01,  ..., 1.2630e+00, -7.7183e-01, 1.2623e+00],\n          [8.4967e-01, 1.1864e+00, -8.1730e-01,  ..., 1.2022e+00, -7.2714e-01, 1.2020e+00],\n          ...,\n          [8.3451e-01, 1.2095e+00, -8.3546e-01,  ..., 1.2287e+00, -7.4377e-01, 1.2282e+00],\n          [8.0669e-01, 1.2467e+00, -8.6684e-01,  ..., 1.2695e+00, -7.7331e-01, 1.2687e+00],\n          [8.0492e-01, 1.2465e+00, -8.6802e-01,  ..., 1.2682e+00, -7.7483e-01, 1.2675e+00]],\n\n         [[3.3044e-01, -1.0916e+00, 4.1452e-02,  ..., 2.7728e-02, -2.4155e-01, 3.3966e-01],\n          [-6.7755e-01, -2.6524e-01, 1.3370e+00,  ..., 8.5552e-01, -1.2770e+00, 1.2111e+00],\n          [-6.1277e-01, -3.2076e-01, 1.2595e+00,  ..., 7.9988e-01, -1.2110e+00, 1.1532e+00],\n          ...,\n          [-6.3082e-01, -3.0635e-01, 1.2818e+00,  ..., 8.1430e-01, -1.2295e+00, 1.1686e+00],\n          [-6.6811e-01, -2.7701e-01, 1.3305e+00,  ..., 8.4374e-01, -1.2679e+00, 1.2000e+00],\n          [-6.4742e-01, -2.9264e-01, 1.3010e+00,  ..., 8.2807e-01, -1.2464e+00, 1.1831e+00]],\n\n         [[2.5404e-01, 3.2281e-01, -2.2825e-01,  ..., 4.7664e-01, -3.0883e-01, -3.3555e-01],\n          [1.2543e+00, 1.2327e+00, -1.2556e+00,  ..., 1.1864e+00, -1.2328e+00, -1.2292e+00],\n          [1.1946e+00, 1.1780e+00, -1.1945e+00,  ..., 1.1427e+00, -1.1774e+00, -1.1754e+00],\n          ...,\n          [1.2299e+00, 1.2085e+00, -1.2311e+00,  ..., 1.1620e+00, -1.2086e+00, -1.2050e+00],\n          [1.2777e+00, 1.2527e+00, -1.2800e+00,  ..., 1.1977e+00, -1.2534e+00, -1.2486e+00],\n          [1.2618e+00, 1.2383e+00, -1.2637e+00,  ..., 1.1866e+00, -1.2388e+00, -1.2345e+00]]],\n\n\n        [[[1.4947e+00, 1.7045e+00, 3.6320e-01,  ..., -1.7501e-01, 1.6384e-01, 1.9950e-01],\n          [7.2946e-01, 8.0652e-01, 1.1702e+00,  ..., 6.8689e-01, -6.8187e-01, -6.0064e-01],\n          [8.3161e-01, 9.2220e-01, 1.0637e+00,  ..., 5.7480e-01, -5.7140e-01, -4.9485e-01],\n          ...,\n          [6.6598e-01, 7.3611e-01, 1.2360e+00,  ..., 7.5545e-01, -7.4965e-01, -6.6599e-01],\n          [7.3900e-01, 8.1822e-01, 1.1600e+00,  ..., 6.7575e-01, -6.7103e-01, -5.9054e-01],\n          [7.5154e-01, 8.3360e-01, 1.1466e+00,  ..., 6.6114e-01, -6.5676e-01, -5.7725e-01]],\n\n         [[4.3962e-01, -1.6009e-01, -1.2242e+00,  ..., 2.8926e-02, -5.5770e-02, -1.0783e+00],\n          [1.1657e+00, -8.3186e-01, -5.8553e-01,  ..., 1.2639e+00, -6.2123e-01, -3.5103e-01],\n          [1.0873e+00, -7.5911e-01, -6.5485e-01,  ..., 1.1372e+00, -5.5962e-01, -4.2949e-01],\n          ...,\n          [1.2296e+00, -8.9286e-01, -5.2637e-01,  ..., 1.3486e+00, -6.7600e-01, -2.8709e-01],\n          [1.1720e+00, -8.3760e-01, -5.8017e-01,  ..., 1.2726e+00, -6.2565e-01, -3.4472e-01],\n          [1.1491e+00, -8.1698e-01, -5.9940e-01,  ..., 1.2301e+00, -6.0955e-01, -3.6767e-01]],\n\n         [[5.9509e-02, 1.6221e+00, -5.7611e-01,  ..., -1.2608e+00, -7.6322e-02, -8.8296e-02],\n          [8.6426e-01, 8.1039e-01, -1.1353e+00,  ..., -2.7688e-01, 9.3999e-01, 6.9205e-01],\n          [7.8034e-01, 8.9505e-01, -1.0771e+00,  ..., -3.7852e-01, 8.3517e-01, 6.1060e-01],\n          ...,\n          [9.0312e-01, 7.7114e-01, -1.1605e+00,  ..., -2.2959e-01, 9.8876e-01, 7.2966e-01],\n          [8.4384e-01, 8.3096e-01, -1.1193e+00,  ..., -3.0081e-01, 9.1541e-01, 6.7209e-01],\n          [8.2780e-01, 8.4718e-01, -1.1094e+00,  ..., -3.2142e-01, 8.9395e-01, 6.5666e-01]],\n\n         ...,\n\n         [[1.5660e+00, 2.7842e-01, -2.9288e-02,  ..., 2.1190e-01, 2.3536e-02, 2.1959e-01],\n          [8.5043e-01, 1.1875e+00, -8.1718e-01,  ..., 1.2041e+00, -7.2667e-01, 1.2039e+00],\n          [9.3396e-01, 1.0857e+00, -7.2661e-01,  ..., 1.0952e+00, -6.3972e-01, 1.0956e+00],\n          ...,\n          [7.8892e-01, 1.2645e+00, -8.8472e-01,  ..., 1.2871e+00, -7.9115e-01, 1.2864e+00],\n          [8.5193e-01, 1.1873e+00, -8.1627e-01,  ..., 1.2043e+00, -7.2551e-01, 1.2040e+00],\n          [8.6612e-01, 1.1682e+00, -8.0024e-01,  ..., 1.1832e+00, -7.1043e-01, 1.1831e+00]],\n\n         [[3.2401e-01, -1.0865e+00, 5.0051e-02,  ..., 3.2832e-02, -2.4819e-01, 3.4508e-01],\n          [-6.2333e-01, -3.1190e-01, 1.2729e+00,  ..., 8.0876e-01, -1.2218e+00, 1.1625e+00],\n          [-5.3236e-01, -3.8753e-01, 1.1596e+00,  ..., 7.3296e-01, -1.1286e+00, 1.0830e+00],\n          ...,\n          [-6.5810e-01, -2.8328e-01, 1.3151e+00,  ..., 8.3745e-01, -1.2574e+00, 1.1927e+00],\n          [-6.2524e-01, -3.1028e-01, 1.2738e+00,  ..., 8.1038e-01, -1.2237e+00, 1.1642e+00],\n          [-5.8899e-01, -3.4114e-01, 1.2300e+00,  ..., 7.7947e-01, -1.1866e+00, 1.1320e+00]],\n\n         [[2.5934e-01, 3.2749e-01, -2.3374e-01,  ..., 4.7996e-01, -3.1360e-01, -3.4011e-01],\n          [1.1946e+00, 1.1771e+00, -1.1947e+00,  ..., 1.1403e+00, -1.1766e+00, -1.1744e+00],\n          [1.0938e+00, 1.0835e+00, -1.0918e+00,  ..., 1.0633e+00, -1.0819e+00, -1.0821e+00],\n          ...,\n          [1.2614e+00, 1.2382e+00, -1.2632e+00,  ..., 1.1874e+00, -1.2386e+00, -1.2344e+00],\n          [1.2026e+00, 1.1837e+00, -1.2031e+00,  ..., 1.1426e+00, -1.1834e+00, -1.1807e+00],\n          [1.1704e+00, 1.1551e+00, -1.1698e+00,  ..., 1.1221e+00, -1.1543e+00, -1.1528e+00]]],\n\n\n        ...,\n\n\n        [[[1.5028e+00, 1.7134e+00, 3.5485e-01,  ..., -1.8367e-01, 1.7241e-01, 2.0781e-01],\n          [7.2490e-01, 8.0286e-01, 1.1745e+00,  ..., 6.9083e-01, -6.8591e-01, -6.0500e-01],\n          [6.4967e-01, 7.1483e-01, 1.2538e+00,  ..., 7.7537e-01, -7.6893e-01, -6.8358e-01],\n          ...,\n          [7.0087e-01, 7.7567e-01, 1.1996e+00,  ..., 7.1714e-01, -7.1192e-01, -6.2986e-01],\n          [7.4614e-01, 8.2704e-01, 1.1524e+00,  ..., 6.6745e-01, -6.6295e-01, -5.8301e-01],\n          [7.1031e-01, 7.8823e-01, 1.1892e+00,  ..., 7.0547e-01, -7.0062e-01, -6.1965e-01]],\n\n         [[4.3245e-01, -1.5320e-01, -1.2309e+00,  ..., 1.9858e-02, -4.9436e-02, -1.0855e+00],\n          [1.1601e+00, -8.2696e-01, -5.9006e-01,  ..., 1.2513e+00, -6.1744e-01, -3.5662e-01],\n          [1.2547e+00, -9.1630e-01, -5.0395e-01,  ..., 1.3919e+00, -6.9627e-01, -2.6190e-01],\n          ...,\n          [1.1892e+00, -8.5426e-01, -5.6388e-01,  ..., 1.2918e+00, -6.4097e-01, -3.2750e-01],\n          [1.1364e+00, -8.0452e-01, -6.1176e-01,  ..., 1.2138e+00, -5.9733e-01, -3.8032e-01],\n          [1.1659e+00, -8.3287e-01, -5.8413e-01,  ..., 1.2513e+00, -6.2324e-01, -3.5086e-01]],\n\n         [[5.1551e-02, 1.6301e+00, -5.6958e-01,  ..., -1.2696e+00, -8.5256e-02, -9.6132e-02],\n          [8.6192e-01, 8.1280e-01, -1.1345e+00,  ..., -2.8069e-01, 9.3587e-01, 6.8989e-01],\n          [9.3439e-01, 7.3968e-01, -1.1863e+00,  ..., -1.9334e-01, 1.0260e+00, 7.6033e-01],\n          ...,\n          [8.8026e-01, 7.9422e-01, -1.1456e+00,  ..., -2.5818e-01, 9.5911e-01, 7.0758e-01],\n          [8.2086e-01, 8.5413e-01, -1.1024e+00,  ..., -3.2910e-01, 8.8611e-01, 6.4978e-01],\n          [8.7049e-01, 8.0423e-01, -1.1418e+00,  ..., -2.7322e-01, 9.4305e-01, 6.9849e-01]],\n\n         ...,\n\n         [[1.5729e+00, 2.7002e-01, -2.1762e-02,  ..., 2.0296e-01, 3.0775e-02, 2.1069e-01],\n          [8.5619e-01, 1.1790e+00, -8.1048e-01,  ..., 1.1946e+00, -7.2046e-01, 1.1944e+00],\n          [7.8860e-01, 1.2665e+00, -8.8550e-01,  ..., 1.2903e+00, -7.9163e-01, 1.2894e+00],\n          ...,\n          [8.1935e-01, 1.2260e+00, -8.5128e-01,  ..., 1.2451e+00, -7.5928e-01, 1.2447e+00],\n          [8.7681e-01, 1.1560e+00, -7.8901e-01,  ..., 1.1702e+00, -6.9949e-01, 1.1703e+00],\n          [8.5097e-01, 1.1849e+00, -8.1608e-01,  ..., 1.2002e+00, -7.2594e-01, 1.2001e+00]],\n\n         [[3.3019e-01, -1.0919e+00, 4.2975e-02,  ..., 2.7392e-02, -2.4191e-01, 3.3946e-01],\n          [-6.2069e-01, -3.1425e-01, 1.2699e+00,  ..., 8.0638e-01, -1.2191e+00, 1.1601e+00],\n          [-6.9168e-01, -2.5520e-01, 1.3589e+00,  ..., 8.6559e-01, -1.2919e+00, 1.2221e+00],\n          ...,\n          [-6.1315e-01, -3.2249e-01, 1.2609e+00,  ..., 7.9820e-01, -1.2115e+00, 1.1521e+00],\n          [-5.7351e-01, -3.5566e-01, 1.2116e+00,  ..., 7.6495e-01, -1.1709e+00, 1.1173e+00],\n          [-6.1726e-01, -3.1498e-01, 1.2589e+00,  ..., 8.0565e-01, -1.2151e+00, 1.1589e+00]],\n\n         [[2.5030e-01, 3.1891e-01, -2.2455e-01,  ..., 4.7254e-01, -3.0495e-01, -3.3162e-01],\n          [1.1693e+00, 1.1532e+00, -1.1690e+00,  ..., 1.1200e+00, -1.1526e+00, -1.1508e+00],\n          [1.2690e+00, 1.2444e+00, -1.2712e+00,  ..., 1.1915e+00, -1.2451e+00, -1.2404e+00],\n          ...,\n          [1.2325e+00, 1.2118e+00, -1.2336e+00,  ..., 1.1659e+00, -1.2118e+00, -1.2084e+00],\n          [1.1688e+00, 1.1539e+00, -1.1681e+00,  ..., 1.1214e+00, -1.1530e+00, -1.1516e+00],\n          [1.2034e+00, 1.1861e+00, -1.2034e+00,  ..., 1.1483e+00, -1.1856e+00, -1.1834e+00]]],\n\n\n        [[[1.5134e+00, 1.7257e+00, 3.4372e-01,  ..., -1.9549e-01, 1.8402e-01, 2.1886e-01],\n          [6.9329e-01, 7.6546e-01, 1.2080e+00,  ..., 7.2664e-01, -7.2104e-01, -6.3813e-01],\n          [6.5724e-01, 7.2434e-01, 1.2457e+00,  ..., 7.6639e-01, -7.6019e-01, -6.7552e-01],\n          ...,\n          [7.6712e-01, 8.5312e-01, 1.1298e+00,  ..., 6.4272e-01, -6.3882e-01, -5.6066e-01],\n          [6.3888e-01, 7.0687e-01, 1.2638e+00,  ..., 7.8410e-01, -7.7805e-01, -6.9365e-01],\n          [6.6468e-01, 7.3561e-01, 1.2370e+00,  ..., 7.5614e-01, -7.5043e-01, -6.6707e-01]],\n\n         [[4.2270e-01, -1.4414e-01, -1.2395e+00,  ..., 3.7169e-03, -4.1733e-02, -1.0953e+00],\n          [1.1891e+00, -8.5367e-01, -5.6475e-01,  ..., 1.3032e+00, -6.3972e-01, -3.2752e-01],\n          [1.2346e+00, -8.9733e-01, -5.2218e-01,  ..., 1.3623e+00, -6.7974e-01, -2.8208e-01],\n          ...,\n          [1.1090e+00, -7.7919e-01, -6.3579e-01,  ..., 1.1693e+00, -5.7628e-01, -4.0777e-01],\n          [1.2498e+00, -9.1275e-01, -5.0668e-01,  ..., 1.3726e+00, -6.9528e-01, -2.6688e-01],\n          [1.2225e+00, -8.8598e-01, -5.3311e-01,  ..., 1.3430e+00, -6.6970e-01, -2.9417e-01]],\n\n         [[4.1760e-02, 1.6399e+00, -5.6284e-01,  ..., -1.2816e+00, -9.7706e-02, -1.0562e-01],\n          [8.8963e-01, 7.8486e-01, -1.1544e+00,  ..., -2.4691e-01, 9.7079e-01, 7.1679e-01],\n          [9.3174e-01, 7.4241e-01, -1.1855e+00,  ..., -1.9832e-01, 1.0205e+00, 7.5795e-01],\n          ...,\n          [8.0213e-01, 8.7305e-01, -1.0907e+00,  ..., -3.5238e-01, 8.6202e-01, 6.3169e-01],\n          [9.4891e-01, 7.2513e-01, -1.1989e+00,  ..., -1.7921e-01, 1.0399e+00, 7.7479e-01],\n          [9.2940e-01, 7.4468e-01, -1.1799e+00,  ..., -1.9902e-01, 1.0200e+00, 7.5532e-01]],\n\n         ...,\n\n         [[1.5821e+00, 2.5832e-01, -1.1668e-02,  ..., 1.9016e-01, 4.0365e-02, 1.9800e-01],\n          [8.1956e-01, 1.2268e+00, -8.5123e-01,  ..., 1.2471e+00, -7.5907e-01, 1.2464e+00],\n          [7.8904e-01, 1.2644e+00, -8.8446e-01,  ..., 1.2874e+00, -7.9091e-01, 1.2866e+00],\n          ...,\n          [8.8586e-01, 1.1428e+00, -7.7844e-01,  ..., 1.1552e+00, -6.8971e-01, 1.1554e+00],\n          [7.8624e-01, 1.2705e+00, -8.8861e-01,  ..., 1.2945e+00, -7.9443e-01, 1.2936e+00],\n          [8.0848e-01, 1.2437e+00, -8.6461e-01,  ..., 1.2660e+00, -7.7130e-01, 1.2653e+00]],\n\n         [[3.4193e-01, -1.1014e+00, 2.7663e-02,  ..., 1.7906e-02, -2.2983e-01, 3.2942e-01],\n          [-6.6079e-01, -2.7898e-01, 1.3183e+00,  ..., 8.4168e-01, -1.2600e+00, 1.1965e+00],\n          [-6.8034e-01, -2.6310e-01, 1.3424e+00,  ..., 8.5766e-01, -1.2800e+00, 1.2134e+00],\n          ...,\n          [-5.5801e-01, -3.6495e-01, 1.1871e+00,  ..., 7.5560e-01, -1.1545e+00, 1.1065e+00],\n          [-6.6952e-01, -2.7558e-01, 1.3313e+00,  ..., 8.4522e-01, -1.2693e+00, 1.2014e+00],\n          [-6.6813e-01, -2.7628e-01, 1.3297e+00,  ..., 8.4450e-01, -1.2678e+00, 1.2005e+00]],\n\n         [[2.3809e-01, 3.0779e-01, -2.1201e-01,  ..., 4.6385e-01, -2.9366e-01, -3.2070e-01],\n          [1.2194e+00, 1.1986e+00, -1.2206e+00,  ..., 1.1547e+00, -1.1987e+00, -1.1953e+00],\n          [1.2572e+00, 1.2335e+00, -1.2592e+00,  ..., 1.1823e+00, -1.2340e+00, -1.2296e+00],\n          ...,\n          [1.1576e+00, 1.1445e+00, -1.1564e+00,  ..., 1.1165e+00, -1.1434e+00, -1.1426e+00],\n          [1.3004e+00, 1.2747e+00, -1.3029e+00,  ..., 1.2179e+00, -1.2755e+00, -1.2704e+00],\n          [1.2739e+00, 1.2504e+00, -1.2759e+00,  ..., 1.1992e+00, -1.2509e+00, -1.2466e+00]]],\n\n\n        [[[1.4825e+00, 1.6909e+00, 3.7582e-01,  ..., -1.6177e-01, 1.5078e-01, 1.8697e-01],\n          [7.6171e-01, 8.4487e-01, 1.1360e+00,  ..., 6.5016e-01, -6.4587e-01, -5.6676e-01],\n          [7.4450e-01, 8.2456e-01, 1.1542e+00,  ..., 6.6962e-01, -6.6495e-01, -5.8478e-01],\n          ...,\n          [6.9887e-01, 7.7311e-01, 1.2017e+00,  ..., 7.1953e-01, -7.1420e-01, -6.3200e-01],\n          [6.7983e-01, 7.5282e-01, 1.2212e+00,  ..., 7.3951e-01, -7.3405e-01, -6.5139e-01],\n          [7.2952e-01, 8.0836e-01, 1.1696e+00,  ..., 6.8552e-01, -6.8075e-01, -6.0012e-01]],\n\n         [[4.5113e-01, -1.7097e-01, -1.2137e+00,  ..., 4.5617e-02, -6.5374e-02, -1.0668e+00],\n          [1.1497e+00, -8.1748e-01, -5.9894e-01,  ..., 1.2335e+00, -6.0992e-01, -3.6705e-01],\n          [1.1453e+00, -8.1306e-01, -6.0338e-01,  ..., 1.2326e+00, -6.0555e-01, -3.7136e-01],\n          ...,\n          [1.1890e+00, -8.5441e-01, -5.6350e-01,  ..., 1.2927e+00, -6.4197e-01, -3.2770e-01],\n          [1.2104e+00, -8.7550e-01, -5.4261e-01,  ..., 1.3143e+00, -6.6233e-01, -3.0631e-01],\n          [1.1560e+00, -8.2386e-01, -5.9253e-01,  ..., 1.2394e+00, -6.1637e-01, -3.6073e-01]],\n\n         [[7.0234e-02, 1.6113e+00, -5.8455e-01,  ..., -1.2485e+00, -6.3780e-02, -7.7787e-02],\n          [8.2657e-01, 8.4841e-01, -1.1092e+00,  ..., -3.2189e-01, 8.9370e-01, 6.5543e-01],\n          [8.3990e-01, 8.3502e-01, -1.1200e+00,  ..., -3.0711e-01, 9.0873e-01, 6.6854e-01],\n          ...,\n          [8.8788e-01, 7.8657e-01, -1.1509e+00,  ..., -2.4854e-01, 9.6913e-01, 7.1496e-01],\n          [9.0581e-01, 7.6856e-01, -1.1669e+00,  ..., -2.3018e-01, 9.8754e-01, 7.3279e-01],\n          [8.6350e-01, 8.1119e-01, -1.1351e+00,  ..., -2.7879e-01, 9.3785e-01, 6.9141e-01]],\n\n         ...,\n\n         [[1.5553e+00, 2.9153e-01, -4.0882e-02,  ..., 2.2600e-01, 1.2422e-02, 2.3360e-01],\n          [8.7020e-01, 1.1606e+00, -7.9475e-01,  ..., 1.1745e+00, -7.0561e-01, 1.1744e+00],\n          [8.5884e-01, 1.1775e+00, -8.0813e-01,  ..., 1.1938e+00, -7.1794e-01, 1.1935e+00],\n          ...,\n          [8.2797e-01, 1.2141e+00, -8.4131e-01,  ..., 1.2325e+00, -7.4996e-01, 1.2321e+00],\n          [8.2163e-01, 1.2242e+00, -8.4908e-01,  ..., 1.2438e+00, -7.5702e-01, 1.2433e+00],\n          [8.5813e-01, 1.1767e+00, -8.0844e-01,  ..., 1.1919e+00, -7.1851e-01, 1.1918e+00]],\n\n         [[3.1268e-01, -1.0769e+00, 6.3814e-02,  ..., 4.2466e-02, -2.5976e-01, 3.5514e-01],\n          [-5.9025e-01, -3.3930e-01, 1.2322e+00,  ..., 7.8127e-01, -1.1879e+00, 1.1336e+00],\n          [-6.0621e-01, -3.2619e-01, 1.2521e+00,  ..., 7.9440e-01, -1.2043e+00, 1.1475e+00],\n          ...,\n          [-6.1974e-01, -3.1850e-01, 1.2732e+00,  ..., 8.0217e-01, -1.2186e+00, 1.1567e+00],\n          [-6.1932e-01, -3.1931e-01, 1.2721e+00,  ..., 8.0140e-01, -1.2182e+00, 1.1560e+00],\n          [-5.9253e-01, -3.4022e-01, 1.2377e+00,  ..., 7.8040e-01, -1.1906e+00, 1.1336e+00]],\n\n         [[2.7224e-01, 3.3946e-01, -2.4691e-01,  ..., 4.8978e-01, -3.2571e-01, -3.5191e-01],\n          [1.1436e+00, 1.1290e+00, -1.1428e+00,  ..., 1.0995e+00, -1.1281e+00, -1.1269e+00],\n          [1.1670e+00, 1.1501e+00, -1.1669e+00,  ..., 1.1150e+00, -1.1495e+00, -1.1475e+00],\n          ...,\n          [1.2057e+00, 1.1862e+00, -1.2064e+00,  ..., 1.1441e+00, -1.1860e+00, -1.1830e+00],\n          [1.2234e+00, 1.2033e+00, -1.2243e+00,  ..., 1.1595e+00, -1.2032e+00, -1.2000e+00],\n          [1.1594e+00, 1.1434e+00, -1.1590e+00,  ..., 1.1100e+00, -1.1427e+00, -1.1410e+00]]]],\n       device='cuda:0', grad_fn=<CloneBackward0>), tensor([[[[ 3.7057e-03,  3.2195e-03,  8.6933e-04,  ..., -2.1755e-02,\n           -6.1692e-03, -1.4586e-02],\n          [ 3.8886e-02, -2.5187e-03, -4.1543e-03,  ..., -1.6268e-02,\n           -6.4880e-02,  7.9820e-03],\n          [ 2.9146e-02, -1.0372e-02, -3.9332e-04,  ..., -3.1920e-02,\n           -7.1784e-02,  2.7001e-02],\n          ...,\n          [ 1.0802e-01, -4.8810e-02, -5.4357e-02,  ...,  4.1423e-02,\n           -1.9098e-02,  4.0279e-02],\n          [-1.3840e-02, -4.2362e-02, -7.4422e-02,  ..., -3.0915e-02,\n            6.7220e-02, -1.5551e-03],\n          [-4.3204e-02, -9.9695e-03, -1.1904e-01,  ..., -7.1242e-02,\n            3.6874e-02, -2.1303e-02]],\n\n         [[-9.9248e-03, -2.7547e-03,  7.4850e-03,  ..., -1.3158e-02,\n           -7.4519e-03, -2.3724e-03],\n          [ 3.9759e-02, -1.1679e-01,  3.2931e-02,  ...,  1.8676e-02,\n            2.0506e-02,  3.3108e-02],\n          [-8.9421e-04, -7.3220e-02, -4.1077e-03,  ...,  4.7623e-03,\n           -1.8686e-02,  3.3818e-02],\n          ...,\n          [-1.2418e-02,  7.9768e-02, -6.2818e-02,  ...,  3.5696e-02,\n           -6.9426e-02, -4.0913e-02],\n          [ 4.3509e-02,  4.9304e-02, -3.8662e-02,  ..., -1.0733e-03,\n           -2.3788e-02, -3.5969e-02],\n          [ 4.5142e-02,  4.9427e-02, -4.7408e-02,  ..., -4.5514e-02,\n           -6.8346e-02, -7.0794e-02]],\n\n         [[-5.0615e-04, -1.0798e-03,  1.7046e-03,  ..., -5.3816e-03,\n           -5.7116e-03,  1.5309e-02],\n          [ 6.0694e-02, -1.4325e-02,  4.1524e-02,  ...,  5.0284e-02,\n           -6.4950e-02,  1.8501e-02],\n          [ 9.2598e-02,  6.1426e-02,  3.7759e-02,  ...,  5.5047e-02,\n           -4.2154e-02,  1.6151e-02],\n          ...,\n          [ 8.9892e-02,  4.4337e-03, -1.4513e-02,  ...,  3.3882e-02,\n            4.6335e-02, -1.7844e-02],\n          [ 1.0365e-01,  6.3125e-02,  2.3575e-03,  ...,  3.8944e-02,\n            2.6812e-02,  3.7226e-02],\n          [ 9.4839e-02,  4.5347e-02,  1.2902e-02,  ...,  3.2417e-02,\n           -6.8662e-03,  7.6712e-02]],\n\n         ...,\n\n         [[-3.1117e-03,  4.2579e-03, -2.5601e-03,  ...,  2.2207e-03,\n            1.5274e-02, -1.7412e-02],\n          [ 1.8837e-02,  4.1317e-02, -4.8754e-02,  ...,  5.0126e-02,\n           -1.2245e-02, -7.3104e-03],\n          [ 3.8001e-02, -1.8330e-02, -1.0634e-01,  ..., -5.8981e-02,\n            5.6208e-02, -5.4266e-02],\n          ...,\n          [ 1.7536e-02,  1.6958e-02, -8.7768e-02,  ..., -5.7929e-02,\n            8.2213e-02, -7.2009e-02],\n          [-4.5164e-03,  4.7888e-02, -6.2056e-02,  ...,  1.7788e-02,\n            5.3656e-02, -7.3253e-02],\n          [ 3.0668e-02,  4.8003e-02, -2.9411e-02,  ...,  4.8273e-02,\n            4.5058e-02, -8.8269e-02]],\n\n         [[-6.3552e-03,  7.6137e-03,  7.4994e-03,  ...,  3.7087e-03,\n            8.5897e-04,  1.0977e-03],\n          [ 6.8928e-02, -3.5736e-02, -4.6490e-02,  ..., -7.7849e-03,\n           -2.2217e-03, -3.9661e-02],\n          [ 7.8655e-02, -7.3876e-02,  1.8349e-02,  ..., -2.5271e-02,\n           -2.2937e-02,  2.9826e-02],\n          ...,\n          [-3.9597e-03, -2.6727e-02,  5.2979e-02,  ..., -1.6940e-02,\n           -3.3870e-05, -1.8635e-02],\n          [ 2.5531e-02,  2.3268e-02, -2.4590e-02,  ...,  3.4486e-02,\n            9.4687e-02, -9.8920e-03],\n          [ 5.5857e-02,  1.1714e-02, -6.3114e-02,  ...,  2.8032e-02,\n            7.0656e-02, -5.3237e-03]],\n\n         [[-9.6889e-03, -6.8591e-03, -1.0816e-02,  ..., -8.1413e-03,\n           -1.6859e-03, -2.0148e-03],\n          [ 3.6199e-02,  9.2330e-02,  9.6504e-02,  ...,  3.7890e-02,\n            2.2634e-02, -7.4406e-03],\n          [-5.8129e-02, -2.1029e-03,  1.0269e-03,  ...,  6.6110e-02,\n            2.5287e-02, -2.6612e-02],\n          ...,\n          [ 7.4781e-03, -2.5366e-02, -1.1944e-02,  ...,  5.2434e-03,\n            1.7325e-02,  4.8229e-03],\n          [-1.8762e-02, -3.9431e-02, -7.4428e-02,  ..., -3.4496e-03,\n           -2.8714e-02,  3.7524e-02],\n          [-7.8980e-02,  3.2399e-02, -6.6124e-02,  ...,  4.2560e-03,\n           -5.2928e-02,  6.3290e-02]]],\n\n\n        [[[ 2.1438e-03,  7.6939e-03,  1.1409e-02,  ..., -1.2703e-02,\n           -3.0485e-03, -1.4242e-02],\n          [-1.6688e-02,  1.5796e-02, -5.0894e-02,  ..., -4.6729e-02,\n            2.2672e-02, -1.5863e-02],\n          [-1.4715e-02,  3.9467e-02, -1.9765e-03,  ...,  3.2461e-03,\n           -1.5803e-02,  3.7217e-02],\n          ...,\n          [-6.1748e-04, -2.6455e-02,  5.3109e-02,  ...,  4.9399e-02,\n            1.7585e-02,  1.8913e-02],\n          [-4.1459e-02, -5.6971e-02,  7.6274e-02,  ...,  1.4691e-02,\n            6.8199e-03, -4.4096e-02],\n          [-2.2736e-02, -4.7921e-02,  1.8168e-02,  ...,  5.2719e-02,\n           -2.1563e-02, -1.8303e-02]],\n\n         [[-1.2420e-02, -1.8269e-03,  4.3336e-03,  ..., -6.1667e-03,\n           -4.0761e-03,  3.8009e-03],\n          [ 4.2775e-02, -1.5588e-02, -5.2556e-02,  ...,  1.7245e-02,\n            3.4686e-02, -9.6206e-02],\n          [ 5.1487e-02, -3.0850e-02, -2.8471e-03,  ...,  1.4287e-02,\n            7.4232e-02, -6.5705e-02],\n          ...,\n          [ 2.0529e-03, -2.2242e-02, -2.2044e-02,  ...,  5.1642e-02,\n           -5.9166e-02,  4.8630e-02],\n          [-2.7172e-02, -2.5544e-02,  2.2385e-02,  ...,  6.3384e-02,\n           -9.1275e-02,  8.0677e-02],\n          [-3.5043e-02,  1.5339e-02, -1.1550e-02,  ...,  7.3286e-02,\n           -5.7204e-03,  9.2850e-02]],\n\n         [[-5.1235e-03, -6.2502e-04,  6.9324e-03,  ..., -1.2951e-02,\n           -5.9125e-03,  1.3353e-02],\n          [ 1.0214e-01,  4.5340e-02,  5.5199e-03,  ...,  2.4288e-02,\n            4.4051e-02,  4.9967e-02],\n          [ 4.1205e-02,  4.3305e-02, -1.0617e-02,  ...,  7.4854e-02,\n           -2.8934e-02, -4.6413e-02],\n          ...,\n          [-4.7518e-02,  7.2693e-03, -3.9326e-02,  ..., -1.0015e-01,\n            3.3021e-02, -9.5059e-02],\n          [-6.3381e-02, -6.8025e-02, -6.2167e-02,  ..., -2.5846e-02,\n            8.9569e-02, -7.5959e-02],\n          [-2.6940e-02, -4.6912e-02, -5.4502e-02,  ..., -6.7730e-02,\n            8.8554e-02, -5.2802e-02]],\n\n         ...,\n\n         [[ 9.9154e-04,  4.7073e-03,  1.6091e-03,  ..., -2.4787e-03,\n            1.5743e-02, -3.8199e-03],\n          [ 6.9804e-02,  3.7993e-03, -3.1277e-02,  ...,  1.9411e-02,\n           -2.6181e-03, -1.0996e-02],\n          [ 5.1922e-02, -2.5828e-03, -3.9170e-02,  ...,  2.7149e-02,\n           -4.1678e-02,  2.6795e-02],\n          ...,\n          [ 4.0802e-02,  8.6568e-03,  6.3216e-02,  ..., -1.4241e-02,\n           -4.9623e-03,  2.4811e-02],\n          [ 5.8305e-02, -2.4880e-02,  1.1268e-01,  ..., -2.2982e-02,\n           -2.2505e-02, -7.1126e-03],\n          [ 5.5549e-02, -1.5139e-02,  7.8659e-02,  ...,  3.5157e-02,\n           -2.1870e-02,  2.7958e-02]],\n\n         [[-6.0436e-03,  7.5554e-03,  9.2602e-03,  ...,  4.9012e-03,\n            9.5230e-04,  3.5779e-03],\n          [ 1.1899e-01,  2.2014e-03, -3.7221e-02,  ...,  4.8879e-02,\n            9.6451e-02, -2.5019e-02],\n          [ 6.5284e-02, -5.8818e-02,  5.0137e-03,  ...,  1.0736e-02,\n            2.8776e-02, -6.2639e-03],\n          ...,\n          [-7.6230e-02, -2.0345e-02,  8.7487e-02,  ...,  8.3695e-04,\n            1.9953e-02,  6.1249e-02],\n          [-5.3451e-02,  1.6461e-02,  7.8369e-02,  ...,  2.3584e-02,\n           -3.6624e-02,  9.0945e-02],\n          [-6.9500e-02, -1.7129e-02,  6.0991e-02,  ...,  7.4341e-03,\n           -7.2406e-03,  6.2889e-02]],\n\n         [[-9.0441e-03, -9.1206e-03, -1.2039e-02,  ..., -1.0236e-02,\n            7.4605e-04, -2.4756e-03],\n          [-4.7759e-02,  3.8138e-02, -1.1489e-02,  ...,  3.5716e-03,\n           -5.5266e-02,  3.5629e-02],\n          [-3.5722e-03,  4.3099e-03,  6.2444e-02,  ...,  2.5674e-02,\n           -2.1978e-02,  1.9555e-02],\n          ...,\n          [-4.4760e-02, -4.6826e-02,  1.7237e-02,  ..., -6.0756e-02,\n            2.2050e-02,  1.7722e-02],\n          [-6.1812e-02, -2.8696e-02,  1.4415e-02,  ..., -4.9440e-02,\n            4.2915e-02, -2.1957e-02],\n          [-9.3581e-02, -3.7533e-02,  3.5555e-02,  ..., -7.3924e-02,\n            3.5278e-02,  2.2444e-04]]],\n\n\n        [[[ 6.7688e-03,  1.2151e-02,  1.4236e-02,  ..., -1.5541e-02,\n           -1.7955e-03, -2.1260e-02],\n          [ 3.3633e-02, -3.1738e-02, -6.6682e-03,  ...,  1.5968e-02,\n           -4.7687e-02,  2.7822e-02],\n          [ 1.9201e-02,  6.1919e-03,  5.7947e-02,  ..., -1.0404e-03,\n            6.2595e-02, -5.2115e-02],\n          ...,\n          [-3.5926e-03, -1.2276e-02,  9.6199e-03,  ..., -9.2107e-03,\n           -2.8563e-03,  6.3472e-03],\n          [ 3.1771e-02, -3.6975e-04,  4.4121e-02,  ...,  6.5510e-02,\n           -5.3717e-02, -1.2616e-02],\n          [ 5.7351e-02,  2.3481e-02,  5.6303e-02,  ...,  6.8676e-02,\n           -2.8002e-02, -4.1537e-02]],\n\n         [[-1.5379e-02, -6.2128e-03,  8.9013e-03,  ..., -7.7359e-03,\n           -1.1298e-02,  5.3138e-03],\n          [-5.1168e-03, -4.1772e-02,  3.2273e-02,  ...,  3.4400e-02,\n           -2.9539e-02,  9.5498e-02],\n          [-1.2147e-01,  2.5382e-02,  5.8437e-02,  ..., -1.7446e-02,\n           -2.1106e-02, -2.8514e-02],\n          ...,\n          [ 5.8858e-02, -4.4776e-02, -3.0835e-02,  ...,  2.0387e-02,\n           -5.1353e-02, -3.8366e-02],\n          [ 4.5947e-02,  1.0744e-03, -6.0874e-03,  ...,  7.7351e-02,\n           -3.7452e-02,  6.9921e-03],\n          [ 3.5393e-02,  5.5540e-02, -3.5911e-02,  ...,  5.1979e-02,\n           -7.0437e-02, -4.9755e-03]],\n\n         [[-6.5919e-03, -4.7698e-03,  6.8514e-03,  ..., -1.3212e-02,\n           -6.9590e-03,  1.0866e-02],\n          [ 1.9684e-02,  2.1290e-02,  2.0079e-02,  ..., -2.0705e-02,\n            7.6862e-03, -3.7535e-02],\n          [-4.3475e-02,  2.3949e-02, -6.1134e-02,  ...,  3.1197e-02,\n            7.3607e-02, -4.0063e-02],\n          ...,\n          [ 7.9435e-02,  2.8491e-02,  3.4241e-02,  ...,  3.0813e-02,\n           -2.8689e-02, -3.2600e-02],\n          [ 5.8145e-02,  3.7961e-02,  7.6110e-02,  ..., -3.9088e-02,\n           -3.9849e-02, -6.9451e-02],\n          [ 2.8585e-02,  9.8507e-03,  1.1956e-01,  ..., -1.2321e-02,\n            3.3425e-03, -5.4235e-02]],\n\n         ...,\n\n         [[-1.9423e-03,  2.3262e-03, -5.9583e-04,  ...,  4.7987e-04,\n            1.8408e-02, -7.9926e-03],\n          [ 1.4733e-01, -2.3809e-02,  5.6976e-02,  ..., -2.0009e-02,\n            3.4863e-02, -1.9913e-02],\n          [-8.5097e-02,  1.4928e-02, -4.9170e-02,  ..., -6.7437e-02,\n           -2.9608e-02,  5.9069e-02],\n          ...,\n          [-1.4891e-03,  3.5726e-02, -3.3787e-02,  ...,  5.1933e-02,\n            2.1603e-02, -1.9865e-02],\n          [ 7.3927e-02, -1.8103e-02,  4.4972e-02,  ..., -3.8876e-02,\n            7.1548e-02,  4.6765e-02],\n          [ 4.5380e-04, -5.0534e-02,  1.5710e-02,  ..., -1.4634e-02,\n            2.9256e-02,  7.2731e-02]],\n\n         [[-9.4201e-03,  1.2063e-02,  1.4791e-02,  ...,  6.8300e-03,\n           -2.3124e-03, -1.5925e-03],\n          [ 1.5030e-02, -3.9917e-02, -4.5076e-02,  ...,  4.6973e-02,\n            1.5427e-02,  3.1331e-02],\n          [ 1.1676e-02,  3.3022e-02,  7.3894e-02,  ...,  5.5475e-02,\n           -4.7974e-02, -3.4902e-02],\n          ...,\n          [ 8.4245e-02, -1.0958e-02, -4.6689e-02,  ..., -4.4871e-02,\n            6.9953e-02, -3.2786e-02],\n          [ 1.9004e-03, -4.3348e-02, -7.8393e-02,  ..., -3.1839e-02,\n            2.3876e-04,  1.5394e-02],\n          [-7.0312e-02, -5.1512e-02,  3.8987e-03,  ..., -1.8674e-02,\n           -1.7709e-02, -4.7346e-03]],\n\n         [[-5.5938e-03, -8.6013e-03, -5.3485e-03,  ..., -5.0152e-03,\n            4.0291e-05, -1.7861e-03],\n          [-3.7046e-02,  9.2476e-03,  4.0434e-02,  ..., -3.2991e-03,\n            4.4283e-03,  5.7179e-02],\n          [ 1.8195e-03, -3.8156e-02, -1.4652e-02,  ...,  1.5070e-02,\n            4.4317e-03, -3.2425e-03],\n          ...,\n          [ 2.7674e-02,  4.4982e-02,  5.4319e-04,  ...,  1.3705e-02,\n            8.2405e-03, -3.4077e-02],\n          [-4.5559e-04,  1.0659e-02,  3.8047e-02,  ...,  5.4592e-02,\n            3.8394e-02,  1.9451e-02],\n          [-1.4618e-03, -2.4183e-02,  3.9285e-02,  ...,  6.2338e-02,\n            9.0237e-04,  1.9820e-02]]],\n\n\n        ...,\n\n\n        [[[ 7.7373e-03,  1.4470e-02,  2.2316e-02,  ..., -8.4624e-03,\n           -1.7049e-03, -2.2756e-02],\n          [ 1.5989e-02, -9.1841e-02, -6.5012e-02,  ...,  1.8689e-02,\n            1.5911e-02,  1.2601e-03],\n          [ 1.7993e-03, -6.7139e-02, -6.1170e-02,  ..., -6.0854e-02,\n           -8.2609e-03, -3.1417e-04],\n          ...,\n          [-9.9805e-03,  1.9730e-03, -6.5398e-02,  ...,  6.7447e-03,\n           -3.3608e-02,  4.7485e-02],\n          [-1.0856e-02,  7.7351e-02,  9.8423e-03,  ...,  1.6438e-02,\n            3.9170e-02,  9.3598e-03],\n          [ 2.8537e-02, -3.1303e-02, -5.5722e-02,  ..., -1.4557e-02,\n            7.6077e-03,  6.5000e-03]],\n\n         [[-1.3469e-02, -9.3413e-03,  8.5402e-03,  ...,  1.8364e-03,\n           -7.9837e-03,  1.4035e-02],\n          [ 2.4580e-02,  1.8376e-02,  3.9391e-02,  ..., -1.3314e-02,\n            7.3462e-03, -6.7893e-02],\n          [ 6.1495e-03, -7.4408e-02,  3.0861e-02,  ..., -1.4668e-02,\n            5.2990e-02, -3.5136e-02],\n          ...,\n          [ 9.7202e-02,  3.5622e-02, -5.2453e-02,  ..., -5.0864e-02,\n           -3.8111e-02, -7.9951e-02],\n          [ 5.9073e-02,  2.2222e-02, -3.7797e-02,  ..., -5.4488e-02,\n           -3.2397e-02, -3.1133e-02],\n          [ 7.9657e-02,  4.7603e-02, -1.2424e-02,  ..., -1.6932e-02,\n           -1.1159e-01,  1.8984e-02]],\n\n         [[-1.3619e-02, -7.6218e-03,  1.5664e-02,  ..., -2.0271e-02,\n           -3.1321e-03,  3.1460e-04],\n          [-1.2860e-02, -3.4094e-02,  1.5870e-02,  ..., -3.4536e-02,\n            4.2241e-02,  3.9871e-02],\n          [ 3.3013e-02,  2.6802e-02,  7.8012e-03,  ...,  4.8697e-02,\n            1.4553e-02,  6.2728e-02],\n          ...,\n          [ 9.6597e-02,  3.6173e-02,  5.8801e-02,  ...,  6.1085e-03,\n           -4.1366e-02,  6.8610e-03],\n          [ 4.3486e-02, -1.0060e-02,  1.1170e-01,  ...,  6.6523e-03,\n           -5.4077e-02, -6.8702e-02],\n          [-2.5265e-03, -7.0904e-02, -1.1741e-02,  ..., -2.3360e-02,\n           -1.0018e-02, -3.0674e-02]],\n\n         ...,\n\n         [[-5.9669e-04, -2.2582e-03,  4.6776e-03,  ..., -6.1425e-03,\n            1.1968e-02,  1.2304e-02],\n          [-6.2511e-02,  1.1634e-02, -5.9952e-03,  ...,  1.1693e-02,\n           -2.0905e-02,  1.5971e-02],\n          [-6.3354e-04,  3.4839e-02,  1.4068e-02,  ..., -2.4120e-03,\n           -1.2672e-02, -5.1207e-02],\n          ...,\n          [-2.3209e-02,  1.8456e-02, -4.4505e-02,  ...,  7.0232e-02,\n            6.7532e-02, -1.1652e-01],\n          [ 1.9177e-02,  3.3884e-03,  3.8994e-02,  ...,  2.1446e-02,\n            2.4466e-02, -3.9942e-03],\n          [ 2.5744e-02,  2.3675e-02, -1.0753e-02,  ...,  5.5708e-02,\n            5.0182e-03, -1.4080e-02]],\n\n         [[-1.3738e-02,  1.1354e-02,  1.7086e-02,  ...,  4.1336e-03,\n           -8.0407e-03,  1.5832e-03],\n          [-1.0630e-03, -6.5636e-03, -7.0829e-03,  ...,  4.3030e-02,\n           -1.1431e-02, -3.8558e-02],\n          [ 6.8405e-02, -3.5542e-02,  2.7185e-03,  ...,  2.1204e-02,\n           -1.4476e-03,  1.1905e-02],\n          ...,\n          [ 4.9186e-02, -5.4595e-02, -2.6509e-02,  ..., -4.5767e-02,\n            2.3547e-02, -6.0867e-02],\n          [-5.2573e-03,  5.7392e-02,  4.1244e-03,  ...,  5.6339e-02,\n            1.0555e-02, -8.1877e-02],\n          [-6.0907e-02,  5.3883e-02, -2.6438e-02,  ..., -4.3041e-02,\n            1.0844e-04, -3.0237e-02]],\n\n         [[ 4.1309e-03, -9.9653e-03, -4.3063e-04,  ..., -1.0642e-03,\n            2.3810e-03, -1.3835e-03],\n          [ 1.1466e-02,  3.7836e-02,  3.1658e-02,  ..., -2.5520e-02,\n            5.0700e-02,  1.4634e-02],\n          [-1.6106e-02,  5.7465e-02,  3.0656e-02,  ..., -2.3338e-03,\n            1.7378e-02,  1.1370e-03],\n          ...,\n          [-1.7076e-02,  4.4039e-02,  5.3114e-03,  ..., -3.2883e-03,\n            6.3086e-02, -6.7993e-02],\n          [ 5.8819e-02, -7.6497e-02,  3.8293e-02,  ..., -9.4679e-03,\n            4.1454e-02, -2.3495e-02],\n          [ 2.4457e-04,  2.5898e-02,  1.0804e-02,  ..., -8.9566e-03,\n            1.7194e-02,  9.7121e-03]]],\n\n\n        [[[ 1.2402e-02,  2.5611e-02,  2.4210e-02,  ..., -9.1018e-03,\n           -1.9776e-03, -2.2665e-02],\n          [-7.0587e-03,  5.1739e-03, -3.5826e-02,  ...,  6.9317e-02,\n           -2.8534e-02,  3.8553e-02],\n          [ 1.1392e-02, -2.8572e-02, -4.8762e-02,  ..., -3.5268e-02,\n           -3.9347e-02,  6.7075e-02],\n          ...,\n          [ 2.1709e-02,  1.8962e-02,  5.9684e-03,  ...,  1.6776e-02,\n            2.4369e-02,  1.3949e-02],\n          [ 2.6432e-02, -7.5063e-02, -6.9923e-02,  ..., -3.6585e-02,\n            3.8435e-02,  7.1618e-03],\n          [-7.2522e-02, -7.0573e-02, -6.6789e-02,  ..., -9.9606e-02,\n            3.0632e-02, -2.8189e-02]],\n\n         [[-1.7612e-02, -1.1625e-02,  1.3112e-02,  ...,  4.7596e-04,\n           -9.0412e-03,  1.1590e-02],\n          [-2.9510e-02,  6.2293e-03,  3.5239e-02,  ...,  1.1572e-02,\n            2.4181e-02, -2.1489e-02],\n          [-5.3557e-03, -8.6219e-02,  2.7165e-02,  ..., -5.2906e-02,\n            1.6730e-02,  6.0395e-03],\n          ...,\n          [ 5.2118e-02,  5.3964e-03,  3.3383e-02,  ...,  4.8842e-04,\n           -3.2178e-02,  7.2981e-02],\n          [ 2.8324e-02,  1.3198e-02,  1.7818e-02,  ..., -4.1474e-02,\n           -1.0746e-01, -4.3387e-02],\n          [ 5.5704e-02,  1.8317e-02, -2.4157e-02,  ...,  2.2709e-03,\n           -5.8316e-02, -2.4219e-02]],\n\n         [[-1.3509e-02, -1.1313e-02,  1.3460e-02,  ..., -1.7781e-02,\n           -7.2874e-03, -5.3031e-03],\n          [-3.9975e-02, -8.9741e-03,  3.4526e-02,  ...,  5.1185e-02,\n            3.0944e-03,  3.8529e-02],\n          [ 5.4299e-02,  4.1914e-02,  7.0644e-03,  ...,  3.7743e-02,\n           -2.6173e-03,  8.0331e-02],\n          ...,\n          [-8.0997e-03, -1.5164e-02, -4.2794e-03,  ..., -2.3963e-02,\n            6.3975e-03, -1.2764e-02],\n          [ 1.7624e-02,  5.4656e-02, -2.3357e-02,  ..., -7.9471e-03,\n            3.8430e-02,  3.7678e-02],\n          [ 3.8788e-02,  6.0394e-02, -1.9011e-02,  ..., -5.6897e-03,\n            1.1425e-02,  3.9939e-02]],\n\n         ...,\n\n         [[-7.5789e-03, -9.6666e-03, -4.4396e-03,  ..., -1.1049e-02,\n            1.0704e-02,  1.0435e-02],\n          [ 1.1792e-02,  9.9379e-03,  1.4131e-02,  ..., -3.3697e-02,\n           -5.7989e-02,  9.1320e-02],\n          [-1.5084e-02,  4.1487e-02,  2.7378e-02,  ..., -3.9116e-04,\n            1.1333e-02, -8.2394e-02],\n          ...,\n          [-2.2855e-03, -5.1662e-02,  2.9713e-02,  ...,  6.7682e-02,\n           -5.7281e-03, -1.2574e-02],\n          [-2.4362e-02,  5.6320e-02, -3.4474e-02,  ...,  5.3650e-02,\n            1.8498e-02, -8.7520e-02],\n          [-6.2522e-02,  3.9196e-02, -5.3038e-02,  ...,  1.6448e-03,\n            2.1889e-02, -7.6631e-02]],\n\n         [[-1.6020e-02,  1.3352e-02,  2.6129e-02,  ...,  6.4621e-03,\n           -1.5813e-02, -7.2650e-03],\n          [ 3.9458e-02,  1.3045e-02, -2.5738e-02,  ...,  6.7501e-03,\n           -7.7692e-02, -7.0050e-02],\n          [ 9.0241e-02, -4.5083e-02,  9.4537e-03,  ...,  3.7554e-02,\n            2.8198e-02,  4.4767e-02],\n          ...,\n          [-7.0531e-02,  7.4275e-02,  4.5503e-03,  ...,  2.4207e-02,\n            1.4740e-02, -5.3299e-02],\n          [ 3.5409e-02, -4.6484e-03,  7.4092e-03,  ..., -7.8867e-03,\n            1.1499e-01,  3.3211e-03],\n          [ 5.2244e-02,  3.4957e-02, -1.5940e-03,  ..., -7.7989e-03,\n            8.6688e-02,  3.0116e-02]],\n\n         [[ 1.2905e-02, -1.5576e-02,  6.9433e-03,  ...,  2.4655e-03,\n            1.3396e-03,  1.9756e-03],\n          [-2.7928e-02,  8.5817e-02,  6.6661e-02,  ..., -1.5890e-02,\n            9.7840e-03,  3.4410e-02],\n          [-3.7531e-02,  5.5230e-02,  3.2979e-02,  ..., -1.8907e-02,\n            2.6788e-02, -3.9720e-03],\n          ...,\n          [ 3.3490e-03, -2.4207e-02,  8.4552e-02,  ..., -3.5316e-02,\n            3.1631e-02,  1.4419e-02],\n          [-1.4523e-03,  3.3342e-02, -3.8705e-02,  ..., -2.6549e-02,\n           -1.7997e-02,  1.1926e-02],\n          [-3.7485e-03,  2.1430e-02, -6.5213e-02,  ...,  5.0561e-03,\n           -3.9285e-02,  2.1364e-02]]],\n\n\n        [[[ 1.9348e-03,  4.0025e-03,  2.2433e-03,  ..., -2.0925e-02,\n           -5.7313e-03, -1.4322e-02],\n          [ 1.3371e-03, -1.2497e-02,  2.3735e-02,  ..., -1.0275e-04,\n           -3.3666e-02, -5.7271e-03],\n          [ 1.6464e-02, -1.5940e-02,  3.9714e-02,  ...,  3.5454e-02,\n           -6.7779e-03,  3.9492e-02],\n          ...,\n          [-3.3887e-02,  1.1553e-02, -9.4191e-03,  ...,  2.4894e-02,\n           -3.8395e-02,  1.5208e-02],\n          [-3.3176e-02, -1.6607e-03, -1.3944e-02,  ..., -1.1409e-02,\n           -2.1103e-02,  1.9946e-02],\n          [-3.3957e-02, -2.7670e-02, -4.0868e-03,  ..., -3.4660e-03,\n           -2.6204e-02,  7.4920e-03]],\n\n         [[-1.0064e-02, -1.9091e-03,  5.5073e-03,  ..., -1.1311e-02,\n           -7.3968e-03, -5.0225e-03],\n          [-3.8262e-02, -6.3424e-02,  1.4609e-02,  ...,  4.1556e-02,\n            5.0916e-02,  3.3448e-02],\n          [ 2.7952e-02, -8.9517e-02, -3.6030e-03,  ..., -1.2521e-02,\n            6.8660e-02, -2.0623e-02],\n          ...,\n          [ 2.7129e-02, -3.6978e-04, -3.9863e-02,  ...,  1.6449e-02,\n           -3.7029e-02, -6.5538e-03],\n          [-4.1183e-03, -3.8813e-02, -4.4298e-02,  ...,  9.1861e-03,\n           -2.7038e-02, -2.1924e-02],\n          [ 2.7175e-02,  2.5961e-03, -3.8468e-02,  ...,  1.7449e-02,\n           -1.2371e-02, -1.7230e-02]],\n\n         [[ 1.1394e-03, -3.3403e-04,  2.2174e-04,  ..., -6.7937e-03,\n           -6.0340e-03,  1.6502e-02],\n          [ 5.9911e-02, -2.5490e-02, -5.4145e-04,  ...,  2.8741e-02,\n            7.4043e-03, -1.3626e-02],\n          [ 4.0116e-02,  2.0764e-02,  1.3851e-02,  ...,  1.3368e-02,\n           -1.8115e-02, -1.2953e-02],\n          ...,\n          [ 2.1537e-02, -1.4484e-02,  8.3494e-02,  ..., -3.2128e-02,\n           -1.0116e-02,  1.5781e-02],\n          [ 6.3741e-02,  2.4447e-02,  6.2761e-02,  ...,  4.2432e-03,\n            1.2644e-02,  5.1255e-02],\n          [ 6.5658e-03,  3.3994e-03,  9.7054e-02,  ..., -7.0484e-03,\n           -2.7572e-03, -1.1064e-02]],\n\n         ...,\n\n         [[-2.6709e-03,  6.8393e-03, -3.6522e-03,  ...,  8.7716e-03,\n            1.8584e-02, -1.9479e-02],\n          [-1.9710e-02,  6.4313e-03, -5.5016e-02,  ..., -4.3388e-02,\n            2.2695e-02,  3.9174e-02],\n          [ 9.9799e-03, -2.0410e-02, -5.6048e-03,  ..., -4.1516e-02,\n            3.5278e-03,  3.7157e-02],\n          ...,\n          [ 4.1750e-03, -2.3694e-02,  2.1074e-02,  ..., -3.6071e-02,\n           -1.0252e-03,  2.9416e-02],\n          [ 1.8057e-02,  3.6688e-02,  4.2039e-02,  ...,  3.4899e-02,\n            6.1235e-02, -5.3554e-02],\n          [-7.5911e-04,  2.0819e-02,  4.0696e-02,  ..., -1.0401e-02,\n            4.5390e-02,  1.2852e-02]],\n\n         [[-4.4483e-03,  7.1055e-03,  6.3319e-03,  ...,  5.3694e-03,\n            5.6400e-03,  1.7728e-03],\n          [ 2.2510e-02,  9.9687e-03,  3.8894e-02,  ...,  6.8914e-02,\n           -1.5017e-02, -2.3686e-02],\n          [ 3.9080e-02,  7.5508e-02,  2.4783e-02,  ..., -7.4383e-03,\n           -2.5187e-02, -6.0507e-02],\n          ...,\n          [-4.8128e-03, -9.2920e-03, -1.6217e-02,  ...,  2.9041e-02,\n           -1.4220e-02,  2.7749e-02],\n          [-1.0056e-02,  1.7662e-02,  5.0257e-03,  ...,  3.0744e-02,\n            6.2502e-02,  6.7595e-03],\n          [-1.4020e-02, -1.5858e-02, -8.7321e-03,  ..., -1.5776e-04,\n            2.0797e-02, -2.6048e-03]],\n\n         [[-1.4015e-02, -5.3757e-03, -1.4927e-02,  ..., -9.8341e-03,\n           -2.4004e-03, -1.7723e-03],\n          [ 5.9837e-02, -3.8612e-02,  1.1591e-02,  ..., -1.1063e-02,\n            7.2766e-02, -6.7703e-02],\n          [-5.1982e-02, -1.9900e-02,  6.8267e-02,  ...,  2.2258e-03,\n            4.7917e-02, -4.6902e-03],\n          ...,\n          [ 1.6166e-02,  1.0950e-02,  1.9353e-02,  ..., -1.3714e-02,\n            5.9764e-02, -4.3139e-02],\n          [-1.5288e-02, -3.1393e-02, -6.8615e-03,  ..., -4.6547e-02,\n            6.1419e-02, -5.1262e-02],\n          [ 1.3078e-02,  6.3603e-03,  6.1701e-03,  ..., -3.4961e-02,\n            8.4372e-02, -6.9004e-02]]]], device='cuda:0',\n       grad_fn=<CloneBackward0>))), hidden_states=None, attentions=None)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mparm_model\u001b[0m \u001b[0;34m= OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput.to\u001b[0m \u001b[0;34m= <built-in method to of Tensor object at 0x7fc598a0e3b0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mhparam\u001b[0m \u001b[0;34m= {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 8, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 4058, 'early_stop': 40580, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.417623575702154e-05\n    weight_decay: 0.0\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.417623575702154e-05\n    weight_decay: 0.0001\n), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc5b7e27210>}\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# muda de torch.Size([2, 100, vocabsize])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# para torch.Size([200, vocabsize])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), *input=(tensor([[ 3381,  4214,     6,  3869,  5410,     ...1,  6343,  9280,   271]],\n       device='cuda:0'),), **kwargs={})\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mforward_call\u001b[0m \u001b[0;34m= <bound method OPTForCausalLM.forward of OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[ 3381,  4214,     6,  3869,  5410,     7, 19202, 11988,  3780,   366,\n          1192,  1931,  1023,   991,     6,   282,   879,  3245,  8470,   493,\n          9240,  1073,   991,   263, 14628,   271,    13,   102,  4202,   354,\n         43486,   475,  6439,   139,    28,   119, 16690,   118,     6,  1794,\n           257,   979,  5410,   364,  8470,  1021,   748,   661,   139,   364,\n          6821, 32126,   241, 21950,  3838, 17254,   364,  2241,   853,   364,\n          7670,  2527,  1192,   162,   687, 14242,   298,   366,   295,  3853,\n          9371,   991,   181, 12969,   181,  6658,   181,  1140,  7485,  4261,\n         33379,   102,  3381,  4214,   364,  1021, 12099, 19689,  2955,  5251,\n          1999,   233,   242,    10,   504,    41,   366,    23,  5079,     6],\n        [10456, 17493,   364, 12899,  1999,    23,  1140, 15054,   281, 10759,\n          2070,  1977,   417,   281,    98,  7805,  8750,  2001, 15369,  3840,\n         28968,   853,    10, 11363,  8604,  3381,  4214,   263,   326,  5874,\n           858,   354,     4,  9860,   241, 11988,  2677,  3175,   158,  9209,\n          9383,   366, 27817,  6510,     6,  2489, 11409,  4318,   118,  3985,\n             6, 25520,  2544, 13862,  3985,     6, 20407,   225, 31376,  3985,\n           364,  5473,  1479, 12376,   366,     6,  1192,  6655,   842,   542,\n           853,   424,  3840, 11363,  1469,   271,  2664,  5563,    29,   326,\n          5874,   858,   354,     6,  6256, 19115,  2955, 17787,  3695, 11326,\n           493,     4,  3676,  4401,  3381,   139,   263, 26447,     6,  5553],\n        [ 1630,  4330, 36279,  3119,  7252,  1151,   139,  3838,   241,   255,\n           894, 11238, 40649,     6,    10,  1029, 17601,  3695, 22423,   263,\n            15,  2158, 33945,  1479,  1069,   281,    15,  2794, 11548,   250,\n           274,  3843,  2750,  7211,  3952, 12686,  9565,   118,   285,  2095,\n           102,  2654,  3578,  5030,   329,  2841, 42429,     6,   364,  1021,\n         18071, 36800, 11194,   263,   188,   469, 12603,    15,  2794,  9565,\n           118, 26585, 13447,   102,  2841, 38974,     4,  6858,  1757,   242,\n         11363,  1168,   700,    10,  2654,  3578,  2953,  6821,   604,   366,\n         13103,   783,  5166,   364, 46198,  5285,     4,    83,  9689,   625,\n          3281,   102,  3381,  4214,  2841,  3055,   438,  6005, 11363, 11278],\n        [ 4448,   139, 14185,   263,   178, 16299,    36,  1366,  5208,    12,\n         45264,    43,    36, 17075,  3578,  1021,  1808, 12713,  2955,   181,\n          1526,   571,  1243,  3330,   322,  4556,   842,   257,   512, 40837,\n           263,  8546,  8152,     6,   226,  5874,   438,  1020,  7505,  3567,\n          1701,  2102,  1717,  1916, 21179,  1140,   438,   324,   263,  3055,\n          1479,  4063,   139,   109, 10512,  6103, 10456,   282,  1020,     4,\n            22,   250,   139, 17035,  1526, 12834,   263,    66,  3985,  6044,\n          1977, 24151,     6,   117, 17128,   718,   126,   326, 13688,  2841,\n           820, 24934,   139,  2841,  2491,   126, 11988,  2841,  9675, 12186,\n           366,  2750, 14237,  3900,  3381,  4214,    13,   424, 11988, 10969],\n        [ 1020,   856,  1242,  2154,   338,  1526,   506,  2684,     4,    83,\n          3594,   102,  3381,  4214, 41591, 10456,   282,  2426,  2955,  3723,\n         19393,   263, 16799,  6709, 15357,   242, 23974,   102,  1020,  7935,\n          1021,  1192,  1021,  7241,   368,  1855,  2583,   263, 15203,   257,\n          1526, 12834,    12, 38606,   991,     4,    83,  3723, 19393,     6,\n           842,  1910,   856,  1242,  2154, 13883,  2095,     6,  2694, 15477,\n          2095,  1021,   257,  3594,  2095, 26031, 15557,  5511,   242,   263,\n         22043, 18756,    66,   763,  1026,   102,     6, 11030,  2628,   102,\n          3304,   338,  1182,  3851,  2968,  1526,   620,  2426,     6,    15,\n          2794,  2628,   281,   542,   808,  4216, 42559,   991,   117,   295],\n        [ 8906,   293,     4,    83, 39788,   700, 23473, 12207,   523,   281,\n           364,  5456,   281,   385,   281, 16022, 13359,  5003, 34904,  1848,\n           853,   281,  2841, 35348,     6, 31556,     6, 16863,    12, 45391,\n             6, 21123,     6,  1646,  4429,     6, 24712,     6, 24649,   364,\n          7571,     4,    83, 12207,  6658,    36,  6412,  3181,   139,    43,\n          7935,  6256,   225,   281,  7252,  5032, 32925,    11, 23092,   646,\n           698,  1442,     4,  1466,  7479,    10,  6821,  2694,   417,  2413,\n           338,  2102, 13736,  3349,  1479,  1178,   757,   366, 10969,   293,\n          3137,  1021,  6769,   337,  1908, 28594,   385,   281, 16022, 13359,\n          5003,   117,    41,   139,   263, 35348,     4,   440, 34265,  1479],\n        [ 3695, 11326,   493,  2955, 41232,   118,  4214,  3977,   661, 17682,\n             4,   384,  3241,   385,  3851,   117,   548,  2269,   281,     6,\n         11988, 14242,  1588, 10968, 15203,   991,    12,  1090,  3137,   910,\n         18615,   281, 26916, 18920,  5655,   364, 38902,   424,  1717,  1916,\n         20435, 19393,  1855,   424,  2095,   263,    22, 20338, 25588,  1297,\n           109,   740,  3209,  1780, 14182,    22,  7215,  8564,   385,   281,\n           952,  5543,  5521,   281,  1297,  1192, 19438,   242, 15009, 32924,\n          8913,   139,   263,   109,   354,  1323,   366,  3840,    10,   856,\n          9854,   242,   364,  7252,  3840,  2664,  6417,    10, 39788,   298,\n          2102,   263,  7252, 10195, 35853, 32924,  8913,   139,   385,   281],\n        [   11, 12170,   139,   364,   842,  1051,   242,  1021,  9486,   991,\n           475,  5655, 35708,  5285,   109,   385,   493,    10,   139, 13011,\n          3381,  1526,    12,  2560,     6,  1021,   257,   842,  1910,     6,\n          7935, 27438,  2102,     6,  9689,   102,  3181,  2102,     6, 21045,\n           139,     6,  2750,  5679,  4347,     6,    10,  4892, 13718,  2102,\n           364,  9031,  4065,  1977, 31375,   366,   328,  1464,   257, 27039,\n             6, 20775,   842,   118,  1021,  1192, 19394,   271, 32888,   405,\n          1879,  5511,   242,    98,  7805,  8002, 33023,     4,  7765, 20782,\n         16743,   842,  1910,  1021, 19749, 18425,   475,  5655, 43486,   364,\n           588,  1192,   364,   257,  1236,  1526,   181,  6343,  9280,   271]],\n       device='cuda:0'),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), input_ids=tensor([[ 3381,  4214,     6,  3869,  5410,     ...1,  6343,  9280,   271]],\n       device='cuda:0'), attention_mask=None, head_mask=None, past_key_values=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mreturn_dict\u001b[0m \u001b[0;34m= True\u001b[0m\n\u001b[1;32m    944\u001b[0m         )\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self=OPTDecoder(\n  (embed_tokens): Embedding(50272, 7..., eps=1e-05, elementwise_affine=True)\n    )\n  )\n), *input=(), **kwargs={'attention_mask': None, 'head_mask': None, 'input_ids': tensor([[ 3381,  4214,     6,  3869,  5410,     ...1,  6343,  9280,   271]],\n       device='cuda:0'), 'inputs_embeds': None, 'output_attentions': False, 'output_hidden_states': False, 'past_key_values': None, 'return_dict': True, 'use_cache': None})\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mforward_call\u001b[0m \u001b[0;34m= <bound method OPTDecoder.forward of OPTDecoder(\n  (embed_tokens): Embedding(50272, 768, padding_idx=1)\n  (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (layers): ModuleList(\n    (0): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (6): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (7): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (8): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (9): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (10): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (11): OPTDecoderLayer(\n      (self_attn): OPTAttention(\n        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (activation_fn): ReLU()\n      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= ()\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {'input_ids': tensor([[ 3381,  4214,     6,  3869,  5410,     7, 19202, 11988,  3780,   366,\n          1192,  1931,  1023,   991,     6,   282,   879,  3245,  8470,   493,\n          9240,  1073,   991,   263, 14628,   271,    13,   102,  4202,   354,\n         43486,   475,  6439,   139,    28,   119, 16690,   118,     6,  1794,\n           257,   979,  5410,   364,  8470,  1021,   748,   661,   139,   364,\n          6821, 32126,   241, 21950,  3838, 17254,   364,  2241,   853,   364,\n          7670,  2527,  1192,   162,   687, 14242,   298,   366,   295,  3853,\n          9371,   991,   181, 12969,   181,  6658,   181,  1140,  7485,  4261,\n         33379,   102,  3381,  4214,   364,  1021, 12099, 19689,  2955,  5251,\n          1999,   233,   242,    10,   504,    41,   366,    23,  5079,     6],\n        [10456, 17493,   364, 12899,  1999,    23,  1140, 15054,   281, 10759,\n          2070,  1977,   417,   281,    98,  7805,  8750,  2001, 15369,  3840,\n         28968,   853,    10, 11363,  8604,  3381,  4214,   263,   326,  5874,\n           858,   354,     4,  9860,   241, 11988,  2677,  3175,   158,  9209,\n          9383,   366, 27817,  6510,     6,  2489, 11409,  4318,   118,  3985,\n             6, 25520,  2544, 13862,  3985,     6, 20407,   225, 31376,  3985,\n           364,  5473,  1479, 12376,   366,     6,  1192,  6655,   842,   542,\n           853,   424,  3840, 11363,  1469,   271,  2664,  5563,    29,   326,\n          5874,   858,   354,     6,  6256, 19115,  2955, 17787,  3695, 11326,\n           493,     4,  3676,  4401,  3381,   139,   263, 26447,     6,  5553],\n        [ 1630,  4330, 36279,  3119,  7252,  1151,   139,  3838,   241,   255,\n           894, 11238, 40649,     6,    10,  1029, 17601,  3695, 22423,   263,\n            15,  2158, 33945,  1479,  1069,   281,    15,  2794, 11548,   250,\n           274,  3843,  2750,  7211,  3952, 12686,  9565,   118,   285,  2095,\n           102,  2654,  3578,  5030,   329,  2841, 42429,     6,   364,  1021,\n         18071, 36800, 11194,   263,   188,   469, 12603,    15,  2794,  9565,\n           118, 26585, 13447,   102,  2841, 38974,     4,  6858,  1757,   242,\n         11363,  1168,   700,    10,  2654,  3578,  2953,  6821,   604,   366,\n         13103,   783,  5166,   364, 46198,  5285,     4,    83,  9689,   625,\n          3281,   102,  3381,  4214,  2841,  3055,   438,  6005, 11363, 11278],\n        [ 4448,   139, 14185,   263,   178, 16299,    36,  1366,  5208,    12,\n         45264,    43,    36, 17075,  3578,  1021,  1808, 12713,  2955,   181,\n          1526,   571,  1243,  3330,   322,  4556,   842,   257,   512, 40837,\n           263,  8546,  8152,     6,   226,  5874,   438,  1020,  7505,  3567,\n          1701,  2102,  1717,  1916, 21179,  1140,   438,   324,   263,  3055,\n          1479,  4063,   139,   109, 10512,  6103, 10456,   282,  1020,     4,\n            22,   250,   139, 17035,  1526, 12834,   263,    66,  3985,  6044,\n          1977, 24151,     6,   117, 17128,   718,   126,   326, 13688,  2841,\n           820, 24934,   139,  2841,  2491,   126, 11988,  2841,  9675, 12186,\n           366,  2750, 14237,  3900,  3381,  4214,    13,   424, 11988, 10969],\n        [ 1020,   856,  1242,  2154,   338,  1526,   506,  2684,     4,    83,\n          3594,   102,  3381,  4214, 41591, 10456,   282,  2426,  2955,  3723,\n         19393,   263, 16799,  6709, 15357,   242, 23974,   102,  1020,  7935,\n          1021,  1192,  1021,  7241,   368,  1855,  2583,   263, 15203,   257,\n          1526, 12834,    12, 38606,   991,     4,    83,  3723, 19393,     6,\n           842,  1910,   856,  1242,  2154, 13883,  2095,     6,  2694, 15477,\n          2095,  1021,   257,  3594,  2095, 26031, 15557,  5511,   242,   263,\n         22043, 18756,    66,   763,  1026,   102,     6, 11030,  2628,   102,\n          3304,   338,  1182,  3851,  2968,  1526,   620,  2426,     6,    15,\n          2794,  2628,   281,   542,   808,  4216, 42559,   991,   117,   295],\n        [ 8906,   293,     4,    83, 39788,   700, 23473, 12207,   523,   281,\n           364,  5456,   281,   385,   281, 16022, 13359,  5003, 34904,  1848,\n           853,   281,  2841, 35348,     6, 31556,     6, 16863,    12, 45391,\n             6, 21123,     6,  1646,  4429,     6, 24712,     6, 24649,   364,\n          7571,     4,    83, 12207,  6658,    36,  6412,  3181,   139,    43,\n          7935,  6256,   225,   281,  7252,  5032, 32925,    11, 23092,   646,\n           698,  1442,     4,  1466,  7479,    10,  6821,  2694,   417,  2413,\n           338,  2102, 13736,  3349,  1479,  1178,   757,   366, 10969,   293,\n          3137,  1021,  6769,   337,  1908, 28594,   385,   281, 16022, 13359,\n          5003,   117,    41,   139,   263, 35348,     4,   440, 34265,  1479],\n        [ 3695, 11326,   493,  2955, 41232,   118,  4214,  3977,   661, 17682,\n             4,   384,  3241,   385,  3851,   117,   548,  2269,   281,     6,\n         11988, 14242,  1588, 10968, 15203,   991,    12,  1090,  3137,   910,\n         18615,   281, 26916, 18920,  5655,   364, 38902,   424,  1717,  1916,\n         20435, 19393,  1855,   424,  2095,   263,    22, 20338, 25588,  1297,\n           109,   740,  3209,  1780, 14182,    22,  7215,  8564,   385,   281,\n           952,  5543,  5521,   281,  1297,  1192, 19438,   242, 15009, 32924,\n          8913,   139,   263,   109,   354,  1323,   366,  3840,    10,   856,\n          9854,   242,   364,  7252,  3840,  2664,  6417,    10, 39788,   298,\n          2102,   263,  7252, 10195, 35853, 32924,  8913,   139,   385,   281],\n        [   11, 12170,   139,   364,   842,  1051,   242,  1021,  9486,   991,\n           475,  5655, 35708,  5285,   109,   385,   493,    10,   139, 13011,\n          3381,  1526,    12,  2560,     6,  1021,   257,   842,  1910,     6,\n          7935, 27438,  2102,     6,  9689,   102,  3181,  2102,     6, 21045,\n           139,     6,  2750,  5679,  4347,     6,    10,  4892, 13718,  2102,\n           364,  9031,  4065,  1977, 31375,   366,   328,  1464,   257, 27039,\n             6, 20775,   842,   118,  1021,  1192, 19394,   271, 32888,   405,\n          1879,  5511,   242,    98,  7805,  8002, 33023,     4,  7765, 20782,\n         16743,   842,  1910,  1021, 19749, 18425,   475,  5655, 43486,   364,\n           588,  1192,   364,   257,  1236,  1526,   181,  6343,  9280,   271]],\n       device='cuda:0'), 'attention_mask': None, 'head_mask': None, 'past_key_values': None, 'inputs_embeds': None, 'use_cache': None, 'output_attentions': False, 'output_hidden_states': False, 'return_dict': True}\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=OPTDecoder(\n  (embed_tokens): Embedding(50272, 7..., eps=1e-05, elementwise_affine=True)\n    )\n  )\n), input_ids=tensor([[ 3381,  4214,     6,  3869,  5410,     ...1,  6343,  9280,   271]],\n       device='cuda:0'), attention_mask=tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38...    0.0000e+00,  0.0000e+00]]]], device='cuda:0'), head_mask=None, past_key_values=None, inputs_embeds=tensor([[[ 3.3685e-02, -1.1329e-01,  3.2283e-03,...ce='cuda:0',\n       grad_fn=<EmbeddingBackward0>), use_cache=True, output_attentions=False, output_hidden_states=False, return_dict=True)\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36muse_cache\u001b[0m \u001b[0;34m= True\u001b[0m\n\u001b[1;32m    705\u001b[0m                 )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self=OPTDecoderLayer(\n  (self_attn): OPTAttention(\n  ...orm((768,), eps=1e-05, elementwise_affine=True)\n), *input=(tensor([[[ 1.5161e-02, -5.5735e+00,  1.5407e-01,... device='cuda:0',\n       grad_fn=<ViewBackward0>),), **kwargs={'attention_mask': tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38...    0.0000e+00,  0.0000e+00]]]], device='cuda:0'), 'layer_head_mask': None, 'output_attentions': False, 'past_key_value': None, 'use_cache': True})\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mforward_call\u001b[0m \u001b[0;34m= <bound method OPTDecoderLayer.forward of OPTDecoderLayer(\n  (self_attn): OPTAttention(\n    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (activation_fn): ReLU()\n  (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[[ 1.5161e-02, -5.5735e+00,  1.5407e-01,  ..., -4.2751e-02,\n           7.9303e-03,  6.8435e-02],\n         [ 6.1017e-02,  7.0739e-02,  1.9584e-02,  ...,  7.3991e-03,\n           8.7466e-02,  6.9748e-02],\n         [ 2.4586e-02, -2.8332e-02,  1.8613e-01,  ..., -2.2883e-01,\n           2.1079e-01, -2.5655e-01],\n         ...,\n         [-5.0684e-02, -1.4093e+00, -2.4313e-01,  ...,  1.3720e-01,\n           1.1183e-01,  1.2209e-01],\n         [-7.7412e-02, -8.0851e-01,  3.5089e-01,  ...,  5.9853e-02,\n           1.8387e-01, -1.4784e-01],\n         [ 8.5011e-02, -2.7398e+00,  4.4637e-01,  ..., -2.4455e-01,\n           3.5386e-01, -3.7460e-01]],\n\n        [[ 2.3795e-01, -4.3947e+00,  9.0753e-02,  ...,  6.8876e-01,\n           1.6079e-01, -1.8652e-02],\n         [ 1.9900e-01, -3.3664e-01, -2.6770e-02,  ...,  1.1363e-01,\n           4.7435e-02,  1.5918e-02],\n         [ 3.2398e-01, -4.2441e-01, -7.4177e-03,  ...,  4.4325e-02,\n           1.8856e-01,  1.2431e-01],\n         ...,\n         [ 5.7008e-01, -6.6913e-01, -4.4702e-01,  ..., -5.2497e-02,\n           4.0805e-01, -1.3253e-01],\n         [ 1.7332e-01, -5.1570e-01, -1.1462e-01,  ..., -5.8885e-02,\n           2.4303e-02, -4.6244e-01],\n         [ 1.0773e-01,  1.9649e-01, -2.0821e-01,  ..., -3.1705e-01,\n           2.3721e-01,  4.3935e-02]],\n\n        [[ 6.6204e-02, -5.1291e+00,  1.1697e-01,  ...,  6.2326e-01,\n           1.2428e-02, -4.4243e-03],\n         [ 1.8003e-01, -1.0339e+00,  3.7381e-02,  ...,  4.7406e-02,\n           1.2397e-01,  1.6555e-01],\n         [ 8.0352e-02, -5.0537e-01,  1.5472e-01,  ..., -9.6174e-02,\n          -6.3835e-02, -1.5509e-01],\n         ...,\n         [ 4.5377e-01,  1.3138e-01,  2.2696e-01,  ..., -2.8229e-01,\n           6.2961e-02,  1.8291e-01],\n         [ 1.3739e-01, -1.2234e+00,  2.0923e-01,  ..., -2.6417e-01,\n          -9.3362e-02,  1.2319e-01],\n         [ 6.2626e-02, -1.2667e+00,  1.7497e-02,  ..., -4.7435e-01,\n          -7.9477e-02, -1.0761e-01]],\n\n        ...,\n\n        [[-2.1033e-02, -5.5516e+00,  1.4151e-01,  ...,  6.4816e-01,\n           3.1782e-02,  3.4932e-02],\n         [ 9.8057e-03,  5.6136e-02,  2.2336e-02,  ...,  2.4888e-01,\n           8.2286e-02,  2.1648e-02],\n         [-9.6850e-03, -3.7792e+00,  9.9711e-02,  ..., -1.5776e-01,\n           1.6745e-01, -1.3542e-01],\n         ...,\n         [-8.2764e-02, -1.8824e-01,  2.5525e-01,  ..., -3.8104e-01,\n           3.5191e-01,  8.4194e-02],\n         [-1.8797e-01, -7.4726e-01, -9.0190e-03,  ...,  1.8182e-02,\n           2.7637e-01,  1.3053e-01],\n         [ 1.6235e-02, -3.4394e-01, -5.7329e-02,  ...,  9.4922e-02,\n           1.7397e-01,  6.3753e-02]],\n\n        [[ 8.0518e-02, -4.0059e+00,  1.5032e-01,  ...,  5.8999e-01,\n           2.1186e-01,  1.4259e-02],\n         [-1.1919e-01, -4.8826e-01, -1.4683e-01,  ..., -9.1029e-02,\n          -5.1047e-02,  7.1935e-02],\n         [-1.2078e-02, -1.5501e-01, -7.4310e-02,  ..., -1.3035e-02,\n           7.5689e-02,  1.3131e-01],\n         ...,\n         [ 7.1008e-02,  1.0868e+00, -1.0811e-01,  ..., -1.4791e-01,\n          -4.4172e-03,  4.3042e-02],\n         [-2.5030e-02, -8.5510e-01, -9.7045e-02,  ..., -1.9079e-01,\n           4.1104e-02, -1.0937e-01],\n         [-1.1238e-01,  7.3344e-01, -1.6430e-01,  ..., -4.5244e-02,\n          -1.7221e-01, -5.6301e-02]],\n\n        [[ 3.5440e-02, -4.2381e+00,  6.9263e-02,  ...,  5.6023e-01,\n           9.0124e-02, -6.0039e-02],\n         [ 6.7279e-02, -5.6068e-01,  7.2137e-02,  ..., -1.1656e-02,\n          -1.2268e-03,  1.1169e-01],\n         [ 3.0978e-01, -5.3138e-01, -1.9897e-02,  ..., -1.4447e-01,\n          -3.3529e-02, -7.0050e-02],\n         ...,\n         [ 1.8019e-01,  7.2196e-01, -1.2601e-02,  ..., -4.6596e-01,\n          -2.9143e-03, -2.1637e-03],\n         [-3.9028e-02, -7.2494e-01,  1.2154e-01,  ..., -2.0286e-01,\n          -3.5536e-02, -1.8279e-01],\n         [-1.7146e-01,  1.0331e+00, -2.5057e-01,  ..., -1.0437e-01,\n           3.7713e-02,  3.7074e-02]]], device='cuda:0',\n       grad_fn=<ViewBackward0>),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {'attention_mask': tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        ...,\n\n\n        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        [[[ 0.0000e+00, -3.4028e+38, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00, -3.4028e+38,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4028e+38,\n           -3.4028e+38, -3.4028e+38],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -3.4028e+38, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -3.4028e+38],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]]], device='cuda:0'), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=OPTDecoderLayer(\n  (self_attn): OPTAttention(\n  ...orm((768,), eps=1e-05, elementwise_affine=True)\n), hidden_states=tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  .....\n       device='cuda:0', grad_fn=<ReluBackward0>), attention_mask=tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38...    0.0000e+00,  0.0000e+00]]]], device='cuda:0'), layer_head_mask=None, output_attentions=False, use_cache=True, past_key_value=None)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mhidden_states\u001b[0m \u001b[0;34m= tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  ..., 0.0000e+00, 9.9608e-02, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n       device='cuda:0', grad_fn=<ReluBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.fc2\u001b[0m \u001b[0;34m= Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self=Linear(in_features=3072, out_features=768, bias=True), *input=(tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  .....\n       device='cuda:0', grad_fn=<ReluBackward0>),), **kwargs={})\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mforward_call\u001b[0m \u001b[0;34m= <bound method Linear.forward of Linear(in_features=3072, out_features=768, bias=True)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  ..., 0.0000e+00, 9.9608e-02, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n       device='cuda:0', grad_fn=<ReluBackward0>),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=Linear(in_features=3072, out_features=768, bias=True), input=tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  .....\n       device='cuda:0', grad_fn=<ReluBackward0>))\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mF.linear\u001b[0m \u001b[0;34m= <function linear at 0x7fc60f5a04d0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  ..., 0.0000e+00, 9.9608e-02, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n       device='cuda:0', grad_fn=<ReluBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.weight\u001b[0m \u001b[0;34m= Parameter containing:\ntensor([[ 1.0483e-03, -7.7878e-03, -9.0114e-03,  ..., -1.3342e-03,\n          1.2033e-02,  7.9609e-03],\n        [-4.6866e-02, -3.4472e-02, -6.3638e-02,  ..., -1.6218e-02,\n         -2.8732e-02,  6.7287e-03],\n        [-4.4598e-03,  1.5761e-02, -7.9800e-03,  ..., -6.1653e-03,\n          1.7925e-02,  7.7807e-03],\n        ...,\n        [ 1.8789e-02,  9.1553e-03, -9.8881e-03,  ..., -2.1792e-03,\n         -1.3324e-02, -1.1625e-03],\n        [-1.8926e-03, -1.0199e-02,  9.1252e-03,  ..., -2.1141e-03,\n          1.6208e-02,  1.7390e-02],\n        [ 3.3168e-05,  1.1934e-03,  6.7774e-03,  ..., -9.5507e-03,\n         -4.4643e-03, -8.0083e-03]], device='cuda:0', requires_grad=True)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.bias\u001b[0m \u001b[0;34m= Parameter containing:\ntensor([ 4.8521e-03,  2.1897e-02,  9.4740e-03, -1.0681e-02,  1.5044e-03,\n         4.5271e-04,  1.5272e-02,  5.9391e-03,  4.6618e-03,  6.7650e-04,\n        -1.2297e-02, -3.7563e-03,  3.5670e-03, -4.2529e-03,  7.9859e-03,\n        -7.4854e-03, -1.0479e-03, -2.3301e-03, -5.1403e-03, -1.1754e-03,\n        -5.3549e-02, -3.0976e-04,  6.0539e-03, -1.6012e-02, -8.1361e-03,\n         1.3506e-04, -1.9545e-03,  3.9966e-03, -7.9100e-03,  3.5905e-03,\n         8.0427e-03, -1.7940e-02, -1.1837e-02,  9.2702e-03, -1.4064e-02,\n         6.5122e-03, -1.3663e-02,  7.4959e-03, -6.2527e-03,  1.2555e-02,\n         3.7583e-03,  5.2666e-03, -1.8315e-03,  1.1555e-02, -1.2019e-02,\n         7.2844e-03, -5.8099e-03,  1.8510e-02, -3.7640e-04, -1.3973e-02,\n         1.0434e-02, -3.3899e-03,  1.6820e-02, -1.4337e-02, -1.2776e-02,\n         1.2438e-03,  4.9739e-03, -2.9592e-03,  8.5997e-03,  7.1787e-03,\n         5.2585e-03,  2.1852e-02,  8.6368e-03, -1.5947e-02, -2.7866e-03,\n        -6.3835e-03,  1.9673e-03, -2.7436e-03, -6.0244e-03,  4.3477e-03,\n         6.8208e-03, -1.4410e-02, -3.1084e-03,  1.0359e-03,  5.0409e-03,\n         3.9560e-03,  1.5779e-03, -4.7672e-03, -3.7576e-04, -2.0302e-03,\n         1.6972e-03, -2.2588e-03,  1.4119e-02,  7.9137e-03,  1.1871e-02,\n        -7.9027e-03,  3.9132e-03, -8.2581e-04,  1.0704e-02, -6.7014e-03,\n        -3.9309e-02, -1.7300e-02,  8.2843e-03,  3.7417e-03, -1.7606e-03,\n        -2.5266e-03,  5.9416e-03,  2.0168e-02, -2.1081e-02,  6.9172e-03,\n         1.2677e-02, -1.3839e-02,  6.9296e-03, -2.8055e-03, -9.1375e-03,\n         1.4408e-02, -1.2299e-02,  1.6568e-02, -6.5706e-03, -1.2223e-02,\n         7.7945e-03,  9.0612e-04,  3.1409e-03, -1.0067e-02, -5.5894e-02,\n        -5.2159e-03,  1.0642e-02, -1.5085e-02, -2.4796e-02, -8.3892e-03,\n        -7.9222e-03, -9.8696e-03, -2.8704e-03,  1.1627e-03,  5.1770e-05,\n        -1.0577e-04,  3.3841e-03,  5.9580e-03,  1.1933e-02,  5.6968e-03,\n         1.8265e-02,  1.6151e-03, -4.2234e-03, -1.2698e-04, -5.2291e-03,\n        -2.7210e-03,  8.0698e-03,  9.9447e-04,  1.7762e-02,  9.1987e-03,\n        -2.7673e-02,  3.0002e-03,  3.3834e-03, -3.0443e-03,  3.1045e-03,\n         3.2182e-04,  5.0196e-03,  8.5745e-03, -1.4722e-03, -1.0774e-02,\n        -7.5623e-03, -8.5885e-04, -8.2788e-03,  1.3704e-02,  1.0896e-02,\n         1.7064e-03,  6.0692e-03,  1.4446e-02,  4.1554e-03,  5.7510e-03,\n         5.1193e-03, -1.8570e-02, -1.6443e-03, -2.6150e-03,  3.9289e-03,\n        -7.4846e-03, -9.3908e-03, -5.0984e-03, -1.0646e-02, -1.7282e-02,\n         6.1938e-03,  2.0931e-02, -1.5833e-02,  6.7478e-03, -4.5674e-02,\n        -3.2354e-03,  1.0683e-02, -2.2775e-02, -1.3241e-02, -1.1633e-02,\n         5.0383e-03,  2.9561e-04,  1.0794e-02, -2.1567e-03, -2.9891e-03,\n        -4.5570e-03, -2.8450e-02, -3.6399e-03,  7.2521e-03, -4.4623e-03,\n        -2.1339e-03,  1.2108e-02, -6.4772e-03,  4.8501e-03, -5.3036e-03,\n         8.8487e-03,  5.8598e-03, -8.0472e-03,  5.9259e-04, -3.2681e-03,\n        -6.6208e-03,  8.3501e-03,  4.5201e-03,  2.5827e-04, -2.9973e-03,\n         2.2560e-02, -2.4859e-03,  1.2619e-02,  1.3537e-02, -1.1576e-03,\n         8.9714e-03,  9.0819e-04, -1.0417e-02, -8.5418e-04, -1.6119e-02,\n         1.2963e-02,  1.0251e-02,  9.6932e-03, -5.4587e-05,  2.0906e-02,\n        -1.2398e-03, -2.2669e-02,  1.4525e-02, -9.2013e-03,  1.4122e-02,\n         5.9847e-03, -7.9777e-03, -1.3821e-02, -1.7088e-02,  1.6989e-02,\n         1.2800e-02, -2.1054e-02, -1.7505e-02,  5.2251e-05,  1.5770e-02,\n        -2.7207e-03, -4.0463e-03, -6.1229e-03,  3.7058e-03,  4.9263e-03,\n         1.1694e-02, -1.3610e-02, -6.9040e-03, -1.1387e-02, -2.1495e-03,\n        -8.1701e-03, -1.4801e-02,  9.2226e-03, -1.5280e-02,  4.8696e-03,\n        -5.6204e-04,  2.7530e-02, -4.4862e-03, -2.0064e-04,  1.8665e-03,\n        -3.7229e-03, -6.3350e-03, -5.9100e-03,  2.6694e-03, -7.9968e-03,\n         6.0988e-03, -7.6362e-03,  7.7755e-03, -1.6421e-02,  2.3481e-03,\n        -1.8722e-02, -2.3150e-02,  1.1144e-02,  1.0771e-02,  2.7008e-04,\n         5.6076e-03, -3.3821e-03,  1.0863e-02, -1.1510e-02, -9.2244e-04,\n         4.7643e-03,  1.2510e-03,  1.5603e-04, -9.4766e-03,  1.6293e-04,\n        -6.8112e-04,  1.3853e-02,  4.2988e-03, -2.4846e-03,  2.9502e-03,\n         2.7906e-03,  8.3292e-03,  2.0841e-02, -6.5192e-03,  4.0946e-03,\n        -4.4378e-03, -4.9862e-03,  1.5562e-02, -1.7550e-03,  5.8463e-03,\n         3.5299e-05, -2.2725e-03, -7.2018e-03, -3.4185e-03,  1.9829e-03,\n         1.0183e-02,  8.5417e-04,  6.6531e-03, -3.7769e-03, -1.7410e-03,\n         2.7847e-03,  5.8457e-04, -2.8352e-04,  8.7203e-03,  1.5451e-05,\n        -6.7773e-03,  2.5684e-02, -6.0656e-03, -1.3382e-02, -1.0669e-02,\n         2.4113e-03, -8.7023e-03,  1.9617e-02, -4.0686e-03, -9.5084e-03,\n         4.1833e-03, -3.4600e-03, -7.7735e-03, -9.9612e-03,  4.5750e-05,\n        -4.5247e-03, -1.0113e-02, -1.6438e-02, -4.2614e-04,  3.9168e-03,\n        -7.4066e-03, -1.5820e-02, -8.0423e-03, -1.3433e-02, -2.1988e-02,\n        -2.4801e-02, -3.8386e-03, -1.0298e-02, -2.5305e-03, -4.3774e-03,\n        -2.3115e-02,  1.9233e-02,  5.7082e-03,  6.9035e-03, -1.6292e-02,\n        -2.7858e-02,  4.5046e-03, -1.1604e-02, -4.5397e-03, -2.6778e-02,\n        -3.1727e-03,  1.9516e-02, -2.5134e-03,  5.3662e-02,  1.3297e-02,\n        -5.3088e-02, -4.1824e-03, -9.4843e-04,  7.6987e-03,  3.1061e-03,\n         1.0933e-02, -1.0215e-02,  4.6496e-03, -1.5380e-02,  6.0454e-03,\n        -1.9853e-03, -9.4607e-04, -1.4682e-02,  1.6984e-04, -9.6661e-04,\n        -2.6289e-03,  5.0183e-03, -7.5690e-03,  1.6436e-03, -1.9193e-02,\n        -3.1853e-02, -8.4748e-03, -2.4048e-03,  1.3622e-02,  6.1662e-03,\n         1.0318e-02,  3.5779e-03,  1.5591e-02,  5.1069e-03, -1.4332e-02,\n        -1.0879e-02,  1.3138e-02,  5.1318e-03, -1.6768e-03, -1.6058e-02,\n        -9.4333e-03,  7.7585e-03, -1.3513e-02,  1.6015e-03, -9.6059e-03,\n         5.8819e-03,  6.2207e-03,  1.5643e-03,  5.5690e-03, -4.3029e-03,\n        -8.9664e-03,  1.2149e-02, -3.5258e-04,  1.1390e-02,  1.0160e-02,\n         2.2349e-02, -5.3247e-03,  1.6526e-02,  2.3989e-04, -8.5838e-03,\n         3.6163e-03,  5.1848e-04,  1.1433e-02, -2.0787e-03, -6.2213e-03,\n         2.0228e-02,  1.0812e-02,  2.0251e-02,  5.9596e-03, -1.5718e-02,\n         7.1512e-03,  4.9057e-03, -2.7291e-01,  7.5315e-03, -2.7525e-02,\n         6.5759e-03, -6.1021e-03,  5.3712e-04, -5.7159e-03, -3.1999e-03,\n         7.1039e-03, -2.6411e-02,  3.8841e-03,  1.3163e-02,  1.0494e-02,\n         1.7441e-02,  7.4625e-03, -3.0912e-03,  8.5485e-03,  1.2453e-02,\n        -7.0957e-03, -4.3735e-03,  1.6716e-03, -4.2584e-03, -6.8505e-03,\n         1.1429e-02,  5.5109e-03,  6.1918e-03,  1.3121e-02, -7.9127e-05,\n         5.8243e-03,  1.3653e-02,  3.9599e-03, -1.4619e-02,  1.5558e-03,\n        -1.0011e-03, -3.2919e-02, -1.1776e-03,  4.6114e-03,  9.7119e-03,\n         1.7938e-02,  1.2150e-02, -7.1375e-03, -2.3015e-03, -9.3771e-03,\n        -1.8237e-03,  7.7706e-03, -1.2758e-02,  8.2311e-03,  1.0718e-02,\n        -1.7488e-02, -1.6008e-02,  3.4320e-03,  4.2032e-03, -1.4721e-02,\n         3.6317e-03, -6.8918e-03, -2.4418e-03, -3.7935e-03, -3.3411e-03,\n        -9.5328e-03, -1.0424e-02, -3.2779e-03, -1.6413e-02, -7.2408e-03,\n        -1.6851e-03,  1.7651e-02, -8.4258e-03,  1.1767e-03,  2.4501e-03,\n         6.9684e-03, -1.5371e-04,  4.9056e-03, -1.5362e-02, -2.3809e-03,\n        -1.3153e-02, -1.3947e-02,  8.4123e-03, -7.1363e-04, -5.1272e-04,\n         5.6414e-03, -1.1419e-03, -5.4236e-03, -2.3558e-04, -1.4465e-02,\n         9.4876e-03,  1.7246e-02,  7.8333e-03, -2.0382e-02,  2.2036e-02,\n         3.9713e-03, -1.7529e-02,  5.9421e-03,  1.2660e-02,  1.7963e-02,\n         1.6849e-02, -4.4247e-03, -8.8774e-03, -1.0036e-02, -2.6192e-02,\n        -6.6488e-03, -1.1037e-02, -5.2248e-04,  8.8847e-03,  2.1577e-02,\n        -1.7690e-02,  6.7719e-02, -1.7940e-03, -7.2144e-04, -4.8792e-03,\n        -5.2160e-04,  2.0387e-02,  2.3457e-02,  2.2987e-03, -2.8230e-03,\n         9.8262e-04,  7.6678e-03, -1.2602e-02,  1.3712e-03, -5.9636e-03,\n         8.4728e-03,  9.7945e-03,  1.8875e-02, -1.1761e-02,  8.4486e-03,\n        -6.5427e-03, -1.0821e-02, -1.8378e-02,  1.1782e-04,  1.4728e-02,\n        -4.5728e-03, -5.3310e-03,  5.8161e-03,  2.5254e-03, -1.5298e-02,\n        -8.2829e-03, -4.9204e-03, -6.7762e-03, -1.3968e-02, -5.0512e-03,\n         2.0694e-03, -2.0547e-03,  1.2989e-02, -1.2561e-02, -3.9346e-03,\n         8.0921e-03,  2.2794e-03, -9.8757e-03,  3.8440e-03,  1.4281e-02,\n         1.4431e-02, -1.2656e-02, -5.6140e-03,  1.8937e-02, -2.8733e-02,\n         9.0419e-04, -8.0504e-03, -1.7042e-02, -1.1202e-02, -1.9735e-03,\n        -7.7217e-03,  6.4953e-03,  1.3603e-02, -4.6683e-03, -2.0009e-02,\n        -5.7123e-03,  1.4727e-02,  1.4844e-03,  1.2351e-02,  5.3493e-03,\n         6.4715e-03,  2.0465e-02, -1.6445e-02,  1.8360e-03,  1.1304e-03,\n         1.0698e-02,  1.3027e-02, -4.1099e-03,  1.2465e-02,  1.4663e-02,\n         1.8791e-02,  6.5558e-03, -2.6639e-03,  1.9077e-02, -1.0572e-02,\n        -7.3378e-03,  9.3518e-03,  1.7210e-02, -7.9083e-03,  1.0630e-02,\n        -4.1561e-04,  2.4730e-03,  1.1711e-03, -2.3229e-03, -1.8869e-02,\n         4.1363e-03,  8.9857e-03,  1.2259e-02,  4.9236e-03, -2.3325e-02,\n         5.7182e-03, -5.1188e-03, -1.0989e-02,  8.8022e-03, -3.9972e-02,\n        -1.2454e-02, -5.1792e-03, -1.3054e-03,  3.8142e-03,  1.3714e-02,\n         1.6481e-04, -4.5547e-03, -1.8836e-02,  2.9413e-03,  2.3549e-02,\n        -3.3217e-03, -4.4010e-03,  1.0472e-02, -5.3070e-02, -1.2433e-02,\n        -8.2471e-03,  5.8499e-03, -4.1150e-03, -2.1084e-02, -4.5903e-03,\n         9.7190e-03, -5.0059e-03, -2.6336e-02,  2.8893e-03, -4.8182e-03,\n         1.1307e-02,  1.0283e-02, -3.6541e-03,  3.4809e-03, -4.4435e-03,\n        -3.8584e-03,  6.6467e-03,  2.6232e-02, -1.0732e-02, -1.0484e-03,\n         8.1112e-04,  1.0185e-02, -1.2516e-02, -8.6969e-03, -1.3058e-02,\n        -5.6110e-03, -5.5140e-03,  7.9961e-03,  3.1677e-03,  1.1114e-02,\n         7.5115e-03,  5.9754e-03, -9.3546e-03,  1.6863e-02, -1.7603e-02,\n         1.4922e-03,  9.7844e-03,  5.9478e-04, -6.9355e-03, -1.1417e-02,\n         1.3672e-02, -6.5205e-03,  8.0334e-03,  9.3448e-03,  1.7244e-02,\n        -2.0423e-02,  1.2507e-02,  1.8644e-03,  8.0372e-03, -1.8640e-02,\n         7.4018e-03,  1.2861e-02,  9.3628e-03, -1.5410e-02,  8.6267e-03,\n         9.1553e-03,  6.7183e-03,  3.5797e-03,  8.5484e-03,  9.9596e-03,\n         9.1772e-03, -3.1773e-05, -5.7101e-03, -1.2084e-02, -2.4658e-03,\n         7.9517e-03, -5.0779e-01,  1.0139e-03,  1.0576e-02, -1.0760e-02,\n         1.2478e-02, -1.7292e-02, -1.4580e-02,  2.0072e-02, -1.6023e-04,\n         2.0238e-03, -1.2025e-02,  4.4631e-03, -5.6513e-03, -1.9817e-02,\n        -6.3167e-03, -2.1827e-02,  3.2357e-03,  2.5435e-02,  4.9387e-03,\n         8.3249e-03, -1.3499e-02, -9.4396e-04,  3.7451e-04,  1.5849e-02,\n         3.8080e-04,  1.0294e-02,  1.1543e-02,  8.3427e-03,  8.3668e-03,\n        -4.9993e-03,  6.7894e-03,  7.6795e-03,  1.5896e-02, -1.1729e-03,\n         2.1644e-02,  6.1628e-03,  4.3696e-03, -1.3227e-02, -1.8129e-02,\n         7.6195e-03, -1.3195e-02, -2.9670e-02, -1.4286e-02,  7.2905e-03,\n         1.1367e-02,  1.3281e-02, -1.6435e-03,  1.2947e-03,  1.2742e-02,\n         4.7769e-02, -2.2950e-03,  9.5310e-04, -8.6626e-03, -3.0132e-03,\n         7.4294e-03,  6.5061e-03,  1.5730e-02,  2.7817e-03,  9.8474e-03,\n        -2.8636e-02,  5.3342e-03,  1.2795e-02], device='cuda:0',\n       requires_grad=True)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input=tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  .....\n       device='cuda:0', grad_fn=<ReluBackward0>), weight=Parameter containing:\ntensor([[ 1.0483e-03, -7.7....0083e-03]], device='cuda:0', requires_grad=True), bias=Parameter containing:\ntensor([ 4.8521e-03,  2.18...-02], device='cuda:0',\n       requires_grad=True))\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mtorch._C._nn.linear\u001b[0m \u001b[0;34m= <built-in function linear>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= tensor([[2.2283e-01, 2.2807e-01, 3.6793e-01,  ..., 0.0000e+00, 9.9608e-02, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n       device='cuda:0', grad_fn=<ReluBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mweight\u001b[0m \u001b[0;34m= Parameter containing:\ntensor([[ 1.0483e-03, -7.7878e-03, -9.0114e-03,  ..., -1.3342e-03,\n          1.2033e-02,  7.9609e-03],\n        [-4.6866e-02, -3.4472e-02, -6.3638e-02,  ..., -1.6218e-02,\n         -2.8732e-02,  6.7287e-03],\n        [-4.4598e-03,  1.5761e-02, -7.9800e-03,  ..., -6.1653e-03,\n          1.7925e-02,  7.7807e-03],\n        ...,\n        [ 1.8789e-02,  9.1553e-03, -9.8881e-03,  ..., -2.1792e-03,\n         -1.3324e-02, -1.1625e-03],\n        [-1.8926e-03, -1.0199e-02,  9.1252e-03,  ..., -2.1141e-03,\n          1.6208e-02,  1.7390e-02],\n        [ 3.3168e-05,  1.1934e-03,  6.7774e-03,  ..., -9.5507e-03,\n         -4.4643e-03, -8.0083e-03]], device='cuda:0', requires_grad=True)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mbias\u001b[0m \u001b[0;34m= Parameter containing:\ntensor([ 4.8521e-03,  2.1897e-02,  9.4740e-03, -1.0681e-02,  1.5044e-03,\n         4.5271e-04,  1.5272e-02,  5.9391e-03,  4.6618e-03,  6.7650e-04,\n        -1.2297e-02, -3.7563e-03,  3.5670e-03, -4.2529e-03,  7.9859e-03,\n        -7.4854e-03, -1.0479e-03, -2.3301e-03, -5.1403e-03, -1.1754e-03,\n        -5.3549e-02, -3.0976e-04,  6.0539e-03, -1.6012e-02, -8.1361e-03,\n         1.3506e-04, -1.9545e-03,  3.9966e-03, -7.9100e-03,  3.5905e-03,\n         8.0427e-03, -1.7940e-02, -1.1837e-02,  9.2702e-03, -1.4064e-02,\n         6.5122e-03, -1.3663e-02,  7.4959e-03, -6.2527e-03,  1.2555e-02,\n         3.7583e-03,  5.2666e-03, -1.8315e-03,  1.1555e-02, -1.2019e-02,\n         7.2844e-03, -5.8099e-03,  1.8510e-02, -3.7640e-04, -1.3973e-02,\n         1.0434e-02, -3.3899e-03,  1.6820e-02, -1.4337e-02, -1.2776e-02,\n         1.2438e-03,  4.9739e-03, -2.9592e-03,  8.5997e-03,  7.1787e-03,\n         5.2585e-03,  2.1852e-02,  8.6368e-03, -1.5947e-02, -2.7866e-03,\n        -6.3835e-03,  1.9673e-03, -2.7436e-03, -6.0244e-03,  4.3477e-03,\n         6.8208e-03, -1.4410e-02, -3.1084e-03,  1.0359e-03,  5.0409e-03,\n         3.9560e-03,  1.5779e-03, -4.7672e-03, -3.7576e-04, -2.0302e-03,\n         1.6972e-03, -2.2588e-03,  1.4119e-02,  7.9137e-03,  1.1871e-02,\n        -7.9027e-03,  3.9132e-03, -8.2581e-04,  1.0704e-02, -6.7014e-03,\n        -3.9309e-02, -1.7300e-02,  8.2843e-03,  3.7417e-03, -1.7606e-03,\n        -2.5266e-03,  5.9416e-03,  2.0168e-02, -2.1081e-02,  6.9172e-03,\n         1.2677e-02, -1.3839e-02,  6.9296e-03, -2.8055e-03, -9.1375e-03,\n         1.4408e-02, -1.2299e-02,  1.6568e-02, -6.5706e-03, -1.2223e-02,\n         7.7945e-03,  9.0612e-04,  3.1409e-03, -1.0067e-02, -5.5894e-02,\n        -5.2159e-03,  1.0642e-02, -1.5085e-02, -2.4796e-02, -8.3892e-03,\n        -7.9222e-03, -9.8696e-03, -2.8704e-03,  1.1627e-03,  5.1770e-05,\n        -1.0577e-04,  3.3841e-03,  5.9580e-03,  1.1933e-02,  5.6968e-03,\n         1.8265e-02,  1.6151e-03, -4.2234e-03, -1.2698e-04, -5.2291e-03,\n        -2.7210e-03,  8.0698e-03,  9.9447e-04,  1.7762e-02,  9.1987e-03,\n        -2.7673e-02,  3.0002e-03,  3.3834e-03, -3.0443e-03,  3.1045e-03,\n         3.2182e-04,  5.0196e-03,  8.5745e-03, -1.4722e-03, -1.0774e-02,\n        -7.5623e-03, -8.5885e-04, -8.2788e-03,  1.3704e-02,  1.0896e-02,\n         1.7064e-03,  6.0692e-03,  1.4446e-02,  4.1554e-03,  5.7510e-03,\n         5.1193e-03, -1.8570e-02, -1.6443e-03, -2.6150e-03,  3.9289e-03,\n        -7.4846e-03, -9.3908e-03, -5.0984e-03, -1.0646e-02, -1.7282e-02,\n         6.1938e-03,  2.0931e-02, -1.5833e-02,  6.7478e-03, -4.5674e-02,\n        -3.2354e-03,  1.0683e-02, -2.2775e-02, -1.3241e-02, -1.1633e-02,\n         5.0383e-03,  2.9561e-04,  1.0794e-02, -2.1567e-03, -2.9891e-03,\n        -4.5570e-03, -2.8450e-02, -3.6399e-03,  7.2521e-03, -4.4623e-03,\n        -2.1339e-03,  1.2108e-02, -6.4772e-03,  4.8501e-03, -5.3036e-03,\n         8.8487e-03,  5.8598e-03, -8.0472e-03,  5.9259e-04, -3.2681e-03,\n        -6.6208e-03,  8.3501e-03,  4.5201e-03,  2.5827e-04, -2.9973e-03,\n         2.2560e-02, -2.4859e-03,  1.2619e-02,  1.3537e-02, -1.1576e-03,\n         8.9714e-03,  9.0819e-04, -1.0417e-02, -8.5418e-04, -1.6119e-02,\n         1.2963e-02,  1.0251e-02,  9.6932e-03, -5.4587e-05,  2.0906e-02,\n        -1.2398e-03, -2.2669e-02,  1.4525e-02, -9.2013e-03,  1.4122e-02,\n         5.9847e-03, -7.9777e-03, -1.3821e-02, -1.7088e-02,  1.6989e-02,\n         1.2800e-02, -2.1054e-02, -1.7505e-02,  5.2251e-05,  1.5770e-02,\n        -2.7207e-03, -4.0463e-03, -6.1229e-03,  3.7058e-03,  4.9263e-03,\n         1.1694e-02, -1.3610e-02, -6.9040e-03, -1.1387e-02, -2.1495e-03,\n        -8.1701e-03, -1.4801e-02,  9.2226e-03, -1.5280e-02,  4.8696e-03,\n        -5.6204e-04,  2.7530e-02, -4.4862e-03, -2.0064e-04,  1.8665e-03,\n        -3.7229e-03, -6.3350e-03, -5.9100e-03,  2.6694e-03, -7.9968e-03,\n         6.0988e-03, -7.6362e-03,  7.7755e-03, -1.6421e-02,  2.3481e-03,\n        -1.8722e-02, -2.3150e-02,  1.1144e-02,  1.0771e-02,  2.7008e-04,\n         5.6076e-03, -3.3821e-03,  1.0863e-02, -1.1510e-02, -9.2244e-04,\n         4.7643e-03,  1.2510e-03,  1.5603e-04, -9.4766e-03,  1.6293e-04,\n        -6.8112e-04,  1.3853e-02,  4.2988e-03, -2.4846e-03,  2.9502e-03,\n         2.7906e-03,  8.3292e-03,  2.0841e-02, -6.5192e-03,  4.0946e-03,\n        -4.4378e-03, -4.9862e-03,  1.5562e-02, -1.7550e-03,  5.8463e-03,\n         3.5299e-05, -2.2725e-03, -7.2018e-03, -3.4185e-03,  1.9829e-03,\n         1.0183e-02,  8.5417e-04,  6.6531e-03, -3.7769e-03, -1.7410e-03,\n         2.7847e-03,  5.8457e-04, -2.8352e-04,  8.7203e-03,  1.5451e-05,\n        -6.7773e-03,  2.5684e-02, -6.0656e-03, -1.3382e-02, -1.0669e-02,\n         2.4113e-03, -8.7023e-03,  1.9617e-02, -4.0686e-03, -9.5084e-03,\n         4.1833e-03, -3.4600e-03, -7.7735e-03, -9.9612e-03,  4.5750e-05,\n        -4.5247e-03, -1.0113e-02, -1.6438e-02, -4.2614e-04,  3.9168e-03,\n        -7.4066e-03, -1.5820e-02, -8.0423e-03, -1.3433e-02, -2.1988e-02,\n        -2.4801e-02, -3.8386e-03, -1.0298e-02, -2.5305e-03, -4.3774e-03,\n        -2.3115e-02,  1.9233e-02,  5.7082e-03,  6.9035e-03, -1.6292e-02,\n        -2.7858e-02,  4.5046e-03, -1.1604e-02, -4.5397e-03, -2.6778e-02,\n        -3.1727e-03,  1.9516e-02, -2.5134e-03,  5.3662e-02,  1.3297e-02,\n        -5.3088e-02, -4.1824e-03, -9.4843e-04,  7.6987e-03,  3.1061e-03,\n         1.0933e-02, -1.0215e-02,  4.6496e-03, -1.5380e-02,  6.0454e-03,\n        -1.9853e-03, -9.4607e-04, -1.4682e-02,  1.6984e-04, -9.6661e-04,\n        -2.6289e-03,  5.0183e-03, -7.5690e-03,  1.6436e-03, -1.9193e-02,\n        -3.1853e-02, -8.4748e-03, -2.4048e-03,  1.3622e-02,  6.1662e-03,\n         1.0318e-02,  3.5779e-03,  1.5591e-02,  5.1069e-03, -1.4332e-02,\n        -1.0879e-02,  1.3138e-02,  5.1318e-03, -1.6768e-03, -1.6058e-02,\n        -9.4333e-03,  7.7585e-03, -1.3513e-02,  1.6015e-03, -9.6059e-03,\n         5.8819e-03,  6.2207e-03,  1.5643e-03,  5.5690e-03, -4.3029e-03,\n        -8.9664e-03,  1.2149e-02, -3.5258e-04,  1.1390e-02,  1.0160e-02,\n         2.2349e-02, -5.3247e-03,  1.6526e-02,  2.3989e-04, -8.5838e-03,\n         3.6163e-03,  5.1848e-04,  1.1433e-02, -2.0787e-03, -6.2213e-03,\n         2.0228e-02,  1.0812e-02,  2.0251e-02,  5.9596e-03, -1.5718e-02,\n         7.1512e-03,  4.9057e-03, -2.7291e-01,  7.5315e-03, -2.7525e-02,\n         6.5759e-03, -6.1021e-03,  5.3712e-04, -5.7159e-03, -3.1999e-03,\n         7.1039e-03, -2.6411e-02,  3.8841e-03,  1.3163e-02,  1.0494e-02,\n         1.7441e-02,  7.4625e-03, -3.0912e-03,  8.5485e-03,  1.2453e-02,\n        -7.0957e-03, -4.3735e-03,  1.6716e-03, -4.2584e-03, -6.8505e-03,\n         1.1429e-02,  5.5109e-03,  6.1918e-03,  1.3121e-02, -7.9127e-05,\n         5.8243e-03,  1.3653e-02,  3.9599e-03, -1.4619e-02,  1.5558e-03,\n        -1.0011e-03, -3.2919e-02, -1.1776e-03,  4.6114e-03,  9.7119e-03,\n         1.7938e-02,  1.2150e-02, -7.1375e-03, -2.3015e-03, -9.3771e-03,\n        -1.8237e-03,  7.7706e-03, -1.2758e-02,  8.2311e-03,  1.0718e-02,\n        -1.7488e-02, -1.6008e-02,  3.4320e-03,  4.2032e-03, -1.4721e-02,\n         3.6317e-03, -6.8918e-03, -2.4418e-03, -3.7935e-03, -3.3411e-03,\n        -9.5328e-03, -1.0424e-02, -3.2779e-03, -1.6413e-02, -7.2408e-03,\n        -1.6851e-03,  1.7651e-02, -8.4258e-03,  1.1767e-03,  2.4501e-03,\n         6.9684e-03, -1.5371e-04,  4.9056e-03, -1.5362e-02, -2.3809e-03,\n        -1.3153e-02, -1.3947e-02,  8.4123e-03, -7.1363e-04, -5.1272e-04,\n         5.6414e-03, -1.1419e-03, -5.4236e-03, -2.3558e-04, -1.4465e-02,\n         9.4876e-03,  1.7246e-02,  7.8333e-03, -2.0382e-02,  2.2036e-02,\n         3.9713e-03, -1.7529e-02,  5.9421e-03,  1.2660e-02,  1.7963e-02,\n         1.6849e-02, -4.4247e-03, -8.8774e-03, -1.0036e-02, -2.6192e-02,\n        -6.6488e-03, -1.1037e-02, -5.2248e-04,  8.8847e-03,  2.1577e-02,\n        -1.7690e-02,  6.7719e-02, -1.7940e-03, -7.2144e-04, -4.8792e-03,\n        -5.2160e-04,  2.0387e-02,  2.3457e-02,  2.2987e-03, -2.8230e-03,\n         9.8262e-04,  7.6678e-03, -1.2602e-02,  1.3712e-03, -5.9636e-03,\n         8.4728e-03,  9.7945e-03,  1.8875e-02, -1.1761e-02,  8.4486e-03,\n        -6.5427e-03, -1.0821e-02, -1.8378e-02,  1.1782e-04,  1.4728e-02,\n        -4.5728e-03, -5.3310e-03,  5.8161e-03,  2.5254e-03, -1.5298e-02,\n        -8.2829e-03, -4.9204e-03, -6.7762e-03, -1.3968e-02, -5.0512e-03,\n         2.0694e-03, -2.0547e-03,  1.2989e-02, -1.2561e-02, -3.9346e-03,\n         8.0921e-03,  2.2794e-03, -9.8757e-03,  3.8440e-03,  1.4281e-02,\n         1.4431e-02, -1.2656e-02, -5.6140e-03,  1.8937e-02, -2.8733e-02,\n         9.0419e-04, -8.0504e-03, -1.7042e-02, -1.1202e-02, -1.9735e-03,\n        -7.7217e-03,  6.4953e-03,  1.3603e-02, -4.6683e-03, -2.0009e-02,\n        -5.7123e-03,  1.4727e-02,  1.4844e-03,  1.2351e-02,  5.3493e-03,\n         6.4715e-03,  2.0465e-02, -1.6445e-02,  1.8360e-03,  1.1304e-03,\n         1.0698e-02,  1.3027e-02, -4.1099e-03,  1.2465e-02,  1.4663e-02,\n         1.8791e-02,  6.5558e-03, -2.6639e-03,  1.9077e-02, -1.0572e-02,\n        -7.3378e-03,  9.3518e-03,  1.7210e-02, -7.9083e-03,  1.0630e-02,\n        -4.1561e-04,  2.4730e-03,  1.1711e-03, -2.3229e-03, -1.8869e-02,\n         4.1363e-03,  8.9857e-03,  1.2259e-02,  4.9236e-03, -2.3325e-02,\n         5.7182e-03, -5.1188e-03, -1.0989e-02,  8.8022e-03, -3.9972e-02,\n        -1.2454e-02, -5.1792e-03, -1.3054e-03,  3.8142e-03,  1.3714e-02,\n         1.6481e-04, -4.5547e-03, -1.8836e-02,  2.9413e-03,  2.3549e-02,\n        -3.3217e-03, -4.4010e-03,  1.0472e-02, -5.3070e-02, -1.2433e-02,\n        -8.2471e-03,  5.8499e-03, -4.1150e-03, -2.1084e-02, -4.5903e-03,\n         9.7190e-03, -5.0059e-03, -2.6336e-02,  2.8893e-03, -4.8182e-03,\n         1.1307e-02,  1.0283e-02, -3.6541e-03,  3.4809e-03, -4.4435e-03,\n        -3.8584e-03,  6.6467e-03,  2.6232e-02, -1.0732e-02, -1.0484e-03,\n         8.1112e-04,  1.0185e-02, -1.2516e-02, -8.6969e-03, -1.3058e-02,\n        -5.6110e-03, -5.5140e-03,  7.9961e-03,  3.1677e-03,  1.1114e-02,\n         7.5115e-03,  5.9754e-03, -9.3546e-03,  1.6863e-02, -1.7603e-02,\n         1.4922e-03,  9.7844e-03,  5.9478e-04, -6.9355e-03, -1.1417e-02,\n         1.3672e-02, -6.5205e-03,  8.0334e-03,  9.3448e-03,  1.7244e-02,\n        -2.0423e-02,  1.2507e-02,  1.8644e-03,  8.0372e-03, -1.8640e-02,\n         7.4018e-03,  1.2861e-02,  9.3628e-03, -1.5410e-02,  8.6267e-03,\n         9.1553e-03,  6.7183e-03,  3.5797e-03,  8.5484e-03,  9.9596e-03,\n         9.1772e-03, -3.1773e-05, -5.7101e-03, -1.2084e-02, -2.4658e-03,\n         7.9517e-03, -5.0779e-01,  1.0139e-03,  1.0576e-02, -1.0760e-02,\n         1.2478e-02, -1.7292e-02, -1.4580e-02,  2.0072e-02, -1.6023e-04,\n         2.0238e-03, -1.2025e-02,  4.4631e-03, -5.6513e-03, -1.9817e-02,\n        -6.3167e-03, -2.1827e-02,  3.2357e-03,  2.5435e-02,  4.9387e-03,\n         8.3249e-03, -1.3499e-02, -9.4396e-04,  3.7451e-04,  1.5849e-02,\n         3.8080e-04,  1.0294e-02,  1.1543e-02,  8.3427e-03,  8.3668e-03,\n        -4.9993e-03,  6.7894e-03,  7.6795e-03,  1.5896e-02, -1.1729e-03,\n         2.1644e-02,  6.1628e-03,  4.3696e-03, -1.3227e-02, -1.8129e-02,\n         7.6195e-03, -1.3195e-02, -2.9670e-02, -1.4286e-02,  7.2905e-03,\n         1.1367e-02,  1.3281e-02, -1.6435e-03,  1.2947e-03,  1.2742e-02,\n         4.7769e-02, -2.2950e-03,  9.5310e-04, -8.6626e-03, -3.0132e-03,\n         7.4294e-03,  6.5061e-03,  1.5730e-02,  2.7817e-03,  9.8474e-03,\n        -2.8636e-02,  5.3342e-03,  1.2795e-02], device='cuda:0',\n       requires_grad=True)\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f=<frame at 0x7fc5d5406b90, file '/home/borela/min.../torch/nn/functional.py', line 1848, code linear>, limit=None)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mformat_list\u001b[0m \u001b[0;34m= <function format_list at 0x7fc70e39fcb0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mextract_stack\u001b[0m \u001b[0;34m= <function extract_stack at 0x7fc70e31cf80>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mf\u001b[0m \u001b[0;34m= <frame at 0x7fc5d5406b90, file '/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/functional.py', line 1848, code linear>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mlimit\u001b[0m \u001b[0;34m= None\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f=<frame at 0x7fc5d5406b90, file '/home/borela/min.../torch/nn/functional.py', line 1848, code linear>, limit=None)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mstack\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mStackSummary.extract\u001b[0m \u001b[0;34m= <bound method StackSummary.extract of <class 'traceback.StackSummary'>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mwalk_stack\u001b[0m \u001b[0;34m= <function walk_stack at 0x7fc70e31f0e0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mf\u001b[0m \u001b[0;34m= <frame at 0x7fc5d5406b90, file '/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/functional.py', line 1848, code linear>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mlimit\u001b[0m \u001b[0;34m= None\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass=<class 'traceback.StackSummary'>, frame_gen=<generator object walk_stack>, limit=None, lookup_lines=True, capture_locals=False)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mlinecache.checkcache\u001b[0m \u001b[0;34m= <function check_linecache_ipython at 0x7fc70d09c710>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mfilename\u001b[0m \u001b[0;34m= '/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/IPython/core/async_helpers.py'\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args=('/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/IPython/core/async_helpers.py',))\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# First call the original checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mlinecache._checkcache_ori\u001b[0m \u001b[0;34m= <function checkcache at 0x7fc70e30e4d0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36margs\u001b[0m \u001b[0;34m= ('/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/IPython/core/async_helpers.py',)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename='/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/IPython/core/async_helpers.py')\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mlen\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mentry\u001b[0m \u001b[0;34m= (5343, 1643818180.0, ['\"\"\"\\n', 'Async helper function that are invalid syntax on Python 3.5 and below.\\n', '\\n', 'This code is best effort, and may have edge cases not behaving as expected. In\\n', 'particular it contain a number of heuristics to detect whether code is\\n', 'effectively async and need to run in an event loop or not.\\n', '\\n', 'Some constructs (like top-level `return`, or `yield`) are taken care of\\n', 'explicitly to actually raise a SyntaxError and stay as close as possible to\\n', 'Python semantics.\\n', '\"\"\"\\n', '\\n', '\\n', 'import ast\\n', 'import sys\\n', 'import asyncio\\n', 'import inspect\\n', 'from textwrap import dedent, indent\\n', '\\n', '\\n', 'class _AsyncIORunner:\\n', '    def __init__(self):\\n', '        self._loop = None\\n', '\\n', '    @property\\n', '    def loop(self):\\n', '        \"\"\"Always returns a non-closed event loop\"\"\"\\n', '        if self._loop is None or self._loop.is_closed():\\n', '            policy = asyncio.get_event_loop_policy()\\n', '            self._loop = policy.new_event_loop()\\n', '            policy.set_event_loop(self._loop)\\n', '        return self._loop\\n', '\\n', '    def __call__(self, coro):\\n', '        \"\"\"\\n', '        Handler for asyncio autoawait\\n', '        \"\"\"\\n', '        return self.loop.run_until_complete(coro)\\n', '\\n', '    def __str__(self):\\n', \"        return 'asyncio'\\n\", '\\n', '_asyncio_runner = _AsyncIORunner()\\n', '\\n', '\\n', 'def _curio_runner(coroutine):\\n', '    \"\"\"\\n', '    handler for curio autoawait\\n', '    \"\"\"\\n', '    import curio\\n', '\\n', '    return curio.run(coroutine)\\n', '\\n', '\\n', 'def _trio_runner(async_fn):\\n', '    import trio\\n', '\\n', '    async def loc(coro):\\n', '        \"\"\"\\n', '        We need the dummy no-op async def to protect from\\n', \"        trio's internal. See https://github.com/python-trio/trio/issues/89\\n\", '        \"\"\"\\n', '        return await coro\\n', '\\n', '    return trio.run(loc, async_fn)\\n', '\\n', '\\n', 'def _pseudo_sync_runner(coro):\\n', '    \"\"\"\\n', '    A runner that does not really allow async execution, and just advance the coroutine.\\n', '\\n', '    See discussion in https://github.com/python-trio/trio/issues/608,\\n', '\\n', '    Credit to Nathaniel Smith\\n', '\\n', '    \"\"\"\\n', '    try:\\n', '        coro.send(None)\\n', '    except StopIteration as exc:\\n', '        return exc.value\\n', '    else:\\n', '        # TODO: do not raise but return an execution result with the right info.\\n', '        raise RuntimeError(\\n', '            \"{coro_name!r} needs a real async loop\".format(coro_name=coro.__name__)\\n', '        )\\n', '\\n', '\\n', 'def _asyncify(code: str) -> str:\\n', '    \"\"\"wrap code in async def definition.\\n', '\\n', '    And setup a bit of context to run it later.\\n', '    \"\"\"\\n', '    res = dedent(\\n', '        \"\"\"\\n', '    async def __wrapper__():\\n', '        try:\\n', '    {usercode}\\n', '        finally:\\n', '            locals()\\n', '    \"\"\"\\n', '    ).format(usercode=indent(code, \" \" * 8))\\n', '    return res\\n', '\\n', '\\n', 'class _AsyncSyntaxErrorVisitor(ast.NodeVisitor):\\n', '    \"\"\"\\n', '    Find syntax errors that would be an error in an async repl, but because\\n', '    the implementation involves wrapping the repl in an async function, it\\n', '    is erroneously allowed (e.g. yield or return at the top level)\\n', '    \"\"\"\\n', '    def __init__(self):\\n', '        if sys.version_info >= (3,8):\\n', \"            raise ValueError('DEPRECATED in Python 3.8+')\\n\", '        self.depth = 0\\n', '        super().__init__()\\n', '\\n', '    def generic_visit(self, node):\\n', '        func_types = (ast.FunctionDef, ast.AsyncFunctionDef)\\n', '        invalid_types_by_depth = {\\n', '            0: (ast.Return, ast.Yield, ast.YieldFrom),\\n', '            1: (ast.Nonlocal,)\\n', '        }\\n', '\\n', '        should_traverse = self.depth < max(invalid_types_by_depth.keys())\\n', '        if isinstance(node, func_types) and should_traverse:\\n', '            self.depth += 1\\n', '            super().generic_visit(node)\\n', '            self.depth -= 1\\n', '        elif isinstance(node, invalid_types_by_depth[self.depth]):\\n', '            raise SyntaxError()\\n', '        else:\\n', '            super().generic_visit(node)\\n', '\\n', '\\n', 'def _async_parse_cell(cell: str) -> ast.AST:\\n', '    \"\"\"\\n', '    This is a compatibility shim for pre-3.7 when async outside of a function\\n', '    is a syntax error at the parse stage.\\n', '\\n', '    It will return an abstract syntax tree parsed as if async and await outside\\n', '    of a function were not a syntax error.\\n', '    \"\"\"\\n', '    if sys.version_info < (3, 7):\\n', '        # Prior to 3.7 you need to asyncify before parse\\n', '        wrapped_parse_tree = ast.parse(_asyncify(cell))\\n', '        return wrapped_parse_tree.body[0].body[0]\\n', '    else:\\n', '        return ast.parse(cell)\\n', '\\n', '\\n', 'def _should_be_async(cell: str) -> bool:\\n', '    \"\"\"Detect if a block of code need to be wrapped in an `async def`\\n', '\\n', \"    Attempt to parse the block of code, it it compile we're fine.\\n\", '    Otherwise we  wrap if and try to compile.\\n', '\\n', '    If it works, assume it should be async. Otherwise Return False.\\n', '\\n', '    Not handled yet: If the block of code has a return statement as the top\\n', '    level, it will be seen as async. This is a know limitation.\\n', '    \"\"\"\\n', '    if sys.version_info > (3, 8):\\n', '        try:\\n', '            code = compile(cell, \"<>\", \"exec\", flags=getattr(ast,\\'PyCF_ALLOW_TOP_LEVEL_AWAIT\\', 0x0))\\n', '            return inspect.CO_COROUTINE & code.co_flags == inspect.CO_COROUTINE\\n', '        except (SyntaxError, MemoryError):\\n', '            return False\\n', '    try:\\n', \"        # we can't limit ourself to ast.parse, as it __accepts__ to parse on\\n\", '        # 3.7+, but just does not _compile_\\n', '        code = compile(cell, \"<>\", \"exec\")\\n', '    except (SyntaxError, MemoryError):\\n', '        try:\\n', '            parse_tree = _async_parse_cell(cell)\\n', '\\n', '            # Raise a SyntaxError if there are top-level return or yields\\n', '            v = _AsyncSyntaxErrorVisitor()\\n', '            v.visit(parse_tree)\\n', '\\n', '        except (SyntaxError, MemoryError):\\n', '            return False\\n', '        return True\\n', '    return False\\n'], '/home/borela/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/IPython/core/async_helpers.py')\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# lazy cache entry, leave it lazy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "treina_grid(hparam, gridparam, model, parm_se_gera_rastro = True, se_treina_poucos_dados=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "xiH41qVuhG2m"
      },
      "outputs": [],
      "source": [
        "path_modelo = f'{prefixo_nome_diretorio}best_model_len_{hparam[\"max_seq_length\"]}.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model, path_modelo) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### batch_size 24  \n",
        "Aumentando o batch size (para otimizar o tempo, já que há espaço na GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[3],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[24],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 45.61\n",
            " used 20.61\n",
            " free 2.01\n",
            " cached 44.25\n",
            " buffers 0.49\n",
            "/nGPU\n",
            "Tue Mar 28 13:15:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "|  0%   50C    P8    26W / 370W |   7807MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "|    0   N/A  N/A    467182      C   .../relevar-busca/bin/python     7743MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu', 'gpu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 28 13:16:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 45%   51C    P8    27W / 370W |   7807MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "|    0   N/A  N/A    467182      C   .../relevar-busca/bin/python     7743MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_workers_dataloader': 0,\n",
              " 'device': device(type='cuda', index=0),\n",
              " 'vocab_size': 50272,\n",
              " 'num_sentenca_train': 249800,\n",
              " 'num_sentenca_valid': 100,\n",
              " 'num_sentenca_test': 100,\n",
              " 'max_seq_length': 100,\n",
              " 'batch_size': 8,\n",
              " 'num_epochs': 3,\n",
              " 'learning_rate': 0.0001,\n",
              " 'fator_corte_loss_maximo': 1,\n",
              " 'decrease_factor_lr': 1e-06,\n",
              " 'weight_decay': 0.0001,\n",
              " 'drop_last': True,\n",
              " 'train_size': 4328981,\n",
              " 'valid_size': 2036,\n",
              " 'test_size': 1143,\n",
              " 'max_examples': 12986943,\n",
              " 'percentual_eval_every_steps': 0.0025,\n",
              " 'eval_every_steps': 4058,\n",
              " 'early_stop': 40580,\n",
              " 'criterion': CrossEntropyLoss(),\n",
              " 'num_params': 125239296,\n",
              " 'amsgrad': False,\n",
              " 'optimizer': Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     initial_lr: 0.0001\n",
              "     lr: 9.995352161245022e-05\n",
              "     weight_decay: 0.0\n",
              " \n",
              " Parameter Group 1\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     initial_lr: 0.0001\n",
              "     lr: 9.995352161245022e-05\n",
              "     weight_decay: 0.0001\n",
              " ),\n",
              " 'scheduler': <torch.optim.lr_scheduler.LambdaLR at 0x7fc5f14cb0d0>}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "hparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rayM8tB9d888",
        "outputId": "f510b233-cdf8-4ae3-f0e9-4b0649f9fa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 3, 'fator_corte_loss_maximo': 1, 'batch_size': 24, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-23\n",
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 24, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 1352, 'early_stop': 13520, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc580ca8050>}\n",
            "Momento: [2023-Mar-28 13:17:12] Métricas iniciais em validação: {'valid/perplexidade': 22.45040379007433} Serão treinadas 12986943 amostras\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ida para os países da época. Essa história também será muito maiúscula. Pessoas estão dando suporte ao time, já que temos muito de triste em ficar juntos. Foi a cem vezes que o Brasil tem crescimento bem acima de 20,2%,\n",
            "Step: 1352 Amostras:32448 de um total de 12986943.000 (0.250%)\n",
            "Momento: [2023-Mar-28 13:25:03] lr: 9.98650e-05 Train loss: 2.9024 perplexidade: 19.7736 Validação perplexidade: 21.3129  novo best valid 21.31286169139667\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal, pois é muito bom. Por ai será muito bom um grande torcedor. Tenho a certeza de que as aventuras são grandes. Não é imparcial imaginar que estejam perigosos! O fim do sonho é fazer as atitudes dos pilotos e dos físico, os pilotos, ou aposentados\n",
            "Step: 2704 Amostras:64896 de um total de 12986943.000 (0.500%)\n",
            "Momento: [2023-Mar-28 13:32:54] lr: 9.97303e-05 Train loss: 2.9021 perplexidade: 19.0152 Validação perplexidade: 20.7228  novo best valid 20.722834155225662\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ética e seguro. Os serviços realizados pelos usuários de serviços dispendiosos na legislação passada são necessários somente mais detalhados do que o \"dobramento do tratamento das crianças\" ou seja, como também pode ser de um lugar pr\n",
            "Step: 4056 Amostras:97344 de um total de 12986943.000 (0.750%)\n",
            "Momento: [2023-Mar-28 13:40:47] lr: 9.95960e-05 Train loss: 2.9670 perplexidade: 18.5515 Validação perplexidade: 20.4471  novo best valid 20.4471404640169\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "All 0 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-23/metadata\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_467182/2716319437.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtreina_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparm_se_gera_rastro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_treina_poucos_dados\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mtreina_grid\u001b[0m \u001b[0;34m= <function treina_grid at 0x7fc591a755f0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mhparam\u001b[0m \u001b[0;34m= {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 24, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 1352, 'early_stop': 13520, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.958284745202347e-05\n    weight_decay: 0.0\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.958284745202347e-05\n    weight_decay: 0.0001\n), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc580ca8050>}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mgridparam\u001b[0m \u001b[0;34m= {'learning_rate': [0.0001], 'num_epochs': [3], 'fator_corte_loss_maximo': [1], 'batch_size': [24], 'decrease_factor_lr': [1e-06], 'weight_decay': [0.0001]}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel\u001b[0m \u001b[0;34m= OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_se_gera_rastro\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mse_treina_poucos_dados\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_467182/2743148588.py\u001b[0m in \u001b[0;36mtreina_grid\u001b[0;34m(hparam={'amsgrad': False, 'batch_size': 24, 'criterion': CrossEntropyLoss(), 'decrease_factor_lr': 1e-06, 'device': device(type='cuda', index=0), 'drop_last': True, 'early_stop': 13520, 'eval_every_steps': 1352, 'fator_corte_loss_maximo': 1, 'learning_rate': 0.0001, ...}, gridparam={'batch_size': [24], 'decrease_factor_lr': [1e-06], 'fator_corte_loss_maximo': [1], 'learning_rate': [0.0001], 'num_epochs': [3], 'weight_decay': [0.0001]}, model=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), parm_se_apenas_uma_validacao=False, parm_se_gera_rastro=True, se_treina_poucos_dados=False)\u001b[0m\n\u001b[1;32m     16\u001b[0m                           \u001b[0mparm_loader_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                           \u001b[0mparm_se_apenas_uma_validacao\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparm_se_apenas_uma_validacao\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                           parm_se_gera_rastro=parm_se_gera_rastro, parm_verbose=True, parm_intervalo_print=1)\n\u001b[0m        \u001b[0;36mparm_se_gera_rastro\u001b[0m \u001b[0;34m= True\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_verbose\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_intervalo_print\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mqtd_experimento\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_467182/2739222982.py\u001b[0m in \u001b[0;36mtreina_modelo\u001b[0;34m(parm_model=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), parm_loader_train=<torch.utils.data.dataloader.DataLoader object>, parm_loader_valid=<torch.utils.data.dataloader.DataLoader object>, parm_loader_test=<torch.utils.data.dataloader.DataLoader object>, hparam={'amsgrad': False, 'batch_size': 24, 'criterion': CrossEntropyLoss(), 'decrease_factor_lr': 1e-06, 'device': device(type='cuda', index=0), 'drop_last': True, 'early_stop': 13520, 'eval_every_steps': 1352, 'fator_corte_loss_maximo': 1, 'learning_rate': 0.0001, ...}, parm_se_apenas_uma_validacao=False, parm_se_gera_rastro=True, parm_verbose=True, parm_intervalo_print=1)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mfator_corte_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fator_corte_loss_maximo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_examples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'criterion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfator_corte_loss\u001b[0m   \u001b[0;31m# ajustando para diminuir a redução na loss quando perto do fim do treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mloss.backward\u001b[0m \u001b[0;34m= <bound method Tensor.backward of tensor(2.9874e+00, device='cuda:0', grad_fn=<MulBackward0>)>\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scheduler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# DÚVIDA: melhor fazer por step treino ou por step validação?  Esse último não impactou mudança.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self=tensor(2.9874e+00, device='cuda:0', grad_fn=<MulBackward0>), gradient=None, retain_graph=None, create_graph=False, inputs=None)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mtorch.autograd.backward\u001b[0m \u001b[0;34m= <function backward at 0x7fc60f670b90>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself\u001b[0m \u001b[0;34m= tensor(2.9874e+00, device='cuda:0', grad_fn=<MulBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mgradient\u001b[0m \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mretain_graph\u001b[0m \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mcreate_graph\u001b[0m \u001b[0;34m= False\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minputs\u001b[0m \u001b[0;34m= None\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors=(tensor(2.9874e+00, device='cuda:0', grad_fn=<MulBackward0>),), grad_tensors=None, retain_graph=False, create_graph=False, grad_variables=None, inputs=())\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mallow_unreachable\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36maccumulate_grad\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "treina_grid(hparam, gridparam, model, parm_se_gera_rastro = True, se_treina_poucos_dados=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### batch_size 72\n",
        "\n",
        "Aumentando de novo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_modelo = f'{prefixo_nome_diretorio}best_model_len_{hparam[\"max_seq_length\"]}.pt'\n",
        "torch.save(model, path_modelo) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[3],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[72],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 45.71\n",
            " used 20.51\n",
            " free 2.07\n",
            " cached 44.27\n",
            " buffers 0.5\n",
            "/nGPU\n",
            "Tue Mar 28 13:45:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 67%   52C    P8    39W / 370W |  11043MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "|    0   N/A  N/A    467182      C   .../relevar-busca/bin/python    10979MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu', 'gpu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 3, 'fator_corte_loss_maximo': 1, 'batch_size': 72, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-24\n",
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 72, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 450, 'early_stop': 4500, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc580c82190>}\n",
            "Momento: [2023-Mar-28 13:45:45] Métricas iniciais em validação: {'valid/perplexidade': 20.28890751644474} Serão treinadas 12986943 amostras\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ixo e pode atrair vários atletas para participarem da Copa Nordeste de 2016, além de conquistar a fé nas modalidades de esportenismo, lazer, alimentação esportiva, trabalho, cultura, lazer, gênero, futebol, esporte, futebol, além de ví\n",
            "Step: 450 Amostras:32400 de um total de 12986943.000 (0.249%)\n",
            "Momento: [2023-Mar-28 13:49:51] lr: 9.99550e-05 Train loss: 2.6441 perplexidade: 16.9585 Validação perplexidade: 19.7745  novo best valid 19.774537580603642\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ida e sai a um momento difícil. O principal foco da nova modalidade é o esporte e a equipe do clube que vem atuando na seleção europeia. É uma grande modalidade de esportes e esportes. É importante lembrar que além desta competição também é importante\n",
            "Step: 900 Amostras:64800 de um total de 12986943.000 (0.499%)\n",
            "Momento: [2023-Mar-28 13:53:59] lr: 9.99101e-05 Train loss: 2.8919 perplexidade: 16.2543 Validação perplexidade: 19.5582  novo best valid 19.55824972633743\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ética porque não tem mais nada. Você não quer estudar e não quer comer o melhor porque quer que você seja. Isso porque além disso, se você está fazendo isso através de jogos que não serve como \"informação\". É interessante também saber que temos\n",
            "Step: 1350 Amostras:97200 de um total de 12986943.000 (0.748%)\n",
            "Momento: [2023-Mar-28 13:58:08] lr: 9.98652e-05 Train loss: 2.7253 perplexidade: 15.7347 Validação perplexidade: 19.4156  novo best valid 19.41558365779286\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ia aparelhar este universo. Ainda existe especialização em esportes e jogos mais simples. Muita gente se empolga com a possibilidade de fazer esquemas, mas não vai fazer o que lhes tira do crescimento\", revela Carlos Nascimento, professor de física eletr\n",
            "Step: 1800 Amostras:129600 de um total de 12986943.000 (0.998%)\n",
            "Momento: [2023-Mar-28 14:02:17] lr: 9.98203e-05 Train loss: 2.7963 perplexidade: 17.1285 Validação perplexidade: 18.8839  novo best valid 18.883947687810426\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal e seria uma verdadeira razão para evitar que o pessoal com muito dinheiro fiquem na casa da família. A seção Pública de Atatiba - www.públicabra.com.br – já faz uma nota no site http://notam.blogspot.com.br/10/\n",
            "Step: 2250 Amostras:162000 de um total de 12986943.000 (1.247%)\n",
            "Momento: [2023-Mar-28 14:06:25] lr: 9.97755e-05 Train loss: 2.7831 perplexidade: 17.0534 Validação perplexidade: 18.6637  novo best valid 18.663684150361423\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ético e social O Procon fez uma série de estudo mais acerca das ações realizadas por profissionais portugueses, com capacidade de mercado e experiente pessoalmente com capacidade de mercado. Isso foi dito pelos empresários locais. O objetivo principal da campanha\n",
            "Step: 2700 Amostras:194400 de um total de 12986943.000 (1.497%)\n",
            "Momento: [2023-Mar-28 14:10:34] lr: 9.97307e-05 Train loss: 2.7764 perplexidade: 16.7948 Validação perplexidade: 18.5775  novo best valid 18.577495481079545\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegítimo, com muita sorte nos campeonatos nacionais, então na primeira fase, e na segunda fase é possível mudar o ciclo de competição. Se quisermos ajudar muito ninguém pode ser ótimo, se é necessário acabar com a Copidinha, mas e\n",
            "Step: 3150 Amostras:226800 de um total de 12986943.000 (1.746%)\n",
            "Momento: [2023-Mar-28 14:14:42] lr: 9.96860e-05 Train loss: 2.8214 perplexidade: 16.6367 Validação perplexidade: 18.3953  novo best valid 18.395274006072565\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é irmão de João Pessoa e o que mais faz do filme, é irmão de Jô (sistema de TV. Como você falou), viu-se a partir das 7:50h00, e está entre os 8 a 8. A série de entrevistas coletadas nos cinemas (Santo Agostinho), viu-se ent\n",
            "Step: 3600 Amostras:259200 de um total de 12986943.000 (1.996%)\n",
            "Momento: [2023-Mar-28 14:18:50] lr: 9.96413e-05 Train loss: 2.8788 perplexidade: 16.4250 Validação perplexidade: 18.2121  novo best valid 18.212133468977363\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal, uma vez que, se o caso dela desejar mudar de fase você seleciona o estilo de cada uma das peças que mais precisarem. Seu interesse seria com muito menos em busca de acessos. Além de ter acesso ao espaço de eventos internacion\n",
            "Step: 4050 Amostras:291600 de um total de 12986943.000 (2.245%)\n",
            "Momento: [2023-Mar-28 14:22:58] lr: 9.95966e-05 Train loss: 2.7724 perplexidade: 16.4934 Validação perplexidade: 18.1923  novo best valid 18.192303616613692\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ético\", disse. Para garantir a presença dessa idéia, ele anunciou o programa \"The New York\", em comunicado na manhã desta quarta-feira, dia 31 no Campus de São Paulo, em Curatiba(BA). Depois disso, a professora do escolas Tereza Cássia Ribeiro e Adeliana Mar\n",
            "Step: 4500 Amostras:324000 de um total de 12986943.000 (2.495%)\n",
            "Momento: [2023-Mar-28 14:27:05] lr: 9.95520e-05 Train loss: 2.7221 perplexidade: 16.2816 Validação perplexidade: 18.0305  novo best valid 18.030513336008337\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é irmã de uma fic no meu avô, a minha tia mora comigo, minha filha namora em Paris quando saiu de casa novamente no hospital da casa que eu ia dormir aqui naquele momento. Fico ansioso por me olhar com todo esse ótimos olhinhos, o arquiteto em\n",
            "Step: 4950 Amostras:356400 de um total de 12986943.000 (2.744%)\n",
            "Momento: [2023-Mar-28 14:31:13] lr: 9.95074e-05 Train loss: 2.7681 perplexidade: 16.1829 Validação perplexidade: 17.9402  novo best valid 17.9402160252834\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegível. Mas, após vários anos fora: é hora de usar esportes e jogos como cintura da competição. Por ser um cara de toda cidade do interior todo, atua como uma organização nacional e interna e, no fundo, não é algo de inusitado.\n",
            "Step: 5400 Amostras:388800 de um total de 12986943.000 (2.994%)\n",
            "Momento: [2023-Mar-28 14:35:22] lr: 9.94629e-05 Train loss: 2.7583 perplexidade: 16.0612 Validação perplexidade: 17.9098  novo best valid 17.909820102648155\n",
            "Testando modelo com perplexidade menor. Frase inicio: Praticar esportes é \n",
            "Frase final gerada: Praticar esportes é ilegal e é uma grande pena comprar roupas, pois ainda há como preguiça seus fãs de jogos. Então se você puder aproveitar mais e mais ainda temos de comprar roupas, o importante é conseguir alguém, porque isso aconteceu quando a gente vai\n",
            "Step: 5850 Amostras:421200 de um total de 12986943.000 (3.243%)\n",
            "Momento: [2023-Mar-28 14:39:30] lr: 9.94184e-05 Train loss: 2.7463 perplexidade: 15.9721 Validação perplexidade: 17.8202  novo best valid 17.820178210848276\n",
            "Step: 6300 Amostras:453600 de um total de 12986943.000 (3.493%)\n",
            "Momento: [2023-Mar-28 14:43:38] lr: 9.93739e-05 Train loss: 2.9033 perplexidade: 15.8414 Validação perplexidade: 17.8652 \n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "All 0 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-24/metadata\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_467182/1512230818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreina_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparm_se_gera_rastro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_treina_poucos_dados\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mresultado\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mtreina_grid\u001b[0m \u001b[0;34m= <function treina_grid at 0x7fc591a755f0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mhparam\u001b[0m \u001b[0;34m= {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'batch_size': 72, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 450, 'early_stop': 4500, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.937009298059599e-05\n    weight_decay: 0.0\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 9.937009298059599e-05\n    weight_decay: 0.0001\n), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fc580c82190>}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mgridparam\u001b[0m \u001b[0;34m= {'learning_rate': [0.0001], 'num_epochs': [3], 'fator_corte_loss_maximo': [1], 'batch_size': [72], 'decrease_factor_lr': [1e-06], 'weight_decay': [0.0001]}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel\u001b[0m \u001b[0;34m= OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_se_gera_rastro\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mse_treina_poucos_dados\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_467182/2743148588.py\u001b[0m in \u001b[0;36mtreina_grid\u001b[0;34m(hparam={'amsgrad': False, 'batch_size': 72, 'criterion': CrossEntropyLoss(), 'decrease_factor_lr': 1e-06, 'device': device(type='cuda', index=0), 'drop_last': True, 'early_stop': 4500, 'eval_every_steps': 450, 'fator_corte_loss_maximo': 1, 'learning_rate': 0.0001, ...}, gridparam={'batch_size': [72], 'decrease_factor_lr': [1e-06], 'fator_corte_loss_maximo': [1], 'learning_rate': [0.0001], 'num_epochs': [3], 'weight_decay': [0.0001]}, model=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), parm_se_apenas_uma_validacao=False, parm_se_gera_rastro=True, se_treina_poucos_dados=False)\u001b[0m\n\u001b[1;32m     16\u001b[0m                           \u001b[0mparm_loader_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                           \u001b[0mparm_se_apenas_uma_validacao\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparm_se_apenas_uma_validacao\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                           parm_se_gera_rastro=parm_se_gera_rastro, parm_verbose=True, parm_intervalo_print=1)\n\u001b[0m        \u001b[0;36mparm_se_gera_rastro\u001b[0m \u001b[0;34m= True\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_verbose\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mparm_intervalo_print\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mqtd_experimento\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_467182/2739222982.py\u001b[0m in \u001b[0;36mtreina_modelo\u001b[0;34m(parm_model=OPTForCausalLM(\n  (model): OPTModel(\n    (decode...n_features=768, out_features=50272, bias=False)\n), parm_loader_train=<torch.utils.data.dataloader.DataLoader object>, parm_loader_valid=<torch.utils.data.dataloader.DataLoader object>, parm_loader_test=<torch.utils.data.dataloader.DataLoader object>, hparam={'amsgrad': False, 'batch_size': 72, 'criterion': CrossEntropyLoss(), 'decrease_factor_lr': 1e-06, 'device': device(type='cuda', index=0), 'drop_last': True, 'early_stop': 4500, 'eval_every_steps': 450, 'fator_corte_loss_maximo': 1, 'learning_rate': 0.0001, ...}, parm_se_apenas_uma_validacao=False, parm_se_gera_rastro=True, parm_verbose=True, parm_intervalo_print=1)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# para torch.Size([2*seq_length])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mloss\u001b[0m \u001b[0;34m= tensor(2.7155e+00, device='cuda:0', grad_fn=<MulBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mnn.functional.cross_entropy\u001b[0m \u001b[0;34m= <function cross_entropy at 0x7fc60f5a2050>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mlogits\u001b[0m \u001b[0;34m= tensor([[-4.8830e-01, -4.8830e-01, 3.2341e+00,  ..., -4.8830e-01, -4.8830e-01, -4.8830e-01],\n        [-9.4175e-01, -9.4175e-01, -2.5085e-01,  ..., -9.4175e-01, -9.4175e-01, -9.4175e-01],\n        [-9.8443e-01, -9.8443e-01, -5.0042e-01,  ..., -9.8443e-01, -9.8443e-01, -9.8443e-01],\n        ...,\n        [-1.1544e+00, -1.1544e+00, -1.5538e+00,  ..., -1.1544e+00, -1.1544e+00, -1.1544e+00],\n        [-1.0603e+00, -1.0603e+00, -2.7908e+00,  ..., -1.0603e+00, -1.0603e+00, -1.0603e+00],\n        [-9.4345e-01, -9.4345e-01, -2.0049e+00,  ..., -9.4345e-01, -9.4345e-01, -9.4345e-01]],\n       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mtarget\u001b[0m \u001b[0;34m= tensor([  991,   366,   475,  ...,  3137,  1021, 33532], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mignore_index\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mparm_model.config.pad_token_id\u001b[0m \u001b[0;34m= 1\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# ipdb.set_trace(context=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mhparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input=tensor([[-4.8830e-01, -4.8830e-01, 3.2341e+00,  ...device='cuda:0', grad_fn=<ReshapeAliasBackward0>), target=tensor([  991,   366,   475,  ...,  3137,  1021, 33532], device='cuda:0'), weight=None, size_average=None, ignore_index=1, reduce=None, reduction='mean', label_smoothing=0.0)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mtorch._C._nn.cross_entropy_loss\u001b[0m \u001b[0;34m= <built-in function cross_entropy_loss>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= tensor([[-4.8830e-01, -4.8830e-01, 3.2341e+00,  ..., -4.8830e-01, -4.8830e-01, -4.8830e-01],\n        [-9.4175e-01, -9.4175e-01, -2.5085e-01,  ..., -9.4175e-01, -9.4175e-01, -9.4175e-01],\n        [-9.8443e-01, -9.8443e-01, -5.0042e-01,  ..., -9.8443e-01, -9.8443e-01, -9.8443e-01],\n        ...,\n        [-1.1544e+00, -1.1544e+00, -1.5538e+00,  ..., -1.1544e+00, -1.1544e+00, -1.1544e+00],\n        [-1.0603e+00, -1.0603e+00, -2.7908e+00,  ..., -1.0603e+00, -1.0603e+00, -1.0603e+00],\n        [-9.4345e-01, -9.4345e-01, -2.0049e+00,  ..., -9.4345e-01, -9.4345e-01, -9.4345e-01]],\n       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mtarget\u001b[0m \u001b[0;34m= tensor([  991,   366,   475,  ...,  3137,  1021, 33532], device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mweight\u001b[0m \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36m_Reduction.get_enum\u001b[0m \u001b[0;34m= <function get_enum at 0x7fc60f7884d0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mreduction\u001b[0m \u001b[0;34m= 'mean'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mignore_index\u001b[0m \u001b[0;34m= 1\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mlabel_smoothing\u001b[0m \u001b[0;34m= 0.0\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/relevar-busca/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f=None, limit=None)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mformat_stack\u001b[0m \u001b[0;34m= <function format_stack at 0x7fc70e31cef0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mf\u001b[0m \u001b[0;34m= None\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mlimit\u001b[0m \u001b[0;34m= None\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "resultado = treina_grid(hparam, gridparam, model, parm_se_gera_rastro = True, se_treina_poucos_dados=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.load_state_dict(resultado['best_model_dict'])\n",
        "model.to(hparam['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_modelo = f'{prefixo_nome_diretorio}best_model_len_{hparam[\"max_seq_length\"]}_final.pt'\n",
        "torch.save(model, path_modelo)    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### batch_size 96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_modelo = f'{prefixo_nome_diretorio}best_model_len_{hparam[\"max_seq_length\"]}_final.pt'\n",
        "model = torch.load(path_modelo) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "gridparam = { \n",
        "               'learning_rate': [ 1e-4],\n",
        "               'num_epochs':[3],\n",
        "               'fator_corte_loss_maximo': [1],\n",
        "               'batch_size':[96],\n",
        "               'decrease_factor_lr': [1e-6],\n",
        "               'weight_decay': [1e-4]\n",
        "             }         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 47.71\n",
            " used 18.46\n",
            " free 0.5\n",
            " cached 47.85\n",
            " buffers 0.55\n",
            "/nGPU\n",
            "Tue Mar 28 17:18:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 57%   43C    P8    26W / 370W |   3293MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1227      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell               13MiB |\n",
            "|    0   N/A  N/A    491803      C   ...vs/treinamento/bin/python     3229MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu', 'gpu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serão 1 experimentações\n",
            "\n",
            "\n",
            "NUM: 1/1 : {'learning_rate': 0.0001, 'num_epochs': 3, 'fator_corte_loss_maximo': 1, 'batch_size': 96, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001} \n",
            "Number of model parameters: 125239296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/treinamento/lib/python3.7/site-packages/ipykernel_launcher.py:40: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/treinamento/lib/python3.7/site-packages/ipykernel_launcher.py:54: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'tuple'>).\n",
            "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
            "        for dictionaries or collections that contain unsupported values.\n",
            "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
            "/home/borela/miniconda3/envs/treinamento/lib/python3.7/site-packages/ipykernel_launcher.py:54: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
            "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
            "        for dictionaries or collections that contain unsupported values.\n",
            "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hparam: {'num_workers_dataloader': 0, 'device': device(type='cuda', index=0), 'vocab_size': 50272, 'num_sentenca_train': 249800, 'num_sentenca_valid': 100, 'num_sentenca_test': 100, 'max_seq_length': 100, 'train_size': 4328981, 'valid_size': 2036, 'test_size': 1143, 'batch_size': 96, 'num_epochs': 3, 'learning_rate': 0.0001, 'fator_corte_loss_maximo': 1, 'decrease_factor_lr': 1e-06, 'weight_decay': 0.0001, 'drop_last': True, 'max_examples': 12986943, 'percentual_eval_every_steps': 0.0025, 'eval_every_steps': 338, 'early_stop': 3380, 'criterion': CrossEntropyLoss(), 'num_params': 125239296, 'amsgrad': False, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    weight_decay: 0.0001\n",
            "), 'scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7fe94405d910>}\n",
            "Momento: [2023-Mar-28 17:19:29] Métricas iniciais em validação: {'valid/perplexidade': 17.79368390030511} Serão treinadas 12986943 amostras\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal. Não sei o que fazer, mas acho muito difícil deixarmos para trás e nós temos um problema com isso\", disse ele em entrevista à revistas \"O Brasil está sempre na mesma posse\". O presidente da Câmara dos Deputados (PT-SP), José Serra Filho afirmou nesta ter sido púrbional no últimpeculpa do governadoras', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 338 Amostras:32448 de um total de 12986943 (0.250%)\n",
            "Momento: [2023-Mar-28 17:23:00] lr: 9.99662e-05 Train loss: 2.6758 perplexidade: 15.7022 Validação perplexidade: 17.9691  novo best valid 17.96911780045298\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não se pode dizer que o futebol brasileiro tem um problema de saúde. O jogo entre os dois times está em uma situaçõe muito grandiosamente complicada e ainda assim com as duas equipes enfrentando algum tipo de erro na partida contra o Flamengo no sistema (com), por exemplos inériciondo-maltino da Copac', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 676 Amostras:64896 de um total de 12986943 (0.500%)\n",
            "Momento: [2023-Mar-28 17:26:31] lr: 9.99324e-05 Train loss: 2.6551 perplexidade: 15.4249 Validação perplexidade: 17.7739  novo best valid 17.77387671786218\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como fazer isso. O que aconteceu? Acho muita coisa de errado e ainda estou falando sobre o assunto: \"A gente vai ter um time grande para se tornarem campeões\". Eles também só querem ser jogados no clube porque precisamos do resultado em campanalmente os outros importantíssimo\", afirmosament', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 1014 Amostras:97344 de um total de 12986943 (0.750%)\n",
            "Momento: [2023-Mar-28 17:30:02] lr: 9.98987e-05 Train loss: 2.7334 perplexidade: 15.1743 Validação perplexidade: 17.7438  novo best valid 17.74382048870334\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética, e não pode ser uma tarefa fácil. Mas acho que o mundo esta muitas vezes em altos índices de competições porque tem um potencial para se tornarmos cada dia melhores do ano passado\", disse ele no Twitter neste domingo (28/06), durante reunião com os atletrâneais da Cidade Sístelaoncieneuetto', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 1352 Amostras:129792 de um total de 12986943 (0.999%)\n",
            "Momento: [2023-Mar-28 17:33:32] lr: 9.98650e-05 Train loss: 2.7839 perplexidade: 15.5394 Validação perplexidade: 17.5174  novo best valid 17.517379866335954\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não temos condições de fazer isso. O que acham? Acho muito importante para o Brasil e outros países do mundo inteiro! Eu sou um pouco inteligente porque está meio desempregado com essa questão da vida adultinha... Mas se você tiver alguma dificança na carreia em suas no correta (com) vou ter u', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 1690 Amostras:162240 de um total de 12986943 (1.249%)\n",
            "Momento: [2023-Mar-28 17:37:01] lr: 9.98313e-05 Train loss: 2.7280 perplexidade: 15.5044 Validação perplexidade: 17.4858  novo best valid 17.485797473867176\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal. Acho que o Brasil não tem acesso às áreas de atuações do país, mas sim da sociedade e dos Estados Unidos\", disse ele em entrevista coletiva com os dirigentes dessa categoria no Rio Grande do Sul (RJ). \"A gente vê isso porque estamos muito bem preparadas para fazulando as pessiais\". O governador desta respe', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 2028 Amostras:194688 de um total de 12986943 (1.499%)\n",
            "Momento: [2023-Mar-28 17:40:33] lr: 9.97976e-05 Train loss: 2.7202 perplexidade: 15.3621 Validação perplexidade: 17.4511  novo best valid 17.451060095389234\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como segurá-lo. O que ele fez? Nós fazemos isso porque o Brasil estava em um momento de crise econômica\", disse a presidente da Federação das Indústrias do Estado (Fiesp), Mariana Lopes Filho, durante reuniões na manhã desta quinta no Palco para discuturnada sobre os problematletivament', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 2366 Amostras:227136 de um total de 12986943 (1.749%)\n",
            "Momento: [2023-Mar-28 17:44:05] lr: 9.97640e-05 Train loss: 2.7372 perplexidade: 15.2863 Validação perplexidade: 17.3876  novo best valid 17.387632701782017\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não sei o que fazer. É uma questão de muita diferença entre os jovens e adultos: a) O pior desempenho do país no Brasil foge da crise financeira; b) A inflação está em altas para as mulheres (e também na maioria dos casais). c) As taxatividades sóides podiam ser baixo-direnais por', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 2704 Amostras:259584 de um total de 12986943 (1.999%)\n",
            "Momento: [2023-Mar-28 17:47:37] lr: 9.97303e-05 Train loss: 2.6841 perplexidade: 15.1352 Validação perplexidade: 17.3371  novo best valid 17.337093460662587\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não pode ser considerado um problema. O que aconteceu com o Brasil foi a falta de recursos para os estados e municípios em geral\", afirmou o presidente da Associações Nacional dos Trabalhadores (ANT), Joaquim Barbosa Filho, no último dia 17/11. A Secretaria do Ministema tambientificando Púbempre', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 3042 Amostras:292032 de um total de 12986943 (2.249%)\n",
            "Momento: [2023-Mar-28 17:51:09] lr: 9.96967e-05 Train loss: 2.6728 perplexidade: 15.2563 Validação perplexidade: 17.2418  novo best valid 17.24176043747499\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como ser um futebol. O que vocês acham? É uma coisa de gente muita responsabilidade e até mesmo pessoais para se sentir bem em suas vidas diárias ou porque estamos falando do nosso time\", disse ele na entrevista à revistas \"O Brasil preciso ter certecnologia\" (comentrevolucenficiencia) daqu', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 3380 Amostras:324480 de um total de 12986943 (2.499%)\n",
            "Momento: [2023-Mar-28 17:54:42] lr: 9.96631e-05 Train loss: 2.8023 perplexidade: 15.1122 Validação perplexidade: 17.1643  novo best valid 17.164274086886667\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como se preocupações. O que aconteceu foi a falta de um ônibus para o trecho da BR-101 e do prédio onde estava sendo construído uma avenida no bairro Jardim Botânicos em Sampa (SP). A cidade também teve muitas difantes na pra quando as ruidas pessenciais rá', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 3718 Amostras:356928 de um total de 12986943 (2.748%)\n",
            "Momento: [2023-Mar-28 17:58:14] lr: 9.96296e-05 Train loss: 2.6082 perplexidade: 15.0332 Validação perplexidade: 17.1523  novo best valid 17.152266467384024\n",
            "Step: 4056 Amostras:389376 de um total de 12986943 (2.998%)\n",
            "Momento: [2023-Mar-28 18:01:46] lr: 9.95960e-05 Train loss: 2.7223 perplexidade: 14.9764 Validação perplexidade: 17.1879 \n",
            "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: HTTPServiceUnavailable\n",
            "Communication with Neptune restored!\n",
            " novo best valid 17.02572269995152; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não temos muito a ver com o que vamos fazer. O Brasil precisa de um bom tempo para se prepararmos e conseguimos melhorias no desempenho do nosso time\", disse ele em entrevista à TV Globo.\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 4394 Amostras:421824 de um total de 12986943 (3.248%)\n",
            "Momento: [2023-Mar-28 18:05:19] lr: 9.95625e-05 Train loss: 2.6061 perplexidade: 14.9036 Validação perplexidade: 17.0257  novo best valid 17.02572269995152; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 4732 Amostras:454272 de um total de 12986943 (3.498%)\n",
            "Momento: [2023-Mar-28 18:08:50] lr: 9.95290e-05 Train loss: 2.6409 perplexidade: 14.8229 Validação perplexidade: 17.0785 \n",
            " novo best valid 17.022793141008364; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não há como se fazer isso. O que vocês acham? Nós temos uma das maiores empresas do mundo e estamos falando de um projeto para o Brasil\", disse ele na entrevista coletiva \"O presidente da Fifa\" José Sarney (PMDB-AL), afirmou nesta quinta-feira novembrada terça feita: \\'Ainda valegria\\'', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 5070 Amostras:486720 de um total de 12986943 (3.748%)\n",
            "Momento: [2023-Mar-28 18:12:29] lr: 9.94956e-05 Train loss: 2.7777 perplexidade: 15.2602 Validação perplexidade: 17.0228  novo best valid 17.022793141008364; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não temos acesso às informações. O que fazer? A gente vai ter de pagar o prejuízo do dinheiro e da diferença entre os jogadores\", disse ele em nota divulgada neste domingo (27). \"Acho muitas vezes um jogo comum para as equipes até porque serem bem no finalizadas na competi-de fortecnologia.\"', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 5408 Amostras:519168 de um total de 12986943 (3.998%)\n",
            "Momento: [2023-Mar-28 18:16:01] lr: 9.94621e-05 Train loss: 2.7740 perplexidade: 15.3278 Validação perplexidade: 16.8628  novo best valid 16.86276425128044\n",
            "Step: 5746 Amostras:551616 de um total de 12986943 (4.247%)\n",
            "Momento: [2023-Mar-28 18:19:33] lr: 9.94287e-05 Train loss: 2.7372 perplexidade: 15.4142 Validação perplexidade: 16.8651 \n",
            " novo best valid 16.820747067547384; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ético e deve ser um bom exemplo para você. Você pode usá-lo com o tempo todo, mas não precisa ficar muitas vezes sempre atento aos seus desejos!\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 6084 Amostras:584064 de um total de 12986943 (4.497%)\n",
            "Momento: [2023-Mar-28 18:23:11] lr: 9.93953e-05 Train loss: 2.7757 perplexidade: 15.3496 Validação perplexidade: 16.8207  novo best valid 16.820747067547384; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ético e não tem condições de se desenvolver. O que você precisa? Acho muito fácil, mas ainda assim ocorreu comigo mesmo! Este blog estava meio sem graça para ser um dos melhores do mundo (e quem sabe) daquela manha vai ter uma ideia diferente outra... Mas tambulouroso… Nóparezavela g', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 6422 Amostras:616512 de um total de 12986943 (4.747%)\n",
            "Momento: [2023-Mar-28 18:26:45] lr: 9.93619e-05 Train loss: 2.7348 perplexidade: 15.2824 Validação perplexidade: 16.7528  novo best valid 16.752801234768434\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética e não deve ser tudo o que faz. É uma boa opção para quem quer se sentir bem, mas ao mesmo tempo pode ter um bom desenvolvimento em suas atividades com muitos outros objetivos: manter os jogadores sempre próximos às necessidade do adversário; garanteira saúdeixando-se da vontensinar no longetamb', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 6760 Amostras:648960 de um total de 12986943 (4.997%)\n",
            "Momento: [2023-Mar-28 18:30:18] lr: 9.93285e-05 Train loss: 2.7415 perplexidade: 15.3634 Validação perplexidade: 16.6717  novo best valid 16.671725828535966\n",
            "Step: 7098 Amostras:681408 de um total de 12986943 (5.247%)\n",
            "Momento: [2023-Mar-28 18:33:50] lr: 9.92952e-05 Train loss: 2.6921 perplexidade: 15.2697 Validação perplexidade: 16.7070 \n",
            "Step: 7436 Amostras:713856 de um total de 12986943 (5.497%)\n",
            "Momento: [2023-Mar-28 18:37:21] lr: 9.92619e-05 Train loss: 2.7203 perplexidade: 15.2720 Validação perplexidade: 16.7557 \n",
            " novo best valid 16.635394089730305; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não pode ser difícil. Acho que ainda há muito tempo eu fui até o Rio de Janeiro para ver se vai melhor do mundo\", disse ele em entrevista à Folha nesta terça-feira (19/11). \"O Brasil estava na primeiro lugar no ranking da categoria profissional com uma medalha olhada porque foirosas nas áróticos brasiliense.\"', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 7774 Amostras:746304 de um total de 12986943 (5.747%)\n",
            "Momento: [2023-Mar-28 18:41:00] lr: 9.92286e-05 Train loss: 2.7380 perplexidade: 15.1840 Validação perplexidade: 16.6354  novo best valid 16.635394089730305; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética, e não deixa a gente se sentir mais feliz. Acho que o futebol tem um papel importante na vida do povo brasileiro\", disse ele em entrevista à revistas Veja nestas terça-feira (27), no Rio Grande do Sul como uma dessas melhores jogadoras da história dos Jogos Olímpicenses. O atletismo foiense tevemuitado porquebr', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 8112 Amostras:778752 de um total de 12986943 (5.996%)\n",
            "Momento: [2023-Mar-28 18:44:33] lr: 9.91953e-05 Train loss: 2.8342 perplexidade: 15.2158 Validação perplexidade: 16.5651  novo best valid 16.56505726472982\n",
            "Step: 8450 Amostras:811200 de um total de 12986943 (6.246%)\n",
            "Momento: [2023-Mar-28 18:48:04] lr: 9.91621e-05 Train loss: 2.7877 perplexidade: 15.1706 Validação perplexidade: 16.6019 \n",
            "Step: 8788 Amostras:843648 de um total de 12986943 (6.496%)\n",
            "Momento: [2023-Mar-28 18:51:36] lr: 9.91289e-05 Train loss: 2.7446 perplexidade: 15.2327 Validação perplexidade: 16.5875 \n",
            "Step: 9126 Amostras:876096 de um total de 12986943 (6.746%)\n",
            "Momento: [2023-Mar-28 18:55:08] lr: 9.90957e-05 Train loss: 2.6886 perplexidade: 15.1663 Validação perplexidade: 16.5826 \n",
            "Step: 9464 Amostras:908544 de um total de 12986943 (6.996%)\n",
            "Momento: [2023-Mar-28 18:58:41] lr: 9.90625e-05 Train loss: 2.7144 perplexidade: 15.0066 Validação perplexidade: 16.5766 \n",
            " novo best valid 16.535084075675158; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, porque não há como se preocuparem em fazer isso. Acho que o Brasil tem um país de muita gente e ainda estamos falando da Copinha do Mundial\", disse ele no Twitter.\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 9802 Amostras:940992 de um total de 12986943 (7.246%)\n",
            "Momento: [2023-Mar-28 19:02:21] lr: 9.90293e-05 Train loss: 2.7459 perplexidade: 15.1090 Validação perplexidade: 16.5351  novo best valid 16.535084075675158; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é irmão de um amigo que namora com o marido, mas ele tem uma vida mais saudável e feliz. Ele também pode ser bebês em casa (como se fosse no Brasil), por exemplo: \"Eu gostaria do meus filhotes\", dizer a atriz sobre suas experiências na áródicas da modalidadeira. Ainda lembalves disse ter', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 10140 Amostras:973440 de um total de 12986943 (7.496%)\n",
            "Momento: [2023-Mar-28 19:05:54] lr: 9.89962e-05 Train loss: 2.7963 perplexidade: 15.0792 Validação perplexidade: 16.5029  novo best valid 16.502886083684455\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegalidade, pois o que mais me incomodou foi a falta de respeito e prazer. Acho um grande problema para mim: não se trata apenas do corpo humano comum (com uma criança em casamentos), mas também da vida adulta\", afirmava ele no site oficial \"O Brasil dos Jogu\". O prólogrosobrementário na águia!', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 10478 Amostras:1005888 de um total de 12986943 (7.745%)\n",
            "Momento: [2023-Mar-28 19:09:26] lr: 9.89631e-05 Train loss: 2.7519 perplexidade: 15.0455 Validação perplexidade: 16.4996  novo best valid 16.499552881866904\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser um bom futebol. O que você quer? Acho muito importante ouvir a torcida do Palmeira, mas também deixou claramente as dificuldades em jogos com os times da casa: \"A gente tem uma chance para se defender\". Eu tenho certeza disso\", disse ele no Twitter.\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 10816 Amostras:1038336 de um total de 12986943 (7.995%)\n",
            "Momento: [2023-Mar-28 19:12:59] lr: 9.89300e-05 Train loss: 2.7115 perplexidade: 14.9278 Validação perplexidade: 16.3995  novo best valid 16.39950191598977\n",
            "Step: 11154 Amostras:1070784 de um total de 12986943 (8.245%)\n",
            "Momento: [2023-Mar-28 19:16:31] lr: 9.88969e-05 Train loss: 2.7155 perplexidade: 14.9347 Validação perplexidade: 16.4257 \n",
            " novo best valid 16.38823043807812; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não pode ser considerado crime. Ainda assim o governador do Estados Unidos e a presidente Dilma Rousseff só se manifestaram em relações às atividades de futebol no Brasil como \"todas as competições\". O que mais me chamou foi um jogo entre os clubes brasileirantes: campeachment (jografista) contra jogadores da Copada', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 11492 Amostras:1103232 de um total de 12986943 (8.495%)\n",
            "Momento: [2023-Mar-28 19:20:10] lr: 9.88639e-05 Train loss: 2.6749 perplexidade: 14.9737 Validação perplexidade: 16.3882  novo best valid 16.38823043807812; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e desrespeitoso. O que acontece com os brasilienses? A polícia de São Paulo, por exemplo, está sendo investigada pela Policia Federal em Goiás (PR), onde foge do local para apuramentos da Operações Lava Jato\", disse o delegado Marcelo Dias dos Santos Silva Pinto Pereira Lima, diretorreiro Ribeiras Araús Barros; Delegratais Roch', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 11830 Amostras:1135680 de um total de 12986943 (8.745%)\n",
            "Momento: [2023-Mar-28 19:23:42] lr: 9.88308e-05 Train loss: 2.6944 perplexidade: 14.8933 Validação perplexidade: 16.3585  novo best valid 16.35853207278755\n",
            "Step: 12168 Amostras:1168128 de um total de 12986943 (8.995%)\n",
            "Momento: [2023-Mar-28 19:27:13] lr: 9.87978e-05 Train loss: 2.6955 perplexidade: 14.8480 Validação perplexidade: 16.3815 \n",
            " novo best valid 16.33325914704408; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não sei o que fazer. O Brasil está muito bem preparado para a Copa do Mundo de 2014 e tem um grande potencial\", disse ele em entrevista à Folha nesta terça-feira (11) na rede social da CBF: \"A gente vai começando uma campanha contra os clubistas\". Ainda assimos? Nó! Eu jama dizinho comentregozemba', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 12506 Amostras:1200576 de um total de 12986943 (9.244%)\n",
            "Momento: [2023-Mar-28 19:30:51] lr: 9.87648e-05 Train loss: 2.6900 perplexidade: 14.8721 Validação perplexidade: 16.3333  novo best valid 16.33325914704408; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e desrespeitar a lei de incentivo à criança, porque o pai não tem conduta para fazer isso. O que aconteceu com os pais? A maioria dos adolescente está em casa próxima da escola onde moramos juntas\", explica Ana Maria Lúcia Pinto Filho (Foto: Reproduçõ/TV)\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 12844 Amostras:1233024 de um total de 12986943 (9.494%)\n",
            "Momento: [2023-Mar-28 19:34:22] lr: 9.87319e-05 Train loss: 2.7195 perplexidade: 14.8379 Validação perplexidade: 16.2765  novo best valid 16.276456977095553\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não temos como fazer isso. Nós estamos em um momento de crise econômica\", disse o presidente da Associação Brasil-Brasília (ABRAS), José Carlos Pimentel Filho Netto, que também foi diretor do Centro Cultural Sesc Rio Branco. \"Estou muito felizinformado porque ainda! Estávaiêncipecujo', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 13182 Amostras:1265472 de um total de 12986943 (9.744%)\n",
            "Momento: [2023-Mar-28 19:37:55] lr: 9.86990e-05 Train loss: 2.7466 perplexidade: 14.9024 Validação perplexidade: 16.2386  novo best valid 16.23858680513752\n",
            "Step: 13520 Amostras:1297920 de um total de 12986943 (9.994%)\n",
            "Momento: [2023-Mar-28 19:41:27] lr: 9.86660e-05 Train loss: 2.6287 perplexidade: 14.8203 Validação perplexidade: 16.3212 \n",
            "Step: 13858 Amostras:1330368 de um total de 12986943 (10.244%)\n",
            "Momento: [2023-Mar-28 19:45:00] lr: 9.86331e-05 Train loss: 2.6855 perplexidade: 14.7902 Validação perplexidade: 16.2748 \n",
            " novo best valid 16.225228159073275; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como ser o único que pode fazer isso. Ainda assim a gente vê um grande desafio para os brasilienses\", afirmou. O presidente da Federação dos Jogos Olímpicos do Rio de Janeiro (FJU), Carlos Alberto Pinto Filho, disse nesta sexta-feira à imprencipereiro: \"Olimonvemário estava em Sust', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 14196 Amostras:1362816 de um total de 12986943 (10.494%)\n",
            "Momento: [2023-Mar-28 19:48:39] lr: 9.86003e-05 Train loss: 2.6803 perplexidade: 14.7998 Validação perplexidade: 16.2252  novo best valid 16.225228159073275; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e desrespeito ao meio ambiente. A Polícia Federal, por sua vez, não tem comunicações telefônicas para o Brasil sobre os riscos de acidentais que podem ser cometidos no país em qualquer momento do anúncio da prisão preventiva dos atleticanos na Copa América 2014 (Foto) O Ministocupresepia já-Sant', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 14534 Amostras:1395264 de um total de 12986943 (10.744%)\n",
            "Momento: [2023-Mar-28 19:52:12] lr: 9.85674e-05 Train loss: 2.7048 perplexidade: 14.8030 Validação perplexidade: 16.1787  novo best valid 16.178726621630073\n",
            "Step: 14872 Amostras:1427712 de um total de 12986943 (10.993%)\n",
            "Momento: [2023-Mar-28 19:55:46] lr: 9.85346e-05 Train loss: 2.6887 perplexidade: 14.8508 Validação perplexidade: 16.1801 \n",
            " novo best valid 16.171838199866862; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegalidade. A maioria dos países que se encontram em situação de risco só podem ser considerados como \"muitos\" ou mais, porque ainda nesses casos os árbitros têm um grande potencial para atingir sua meta e fazer uma boa estratégia no combate às do trabalho (comunidoso). O Brasilamento-lário', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 15210 Amostras:1460160 de um total de 12986943 (11.243%)\n",
            "Momento: [2023-Mar-28 19:59:27] lr: 9.85018e-05 Train loss: 2.6837 perplexidade: 14.8083 Validação perplexidade: 16.1718  novo best valid 16.171838199866862; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 15548 Amostras:1492608 de um total de 12986943 (11.493%)\n",
            "Momento: [2023-Mar-28 20:03:00] lr: 9.84690e-05 Train loss: 2.7533 perplexidade: 14.7657 Validação perplexidade: 16.1978 \n",
            " novo best valid 16.148268260045615; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não pode ser um ato de corrupções. O que mais me chamou a atenção foi o fatídico acidente do trabalho com uma criança em casa e na escola está sendo investigado por policiais militares da Policia Civil (PM) no Rio Grande dos Sul\", afirmaram os agentes federatéticos para as autuetaprevolucionados pelas dessa', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 15886 Amostras:1525056 de um total de 12986943 (11.743%)\n",
            "Momento: [2023-Mar-28 20:06:39] lr: 9.84362e-05 Train loss: 2.7553 perplexidade: 14.7537 Validação perplexidade: 16.1483  novo best valid 16.148268260045615; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética e não deve ser um exercício físico. É importante que você seja mais flexivel para ajudá-los, pois o corpo também pode estragar as vidas das crianças como uma forma simples de atingir os objetivos do desenvolvimento social\", explicou ele em entrevistando sobre suplementado na escola no programador da prólogia', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 16224 Amostras:1557504 de um total de 12986943 (11.993%)\n",
            "Momento: [2023-Mar-28 20:10:13] lr: 9.84035e-05 Train loss: 2.6639 perplexidade: 14.6871 Validação perplexidade: 16.1300  novo best valid 16.129995648710345\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética e não deve ser uma boa ideia. O que você acha? Aproximadamente 50% dos jovens estudantes da Escola Superior do Rio Grande, em São Paulo (SP), também participaram das atividades desenvolvidos pelo Instituto Brasileiro para o Desafio Educativo – BEM-Estar Social na Águerjul/CESPúbemprecidadeiado', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 16562 Amostras:1589952 de um total de 12986943 (12.243%)\n",
            "Momento: [2023-Mar-28 20:13:48] lr: 9.83708e-05 Train loss: 2.6865 perplexidade: 14.7792 Validação perplexidade: 16.0775  novo best valid 16.07752158512015\n",
            "Step: 16900 Amostras:1622400 de um total de 12986943 (12.493%)\n",
            "Momento: [2023-Mar-28 20:17:20] lr: 9.83381e-05 Train loss: 2.5925 perplexidade: 14.6968 Validação perplexidade: 16.1339 \n",
            "Step: 17238 Amostras:1654848 de um total de 12986943 (12.742%)\n",
            "Momento: [2023-Mar-28 20:20:52] lr: 9.83054e-05 Train loss: 2.6293 perplexidade: 14.6580 Validação perplexidade: 16.0878 \n",
            "Step: 17576 Amostras:1687296 de um total de 12986943 (12.992%)\n",
            "Momento: [2023-Mar-28 20:24:22] lr: 9.82728e-05 Train loss: 2.6783 perplexidade: 14.6667 Validação perplexidade: 16.0845 \n",
            " novo best valid 16.070962892281095; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ético e não deixa a criança mais feliz. Ainda que você tenha uma vida melhor, elas só podem ser bem-vindos quando estiverem em um lugar comum para todos os gostam? Seja qual for o seu caso!\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 17914 Amostras:1719744 de um total de 12986943 (13.242%)\n",
            "Momento: [2023-Mar-28 20:28:00] lr: 9.82401e-05 Train loss: 2.6796 perplexidade: 14.7082 Validação perplexidade: 16.0710  novo best valid 16.070962892281095; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal. A gente não tem acesso à internet, mas sim de uma forma que ocorreu em outros países do mundo\", disse ele na sexta-feira (19). \"Eles estavam muitas vezes com medos e faltava aprender para conseguir um lugar no Brasil.\" O projeto foi criado pela por meio@gmailhozinho\" [Fotodoutrosobrinform', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 18252 Amostras:1752192 de um total de 12986943 (13.492%)\n",
            "Momento: [2023-Mar-28 20:31:32] lr: 9.82075e-05 Train loss: 2.6485 perplexidade: 14.6262 Validação perplexidade: 16.0504  novo best valid 16.050356181409136\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não deixa a cidade ser mais bonita. Ainda que o tempo seja menor do que os outros e isso aconteça comigo mesmo na minha vida profissional\", afirmou ele em entrevista à Agência Brasil News neste domingo (11) no site da CBF sobre as atividades físicas dos jogadenses: \"A partir para umais\" - O presidenteletos tem', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 18590 Amostras:1784640 de um total de 12986943 (13.742%)\n",
            "Momento: [2023-Mar-28 20:35:03] lr: 9.81749e-05 Train loss: 2.7808 perplexidade: 14.5880 Validação perplexidade: 16.0113  novo best valid 16.011343964826107\n",
            "Step: 18928 Amostras:1817088 de um total de 12986943 (13.992%)\n",
            "Momento: [2023-Mar-28 20:38:34] lr: 9.81424e-05 Train loss: 2.6642 perplexidade: 14.6818 Validação perplexidade: 16.0369 \n",
            " novo best valid 15.956769082944177; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal. O que mais chamou a atenção foi o fato de os jogadores terem sidos vítimas da ditadura militar, eles nada disseram sobre isso\", disse ele em entrevista à BBC Brasil nesta segunda-feira (17) na TV Globo. \"O problema do futebol brasiliense está no momento piorquerozinho\", comentreformado por um dos', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 19266 Amostras:1849536 de um total de 12986943 (14.242%)\n",
            "Momento: [2023-Mar-28 20:42:09] lr: 9.81098e-05 Train loss: 2.6701 perplexidade: 14.5776 Validação perplexidade: 15.9568  novo best valid 15.956769082944177; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não se pode ficá-lo com a bola. Se você for um jogador de basquete ou uma das maiores torcidas do mundo que joga no Brasil e tem alguma chance para ganhar na Copa Libertadores da América? Este anúncio foi feito por mim mesmo em 2009 pelos meus campereis@gmailia | Obracreditelisa - 15/11', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 19604 Amostras:1881984 de um total de 12986943 (14.491%)\n",
            "Momento: [2023-Mar-28 20:45:40] lr: 9.80773e-05 Train loss: 2.6664 perplexidade: 14.6675 Validação perplexidade: 15.9541  novo best valid 15.95406837563042\n",
            "Step: 19942 Amostras:1914432 de um total de 12986943 (14.741%)\n",
            "Momento: [2023-Mar-28 20:49:10] lr: 9.80448e-05 Train loss: 2.7111 perplexidade: 14.5342 Validação perplexidade: 15.9843 \n",
            " novo best valid 15.908214665267257; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não pode ser punido por crime de responsabilidade. O governo do Estado tem que se preocupa com a saúde e o bem-estar dos cidadãos\", afirmou neste domingo (13) um grupo formada pelas mulheres da região Norte para atender as famílias desses municípios em situais próximadas no interiorana: \"A\" – A Policiais', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 20280 Amostras:1946880 de um total de 12986943 (14.991%)\n",
            "Momento: [2023-Mar-28 20:52:47] lr: 9.80123e-05 Train loss: 2.6255 perplexidade: 14.5645 Validação perplexidade: 15.9082  novo best valid 15.908214665267257; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 20618 Amostras:1979328 de um total de 12986943 (15.241%)\n",
            "Momento: [2023-Mar-28 20:56:18] lr: 9.79799e-05 Train loss: 2.5811 perplexidade: 14.5206 Validação perplexidade: 15.9178 \n",
            " novo best valid 15.88615309070078; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegalidade, não importa o que seja. O próprio corpo físico deve ser considerado com base no artigo 5º da Constituiçõe Federal e na Lei Complementares N° 999/90 (Lei 8.838 /91)\". A lei complementaria aos princípios constitucionais do direito à saúde: \"Aplicidadescio\",dia-princidentificadas em at', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 20956 Amostras:2011776 de um total de 12986943 (15.491%)\n",
            "Momento: [2023-Mar-28 20:59:53] lr: 9.79474e-05 Train loss: 2.6189 perplexidade: 14.5375 Validação perplexidade: 15.8862  novo best valid 15.88615309070078; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 21294 Amostras:2044224 de um total de 12986943 (15.741%)\n",
            "Momento: [2023-Mar-28 21:03:23] lr: 9.79150e-05 Train loss: 2.6325 perplexidade: 14.5349 Validação perplexidade: 15.9245 \n",
            "Step: 21632 Amostras:2076672 de um total de 12986943 (15.990%)\n",
            "Momento: [2023-Mar-28 21:06:52] lr: 9.78826e-05 Train loss: 2.7250 perplexidade: 14.4874 Validação perplexidade: 15.9071 \n",
            " novo best valid 15.880731320869232; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não se pode fazer isso. O que você acha de \"tudo\" e o quanto mais próximo do atleta for para ser um jogador experiente? Ninguém sabe com quem vai ganhar tíquete no mundial contra os brasilienses! Acho bem legal essas coisas porque ele tem umações em campos (nada) na partida pra criana pass', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 21970 Amostras:2109120 de um total de 12986943 (16.240%)\n",
            "Momento: [2023-Mar-28 21:10:27] lr: 9.78502e-05 Train loss: 2.6528 perplexidade: 14.4676 Validação perplexidade: 15.8807  novo best valid 15.880731320869232; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegítimo, porque não temos a menor ideia de como fazer um jogo. O que aconteceu? Acho muita coisa boas e o pessoa está sempre bem em casa! E se você tiver alguém para jogar no banheiro do quartel da cidade (ou quando for) vai ter umaçõeira na portaria pra vergonha... Mas isso amiga:', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 22308 Amostras:2141568 de um total de 12986943 (16.490%)\n",
            "Momento: [2023-Mar-28 21:13:58] lr: 9.78179e-05 Train loss: 2.6884 perplexidade: 14.4843 Validação perplexidade: 15.8752  novo best valid 15.875238225525974\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não deixe seu comentário. O que você acha do futebol? Nós temos um time muito bem preparado para o jogo, mas vamos continuamente jogando em casa\", disse ele à Agencia Brasil-Americana (Brasília) nesta quinta feira(22). Ainda assim foi diferentementenquem os times brasilienses no primeiro: \"O Corinthians\" pode', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 22646 Amostras:2174016 de um total de 12986943 (16.740%)\n",
            "Momento: [2023-Mar-28 21:17:30] lr: 9.77855e-05 Train loss: 2.6578 perplexidade: 14.4057 Validação perplexidade: 15.8547  novo best valid 15.854671695087786\n",
            "Step: 22984 Amostras:2206464 de um total de 12986943 (16.990%)\n",
            "Momento: [2023-Mar-28 21:21:00] lr: 9.77532e-05 Train loss: 2.6812 perplexidade: 14.4509 Validação perplexidade: 15.8624 \n",
            "Step: 23322 Amostras:2238912 de um total de 12986943 (17.240%)\n",
            "Momento: [2023-Mar-28 21:24:31] lr: 9.77210e-05 Train loss: 2.6546 perplexidade: 14.4279 Validação perplexidade: 15.8642 \n",
            "Step: 23660 Amostras:2271360 de um total de 12986943 (17.490%)\n",
            "Momento: [2023-Mar-28 21:28:02] lr: 9.76887e-05 Train loss: 2.6545 perplexidade: 14.4543 Validação perplexidade: 15.8719 \n",
            " novo best valid 15.847672231139995; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não se preocupe com a situaçao. O que vocês podem fazer para conseguir? Acho muito importante ouvirmos falando sobre isso eu estou tentativa de mexer no assunto do jogo contra os jogadores da Selecionadoria Brasil-Americana (Sindicato dos Jografistas), em Sérvário/RS\", disse nesta Silva Nettamb', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 23998 Amostras:2303808 de um total de 12986943 (17.739%)\n",
            "Momento: [2023-Mar-28 21:31:37] lr: 9.76564e-05 Train loss: 2.6948 perplexidade: 14.4227 Validação perplexidade: 15.8477  novo best valid 15.847672231139995; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não se preocupe com a sua vida. Você pode ser um dos melhores do mundo, mas o que você precisa? Aprenda-se de uma maneira mais saudável para ganhar dinheiro em casos onde os jogadores estiverem dispostas às necessidades da criança (ou ela tem algulada) no trabalado na horroficódiaisio.', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 24336 Amostras:2336256 de um total de 12986943 (17.989%)\n",
            "Momento: [2023-Mar-28 21:35:08] lr: 9.76242e-05 Train loss: 2.6898 perplexidade: 14.4531 Validação perplexidade: 15.7998  novo best valid 15.799773198367264\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser punido como crime de responsabilidade. O que aconteceu? A lei do futebol, por exemplo: \"A gente tem muita coisa a ver\", mas o fato da equipe ter sido investigada pelos policiais federais foi um dos principais motivadores para isso.\"\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 24674 Amostras:2368704 de um total de 12986943 (18.239%)\n",
            "Momento: [2023-Mar-28 21:38:39] lr: 9.75920e-05 Train loss: 2.6473 perplexidade: 14.4396 Validação perplexidade: 15.7609  novo best valid 15.760882440039877\n",
            "Step: 25012 Amostras:2401152 de um total de 12986943 (18.489%)\n",
            "Momento: [2023-Mar-28 21:42:09] lr: 9.75598e-05 Train loss: 2.6539 perplexidade: 14.3817 Validação perplexidade: 15.7694 \n",
            "Step: 25350 Amostras:2433600 de um total de 12986943 (18.739%)\n",
            "Momento: [2023-Mar-28 21:45:40] lr: 9.75277e-05 Train loss: 2.7392 perplexidade: 14.3806 Validação perplexidade: 15.7658 \n",
            "Step: 25688 Amostras:2466048 de um total de 12986943 (18.989%)\n",
            "Momento: [2023-Mar-28 21:49:11] lr: 9.74955e-05 Train loss: 2.7109 perplexidade: 14.4708 Validação perplexidade: 15.8045 \n",
            " novo best valid 15.752766673379947; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser uma tarefa fácil. A maioria dos jogadores que estiverem em um time devem ter acesso à internet, porém o único objetivo do jogo foi criado para garantir os direitos da torcida no Brasil: \"Apesas com as grandezades dessa competência sória muita\", mas sempre tem sido quem todos!\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 26026 Amostras:2498496 de um total de 12986943 (19.239%)\n",
            "Momento: [2023-Mar-28 21:52:49] lr: 9.74634e-05 Train loss: 2.6188 perplexidade: 14.3933 Validação perplexidade: 15.7528  novo best valid 15.752766673379947; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegalidade. Apenas um exemplo de que o Brasil não tem competência para fazer a Copa do Mundo, mas sim uma grande vitória da modalística brasiliense no ano passado e na prático-jogos olímpicos em 2014: \"A gente vai ter muita coisa boazinha pra segurando as pessoaçõperdias\". Eupliceiros', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 26364 Amostras:2530944 de um total de 12986943 (19.488%)\n",
            "Momento: [2023-Mar-28 21:56:19] lr: 9.74313e-05 Train loss: 2.7094 perplexidade: 14.4668 Validação perplexidade: 15.7122  novo best valid 15.712164858217093\n",
            "Step: 26702 Amostras:2563392 de um total de 12986943 (19.738%)\n",
            "Momento: [2023-Mar-28 21:59:50] lr: 9.73992e-05 Train loss: 2.6741 perplexidade: 14.3271 Validação perplexidade: 15.7454 \n",
            " novo best valid 15.703711178361328; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser punido com multa de R$ 1.000,00 (um mil reais) por danos morais O Ministério Público Federal do Paraná informou que o ex-prefeito da cidade estava empenhado na construção dos carros para acessórios no local onde foi instalada uma fazônicao automotecaecida calibrecementeira – um tamb', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 27040 Amostras:2595840 de um total de 12986943 (19.988%)\n",
            "Momento: [2023-Mar-28 22:03:27] lr: 9.73672e-05 Train loss: 2.7552 perplexidade: 14.3818 Validação perplexidade: 15.7037  novo best valid 15.703711178361328; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 27378 Amostras:2628288 de um total de 12986943 (20.238%)\n",
            "Momento: [2023-Mar-28 22:06:56] lr: 9.73352e-05 Train loss: 2.7099 perplexidade: 14.2975 Validação perplexidade: 15.7495 \n",
            " novo best valid 15.627467125034597; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não sei como funciona. Acho que a maioria dos jovens deve ser muitas vezes até mesmo um pouco diferente do outro e isso tudo ocorreu porque os jogadores só podem fazer uma boa partida para criarem sucessos em alguns momentos da vida (comuns). Eles tem certecnologias,podermasseja falando', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 27716 Amostras:2660736 de um total de 12986943 (20.488%)\n",
            "Momento: [2023-Mar-28 22:10:33] lr: 9.73031e-05 Train loss: 2.6327 perplexidade: 14.3400 Validação perplexidade: 15.6275  novo best valid 15.627467125034597; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 28054 Amostras:2693184 de um total de 12986943 (20.738%)\n",
            "Momento: [2023-Mar-28 22:14:03] lr: 9.72712e-05 Train loss: 2.6506 perplexidade: 14.2735 Validação perplexidade: 15.7763 \n",
            "Step: 28392 Amostras:2725632 de um total de 12986943 (20.987%)\n",
            "Momento: [2023-Mar-28 22:17:33] lr: 9.72392e-05 Train loss: 2.6483 perplexidade: 14.3059 Validação perplexidade: 15.7411 \n",
            "Step: 28730 Amostras:2758080 de um total de 12986943 (21.237%)\n",
            "Momento: [2023-Mar-28 22:21:03] lr: 9.72072e-05 Train loss: 2.6345 perplexidade: 14.2525 Validação perplexidade: 15.7174 \n",
            "Step: 29068 Amostras:2790528 de um total de 12986943 (21.487%)\n",
            "Momento: [2023-Mar-28 22:24:34] lr: 9.71753e-05 Train loss: 2.5495 perplexidade: 14.3067 Validação perplexidade: 15.6850 \n",
            "Step: 29406 Amostras:2822976 de um total de 12986943 (21.737%)\n",
            "Momento: [2023-Mar-28 22:28:03] lr: 9.71434e-05 Train loss: 2.6515 perplexidade: 14.2842 Validação perplexidade: 15.6335 \n",
            "Step: 29744 Amostras:2855424 de um total de 12986943 (21.987%)\n",
            "Momento: [2023-Mar-28 22:31:33] lr: 9.71115e-05 Train loss: 2.7439 perplexidade: 14.2301 Validação perplexidade: 15.6430 \n",
            " novo best valid 15.578944735621493; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não tem a menor dúvida de que o Brasil está em uma situaçõe muito complicada. O presidente da Federação Internacional dos Jogos Olímpicos (FITO), Paulo Roberto, disse nesta segunda-feira à imprensa na última quarta feita no Rio Grande do Sul por um acordenormônhoje com os jogadoresirosso para as', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 30082 Amostras:2887872 de um total de 12986943 (22.237%)\n",
            "Momento: [2023-Mar-28 22:35:10] lr: 9.70796e-05 Train loss: 2.6576 perplexidade: 14.2583 Validação perplexidade: 15.5789  novo best valid 15.578944735621493; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 30420 Amostras:2920320 de um total de 12986943 (22.487%)\n",
            "Momento: [2023-Mar-28 22:38:40] lr: 9.70478e-05 Train loss: 2.6796 perplexidade: 14.3338 Validação perplexidade: 15.5800 \n",
            " novo best valid 15.55184679309109; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser punido. A prática de atos ilícitos, por exemplo: a violência contra o meio ambiente em que vivemos; os crimes cometidos pelas autorizações do Ministério Pública Federal (MPF); as condutas criminais dos agentes penitenciais da Policia Civil no Estadora Nacional-Geral dizador Adjunto comum', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 30758 Amostras:2952768 de um total de 12986943 (22.736%)\n",
            "Momento: [2023-Mar-28 22:42:18] lr: 9.70160e-05 Train loss: 2.6756 perplexidade: 14.3032 Validação perplexidade: 15.5518  novo best valid 15.55184679309109; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 31096 Amostras:2985216 de um total de 12986943 (22.986%)\n",
            "Momento: [2023-Mar-28 22:45:48] lr: 9.69842e-05 Train loss: 2.7385 perplexidade: 14.2818 Validação perplexidade: 15.5690 \n",
            "Step: 31434 Amostras:3017664 de um total de 12986943 (23.236%)\n",
            "Momento: [2023-Mar-28 22:49:18] lr: 9.69524e-05 Train loss: 2.6762 perplexidade: 14.2779 Validação perplexidade: 15.5867 \n",
            "Step: 31772 Amostras:3050112 de um total de 12986943 (23.486%)\n",
            "Momento: [2023-Mar-28 22:52:49] lr: 9.69206e-05 Train loss: 2.7340 perplexidade: 14.2447 Validação perplexidade: 15.5526 \n",
            " novo best valid 15.51360064491182; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como fazer isso. O que você precisa para se manter no mundial? Acho muita coisa boas e ainda estou pensando em quem vai ser o melhor jogador de basquete do Brasil na minha opinião\", afirmam os atletas da Federaçao Paulista dos Jogos Olímpicos (Fina) neste-Rio 2016/2016.\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 32110 Amostras:3082560 de um total de 12986943 (23.736%)\n",
            "Momento: [2023-Mar-28 22:56:26] lr: 9.68889e-05 Train loss: 2.6357 perplexidade: 14.2771 Validação perplexidade: 15.5136  novo best valid 15.51360064491182; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 32448 Amostras:3115008 de um total de 12986943 (23.986%)\n",
            "Momento: [2023-Mar-28 22:59:56] lr: 9.68572e-05 Train loss: 2.7238 perplexidade: 14.2497 Validação perplexidade: 15.5161 \n",
            "Step: 32786 Amostras:3147456 de um total de 12986943 (24.236%)\n",
            "Momento: [2023-Mar-28 23:03:26] lr: 9.68255e-05 Train loss: 2.5757 perplexidade: 14.2453 Validação perplexidade: 15.5331 \n",
            "Step: 33124 Amostras:3179904 de um total de 12986943 (24.485%)\n",
            "Momento: [2023-Mar-28 23:06:57] lr: 9.67938e-05 Train loss: 2.6111 perplexidade: 14.2225 Validação perplexidade: 15.5346 \n",
            "Step: 33462 Amostras:3212352 de um total de 12986943 (24.735%)\n",
            "Momento: [2023-Mar-28 23:10:27] lr: 9.67621e-05 Train loss: 2.6157 perplexidade: 14.1921 Validação perplexidade: 15.5970 \n",
            "Step: 33800 Amostras:3244800 de um total de 12986943 (24.985%)\n",
            "Momento: [2023-Mar-28 23:13:57] lr: 9.67305e-05 Train loss: 2.6763 perplexidade: 14.2417 Validação perplexidade: 15.5180 \n",
            "Step: 34138 Amostras:3277248 de um total de 12986943 (25.235%)\n",
            "Momento: [2023-Mar-28 23:17:27] lr: 9.66989e-05 Train loss: 2.6101 perplexidade: 14.1148 Validação perplexidade: 15.5362 \n",
            "Step: 34476 Amostras:3309696 de um total de 12986943 (25.485%)\n",
            "Momento: [2023-Mar-28 23:20:57] lr: 9.66673e-05 Train loss: 2.5827 perplexidade: 14.1576 Validação perplexidade: 15.5584 \n",
            " novo best valid 15.499070033730769; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética, mas não tem jeito. Acho que a gente vai ficando muito tempo para pensarmos em competições e jogadores comuns\", afirmou o presidente da Associação Brasileira de Futebol (ABF) José Carlos Gomes Filho. \"Nós estamos trabalhando no nosso país há algostas difrida do mundial\". O atletismo tambiental', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 34814 Amostras:3342144 de um total de 12986943 (25.735%)\n",
            "Momento: [2023-Mar-28 23:24:34] lr: 9.66357e-05 Train loss: 2.6156 perplexidade: 14.1070 Validação perplexidade: 15.4991  novo best valid 15.499070033730769; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não se pode fazer isso. O que você precisa para ganhar dinheiro? A prática de atividades e ações só podem ser feitas quando ocorreu algum crime em um local onde houve uma grande quantidade de jogadores como os do Flamengo (que também estavam no gramado), por caíramas-bronteres na b', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 35152 Amostras:3374592 de um total de 12986943 (25.984%)\n",
            "Momento: [2023-Mar-28 23:28:05] lr: 9.66042e-05 Train loss: 2.6938 perplexidade: 14.1669 Validação perplexidade: 15.4761  novo best valid 15.476060694266156\n",
            "Step: 35490 Amostras:3407040 de um total de 12986943 (26.234%)\n",
            "Momento: [2023-Mar-28 23:31:36] lr: 9.65726e-05 Train loss: 2.5110 perplexidade: 14.2358 Validação perplexidade: 15.5596 \n",
            "Step: 35828 Amostras:3439488 de um total de 12986943 (26.484%)\n",
            "Momento: [2023-Mar-28 23:35:06] lr: 9.65411e-05 Train loss: 2.5992 perplexidade: 14.1528 Validação perplexidade: 15.5017 \n",
            "Step: 36166 Amostras:3471936 de um total de 12986943 (26.734%)\n",
            "Momento: [2023-Mar-28 23:38:36] lr: 9.65096e-05 Train loss: 2.6542 perplexidade: 14.1573 Validação perplexidade: 15.4829 \n",
            "Step: 36504 Amostras:3504384 de um total de 12986943 (26.984%)\n",
            "Momento: [2023-Mar-28 23:42:06] lr: 9.64782e-05 Train loss: 2.6434 perplexidade: 14.1935 Validação perplexidade: 15.4814 \n",
            " novo best valid 15.429472280315537; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como fazer. É um atleta que seja muitas vezes prejudicado pelos jogadores e pela torcida do time\", explica o treinador da equipe de Londres (EUA). \"O meu objetivo era ajudá-lo na preparação para as competiões\". O técnico argentino afirmano disse teria umaisperfeito sobreza no clube', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 36842 Amostras:3536832 de um total de 12986943 (27.234%)\n",
            "Momento: [2023-Mar-28 23:45:43] lr: 9.64467e-05 Train loss: 2.6071 perplexidade: 14.1100 Validação perplexidade: 15.4295  novo best valid 15.429472280315537; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 37180 Amostras:3569280 de um total de 12986943 (27.484%)\n",
            "Momento: [2023-Mar-28 23:49:12] lr: 9.64153e-05 Train loss: 2.6625 perplexidade: 14.0868 Validação perplexidade: 15.4496 \n",
            "Step: 37518 Amostras:3601728 de um total de 12986943 (27.733%)\n",
            "Momento: [2023-Mar-28 23:52:41] lr: 9.63839e-05 Train loss: 2.6173 perplexidade: 14.0670 Validação perplexidade: 15.4391 \n",
            " novo best valid 15.402554064251982; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética e não se preocupa em fazer o que quiser. Aprender a treinar, ter uma boa vontade de jogos para os amiguinhos do mundo inteiro pode ser um desafio!\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 37856 Amostras:3634176 de um total de 12986943 (27.983%)\n",
            "Momento: [2023-Mar-28 23:56:18] lr: 9.63525e-05 Train loss: 2.6707 perplexidade: 14.1040 Validação perplexidade: 15.4026  novo best valid 15.402554064251982; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não se preocupa com o futebol. Acho que a maioria dos brasileiros sabe disso e quer saber de tudo isto\", afirmou ele em entrevista à Rádio Jornalismo FM (www.jurisprudenciavitro). O presidente da Federações Paulistas do Comércio Exterior Brasilosul-RJus/RS, Paulo Roberto Carlos Lopes Filho', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 38194 Amostras:3666624 de um total de 12986943 (28.233%)\n",
            "Momento: [2023-Mar-28 23:59:50] lr: 9.63211e-05 Train loss: 2.5788 perplexidade: 14.1384 Validação perplexidade: 15.3634  novo best valid 15.363401887553193\n",
            "Step: 38532 Amostras:3699072 de um total de 12986943 (28.483%)\n",
            "Momento: [2023-Mar-29 00:03:19] lr: 9.62898e-05 Train loss: 2.6431 perplexidade: 14.0475 Validação perplexidade: 15.3978 \n",
            "Step: 38870 Amostras:3731520 de um total de 12986943 (28.733%)\n",
            "Momento: [2023-Mar-29 00:06:50] lr: 9.62584e-05 Train loss: 2.6447 perplexidade: 14.0592 Validação perplexidade: 15.3958 \n",
            "Step: 39208 Amostras:3763968 de um total de 12986943 (28.983%)\n",
            "Momento: [2023-Mar-29 00:10:20] lr: 9.62271e-05 Train loss: 2.6141 perplexidade: 14.0939 Validação perplexidade: 15.3893 \n",
            "Step: 39546 Amostras:3796416 de um total de 12986943 (29.233%)\n",
            "Momento: [2023-Mar-29 00:13:50] lr: 9.61958e-05 Train loss: 2.5916 perplexidade: 14.1128 Validação perplexidade: 15.4209 \n",
            " novo best valid 15.349826303247545; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como se preocupa em fazer uma boate. Acho que o povo brasiliense está muitíssimo enganado e a gente vai dizer: \"Nós somos um país de corrupçao\". Eu sou contra isso mesmo! Ninguém sabe quem foi essas coisas do mundialismo da modalidade… Mas tambiente os carros negatores podres', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 39884 Amostras:3828864 de um total de 12986943 (29.482%)\n",
            "Momento: [2023-Mar-29 00:17:27] lr: 9.61646e-05 Train loss: 2.6266 perplexidade: 14.0958 Validação perplexidade: 15.3498  novo best valid 15.349826303247545; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 40222 Amostras:3861312 de um total de 12986943 (29.732%)\n",
            "Momento: [2023-Mar-29 00:20:57] lr: 9.61333e-05 Train loss: 2.7011 perplexidade: 14.0450 Validação perplexidade: 15.3759 \n",
            "Step: 40560 Amostras:3893760 de um total de 12986943 (29.982%)\n",
            "Momento: [2023-Mar-29 00:24:27] lr: 9.61021e-05 Train loss: 2.6160 perplexidade: 14.0088 Validação perplexidade: 15.3583 \n",
            " novo best valid 15.311132001076706; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem a ver com o que se trata. O Brasil precisa de um sistema educativo para garantir sua autonomia e independência em relacionamentos sociais\", afirmou ele na entrevista à revistas \"O Estado do Rio Grande\" (Editor: http://www.youtube.com/watch?v=1x0)\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 40898 Amostras:3926208 de um total de 12986943 (30.232%)\n",
            "Momento: [2023-Mar-29 00:28:03] lr: 9.60709e-05 Train loss: 2.6439 perplexidade: 14.0529 Validação perplexidade: 15.3111  novo best valid 15.311132001076706; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser punido como um crime. A prática de exercícios físico-quimicamente corretivo, por exemplo, tem sido uma das mais importantes ferramenta para ajudarmos os jovens na busca da melhor qualidade do nosso pais\", disse o presidente em entrevista à revista \"O Estado\". O evento aconteou que foiçõesquerê', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 41236 Amostras:3958656 de um total de 12986943 (30.482%)\n",
            "Momento: [2023-Mar-29 00:31:34] lr: 9.60397e-05 Train loss: 2.6837 perplexidade: 14.0528 Validação perplexidade: 15.3035  novo best valid 15.303517127070343\n",
            "Step: 41574 Amostras:3991104 de um total de 12986943 (30.732%)\n",
            "Momento: [2023-Mar-29 00:35:04] lr: 9.60085e-05 Train loss: 2.6582 perplexidade: 14.0514 Validação perplexidade: 15.3469 \n",
            "Step: 41912 Amostras:4023552 de um total de 12986943 (30.982%)\n",
            "Momento: [2023-Mar-29 00:38:34] lr: 9.59774e-05 Train loss: 2.6192 perplexidade: 14.0298 Validação perplexidade: 15.3539 \n",
            "Step: 42250 Amostras:4056000 de um total de 12986943 (31.231%)\n",
            "Momento: [2023-Mar-29 00:42:05] lr: 9.59463e-05 Train loss: 2.6875 perplexidade: 13.9800 Validação perplexidade: 15.3510 \n",
            "Step: 42588 Amostras:4088448 de um total de 12986943 (31.481%)\n",
            "Momento: [2023-Mar-29 00:45:35] lr: 9.59152e-05 Train loss: 2.7149 perplexidade: 14.0875 Validação perplexidade: 15.3297 \n",
            "Step: 42926 Amostras:4120896 de um total de 12986943 (31.731%)\n",
            "Momento: [2023-Mar-29 00:49:05] lr: 9.58841e-05 Train loss: 2.6114 perplexidade: 13.9983 Validação perplexidade: 15.3423 \n",
            " novo best valid 15.271220110245904; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não há como fazer isso. O que vocês acham? Ninguém pode ser punido por crime de homicídio qualificado e ou seja um criminoso violenta (com penas maiores). Mas quem estiver em uma situação diferente do outro vive na razoa da prisãoporrogueira\", afirmossegurou elogrólicenciô', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 43264 Amostras:4153344 de um total de 12986943 (31.981%)\n",
            "Momento: [2023-Mar-29 00:52:43] lr: 9.58530e-05 Train loss: 2.6123 perplexidade: 14.0080 Validação perplexidade: 15.2712  novo best valid 15.271220110245904; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 43602 Amostras:4185792 de um total de 12986943 (32.231%)\n",
            "Momento: [2023-Mar-29 00:56:13] lr: 9.58220e-05 Train loss: 2.7470 perplexidade: 14.0386 Validação perplexidade: 15.3058 \n",
            "Step: 43940 Amostras:4218240 de um total de 12986943 (32.481%)\n",
            "Momento: [2023-Mar-29 00:59:43] lr: 9.57909e-05 Train loss: 2.6526 perplexidade: 13.9590 Validação perplexidade: 15.3064 \n",
            "Step: 44278 Amostras:4250688 de um total de 12986943 (32.730%)\n",
            "Momento: [2023-Mar-29 01:03:14] lr: 9.57599e-05 Train loss: 2.5813 perplexidade: 14.0301 Validação perplexidade: 15.2965 \n",
            "Step: 44616 Amostras:4283136 de um total de 12986943 (32.980%)\n",
            "Momento: [2023-Mar-29 01:06:43] lr: 9.57290e-05 Train loss: 2.6319 perplexidade: 13.9983 Validação perplexidade: 15.3431 \n",
            "Step: 44954 Amostras:4315584 de um total de 12986943 (33.230%)\n",
            "Momento: [2023-Mar-29 01:10:13] lr: 9.56980e-05 Train loss: 2.6052 perplexidade: 14.0156 Validação perplexidade: 15.3479 \n",
            "Step: 45292 Amostras:4348032 de um total de 12986943 (33.480%)\n",
            "Momento: [2023-Mar-29 01:13:44] lr: 9.56670e-05 Train loss: 2.5339 perplexidade: 13.9076 Validação perplexidade: 15.3209 \n",
            "Step: 45630 Amostras:4380480 de um total de 12986943 (33.730%)\n",
            "Momento: [2023-Mar-29 01:17:13] lr: 9.56361e-05 Train loss: 2.5329 perplexidade: 13.7630 Validação perplexidade: 15.2811 \n",
            "Step: 45968 Amostras:4412928 de um total de 12986943 (33.980%)\n",
            "Momento: [2023-Mar-29 01:20:43] lr: 9.56052e-05 Train loss: 2.6287 perplexidade: 13.8425 Validação perplexidade: 15.3000 \n",
            "Step: 46306 Amostras:4445376 de um total de 12986943 (34.230%)\n",
            "Momento: [2023-Mar-29 01:24:13] lr: 9.55743e-05 Train loss: 2.6993 perplexidade: 13.8353 Validação perplexidade: 15.3160 \n",
            " novo best valid 15.24981061191329; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ético e não deixa que o público se torne mais atraente. O próximo passatempo será a Copinha do Mundo, em Sampa (SP), no dia 27/10 às 16h30min da manhã deste sábados na sede dos Jogos Olímpicos Rio 2016: A primeira edição foiça-liminar promovônaconteirante aconio comem', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 46644 Amostras:4477824 de um total de 12986943 (34.479%)\n",
            "Momento: [2023-Mar-29 01:27:47] lr: 9.55435e-05 Train loss: 2.6275 perplexidade: 13.7910 Validação perplexidade: 15.2498  novo best valid 15.24981061191329; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 46982 Amostras:4510272 de um total de 12986943 (34.729%)\n",
            "Momento: [2023-Mar-29 01:31:18] lr: 9.55126e-05 Train loss: 2.6975 perplexidade: 13.8471 Validação perplexidade: 15.2566 \n",
            "Step: 47320 Amostras:4542720 de um total de 12986943 (34.979%)\n",
            "Momento: [2023-Mar-29 01:34:47] lr: 9.54818e-05 Train loss: 2.5634 perplexidade: 13.8358 Validação perplexidade: 15.3053 \n",
            " novo best valid 15.241432826413604; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não tem acesso à internet. O que você precisa saber para onde irá seu corpo? Você pode fazer uma pesquisinha sobre o assunto, mas vai ver como ele funciona: \"Nós estamos falando de um modalidade mais simples\". A pré-temporada do Praticante Esporta foi em Londas, por exemplificado na última. Ele', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 47658 Amostras:4575168 de um total de 12986943 (35.229%)\n",
            "Momento: [2023-Mar-29 01:38:21] lr: 9.54510e-05 Train loss: 2.6739 perplexidade: 13.8171 Validação perplexidade: 15.2414  novo best valid 15.241432826413604; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ética e não deixa a criança com problemas. A prática da atividade físico pode ser um grande desafio para quem tem dificuldades em se adaptando às necessidades do corpo, principalmente no que tange o uso dos equipamentos médicos\", explora Ana Paula Souza Santos (foto), uma joveleira-branco profissionada por causa na águerç�', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 47996 Amostras:4607616 de um total de 12986943 (35.479%)\n",
            "Momento: [2023-Mar-29 01:41:52] lr: 9.54202e-05 Train loss: 2.6155 perplexidade: 13.8694 Validação perplexidade: 15.2052  novo best valid 15.205174300358754\n",
            "Step: 48334 Amostras:4640064 de um total de 12986943 (35.729%)\n",
            "Momento: [2023-Mar-29 01:45:21] lr: 9.53894e-05 Train loss: 2.6541 perplexidade: 13.8784 Validação perplexidade: 15.2319 \n",
            "Step: 48672 Amostras:4672512 de um total de 12986943 (35.979%)\n",
            "Momento: [2023-Mar-29 01:48:51] lr: 9.53587e-05 Train loss: 2.6078 perplexidade: 13.8322 Validação perplexidade: 15.2417 \n",
            "Step: 49010 Amostras:4704960 de um total de 12986943 (36.228%)\n",
            "Momento: [2023-Mar-29 01:52:22] lr: 9.53280e-05 Train loss: 2.6040 perplexidade: 13.7775 Validação perplexidade: 15.2272 \n",
            " novo best valid 15.191626422032718; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como se preocupá-los. O que você acha da vida? Acho muita coisa errada e ouvi dizer: \"Ela estava em uma situaçao de desenvolvimentos sociais\". Eles só pensam no futebol quando elas chegam à final do campeonato brasileiro (Foto)\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 49348 Amostras:4737408 de um total de 12986943 (36.478%)\n",
            "Momento: [2023-Mar-29 01:55:59] lr: 9.52973e-05 Train loss: 2.7262 perplexidade: 13.8345 Validação perplexidade: 15.1916  novo best valid 15.191626422032718; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 49686 Amostras:4769856 de um total de 12986943 (36.728%)\n",
            "Momento: [2023-Mar-29 01:59:28] lr: 9.52666e-05 Train loss: 2.6451 perplexidade: 13.9094 Validação perplexidade: 15.2274 \n",
            "Step: 50024 Amostras:4802304 de um total de 12986943 (36.978%)\n",
            "Momento: [2023-Mar-29 02:02:58] lr: 9.52359e-05 Train loss: 2.5396 perplexidade: 13.8499 Validação perplexidade: 15.2057 \n",
            " novo best valid 15.177395143728084; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal. A prática de atividades físicas, como a doação e o uso da terra para fins educativos nada mais só seria uma questõe que se tornaria imprescindível às pessoas em geral\", explicou ele no texto \"Apesar dos desafios\". O presidente afirmando as dificadas na águinvenciêncora: \\'nã', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 50362 Amostras:4834752 de um total de 12986943 (37.228%)\n",
            "Momento: [2023-Mar-29 02:06:35] lr: 9.52053e-05 Train loss: 2.6481 perplexidade: 13.7908 Validação perplexidade: 15.1774  novo best valid 15.177395143728084; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ético e não deixa a criança com dificuldades. A prática física, por exemplo, pode ser uma boa opção para quem quer que seja o melhor do mundo\", explicou ela em entrevista coletiva neste sábado (20). \"A gente tem um bom tempo na academia pra treinar muitas atividadeiras tambéticos olhandua\", mas vai sim', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 50700 Amostras:4867200 de um total de 12986943 (37.478%)\n",
            "Momento: [2023-Mar-29 02:10:06] lr: 9.51746e-05 Train loss: 2.7320 perplexidade: 13.7580 Validação perplexidade: 15.1678  novo best valid 15.167840122945144\n",
            "Step: 51038 Amostras:4899648 de um total de 12986943 (37.727%)\n",
            "Momento: [2023-Mar-29 02:13:36] lr: 9.51440e-05 Train loss: 2.5939 perplexidade: 13.7640 Validação perplexidade: 15.1866 \n",
            " novo best valid 15.161242681029378; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegalidade e não tem como seguir em frente. Ainda mais quando você está pronto para a competição, o que pode ser uma boa opções de lazer só vai acontecer no dia-dia (10/12), às 19h30min na Arena Pernambuco do Esportivo.\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 51376 Amostras:4932096 de um total de 12986943 (37.977%)\n",
            "Momento: [2023-Mar-29 02:17:13] lr: 9.51135e-05 Train loss: 2.6698 perplexidade: 13.8417 Validação perplexidade: 15.1612  novo best valid 15.161242681029378; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Step: 51714 Amostras:4964544 de um total de 12986943 (38.227%)\n",
            "Momento: [2023-Mar-29 02:20:43] lr: 9.50829e-05 Train loss: 2.5727 perplexidade: 13.7858 Validação perplexidade: 15.1899 \n",
            " novo best valid 15.142107412445258; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não tem como fazer. O que você acha? A prática de exercícios, por exemplo: \"A gente só pode se sentir bem quando estamos em um lugar onde ocorre uma grande dificuldade para alcançarmos os objetivos\". Eu também tenho muita coisa na carneira (oura) do corrato! NÃnãs', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 52052 Amostras:4996992 de um total de 12986943 (38.477%)\n",
            "Momento: [2023-Mar-29 02:24:21] lr: 9.50523e-05 Train loss: 2.5994 perplexidade: 13.8014 Validação perplexidade: 15.1421  novo best valid 15.142107412445258; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal, mas não tem como fazer isso. Ainda que o atleta tenha um bom desempenho no treino de hoje (e a sugestões do técnico) ele pode ser uma boas opções para quando seu corpo estiver em equilíbrio e saudade da musculatura\", explica Renato Augustini Júnbsp... O prádia japonêsseguitos', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 52390 Amostras:5029440 de um total de 12986943 (38.727%)\n",
            "Momento: [2023-Mar-29 02:27:52] lr: 9.50218e-05 Train loss: 2.7221 perplexidade: 13.8457 Validação perplexidade: 15.1290  novo best valid 15.128987418459351\n",
            "Step: 52728 Amostras:5061888 de um total de 12986943 (38.977%)\n",
            "Momento: [2023-Mar-29 02:31:22] lr: 9.49913e-05 Train loss: 2.6271 perplexidade: 13.8143 Validação perplexidade: 15.1889 \n",
            "Step: 53066 Amostras:5094336 de um total de 12986943 (39.227%)\n",
            "Momento: [2023-Mar-29 02:34:53] lr: 9.49608e-05 Train loss: 2.5711 perplexidade: 13.8064 Validação perplexidade: 15.1750 \n",
            "Step: 53404 Amostras:5126784 de um total de 12986943 (39.476%)\n",
            "Momento: [2023-Mar-29 02:38:23] lr: 9.49303e-05 Train loss: 2.6105 perplexidade: 13.8105 Validação perplexidade: 15.1642 \n",
            "Step: 53742 Amostras:5159232 de um total de 12986943 (39.726%)\n",
            "Momento: [2023-Mar-29 02:41:52] lr: 9.48999e-05 Train loss: 2.6504 perplexidade: 13.8138 Validação perplexidade: 15.1574 \n",
            "Step: 54080 Amostras:5191680 de um total de 12986943 (39.976%)\n",
            "Momento: [2023-Mar-29 02:45:21] lr: 9.48695e-05 Train loss: 2.6146 perplexidade: 13.7943 Validação perplexidade: 15.1752 \n",
            " novo best valid 15.10825107622996; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ilegal e não pode ser um ato de férias. O que ocorre com a família? A maioria dos jovens, por sua vez, tem uma visibilidade muito grande para serem vistos em todo mundo pelo menino na escola: \"A gente sabe disso\". Eles têm algum problemaúnho no corpo do cário da sociedoso (que ele', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 54418 Amostras:5224128 de um total de 12986943 (40.226%)\n",
            "Momento: [2023-Mar-29 02:48:59] lr: 9.48390e-05 Train loss: 2.6706 perplexidade: 13.8016 Validação perplexidade: 15.1083  novo best valid 15.10825107622996; modelo salvo em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            "Testando modelo com perplexidade menor. Frase gerada: ('Praticar esportes é ingerir alimentos que não só ajudam o coraçãozinho, mas também auxiliá-lo na hora de fazer exercícios. Alguns dos principais benefícios do uso da pranchinha: 1) O ideal para quem quer aprender comercialmente pode ser comprado em lojas virtuais e atendendo as necessidades das; 2), polegria (1). Aprenda umaio', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Step: 54756 Amostras:5256576 de um total de 12986943 (40.476%)\n",
            "Momento: [2023-Mar-29 02:52:29] lr: 9.48087e-05 Train loss: 2.6041 perplexidade: 13.8063 Validação perplexidade: 15.0933  novo best valid 15.093291298301498\n",
            "Step: 55094 Amostras:5289024 de um total de 12986943 (40.726%)\n",
            "Momento: [2023-Mar-29 02:55:59] lr: 9.47783e-05 Train loss: 2.6459 perplexidade: 13.8365 Validação perplexidade: 15.1207 \n",
            "Step: 55432 Amostras:5321472 de um total de 12986943 (40.976%)\n",
            "Momento: [2023-Mar-29 02:59:28] lr: 9.47479e-05 Train loss: 2.5955 perplexidade: 13.7221 Validação perplexidade: 15.1307 \n",
            "Step: 55770 Amostras:5353920 de um total de 12986943 (41.225%)\n",
            "Momento: [2023-Mar-29 03:02:58] lr: 9.47176e-05 Train loss: 2.6002 perplexidade: 13.8022 Validação perplexidade: 15.1225 \n",
            "Step: 56108 Amostras:5386368 de um total de 12986943 (41.475%)\n",
            "Momento: [2023-Mar-29 03:06:28] lr: 9.46873e-05 Train loss: 2.5988 perplexidade: 13.7726 Validação perplexidade: 15.1451 \n",
            "Step: 56446 Amostras:5418816 de um total de 12986943 (41.725%)\n",
            "Momento: [2023-Mar-29 03:09:59] lr: 9.46570e-05 Train loss: 2.6126 perplexidade: 13.8325 Validação perplexidade: 15.1360 \n",
            "Step: 56784 Amostras:5451264 de um total de 12986943 (41.975%)\n",
            "Momento: [2023-Mar-29 03:13:29] lr: 9.46267e-05 Train loss: 2.6899 perplexidade: 13.7866 Validação perplexidade: 15.1061 \n",
            "Step: 57122 Amostras:5483712 de um total de 12986943 (42.225%)\n",
            "Momento: [2023-Mar-29 03:16:59] lr: 9.45965e-05 Train loss: 2.5949 perplexidade: 13.7151 Validação perplexidade: 15.1101 \n",
            "Step: 57460 Amostras:5516160 de um total de 12986943 (42.475%)\n",
            "Momento: [2023-Mar-29 03:20:29] lr: 9.45662e-05 Train loss: 2.5962 perplexidade: 13.8014 Validação perplexidade: 15.1278 \n",
            "Step: 57798 Amostras:5548608 de um total de 12986943 (42.725%)\n",
            "Momento: [2023-Mar-29 03:23:59] lr: 9.45360e-05 Train loss: 2.5944 perplexidade: 13.7494 Validação perplexidade: 15.1724 \n",
            "Parando por critério de early_stop no step 58136 sendo best_step 54756 e ealy_stop 3380\n",
            "Tempo gasto total 36479.14503, steps: 58136, tempo por step   0.62748\n",
            "Final: Step: 58136 Amostras:5581056  42.974%  Momento: [2023-Mar-29 03:27:28] lr:9.45058e-05 Train loss: 2.6076 Train perplexidade: 13.6957 Validação perplexidade: 15.1403 \n",
            "Modelo com melhor resultado em validação (step 54756) salvo após treino em /home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/best_model_len_100_inicio_treino_[2023-Mar-28 17:19:27].pt\n",
            " Resultado com dados de teste para modelo treinado: {'test/perplexidade': 12.901404185314464}\n",
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: ('Praticar esportes é ingerir bebida alcoólica e fazer exercícios de maneira saudável. A praticidade do atletismo pode ser feita com o objetivo principal, seja na formatação dos músculos (cortinas), no corpo humano...\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 7 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 7 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-28/metadata\n"
          ]
        }
      ],
      "source": [
        "resultado = treina_grid(hparam, gridparam, model, parm_se_gera_rastro = True, se_treina_poucos_dados=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frase inicio: Praticar esportes é \n",
            "Frase final gerada: ('Praticar esportes é ingerir bebida alcoólica e fazer exercícios de maneira saudável. A praticidade do atletismo pode ser feita com o objetivo principal, seja na formatação dos músculos (cortinas), no corpo humano...\\n', {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5})\n"
          ]
        }
      ],
      "source": [
        "frase_inicio = \"Praticar esportes é \"\n",
        "frase_final = continuar_frase_pipeline(frase_inicio, model)\n",
        "print(f\"Frase inicio: {frase_inicio}\")\n",
        "print(f\"Frase final gerada: {frase_final}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Avaliação de consumo GPU RTX-3090 (24gb)\n",
        "\n",
        "Para: max_examples= 3 épocas x 4328981 train_size = 12986943 <br>\n",
        "    seq_len = 100, batch_size=8  :: gpu   7807MiB; tempo: 0,25% a cada 18 minutos!  <br>\n",
        "    seq_len = 100, batch_size=24 :: gpu  11043MiB; tempo: 0,25% a cada  8 minutos! <br>\n",
        "    seq_len = 100, batch_size=72 :: gpu  18235MiB; tempo: 0,25% a cada  4 minutos! <br>\n",
        "    seq_len = 100, batch_size=96 :: gpu  19191; tempo: 0,25% a cada  3:30 minutos! <br>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Publicar modelo no repositório HF"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Criar dataset com seq_length 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4Cw7LANFEQ",
        "outputId": "424328ae-2eb5-40ae-c83d-9c39571d53f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_sentencas: 250000\n",
            "hparam['max_seq_length']: 250\n"
          ]
        }
      ],
      "source": [
        "#@title Seleção datasets\n",
        "hparam['num_sentenca_train'] = 249800 #@param [800, 249800] {type:'raw'}\n",
        "hparam['num_sentenca_valid'] = 100 # 100\n",
        "hparam['num_sentenca_test'] = 100 # 100\n",
        "hparam['max_seq_length'] = 250 #@param [9, 50, 100, 250] {type:'raw'}\n",
        "total_sentencas = hparam['num_sentenca_train']+hparam['num_sentenca_valid']+hparam['num_sentenca_test']\n",
        "print(f\"total_sentencas: {total_sentencas}\")\n",
        "print(f\"hparam['max_seq_length']: {hparam['max_seq_length']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyq5iuj_NFEQ",
        "outputId": "80d50cc5-a8a5-49b8-fd37-2355d7a59d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/dataset_max_seq_250_text_\n"
          ]
        }
      ],
      "source": [
        "# prefixo_nome_diretorio = '/content/drive/My Drive/treinamento/202301_IA368DD/aula4/'\n",
        "prefixo_nome_diretorio = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local/aula4/'\n",
        "\n",
        "infixo_nome= 'dataset_max_seq_'+ str(hparam['max_seq_length'])+'_text_'\n",
        "print(prefixo_nome_diretorio + infixo_nome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVTsirIiqJUB",
        "outputId": "082d20fa-1596-48fc-a5ab-85ae097adb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "#@title Boolean fields\n",
        "datasets_carregados_previamente = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(datasets_carregados_previamente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUqNOdVlGEYd"
      },
      "outputs": [],
      "source": [
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "manqs7PdtOjQ"
      },
      "outputs": [],
      "source": [
        "if datasets_carregados_previamente:\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_test'])+'_test.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  test_dataset = torch.load(buffer)\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_valid'])+'_valid.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  valid_dataset = torch.load(buffer)\n",
        "  with open(prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_train'])+'_train.pt','rb') as f:\n",
        "    buffer = io.BytesIO(f.read())\n",
        "  train_dataset = torch.load(buffer)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqEpL-vBtOjR",
        "outputId": "3e311b73-5ded-4d48-ee40-8a729f281b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-29 03:27:37--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.162.112, 142.251.128.144, 142.251.128.80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.162.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1230909256 (1,1G) [text/plain]\n",
            "Saving to: ‘sample-1gb.txt’\n",
            "\n",
            "sample-1gb.txt      100%[===================>]   1,15G  8,99MB/s    in 2m 7s   \n",
            "\n",
            "2023-03-29 03:29:45 (9,26 MB/s) - ‘sample-1gb.txt’ saved [1230909256/1230909256]\n",
            "\n",
            "250000\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  !wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
        "  texts = open('sample-1gb.txt').readlines() \n",
        "  assert total_sentencas <= len(texts), f\"total sentencas deve ser <= len(texts)\"\n",
        "  # texts = texts[:total]  \n",
        "  print(len(texts)) # 250000 \n",
        "\n",
        "  # carga total para treino:\n",
        "  # hparam['num_sentenca_train'] = total_sentencas - (hparam['num_sentenca_valid'] + hparam['num_sentenca_test'])\n",
        "  # train_texts = texts[:hparam['num_sentenca_train'] ]\n",
        "  # hparam['num_sentenca_train'] = len(train_texts)\n",
        "  # carga parcial para treino:\n",
        "  \n",
        "  #ipdb.set_trace(context=6)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5uOibekNNNl",
        "outputId": "ea6d8a6a-afe2-4ca4-838d-83408cfa615d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carregando valid_dataset\n",
            " len(texts) 100; tamanho_batch 50;  num_batch_entrada 2 \n",
            " max_seq_length 250; Mas salvando sentence_length 251\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 464; Momento: [2023-Mar-29 03:29:47]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-29 03:29:48]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-29 03:29:48]\n",
            "Carregado dataset com 818 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  valid_texts = texts[-(hparam['num_sentenca_valid'] + hparam['num_sentenca_test']):-hparam['num_sentenca_test']]\n",
        "  print(\"carregando valid_dataset\")\n",
        "  valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(valid_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_valid'])+'_valid.pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xkTjjo1NNNm",
        "outputId": "fb3a486b-76c4-4949-9ae4-649a904b11f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carregando test_dataset\n",
            " len(texts) 100; tamanho_batch 50;  num_batch_entrada 2 \n",
            " max_seq_length 250; Mas salvando sentence_length 251\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 246; Momento: [2023-Mar-29 03:29:48]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-29 03:29:48]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-29 03:29:48]\n",
            "Carregado dataset com 459 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  print(\"carregando test_dataset\")\n",
        "  test_texts = texts[-hparam['num_sentenca_test']:]  \n",
        "  test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(test_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_test'])+'_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhKHOXM5NNNm",
        "outputId": "8c6ee036-0ce2-4740-a92f-53aa16e77777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carregando train_dataset\n",
            " len(texts) 249800; tamanho_batch 50;  num_batch_entrada 4996 \n",
            " max_seq_length 250; Mas salvando sentence_length 251\n",
            "\t === ndx_batch+1: 1  self.qtd_sequencia: 395; Momento: [2023-Mar-29 03:29:49]\n",
            "\t === ndx_batch+1: 101  self.qtd_sequencia: 36858; Momento: [2023-Mar-29 03:30:12]\n",
            "\t === ndx_batch+1: 201  self.qtd_sequencia: 74170; Momento: [2023-Mar-29 03:30:35]\n",
            "\t === ndx_batch+1: 301  self.qtd_sequencia: 108484; Momento: [2023-Mar-29 03:30:55]\n",
            "\t === ndx_batch+1: 401  self.qtd_sequencia: 140623; Momento: [2023-Mar-29 03:31:14]\n",
            "\t === ndx_batch+1: 501  self.qtd_sequencia: 176186; Momento: [2023-Mar-29 03:31:35]\n",
            "\t === ndx_batch+1: 601  self.qtd_sequencia: 209084; Momento: [2023-Mar-29 03:31:55]\n",
            "\t === ndx_batch+1: 701  self.qtd_sequencia: 244446; Momento: [2023-Mar-29 03:32:15]\n",
            "\t === ndx_batch+1: 801  self.qtd_sequencia: 277510; Momento: [2023-Mar-29 03:32:35]\n",
            "\t === ndx_batch+1: 901  self.qtd_sequencia: 310809; Momento: [2023-Mar-29 03:32:54]\n",
            "\t === ndx_batch+1: 1001  self.qtd_sequencia: 346399; Momento: [2023-Mar-29 03:33:14]\n",
            "\t === ndx_batch+1: 1101  self.qtd_sequencia: 380488; Momento: [2023-Mar-29 03:33:34]\n",
            "\t === ndx_batch+1: 1201  self.qtd_sequencia: 416829; Momento: [2023-Mar-29 03:33:55]\n",
            "\t === ndx_batch+1: 1301  self.qtd_sequencia: 452653; Momento: [2023-Mar-29 03:34:16]\n",
            "\t === ndx_batch+1: 1401  self.qtd_sequencia: 486968; Momento: [2023-Mar-29 03:34:36]\n",
            "\t === ndx_batch+1: 1501  self.qtd_sequencia: 520214; Momento: [2023-Mar-29 03:34:55]\n",
            "\t === ndx_batch+1: 1601  self.qtd_sequencia: 555729; Momento: [2023-Mar-29 03:35:15]\n",
            "\t === ndx_batch+1: 1701  self.qtd_sequencia: 591154; Momento: [2023-Mar-29 03:35:36]\n",
            "\t === ndx_batch+1: 1801  self.qtd_sequencia: 626593; Momento: [2023-Mar-29 03:35:56]\n",
            "\t === ndx_batch+1: 1901  self.qtd_sequencia: 662568; Momento: [2023-Mar-29 03:36:17]\n",
            "\t === ndx_batch+1: 2001  self.qtd_sequencia: 697953; Momento: [2023-Mar-29 03:36:37]\n",
            "\t === ndx_batch+1: 2101  self.qtd_sequencia: 733846; Momento: [2023-Mar-29 03:36:57]\n",
            "\t === ndx_batch+1: 2201  self.qtd_sequencia: 769210; Momento: [2023-Mar-29 03:37:18]\n",
            "\t === ndx_batch+1: 2301  self.qtd_sequencia: 804835; Momento: [2023-Mar-29 03:37:38]\n",
            "\t === ndx_batch+1: 2401  self.qtd_sequencia: 837591; Momento: [2023-Mar-29 03:37:57]\n",
            "\t === ndx_batch+1: 2501  self.qtd_sequencia: 870925; Momento: [2023-Mar-29 03:38:16]\n",
            "\t === ndx_batch+1: 2601  self.qtd_sequencia: 905974; Momento: [2023-Mar-29 03:38:35]\n",
            "\t === ndx_batch+1: 2701  self.qtd_sequencia: 940522; Momento: [2023-Mar-29 03:38:55]\n",
            "\t === ndx_batch+1: 2801  self.qtd_sequencia: 974129; Momento: [2023-Mar-29 03:39:14]\n",
            "\t === ndx_batch+1: 2901  self.qtd_sequencia: 1006988; Momento: [2023-Mar-29 03:39:33]\n",
            "\t === ndx_batch+1: 3001  self.qtd_sequencia: 1040076; Momento: [2023-Mar-29 03:39:53]\n",
            "\t === ndx_batch+1: 3101  self.qtd_sequencia: 1075289; Momento: [2023-Mar-29 03:40:13]\n",
            "\t === ndx_batch+1: 3201  self.qtd_sequencia: 1111207; Momento: [2023-Mar-29 03:40:33]\n",
            "\t === ndx_batch+1: 3301  self.qtd_sequencia: 1148109; Momento: [2023-Mar-29 03:40:54]\n",
            "\t === ndx_batch+1: 3401  self.qtd_sequencia: 1181093; Momento: [2023-Mar-29 03:41:13]\n",
            "\t === ndx_batch+1: 3501  self.qtd_sequencia: 1215902; Momento: [2023-Mar-29 03:41:32]\n",
            "\t === ndx_batch+1: 3601  self.qtd_sequencia: 1253214; Momento: [2023-Mar-29 03:41:54]\n",
            "\t === ndx_batch+1: 3701  self.qtd_sequencia: 1287298; Momento: [2023-Mar-29 03:42:13]\n",
            "\t === ndx_batch+1: 3801  self.qtd_sequencia: 1324250; Momento: [2023-Mar-29 03:42:34]\n",
            "\t === ndx_batch+1: 3901  self.qtd_sequencia: 1357507; Momento: [2023-Mar-29 03:42:53]\n",
            "\t === ndx_batch+1: 4001  self.qtd_sequencia: 1393552; Momento: [2023-Mar-29 03:43:15]\n",
            "\t === ndx_batch+1: 4101  self.qtd_sequencia: 1426416; Momento: [2023-Mar-29 03:43:33]\n",
            "\t === ndx_batch+1: 4201  self.qtd_sequencia: 1461152; Momento: [2023-Mar-29 03:43:53]\n",
            "\t === ndx_batch+1: 4301  self.qtd_sequencia: 1498019; Momento: [2023-Mar-29 03:44:14]\n",
            "\t === ndx_batch+1: 4401  self.qtd_sequencia: 1530231; Momento: [2023-Mar-29 03:44:32]\n",
            "\t === ndx_batch+1: 4501  self.qtd_sequencia: 1565517; Momento: [2023-Mar-29 03:44:52]\n",
            "\t === ndx_batch+1: 4601  self.qtd_sequencia: 1602468; Momento: [2023-Mar-29 03:45:13]\n",
            "\t === ndx_batch+1: 4701  self.qtd_sequencia: 1638801; Momento: [2023-Mar-29 03:45:33]\n",
            "\t === ndx_batch+1: 4801  self.qtd_sequencia: 1674431; Momento: [2023-Mar-29 03:45:54]\n",
            "\t === ndx_batch+1: 4901  self.qtd_sequencia: 1710527; Momento: [2023-Mar-29 03:46:14]\n",
            "\tVou converter lista para tensor;  Momento: [2023-Mar-29 03:46:31]\n",
            "\tConvertido: lista para tensor;  Momento: [2023-Mar-29 03:46:44]\n",
            "Carregado dataset com 1740410 sentenças\n"
          ]
        }
      ],
      "source": [
        "if not datasets_carregados_previamente:\n",
        "  assert hparam['num_sentenca_train'] <=  total_sentencas - (hparam['num_sentenca_valid'] + hparam['num_sentenca_test']), f\"Dados de treino não podem conter dados de validação/teste\"\n",
        "  train_texts = texts[:hparam['num_sentenca_train'] ]\n",
        "\n",
        "  print(\"carregando train_dataset\")\n",
        "  train_dataset = MyDataset(texts=train_texts, tokenizer=tokenizer, max_seq_length=hparam['max_seq_length'])\n",
        "  torch.save(train_dataset, prefixo_nome_diretorio+infixo_nome+str(hparam['num_sentenca_train'])+'_train.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqQfYzEBtOjS",
        "outputId": "4ed7c200-cb75-4b86-a6fb-c6765548654b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training examples: 1740410\n",
            "valid examples: 818\n",
            "test examples: 459\n"
          ]
        }
      ],
      "source": [
        "hparam['train_size'] = len(train_dataset) \n",
        "hparam['valid_size'] = len(valid_dataset) \n",
        "hparam['test_size'] = len(test_dataset) \n",
        "\n",
        "\n",
        "print(f\"training examples: {hparam['train_size']}\")\n",
        "print(f\"valid examples: {hparam['valid_size']}\")\n",
        "print(f\"test examples: {hparam['test_size']}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYIgLS4tOjT"
      },
      "source": [
        "\n",
        "* Testado overfit: OK\n",
        "\n",
        "* Testado conjunto menor: Ok\n",
        "\n",
        "        max_seq_length=100, train_senteces = 800\n",
        "\n",
        "            training examples: 14573 <br>\n",
        "            valid examples: 2036 <br>\n",
        "            test examples: 1143 <br>\n",
        "\n",
        "* max_seq_length=100, train_senteces = 249800\n",
        "\n",
        "    Treino:<br>\n",
        "    \n",
        "        Parametros: \n",
        "            batch size: 96 \n",
        "            num_epochs: 3 \n",
        "            valid examples: 2036 \n",
        "            test examples: 1143 \n",
        "            training examples por época: 4328981  \n",
        "            training examples total: 12986943 \n",
        "            eval_every_steps: 338 \n",
        "            early_stop: 3380 (steps) \n",
        "\n",
        "        Contexto:\n",
        "            tempo total: 10.3 horas \n",
        "            best_step: 54756  \n",
        "            treino por step: 0.627s  \n",
        "            n_examples: 5581056 (42,97%) \n",
        "\n",
        "        perplexidade\n",
        "            treino: 13.69  \n",
        "            teste: 12.90 \n",
        "            validação: 15.14 \n",
        "\n",
        "        Teste geração frase: \n",
        "            {'max_length': 120, 'top_p ': 1, 'temperature': 0, 'top_k': 50, 'repetition_penalty': 1.5} \n",
        "            '[Praticar esportes é] ingerir bebida alcoólica e fazer exercícios de maneira  saudável. A praticidade do atletismo pode ser feita com o objetivo principal, seja na formatação dos músculos (cortinas), no corpo humano...\\n', \n",
        "\n",
        "        [link rastro](https://app.neptune.ai/marcusborela/IA386DD/e/IAD-28/metadata) <br>\n",
        "\n",
        "* Futuro: max_seq_length=250, train_senteces = 249800\n",
        "\n",
        "        training examples: 1740410 <br>\n",
        "        valid examples: 818 <br>\n",
        "        test examples: 459 <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BmRLgbyi_Dvg",
        "0Upk7A-8Zdnd",
        "wew-gFbWeBTq"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "treinamento",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1ab495da778b1490ad558dda332ee040d5168b43173ece1a71630fac661e6874"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
