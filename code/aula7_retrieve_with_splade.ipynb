{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 8 - SPLADE\n",
    "\n",
    "[Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
    "\n",
    "Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado do Exercício"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar a fase de indexação e buscas de um modelo sparso\n",
    "\n",
    "Usar este modelo SPLADE já treinado naver/splade_v2_distil (do distilbert) ou splade-cocondenser-selfdistil (do BERT-base 110M params). Mais informações sobre os modelos estão neste artigo: https://arxiv.org/pdf/2205.04733.pdf\n",
    "Não é necessário treinar o modelo\n",
    "Avaliar nDCG@10 no TREC-COVID e comparar resultados com o BM25 e buscador denso da semana passada\n",
    "\n",
    "A dificuldade do exercício está em implementar a função de busca e ranqueamento usada pelo SPLADE. A implementação deve ser codificada e usar implementação do SPLADE apenas para comparação. A implementação do índice invertido é apenas um \"dicionário python\".\n",
    "\n",
    "Fazer a comparação dos seus resultados com a busca \"original\" do SPLADE.\n",
    "Medir latência (s/query)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4AJiH6lQQHc5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import  BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRETORIO_LOCAL = '/home/borela/fontes/deep_learning_em_buscas_unicamp/local'\n",
    "DIRETORIO_TRABALHO = F'{DIRETORIO_LOCAL}/splade'\n",
    "DIRETORIO_TREC_COVID = F'{DIRETORIO_LOCAL}/trec_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta já existia!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DIRETORIO_LOCAL):\n",
    "    print('pasta já existia!')\n",
    "else:\n",
    "    os.makedirs(DIRETORIO_LOCAL)\n",
    "    print('pasta criada!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta já existia!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DIRETORIO_TRABALHO):\n",
    "    print('pasta já existia!')\n",
    "else:\n",
    "    os.makedirs(DIRETORIO_TRABALHO)\n",
    "    print('pasta criada!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta já existia!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DIRETORIO_TREC_COVID):\n",
    "    print('pasta já existia!')\n",
    "else:\n",
    "    os.makedirs(DIRETORIO_TREC_COVID)\n",
    "    print('pasta criada!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zwyzd_W8gOAL"
   },
   "outputs": [],
   "source": [
    "DIRETORIO_RUN = f\"{DIRETORIO_TRABALHO}/runs\"\n",
    "CAMINHO_RUN = f\"{DIRETORIO_RUN}/run-trec-covid-bm25.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta já existia!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DIRETORIO_RUN):\n",
    "    print('pasta já existia!')\n",
    "else:\n",
    "    os.makedirs(DIRETORIO_RUN)\n",
    "    print('pasta criada!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DKAZ8CWCAM3-"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9XgIWvkkH-kn"
   },
   "outputs": [],
   "source": [
    "def mostra_memoria(lista_mem=['cpu']):\n",
    "  \"\"\"\n",
    "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
    "\n",
    "  Parâmetros:\n",
    "  -----------\n",
    "  lista_mem : list, opcional\n",
    "      Lista com strings 'cpu' e/ou 'gpu'. \n",
    "      'cpu' - exibe informações de memória da CPU.\n",
    "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
    "      O valor padrão é ['cpu'].\n",
    "\n",
    "  Saída:\n",
    "  -------\n",
    "  A função não retorna nada, apenas exibe as informações na tela.\n",
    "\n",
    "  Exemplo de uso:\n",
    "  ---------------\n",
    "  Para exibir informações de memória da CPU:\n",
    "      mostra_memoria(['cpu'])\n",
    "\n",
    "  Para exibir informações de memória da CPU e GPU:\n",
    "      mostra_memoria(['cpu', 'gpu'])\n",
    "  \n",
    "  Autor: Marcus Vinícius Borela de Castro\n",
    "\n",
    "  \"\"\"  \n",
    "  if 'cpu' in lista_mem:\n",
    "    vm = virtual_memory()\n",
    "    ram={}\n",
    "    ram['total']=round(vm.total / 1e9,2)\n",
    "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
    "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
    "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
    "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
    "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
    "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
    "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
    "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
    "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
    "    print('/nGPU')\n",
    "    gpu_info = !nvidia-smi\n",
    "  if 'gpu' in lista_mem:\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "      print('Not connected to a GPU')\n",
    "    else:\n",
    "      print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dri9iiMAvCT",
    "outputId": "53aebd5a-e29f-4c8e-d233-5221aae9f9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 45.85\n",
      " used 20.2\n",
      " free 26.8\n",
      " cached 19.02\n",
      " buffers 1.34\n",
      "/nGPU\n",
      "Sun Apr 23 16:17:21 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 59%   45C    P8    30W / 370W |  11513MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A     53767      C   .../relevar-busca/bin/python    11453MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achvQ78sa3p3"
   },
   "source": [
    "## Fixando as seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AG9RjMb8Qlot"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bkETIyWGkbOf"
   },
   "outputs": [],
   "source": [
    "def inicializa_seed(num_semente:int=123):\n",
    "  \"\"\"\n",
    "  Inicializa as sementes para garantir a reprodutibilidade dos resultados do modelo.\n",
    "  Essa é uma prática recomendada, já que a geração de números aleatórios pode influenciar os resultados do modelo.\n",
    "  Além disso, a função também configura as sementes da GPU para garantir a reprodutibilidade quando se utiliza aceleração por GPU. \n",
    "  \n",
    "  Args:\n",
    "      num_semente (int): número da semente a ser utilizada para inicializar as sementes das bibliotecas.\n",
    "  \n",
    "  References:\n",
    "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
    "  \"\"\"\n",
    "  # Define as sementes das bibliotecas random, numpy e pytorch\n",
    "  random.seed(num_semente)\n",
    "  np.random.seed(num_semente)\n",
    "  torch.manual_seed(num_semente)\n",
    "  \n",
    "  # Define as sementes da GPU\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "  #torch.cuda.manual_seed(num_semente)\n",
    "  #Cuda algorithms\n",
    "  #torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ViMcw_kVkbOf"
   },
   "outputs": [],
   "source": [
    "num_semente=123\n",
    "inicializa_seed(num_semente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v2gtkEPhA0t"
   },
   "source": [
    "## Preparando para debug e display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BJ6S4P5Hw4iG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rnR2kDS_2FgZ"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjrlXHq1hC8n",
    "outputId": "18d94254-4bf8-4632-ef84-012727944746"
   },
   "outputs": [],
   "source": [
    "# https://zohaib.me/debugging-in-google-collab-notebook/\n",
    "# !pip install -Uqq ipdb\n",
    "import ipdb\n",
    "# %pdb off # desativa debug em exceção\n",
    "# %pdb on  # ativa debug em exceção\n",
    "# ipdb.set_trace(context=8)  para execução nesse ponto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wQ5pmlOHxHhk"
   },
   "outputs": [],
   "source": [
    "def config_display():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de display do Pandas.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configurando formato saída Pandas\n",
    "  # define o número máximo de colunas que serão exibidas\n",
    "  pd.options.display.max_columns = None\n",
    "\n",
    "  # define a largura máxima de uma linha\n",
    "  pd.options.display.width = 1000\n",
    "\n",
    "  # define o número máximo de linhas que serão exibidas\n",
    "  pd.options.display.max_rows = 100\n",
    "\n",
    "  # define o número máximo de caracteres por coluna\n",
    "  pd.options.display.max_colwidth = 50\n",
    "\n",
    "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
    "  pd.options.display.show_dimensions = True\n",
    "\n",
    "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
    "  pd.options.display.precision = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "b2tDy72ATNHs"
   },
   "outputs": [],
   "source": [
    "def config_debug():\n",
    "  \"\"\"\n",
    "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
    "  transformers e datasets.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define opções de impressão de tensores para o modo científico\n",
    "  torch.set_printoptions(sci_mode=True) \n",
    "  \"\"\"\n",
    "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
    "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345, \n",
    "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores \n",
    "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite \n",
    "    uma melhor compreensão dos números envolvidos.  \n",
    "  \"\"\"\n",
    "\n",
    "  # Habilita detecção de anomalias no autograd do PyTorch\n",
    "  torch.autograd.set_detect_anomaly(True)\n",
    "  \"\"\"\n",
    "    Permite identificar operações que podem causar problemas de estabilidade numérica, \n",
    "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada, \n",
    "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores \n",
    "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch \n",
    "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido \n",
    "    antes que se torne um problema maior.\n",
    "\n",
    "    É importante notar que a detecção de anomalias pode ter um impacto significativo \n",
    "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
    "    ela deve ser usada com cautela e apenas para depuração.\n",
    "  \"\"\"\n",
    "\n",
    "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
    "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "  \"\"\"\n",
    "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada. \n",
    "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado \n",
    "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
    "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona, \n",
    "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração \n",
    "    e removê-lo após a solução do problema.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define o nível de verbosity do pacote transformers para info\n",
    "  # transformers.utils.logging.set_verbosity_info() \n",
    "  \n",
    "  \n",
    "  \"\"\"\n",
    "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers \n",
    "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
    "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
    "\n",
    "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa \n",
    "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
    "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
    "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a \n",
    "    necessidade de cada tarefa.\n",
    "  \n",
    "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e \n",
    "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
    "  \n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Define o modo verbose do xmode, que é utilizado no debug\n",
    "  # %xmode Verbose \n",
    "\n",
    "  \"\"\"\n",
    "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
    "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
    "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais \n",
    "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
    "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
    "\n",
    "    Caso queira desabilitar o modo verbose e utilizar o modo plain, \n",
    "    comentar a linha acima e descomentar a linha abaixo:\n",
    "    %xmode Plain\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "    Dica:\n",
    "    1.  pdb (Python Debugger)\n",
    "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro \n",
    "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
    "\n",
    "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações \n",
    "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug \n",
    "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
    "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
    "\n",
    "\n",
    "    2. ipdb\n",
    "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
    "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
    "      \n",
    "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do \n",
    "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha, \n",
    "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute \n",
    "      comandos para investigar o comportamento.\n",
    "\n",
    "      Durante a depuração, você pode usar comandos:\n",
    "        next (para executar a próxima linha de código), \n",
    "        step (para entrar em uma função chamada na próxima linha de código) \n",
    "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
    "\n",
    "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
    "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
    "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Tb4aqtcExR84"
   },
   "outputs": [],
   "source": [
    "config_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5Bq4043fkfh",
    "outputId": "fa8e5db1-1feb-4393-fb66-d394d1ad693c"
   },
   "outputs": [],
   "source": [
    "config_debug()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixando o dataset para avaliação (trec-covid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JVM_PATH'] = '/usr/lib/jvm/java-11-openjdk-amd64/lib/server/libjvm.so'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import get_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 queries total\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics('covid-round5')\n",
    "print(f'{len(topics)} queries total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is known about an mRNA vaccine for the SARS-CoV-2 virus?',\n",
       " 'query': 'mRNA vaccine coronavirus',\n",
       " 'narrative': 'Looking for studies specifically focusing on mRNA vaccines for COVID-19, including how mRNA vaccines work, why they are promising, and any results from actual clinical studies.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevância (qrel) de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já existia\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f'{DIRETORIO_TREC_COVID}/test.tsv'):\n",
    "    !wget https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv\n",
    "    !mv test.tsv {DIRETORIO_LOCAL}/\n",
    "else:\n",
    "    print('Arquivo já existia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel = pd.read_csv(f\"{DIRETORIO_TREC_COVID}/test.tsv\", sep=\"\\t\", header=None, \n",
    "                   skiprows=1, names=[\"query\", \"docid\", \"rel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>g7dhmyyo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query     docid  rel\n",
       "0      1  005b2j4b    2\n",
       "1      1  00fmeepz    1\n",
       "2      1  g7dhmyyo    2\n",
       "3      1  0194oljo    1\n",
       "4      1  021q9884    1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>g7dhmyyo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query     docid  rel\n",
       "0      1  005b2j4b    2\n",
       "1      1  00fmeepz    1\n",
       "2      1  g7dhmyyo    2\n",
       "3      1  0194oljo    1\n",
       "4      1  021q9884    1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel[\"q0\"] = \"q0\"\n",
    "qrel_dict = qrel.to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '005b2j4b', 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrel_dict['query'][0], qrel_dict['docid'][0], qrel_dict['rel'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos a serem indexados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Já existia a pasta\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f\"{DIRETORIO_TREC_COVID}/corpus.jsonl.gz\"):\n",
    "    !wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz\n",
    "    !mv corpus.jsonl.gz {DIRETORIO_TREC_COVID}\n",
    "    print('Baixado')\n",
    "else:\n",
    "    print('Já existia a pasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descompacte o arquivo para a memória\n",
    "with gzip.open(f'{DIRETORIO_TREC_COVID}/corpus.jsonl.gz', 'rt') as f:\n",
    "    # Leia o conteúdo do arquivo descompactado\n",
    "    corpus = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> len(corpus): 171332 corpus[0] {'_id': 'ug7v899j', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia', 'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'metadata': {'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35282/', 'pubmed_id': '11472636'}}\n"
     ]
    }
   ],
   "source": [
    "# Exiba os dados carregados\n",
    "print(f\"{type(corpus)} len(corpus): {len(corpus)} corpus[0] {corpus[0]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'title', 'text', 'metadata'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lista_doc_id_passage) 171332\n"
     ]
    }
   ],
   "source": [
    "lista_doc_id_passage = [doc['_id'] for doc in corpus]\n",
    "print(f\"len(lista_doc_id_passage) {len(lista_doc_id_passage)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos de teste de encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Com modelo 'naver/splade-cocondenser-ensembledistil' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NOME_MODELO = \"naver/splade-cocondenser-ensembledistil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se tiver que treinar os modelos, abre\n",
    "model = BertForMaskedLM.from_pretrained(PATH_NOME_MODELO).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"naver/splade-cocondenser-ensembledistil\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PATH_NOME_MODELO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = [\"He likes to eat pizza in the pizza hut.\", \"So many sentences are transformed in just a summary.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Tokenize sentences\n",
    "encoded_input = tokenizer(sentences_test,\n",
    "                                add_special_tokens=True,\n",
    "                                return_special_tokens_mask=True, \n",
    "                                padding=True, \n",
    "                                truncation=True,\n",
    "                                max_length=model.config.max_position_embeddings,\n",
    "                                return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2002,  7777,  2000,  4521, 10733,  1999,  1996, 10733, 12570,\n",
      "          1012,   102],\n",
      "        [  101,  2061,  2116, 11746,  2024,  8590,  1999,  2074,  1037, 12654,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "após BatchEncoding {'input_ids': tensor([[  101,  2002,  7777,  2000,  4521, 10733,  1999,  1996, 10733, 12570,\n",
      "          1012,   102],\n",
      "        [  101,  2061,  2116, 11746,  2024,  8590,  1999,  2074,  1037, 12654,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# print(f\"padded_batch {padded_batch}\")\n",
    "encoded_input = BatchEncoding(encoded_input)\n",
    "print('após BatchEncoding',encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2002,  7777,  2000,  4521, 10733,  1999,  1996, 10733, 12570,\n",
      "          1012,   102],\n",
      "        [  101,  2061,  2116, 11746,  2024,  8590,  1999,  2074,  1037, 12654,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Move os dados para o dispositivo especificado (CPU ou GPU)\n",
    "encoded_input_cuda = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**{k:v for k,v in encoded_input_cuda.items() if k != 'special_tokens_mask'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 30522])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_output.logits.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logits.shape[0]== len(sentences_test), f\"logits.shape[0] deveria ser igual a len(sentences_test), mas {logits.shape[0]} != {len(sentences_test)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logits.shape[2]== model.config.vocab_size, f\"logits.shape[2] deveria ser igual model.config.vocab_size, mas {logits.shape[2]} != {model.config.vocab_size}  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logits.shape[1] ==  encoded_input['input_ids'].shape[1], f\"logits.shape[1] ==  encoded_input['input_ids'].shape[1], mas {logits.shape[1]} != {encoded_input['input_ids'].shape[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.3893e+00, -8.2711e+00, -7.5464e+00,  ..., -8.2343e+00, -7.9853e+00, -6.5540e+00],\n",
       "         [-1.0677e+01, -1.0912e+01, -1.0080e+01,  ..., -1.1239e+01, -1.0584e+01, -1.0340e+01],\n",
       "         [-1.4504e+01, -1.3984e+01, -1.2138e+01,  ..., -1.3341e+01, -1.2841e+01, -1.3440e+01],\n",
       "         ...,\n",
       "         [-1.1762e+01, -1.0844e+01, -9.7184e+00,  ..., -1.0512e+01, -9.5783e+00, -1.1487e+01],\n",
       "         [-2.0236e+01, -1.6708e+01, -1.5875e+01,  ..., -1.6253e+01, -1.5220e+01, -1.7133e+01],\n",
       "         [-1.8771e+01, -1.5834e+01, -1.5056e+01,  ..., -1.5429e+01, -1.4388e+01, -1.6011e+01]],\n",
       "\n",
       "        [[-6.0462e+00, -8.0102e+00, -7.5297e+00,  ..., -7.4065e+00, -7.7055e+00, -6.0488e+00],\n",
       "         [-7.0161e+00, -8.8562e+00, -8.1384e+00,  ..., -8.3108e+00, -8.6692e+00, -6.5108e+00],\n",
       "         [-9.4267e+00, -1.0151e+01, -9.3825e+00,  ..., -9.5120e+00, -9.9987e+00, -7.8468e+00],\n",
       "         ...,\n",
       "         [-2.0250e+01, -1.6809e+01, -1.7290e+01,  ..., -1.6032e+01, -1.6992e+01, -1.9700e+01],\n",
       "         [-1.8281e+01, -1.5261e+01, -1.4755e+01,  ..., -1.5178e+01, -1.3872e+01, -1.5943e+01],\n",
       "         [-1.7309e+01, -1.4686e+01, -1.4178e+01,  ..., -1.4615e+01, -1.3293e+01, -1.5177e+01]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_relu = relu(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 30522])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_relu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_nao_zero = torch.nonzero(logits_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([248, 3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_nao_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices não nulos para a amostra 0:\n",
      "tensor([   0,    0, 2002])\n",
      "Indices não nulos para a amostra 1:\n",
      "tensor([   0,    0, 2010])\n"
     ]
    }
   ],
   "source": [
    "for i in range(logits_relu.shape[0]):\n",
    "    indices = indices_nao_zero[i]\n",
    "    print(f\"Indices não nulos para a amostra {i}:\")\n",
    "    print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0, 2002],\n",
      "        [   0,    0, 2010],\n",
      "        [   0,    0, 2032]])\n"
     ]
    }
   ],
   "source": [
    "print(indices_nao_zero[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para a parte 0: tensor([1.4559e+00, 9.9054e-01, 6.2091e-01])\n",
      "Valores para a parte 1: tensor([1.2144e+00, 2.6016e-01, 3.4094e-02])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_splits = 2  # dividir indices_nao_zero em duas partes\n",
    "batch_size = indices_nao_zero.shape[0] // num_splits  # indices_nao_zero.shape[0]\n",
    "\n",
    "splits = torch.split(indices_nao_zero, batch_size, dim=0)\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    # encontre os valores não-zero correspondentes em logits_relu para esta parte de indices_nao_zero\n",
    "    values = logits_relu[split[:, 0], split[:, 1], split[:, 2]]\n",
    "    print(f\"Valores para a parte {i}: {values[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = logits_relu[indices_nao_zero[:, 0], indices_nao_zero[:, 1], indices_nao_zero[:, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([248]) tensor([1.4559e+00, 9.9054e-01, 6.2091e-01])\n"
     ]
    }
   ],
   "source": [
    "print(valores.shape, valores[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando \"sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wj_sum = torch.sum(torch.log(1 + relu(logits)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30522])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "wj_sum_sparse = wj_sum.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([2, 30522]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wj_sum_sparse), wj_sum_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                            0,     0,     0,     0,     0,     0,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                            1,     1,     1],\n",
       "                       [ 1996,  1999,  2002,  2006,  2010,  2012,  2032,  2066,\n",
       "                         2079,  2123,  2215,  2252,  2283,  2359,  2365,  2370,\n",
       "                         2377,  2380,  2567,  2677,  2720,  2767,  2806,  2833,\n",
       "                         2839,  3124,  3309,  3347,  3364,  3531,  3573,  3782,\n",
       "                         3829,  3899,  3942,  4147,  4435,  4497,  4521,  4605,\n",
       "                         4666,  4669,  4713,  4825,  5404,  5440,  5470,  5542,\n",
       "                         5965,  5983,  6240,  6805,  6871,  7668,  7777,  7884,\n",
       "                         8738,  8808,  9065,  9383,  9544,  9841, 10026, 10733,\n",
       "                        11345, 11642, 12570, 15890, 18959, 19782,  1999,  2061,\n",
       "                         2062,  2074,  2116,  2205,  2261,  2338,  2411,  2466,\n",
       "                         2561,  2671,  2726,  2773,  2878,  2904,  3191,  3213,\n",
       "                         3365,  3375,  3432,  3674,  3722,  3752,  3793,  3975,\n",
       "                         4471,  4489,  4706,  4800,  4807,  5290,  5449,  5530,\n",
       "                         6251,  6310,  6412,  6796,  7063,  7065,  7680,  8553,\n",
       "                         8590,  8651,  9185,  9491, 10938, 11091, 11746, 12654,\n",
       "                        14686, 20423, 21743]]),\n",
       "       values=tensor([3.3765e-01, 6.8221e-01, 4.8495e+00, 5.0857e-02,\n",
       "                      3.3637e+00, 3.5866e-01, 1.5645e+00, 1.2889e+00,\n",
       "                      2.9630e-01, 2.5375e-01, 8.0036e-01, 1.5264e+00,\n",
       "                      1.9450e-01, 2.1403e-01, 2.0454e-01, 6.7570e-01,\n",
       "                      4.5247e-02, 1.9230e-01, 1.0644e-01, 1.8475e-01,\n",
       "                      4.3446e-02, 1.0081e-01, 6.0399e-01, 1.2967e+00,\n",
       "                      7.5185e-01, 4.8301e-01, 1.3395e+00, 9.4893e-01,\n",
       "                      3.3028e-01, 4.5783e-02, 7.9688e-02, 8.9580e-02,\n",
       "                      7.0697e-02, 3.7899e-03, 1.0442e+00, 3.2475e-01,\n",
       "                      2.6173e-02, 7.7419e-01, 4.2067e+00, 1.0101e+00,\n",
       "                      6.8446e-02, 1.4382e+00, 1.5299e-02, 5.9321e+00,\n",
       "                      1.8476e-01, 3.2477e-01, 2.1513e-01, 3.0995e-03,\n",
       "                      1.0105e-02, 1.0006e+00, 2.4887e-01, 2.7209e-01,\n",
       "                      5.6824e-01, 1.0535e-01, 1.2577e+00, 3.4756e-01,\n",
       "                      1.8002e-01, 3.2390e-02, 2.0112e-01, 9.5720e-05,\n",
       "                      5.7925e-01, 9.2031e-02, 1.7398e-01, 9.0861e+00,\n",
       "                      2.0686e+00, 5.0254e-01, 3.2271e+00, 1.0596e+00,\n",
       "                      6.9909e-02, 1.0345e-01, 6.5284e-02, 7.1823e-01,\n",
       "                      6.4278e-02, 1.3465e+00, 1.4859e+00, 2.3125e-02,\n",
       "                      1.1274e-01, 1.0177e-01, 1.8514e-01, 1.4240e+00,\n",
       "                      3.2012e-01, 7.9914e-03, 6.0099e-02, 3.3288e-01,\n",
       "                      1.9278e-01, 1.9380e-01, 1.0715e-01, 9.2200e-01,\n",
       "                      6.8181e-02, 8.9910e-01, 4.1734e-01, 4.5196e-01,\n",
       "                      3.5879e-01, 9.8448e-01, 1.7755e+00, 5.9911e-02,\n",
       "                      2.7286e-01, 7.9708e-01, 2.2653e+00, 2.6121e-01,\n",
       "                      9.2732e-03, 1.6598e-01, 2.8045e-01, 6.9186e-01,\n",
       "                      3.9869e+00, 2.5387e-01, 6.3176e-01, 9.1688e-02,\n",
       "                      2.4171e-02, 3.1174e-01, 1.1770e+00, 6.2285e-02,\n",
       "                      1.3529e+00, 1.5620e+00, 2.5692e-01, 1.1482e-01,\n",
       "                      1.6596e+00, 2.7948e-01, 1.8603e+00, 2.6564e+00,\n",
       "                      1.2008e+00, 3.0817e-01, 1.3502e-01]),\n",
       "       size=(2, 30522), nnz=123, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_sum_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([123]), tensor(9.5482e+01))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_sum_sparse._values().shape, wj_sum_sparse._values().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compreendendo tensor esparso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 123])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_sum_sparse._indices().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subwords/tokens de entrada: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de subwords/tokens de entrada: {wj_sum_sparse._indices()[0].max() + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de elementos no vetor esparso: 123\n"
     ]
    }
   ],
   "source": [
    "print(f\"Qtd de elementos no vetor esparso: {wj_sum_sparse._nnz()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 61044, 123)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_sum_sparse.element_size(), wj_sum_sparse.sparse_dim(), wj_sum_sparse.numel(), wj_sum_sparse._nnz()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando economia de espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_vetor_esparso = wj_sum_sparse.element_size() * wj_sum_sparse._nnz()  # calcula o tamanho em bytes do tensor esparso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_vetor_contiguo = wj_sum.element_size() * wj_sum.numel()  # calcula o tamanho em bytes do tensor esparso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória vetor esparso 492 x contíguo 244176 0.201%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memória vetor esparso {mem_vetor_esparso} x contíguo {mem_vetor_contiguo} {round(100*mem_vetor_esparso/mem_vetor_contiguo,3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6]])\n",
    "\n",
    "unique_vals, counts = torch.unique(wj_sum_sparse._indices()[0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict = {val.item(): count.item() for val, count in zip(unique_vals, counts)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 70, 1: 53}\n"
     ]
    }
   ],
   "source": [
    "print(counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dict = {}\n",
    "unique_indices = torch.unique(wj_sum_sparse._indices()[0])\n",
    "for idx in unique_indices:\n",
    "    values_dict[idx.item()] = wj_sum_sparse._indices()[1][wj_sum_sparse._indices()[0] == idx].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1996, 1999, 2002, 2006, 2010, 2012, 2032, 2066, 2079, 2123, 2215, 2252, 2283, 2359, 2365, 2370, 2377, 2380, 2567, 2677, 2720, 2767, 2806, 2833, 2839, 3124, 3309, 3347, 3364, 3531, 3573, 3782, 3829, 3899, 3942, 4147, 4435, 4497, 4521, 4605, 4666, 4669, 4713, 4825, 5404, 5440, 5470, 5542, 5965, 5983, 6240, 6805, 6871, 7668, 7777, 7884, 8738, 8808, 9065, 9383, 9544, 9841, 10026, 10733, 11345, 11642, 12570, 15890, 18959, 19782], 1: [1999, 2061, 2062, 2074, 2116, 2205, 2261, 2338, 2411, 2466, 2561, 2671, 2726, 2773, 2878, 2904, 3191, 3213, 3365, 3375, 3432, 3674, 3722, 3752, 3793, 3975, 4471, 4489, 4706, 4800, 4807, 5290, 5449, 5530, 6251, 6310, 6412, 6796, 7063, 7065, 7680, 8553, 8590, 8651, 9185, 9491, 10938, 11091, 11746, 12654, 14686, 20423, 21743]}\n"
     ]
    }
   ],
   "source": [
    "print(values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 0:He likes to eat pizza in the pizza hut.\n",
      "[1996, 1999, 2002, 2006, 2010, 2012, 2032, 2066, 2079, 2123, 2215, 2252, 2283, 2359, 2365, 2370, 2377, 2380, 2567, 2677, 2720, 2767, 2806, 2833, 2839, 3124, 3309, 3347, 3364, 3531, 3573, 3782, 3829, 3899, 3942, 4147, 4435, 4497, 4521, 4605, 4666, 4669, 4713, 4825, 5404, 5440, 5470, 5542, 5965, 5983, 6240, 6805, 6871, 7668, 7777, 7884, 8738, 8808, 9065, 9383, 9544, 9841, 10026, 10733, 11345, 11642, 12570, 15890, 18959, 19782]\n",
      "['the', 'in', 'he', 'on', 'his', 'at', 'him', 'like', 'do', 'don', 'want', 'club', 'party', 'wanted', 'son', 'himself', 'play', 'park', 'brother', 'mouth', 'mr', 'friend', 'style', 'food', 'character', 'guy', 'hotel', 'bar', 'actor', 'please', 'store', 'chris', 'kitchen', 'dog', 'visit', 'wearing', 'brand', 'shop', 'eat', 'bowl', 'mix', 'liked', 'factory', 'restaurant', 'beer', 'favorite', 'fan', 'cousin', 'fred', 'eating', 'meat', 'bite', 'preferred', 'cafe', 'likes', 'restaurants', 'diet', 'cheese', 'booth', 'mcdonald', 'prefer', 'dish', 'chef', 'pizza', 'pie', 'sandwich', 'hut', 'burger', 'dislike', 'snack'] \n",
      "pos 1:So many sentences are transformed in just a summary.\n",
      "[1999, 2061, 2062, 2074, 2116, 2205, 2261, 2338, 2411, 2466, 2561, 2671, 2726, 2773, 2878, 2904, 3191, 3213, 3365, 3375, 3432, 3674, 3722, 3752, 3793, 3975, 4471, 4489, 4706, 4800, 4807, 5290, 5449, 5530, 6251, 6310, 6412, 6796, 7063, 7065, 7680, 8553, 8590, 8651, 9185, 9491, 10938, 11091, 11746, 12654, 14686, 20423, 21743]\n",
      "['in', 'so', 'more', 'just', 'many', 'too', 'few', 'book', 'often', 'story', 'total', 'science', 'thomas', 'word', 'whole', 'changed', 'read', 'writer', 'numerous', 'complex', 'simply', 'multiple', 'simple', 'reading', 'text', 'split', 'message', 'difference', 'literary', 'multi', 'communication', 'reform', 'translation', 'pages', 'sentence', 'modified', 'description', 'justin', 'madison', 'rev', 'sum', '##bility', 'transformed', 'transformation', 'reflection', 'essay', 'transform', 'citation', 'sentences', 'summary', 'quote', 'paragraph', 'transforms'] \n"
     ]
    }
   ],
   "source": [
    "for pos, list_tokens in values_dict.items():\n",
    "    print(f\"pos {pos}:{sentences_test[pos]}\")\n",
    "    print(f\"{list_tokens}\")\n",
    "    print(f\"{tokenizer.convert_ids_to_tokens(list_tokens)} \")\n",
    "\n",
    "# tokenizer.convert_ids_to_tokens(wj_sparse._indices()[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "wj_max, _ = torch.max(torch.log(1 + relu(logits)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "wj_max_sparse = wj_max.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30522])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_max_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 70, 1: 53}\n"
     ]
    }
   ],
   "source": [
    "# t = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6]])\n",
    "unique_vals, counts = torch.unique(wj_max_sparse._indices()[0], return_counts=True)\n",
    "counts_dict = {val.item(): count.item() for val, count in zip(unique_vals, counts)}\n",
    "print(counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dict = {}\n",
    "unique_indices = torch.unique(wj_max_sparse._indices()[0])\n",
    "for idx in unique_indices:\n",
    "    values_dict[idx.item()] = wj_max_sparse._indices()[1][wj_max_sparse._indices()[0] == idx].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1996, 1999, 2002, 2006, 2010, 2012, 2032, 2066, 2079, 2123, 2215, 2252, 2283, 2359, 2365, 2370, 2377, 2380, 2567, 2677, 2720, 2767, 2806, 2833, 2839, 3124, 3309, 3347, 3364, 3531, 3573, 3782, 3829, 3899, 3942, 4147, 4435, 4497, 4521, 4605, 4666, 4669, 4713, 4825, 5404, 5440, 5470, 5542, 5965, 5983, 6240, 6805, 6871, 7668, 7777, 7884, 8738, 8808, 9065, 9383, 9544, 9841, 10026, 10733, 11345, 11642, 12570, 15890, 18959, 19782], 1: [1999, 2061, 2062, 2074, 2116, 2205, 2261, 2338, 2411, 2466, 2561, 2671, 2726, 2773, 2878, 2904, 3191, 3213, 3365, 3375, 3432, 3674, 3722, 3752, 3793, 3975, 4471, 4489, 4706, 4800, 4807, 5290, 5449, 5530, 6251, 6310, 6412, 6796, 7063, 7065, 7680, 8553, 8590, 8651, 9185, 9491, 10938, 11091, 11746, 12654, 14686, 20423, 21743]}\n"
     ]
    }
   ],
   "source": [
    "print(values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 0:He likes to eat pizza in the pizza hut.\n",
      "[1996, 1999, 2002, 2006, 2010, 2012, 2032, 2066, 2079, 2123, 2215, 2252, 2283, 2359, 2365, 2370, 2377, 2380, 2567, 2677, 2720, 2767, 2806, 2833, 2839, 3124, 3309, 3347, 3364, 3531, 3573, 3782, 3829, 3899, 3942, 4147, 4435, 4497, 4521, 4605, 4666, 4669, 4713, 4825, 5404, 5440, 5470, 5542, 5965, 5983, 6240, 6805, 6871, 7668, 7777, 7884, 8738, 8808, 9065, 9383, 9544, 9841, 10026, 10733, 11345, 11642, 12570, 15890, 18959, 19782]\n",
      "['the', 'in', 'he', 'on', 'his', 'at', 'him', 'like', 'do', 'don', 'want', 'club', 'party', 'wanted', 'son', 'himself', 'play', 'park', 'brother', 'mouth', 'mr', 'friend', 'style', 'food', 'character', 'guy', 'hotel', 'bar', 'actor', 'please', 'store', 'chris', 'kitchen', 'dog', 'visit', 'wearing', 'brand', 'shop', 'eat', 'bowl', 'mix', 'liked', 'factory', 'restaurant', 'beer', 'favorite', 'fan', 'cousin', 'fred', 'eating', 'meat', 'bite', 'preferred', 'cafe', 'likes', 'restaurants', 'diet', 'cheese', 'booth', 'mcdonald', 'prefer', 'dish', 'chef', 'pizza', 'pie', 'sandwich', 'hut', 'burger', 'dislike', 'snack'] \n",
      "pos 1:So many sentences are transformed in just a summary.\n",
      "[1999, 2061, 2062, 2074, 2116, 2205, 2261, 2338, 2411, 2466, 2561, 2671, 2726, 2773, 2878, 2904, 3191, 3213, 3365, 3375, 3432, 3674, 3722, 3752, 3793, 3975, 4471, 4489, 4706, 4800, 4807, 5290, 5449, 5530, 6251, 6310, 6412, 6796, 7063, 7065, 7680, 8553, 8590, 8651, 9185, 9491, 10938, 11091, 11746, 12654, 14686, 20423, 21743]\n",
      "['in', 'so', 'more', 'just', 'many', 'too', 'few', 'book', 'often', 'story', 'total', 'science', 'thomas', 'word', 'whole', 'changed', 'read', 'writer', 'numerous', 'complex', 'simply', 'multiple', 'simple', 'reading', 'text', 'split', 'message', 'difference', 'literary', 'multi', 'communication', 'reform', 'translation', 'pages', 'sentence', 'modified', 'description', 'justin', 'madison', 'rev', 'sum', '##bility', 'transformed', 'transformation', 'reflection', 'essay', 'transform', 'citation', 'sentences', 'summary', 'quote', 'paragraph', 'transforms'] \n"
     ]
    }
   ],
   "source": [
    "for pos, list_tokens in values_dict.items():\n",
    "    print(f\"pos {pos}:{sentences_test[pos]}\")\n",
    "    print(f\"{list_tokens}\")\n",
    "    print(f\"{tokenizer.convert_ids_to_tokens(list_tokens)} \")\n",
    "\n",
    "# tokenizer.convert_ids_to_tokens(wj_sparse._indices()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.4646e+01), tensor(9.5482e+01))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wj_max_sparse._values().sum(), wj_sum_sparse._values().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão dos experimentos\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expansões se equivalem, seja usando max ou sum, pois as 123 tokens (após expansão) são as 123 não zeradas do logits (após relu).\n",
    "O que muda são os valores dos scores das tokens, o que pode influenciar na busca (sum:95, max:54)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_subword_importante(\n",
    "    parm_model: BertForMaskedLM,\n",
    "    parm_tokenizer: BertTokenizer,\n",
    "    parm_sentences: list,\n",
    "    parm_ind_agg: str = 'max'\n",
    "  )  -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Retorna um tensor esparso contendo as subpalavras importantes de cada sentença fornecida.\n",
    "\n",
    "    Args:\n",
    "        parm_model (BertForMaskedLM): O modelo pré-treinado utilizado para geração de embeddings.\n",
    "        parm_tokenizer (BertTokenizer): O tokenizador associado ao modelo pré-treinado.\n",
    "        parm_sentences (list): A lista de sentenças para as quais se deseja obter as subpalavras importantes.\n",
    "        parm_ind_agg (str): O método de agregação de índices para geração do tensor esparso. As opções são:\n",
    "                                            - 'max': para agregar índices por meio do valor máximo naquela posição;\n",
    "                                            - 'sum': para agregar índices por meio da soma naquela posição.\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: O tensor esparso com as subpalavras importantes de cada sentença.\n",
    "    \"\"\"\n",
    "    encoded_input = parm_tokenizer(parm_sentences,\n",
    "                                    add_special_tokens=True,\n",
    "                                    return_special_tokens_mask=True, \n",
    "                                    padding=True, \n",
    "                                    truncation=True,\n",
    "                                    max_length=model.config.max_position_embeddings,\n",
    "                                    return_tensors='pt')\n",
    "\n",
    "\n",
    "\n",
    "    encoded_input = BatchEncoding(encoded_input)\n",
    "    encoded_input_cuda = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        model_output = parm_model(**{k:v for k,v in encoded_input_cuda.items() if k != 'special_tokens_mask'})\n",
    "    logits = model_output.logits.cpu()    \n",
    "    assert logits.shape[0]== len(parm_sentences), f\"logits.shape[0] deveria ser igual a len(sentences_test), mas {logits.shape[0]} != {len(sentences_test)}\"\n",
    "    assert logits.shape[2]== model.config.vocab_size, f\"logits.shape[2] deveria ser igual model.config.vocab_size, mas {logits.shape[2]} != {model.config.vocab_size}  \"\n",
    "    assert logits.shape[1] ==  encoded_input['input_ids'].shape[1], f\"logits.shape[1] ==  encoded_input['input_ids'].shape[1], mas {logits.shape[1]} != {encoded_input['input_ids'].shape[1]}\"\n",
    "    # print(f\"model_output.logits.shape {model_output.logits.shape}\")\n",
    "    if parm_ind_agg == 'max':\n",
    "        wj, _ = torch.max(torch.log(1 + relu(logits)), dim=1)\n",
    "    elif parm_ind_agg == 'sum':\n",
    "        wj = torch.sum(torch.log(1 + relu(logits)), dim=1)\n",
    "    else:\n",
    "        raise Exception(f\"parm_ind_aggregation_method deve ser 'max' ou 'sum', mas passado {parm_ind_aggregation_method}!\")\n",
    "    \n",
    "    return wj.to_sparse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 30522])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_tokens_expansao(parm_expansao: torch.Tensor, \n",
    "                            parm_tokenizer: BertTokenizer,\n",
    "                            parm_sentences: list) -> dict:\n",
    "    \"\"\"\n",
    "    Retorna um dicionário com informações sobre as expansões de tokens para cada sentença.\n",
    "\n",
    "    Args:\n",
    "        parm_expansao (torch.Tensor): Tensor esparso com as expansões de tokens.\n",
    "        parm_tokenizer (BertTokenizer): Tokenizer associado ao modelo BERT utilizado nas expansões.\n",
    "        parm_sentences (list): Lista de sentenças utilizadas para gerar as expansões.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com informações sobre as expansões de tokens para cada sentença. Cada item do dicionário\n",
    "        contém as seguintes chaves:\n",
    "        - 'text': A sentença original.\n",
    "        - 'list_tokens_expanded': Uma lista de tokens correspondente às expansões geradas para a sentença.\n",
    "    \"\"\"\n",
    "    # Verifica se o número de linhas da expansão é igual ao número de sentenças\n",
    "    assert parm_expansao.shape[0] == len(parm_sentences), f\"parm_expansao.shape[0] == len(parm_sentences), mas {parm_expansao.shape[0]} != {len(parm_sentences)}\"\n",
    "\n",
    "    # Cria um dicionário para armazenar os valores únicos presentes na expansão\n",
    "    values_dict = {}\n",
    "    # Obtém os índices únicos da expansão\n",
    "    unique_indices = torch.unique(parm_expansao._indices()[0])\n",
    "    # Para cada índice único, armazena a lista de tokens correspondente\n",
    "    for idx in unique_indices:\n",
    "        values_dict[idx.item()] = parm_expansao._indices()[1][parm_expansao._indices()[0] == idx].tolist()\n",
    "\n",
    "    # Cria um dicionário de retorno com as informações de cada sentença\n",
    "    dict_retorno = {}\n",
    "    for pos, list_tokens in values_dict.items():\n",
    "        dict_retorno[pos] = {'text': parm_sentences[pos], \n",
    "                             'list_tokens_expanded': parm_tokenizer.convert_ids_to_tokens(list_tokens)}\n",
    "    return dict_retorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expande_corpus(parm_model:BertForMaskedLM,\n",
    "                   parm_tokenizer:BertTokenizer, \n",
    "                   parm_sentences:list,\n",
    "                   parm_batch_size:int=64,\n",
    "                   parm_ind_agg:str='max') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gera a expansão de palavras para um conjunto de documentos usando um modelo de linguagem BERT pré-treinado.\n",
    "\n",
    "    Args:\n",
    "    - parm_model: modelo BERT pré-treinado.\n",
    "    - parm_tokenizer: tokenizer associado ao modelo BERT.\n",
    "    - parm_sentences: lista contendo os documentos (em forma de string) a serem expandidos.\n",
    "    - parm_batch_size: tamanho do batch para processamento em lotes. O padrão é 64.\n",
    "    - parm_ind_agg: método de agregação de subpalavras. As opções são: 'max' (valor máximo) ou 'sum' (soma).\n",
    "    O padrão é 'max'.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: matriz esparsa com a representação das subpalavras importantes para cada documento.\n",
    "    \"\"\"\n",
    "    # Matriz esparsa que irá armazenar as subpalavras importantes para cada documento.\n",
    "    expansoes = None  \n",
    "    # Calcula a quantidade de lotes a serem processados.\n",
    "    qtd_bloco = math.ceil(len(parm_sentences) / parm_batch_size)  \n",
    "    # Cria uma barra de progresso para acompanhar o processamento.    \n",
    "    pbar = tqdm(range(qtd_bloco))  \n",
    "    for ndx in range(qtd_bloco):\n",
    "        # Seleciona o conjunto de documentos a serem processados no lote atual.\n",
    "        lista_doctos = [docto['title'] + ' ' + docto['text'] for docto in parm_sentences[ndx*parm_batch_size:ndx*parm_batch_size+parm_batch_size]]\n",
    "        # Chama a função que gera a expansão de subpalavras      \n",
    "        expansoes_batch = retorna_subword_importante(parm_model=parm_model, parm_tokenizer=parm_tokenizer, parm_sentences=lista_doctos, parm_ind_agg=parm_ind_agg)\n",
    "        # Concatena as expansões geradas no lote atual com as expansões já armazenadas na matriz esparsa.\n",
    "        if expansoes is None:\n",
    "            expansoes = expansoes_batch\n",
    "        else:\n",
    "            expansoes = torch.cat( (expansoes, expansoes_batch), dim=0)\n",
    "       \n",
    "        # Atualiza a barra de progresso.\n",
    "        pbar.update(n=1)\n",
    "        pbar.refresh()\n",
    "\n",
    "    # Exibe a mensagem indicando a quantidade de expansões geradas e retorna a matriz esparsa com as subpalavras importantes.\n",
    "    print(f\"Expansões geradas: shape {expansoes.shape}\")\n",
    "    return expansoes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validando função a partir dos experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansao = retorna_subword_importante(model, tokenizer, sentences_test,'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([123]), torch.Size([123]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expansao._values().shape, wj_max_sparse._values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(wj_max_sparse._values(), expansao._values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansao = retorna_subword_importante(model, tokenizer, sentences_test,'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([123]), torch.Size([123]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expansao._values().shape, wj_sum_sparse._values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(wj_sum_sparse._values(), expansao._values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'text': 'He likes to eat pizza in the pizza hut.', 'list_tokens_expanded': ['the', 'in', 'he', 'on', 'his', 'at', 'him', 'like', 'do', 'don', 'want', 'club', 'party', 'wanted', 'son', 'himself', 'play', 'park', 'brother', 'mouth', 'mr', 'friend', 'style', 'food', 'character', 'guy', 'hotel', 'bar', 'actor', 'please', 'store', 'chris', 'kitchen', 'dog', 'visit', 'wearing', 'brand', 'shop', 'eat', 'bowl', 'mix', 'liked', 'factory', 'restaurant', 'beer', 'favorite', 'fan', 'cousin', 'fred', 'eating', 'meat', 'bite', 'preferred', 'cafe', 'likes', 'restaurants', 'diet', 'cheese', 'booth', 'mcdonald', 'prefer', 'dish', 'chef', 'pizza', 'pie', 'sandwich', 'hut', 'burger', 'dislike', 'snack']}, 1: {'text': 'So many sentences are transformed in just a summary.', 'list_tokens_expanded': ['in', 'so', 'more', 'just', 'many', 'too', 'few', 'book', 'often', 'story', 'total', 'science', 'thomas', 'word', 'whole', 'changed', 'read', 'writer', 'numerous', 'complex', 'simply', 'multiple', 'simple', 'reading', 'text', 'split', 'message', 'difference', 'literary', 'multi', 'communication', 'reform', 'translation', 'pages', 'sentence', 'modified', 'description', 'justin', 'madison', 'rev', 'sum', '##bility', 'transformed', 'transformation', 'reflection', 'essay', 'transform', 'citation', 'sentences', 'summary', 'quote', 'paragraph', 'transforms']}}\n"
     ]
    }
   ],
   "source": [
    "dict_retorno = retorna_tokens_expansao(expansao, tokenizer,sentences_test)\n",
    "print(dict_retorno)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expandindo documentos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um subconjunto para testar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:01<00:00,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansoes geradas: shape {expansoes.shape}\n",
      "CPU times: user 4min 12s, sys: 1min 30s, total: 5min 42s\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_expanded = expande_corpus(parm_model=model,\n",
    "                   parm_tokenizer=tokenizer, \n",
    "                   parm_sentences=corpus[:1024],\n",
    "                   parm_batch_size=64,\n",
    "                   parm_ind_agg='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_doctos = [docto['title'] + ' ' + docto['text'] for docto in corpus[:1024]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'list_tokens_expanded': ['%', '.', '3', '5', 'm', 'p', 'w', 'was', 'with', 'at', 'had', 'my', 'have', 'all', 'been', '##e', 'school', 'years', 'most', 'many', 'university', 'because', 'high', 'group', 'found', 'early', 'december', 'age', 'population', 'death', 'king', 'children', 'died', '14', 'published', 'community', 'round', '##z', 'record', 'case', 'present', 'total', 'records', '1998', 'common', 'similar', '1997', 'yes', 'result', 'child', 'schools', 'features', 'china', '##ma', '40', 'japan', 'hospital', 'data', 'groups', 'fall', '##as', 'breath', 'african', 'africa', 'prince', 'culture', 'die', 'review', 'baby', 'upper', 'associated', 'feature', 'rate', 'cultural', 'results', 'likely', 'spring', '##co', 'pre', 'spread', 'chart', 'sign', 'acquired', 'percent', 'positive', '33', 'israel', 'iran', 'kids', 'communities', 'quarter', '75', 'disease', '##mi', 'reviews', 'identified', 'falling', 'publishing', 'methods', '##ial', 'proved', 'ep', 'patients', '##ide', 'egypt', '##ul', 'affected', 'kings', 'breathing', 'concluded', 'universities', '##ise', 'iraq', 'patient', 'treated', 'prove', 'frequency', 'exposed', 'charts', 'rates', '82', 'characteristics', 'clinical', 'identify', 'recovered', '##ology', 'conclusion', 'recovery', 'isolated', '##ties', 'nigeria', 'affect', 'sultan', 'acquisition', 'survival', 'quarters', 'syria', 'objective', 'virus', 'shah', 'symptoms', 'saudi', 'reviewed', 'characteristic', 'hospitals', '##ella', 'cultures', 'casualties', '##mp', 'healing', 'lungs', 'recover', 'infection', '##com', 'fever', 'arabia', 'clinic', 'admission', 'hiv', 'morocco', 'specimens', 'proven', 'im', 'abdul', 'underlying', 'bacteria', 'infected', 'infant', 'fatal', 'babies', 'heal', 'objectives', 'lung', 'specimen', 'dubai', 'toxic', 'transmitted', 'isolation', 'qatar', '##nch', 'micro', 'complications', 'tract', 'haiti', 'traits', 'dose', '##cos', 'mortality', 'cr', '##ep', '##itis', 'arabian', 'yemen', 'abdullah', 'similarity', '##cci', 'somalia', '##rb', '##itated', 'infections', 'retrospective', 'probable', 'tuberculosis', '##azi', 'epidemic', 'respiratory', 'fungus', 'infants', 'bacterial', 'vaccine', 'tsar', 'uneven', '##dah', '##itation', 'pneumonia', 'como', 'malaria', 'cough', 'abd', 'pl', '##iol', '##hee', 'parasite', 'coughing', '##plication', '##rani', '##biology', '##rom', '##pha', 'tracts', 'retro', 'bro', '##mun', 'preschool', '##oco', '##bola', 'jed', 'influenza', '##tf', '##pl', 'cholera', '##biotic', 'cdc', 'pathogen', 'isolate', '##ccus', '##idi', 'hepatitis', 'mala']}\n"
     ]
    }
   ],
   "source": [
    "dict_retorno = retorna_tokens_expansao(corpus_expanded,\n",
    "                                       tokenizer, \n",
    "                                       lista_doctos)\n",
    "print(dict_retorno[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expandindo todo o corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 43.13\n",
      " used 22.96\n",
      " free 24.07\n",
      " cached 18.98\n",
      " buffers 1.34\n",
      "/nGPU\n",
      "Sun Apr 23 16:54:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 45%   49C    P8    26W / 370W |  19022MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A     53767      C   .../relevar-busca/bin/python    11453MiB |\n",
      "|    0   N/A  N/A    209856      C   ...treinapython39/bin/python     7509MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [2:55:18<00:00,  3.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansoes geradas: shape {expansoes.shape}\n",
      "CPU times: user 12h 16min 50s, sys: 4h 22min 20s, total: 16h 39min 11s\n",
      "Wall time: 2h 55min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_expanded = expande_corpus(parm_model=model,\n",
    "                   parm_tokenizer=tokenizer, \n",
    "                   parm_sentences=corpus,\n",
    "                   parm_batch_size=64,\n",
    "                   parm_ind_agg='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime RAM in gb: \n",
      " total 67.35\n",
      " available 42.73\n",
      " used 23.43\n",
      " free 23.29\n",
      " cached 19.27\n",
      " buffers 1.36\n",
      "/nGPU\n",
      "Sun Apr 23 19:49:56 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 82%   71C    P2   174W / 370W |  19022MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1245      G   /usr/lib/xorg/Xorg                 46MiB |\n",
      "|    0   N/A  N/A      1384      G   /usr/bin/gnome-shell                9MiB |\n",
      "|    0   N/A  N/A     53767      C   .../relevar-busca/bin/python    11453MiB |\n",
      "|    0   N/A  N/A    209856      C   ...treinapython39/bin/python     7509MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostra_memoria(['cpu','gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171332"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([171332, 30522])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMINHO_ARQUIVO_EXPANSAO = f\"{DIRETORIO_TRABALHO}/expansao_corpus_max.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAMINHO_ARQUIVO_EXPANSAO, 'wb') as outputFile:\n",
    "    pickle.dump(corpus_expanded, outputFile, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando as buscas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os dados salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAMINHO_ARQUIVO_EXPANSAO, \"rb\") as f:\n",
    "  corpus_expanded = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_tensor_unitario_em_tokens(parm_tokenizer:BertTokenizer, parm_texto:str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Função que recebe um tokenizer e um parm_ e retorna um tensor com um único valor 1.0 em cada posição\n",
    "    correspondente aos tokens do texto.\n",
    "\n",
    "    :param parm_tokenizer: objeto BertTokenizer, responsável por tokenizar o texto\n",
    "    :type parm_tokenizer: BertTokenizer\n",
    "    :param texto: texto a ser tokenizado\n",
    "    :type texto: str\n",
    "    :return: tensor com um único valor 1.0 em cada posição correspondente aos tokens do texto\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(parm_texto, str) or ( isinstance(parm_texto, list) and  isinstance(parm_texto[0], str)   ) , f\"parm_texto deve ser str ou lista com uma string\"\n",
    "    # tokeniza o texto e obtém a lista de tokens\n",
    "    lista_tokens = parm_tokenizer(parm_texto,\n",
    "                                    add_special_tokens=False,\n",
    "                                    return_special_tokens_mask=False, \n",
    "                                    padding=False, \n",
    "                                    truncation=True,\n",
    "                                    max_length=tokenizer.max_len_single_sentence,\n",
    "                                    return_tensors='pt')['input_ids']\n",
    "    \n",
    "\n",
    "    # cria um tensor vazio com o tamanho do vocabulário\n",
    "    tensor = torch.zeros(parm_tokenizer.vocab_size, dtype=torch.float32)\n",
    "\n",
    "    # define os valores 1.0 nas posições correspondentes aos índices dos tokens na lista\n",
    "    tensor[lista_tokens] = 1.0\n",
    "\n",
    "    return tensor.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_queries_busca_exaustiva(parm_model:BertForMaskedLM,\n",
    "                                    parm_tokenizer:BertTokenizer, \n",
    "                                    parm_dict_queries:{},\n",
    "                                    parm_corpus_expanded, \n",
    "                                    parm_lista_doc_id_passage:list,\n",
    "                                    parm_num_max_hits:int=1000,\n",
    "                                    parm_se_query_expansion:bool=False,\n",
    "                                    parm_ind_agg_se_query_expansion:str=None):\n",
    "    assert parm_ind_agg_se_query_expansion and parm_se_query_expansion or not parm_se_query_expansion, \\\n",
    "        f\"se parm_se_query_expansion {parm_se_query_expansion} for True, deve-se informar parm_ind_agg_se_query_expansion {parm_ind_agg_se_query_expansion}\"\n",
    "    tempos = []\n",
    "    with open(CAMINHO_RUN, 'w') as runfile:\n",
    "        for cnt, (query_id, value) in enumerate(parm_dict_queries.items()):\n",
    "            # print(id, value)\n",
    "            tempo_inicio = time.time()\n",
    "            if parm_se_query_expansion:\n",
    "                query_scores = retorna_subword_importante(parm_model=parm_model,\n",
    "                                                parm_tokenizer=parm_tokenizer,\n",
    "                                                parm_sentences=[value['question']], \n",
    "                                                parm_ind_agg=parm_ind_agg_se_query_expansion).to_dense().squeeze()\n",
    "            else:\n",
    "                query_scores = retorna_tensor_unitario_em_tokens(parm_tokenizer, value['question'])\n",
    "\n",
    "            # Pega os primeiros 1000 resultados\n",
    "            score = torch.matmul(parm_corpus_expanded, query_scores)\n",
    "            # Ordena\n",
    "            sorted_score, indices_score = torch.sort(score, descending=True)\n",
    "\n",
    "            # parm_num_max_hits primeiros\n",
    "            sorted_score = sorted_score[0:parm_num_max_hits]\n",
    "            indices_score = indices_score[0:parm_num_max_hits]\n",
    "\n",
    "            if cnt % 5 == 0:\n",
    "                print(f'{cnt} queries completadas')\n",
    "\n",
    "            # ids dos documentos\n",
    "            ids_docs = [parm_lista_doc_id_passage[i] for i in indices_score]\n",
    "\n",
    "            tempos.append(time.time() - tempo_inicio)\n",
    "\n",
    "            #  query  q0     docid  rank     score    descr    \n",
    "            for i, (id_doc, score) in enumerate(zip(ids_docs, sorted_score)):\n",
    "                texto_docto = f'{query_id} Q0 {id_doc} {i+1} {float(score):.6f} Pesquisa\\n'\n",
    "                _ = runfile.write(texto_docto)\n",
    "    return tempos\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLADE misto (max e com expansão para query com max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 queries completadas\n",
      "5 queries completadas\n",
      "10 queries completadas\n",
      "15 queries completadas\n",
      "20 queries completadas\n",
      "25 queries completadas\n",
      "30 queries completadas\n",
      "35 queries completadas\n",
      "40 queries completadas\n",
      "45 queries completadas\n",
      "CPU times: user 1min 58s, sys: 1.39 s, total: 2min\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tempo_gasto = run_all_queries_busca_exaustiva(parm_model=model,\n",
    "                                    parm_tokenizer=tokenizer, \n",
    "                                    parm_dict_queries=topics,\n",
    "                                    parm_corpus_expanded=corpus_expanded, \n",
    "                                    parm_lista_doc_id_passage=lista_doc_id_passage,\n",
    "                                    parm_num_max_hits=1000,\n",
    "                                    parm_se_query_expansion=True,\n",
    "                                    parm_ind_agg_se_query_expansion='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.0000000\n",
       "mean      0.4290435\n",
       "std       0.0191951\n",
       "min       0.4104669\n",
       "25%       0.4166406\n",
       "50%       0.4214497\n",
       "75%       0.4349711\n",
       "max       0.5177207\n",
       "Name: tempo_gasto, Length: 8, dtype: float64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tempos = pd.DataFrame({'tempo_gasto': tempo_gasto})\n",
    "df_tempos['tempo_gasto'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAHHCAYAAABz3mgLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUe0lEQVR4nO3dd1gU1/4/8PfSlrYLoiBSRexdbNduYkE02K4xsUSsXBWvUa4aTWJBv0qiscUYUXMtiVE0ikpiFzX2xIa9N2TFLh0B2fP7wx97XSkCLgyzvl/PMw/umTNzPnN2cT+cOTOjEEIIEBEREZVyJlIHQERERFQQTFqIiIhIFpi0EBERkSwwaSEiIiJZYNJCREREssCkhYiIiGSBSQsRERHJApMWIiIikgUmLURERCQLTFpIMtOmTYNCoSiRttq2bYu2bdvqXh84cAAKhQIbN240WBt37tyBQqHAqlWrCr3txo0bYW9vjxYtWuD69esIDAzEggULDBZbfhQKBaZNm1YibdH/lOTn3xiNGzcOKpUKAQEBePbsGWrWrIno6Gipw6JixqSFDGLVqlVQKBS6xdLSEi4uLvD19cX333+PpKQkg7Rz//59TJs2zej+c5o9ezYCAwNRoUIFVK9eHREREejevbvUYRVK9pfw25bXk0cynDd/B99cjh8/LnWIBpOcnIwlS5Zg+vTpuHjxIsqVKwdbW1vUrVtX6tComJlJHQAZl+nTp8PLywuZmZl48OABDhw4gDFjxmDevHmIjIzU+0/l66+/xsSJEwu1//v37yMkJAQVK1ZE/fr1C7zd7t27C9VOUXh6eiItLQ3m5uaF3va3336Dq6srzMzM8PjxY6hUKlhaWhZDlMWnZ8+eqFy5su51cnIyRowYgR49eqBnz5668vLly0sR3nsj+3fwTa+/N3JnaWmJS5cuwdPTE2PHjsX9+/fh7OwMExP+HW7smLSQQfn5+aFRo0a615MmTcK+ffvw0UcfoWvXrrh8+TKsrKwAAGZmZjAzK96PYGpqKqytrWFhYVGs7QDQjTAVhaenp+7fjo6OhgqpRNWtW1cvKX3y5AlGjBiBunXron///hJG9n5583fQGJmZmen9zri4uEgYDZUkpqVU7D788ENMnjwZd+/exZo1a3TluZ3T37NnD1q2bAl7e3vY2tqiWrVq+PLLLwG8mofSuHFjAMCgQYN0w97Zc0jatm2L2rVr49SpU2jdujWsra112745pyVbVlYWvvzySzg7O8PGxgZdu3bFvXv39OpUrFgRAwcOzLHtm/vMa07LlStX0Lt3bzg6OsLKygrVqlXDV199pVt/+/ZtjBgxAlWrVoWVlRXKli2Ljz/+GHfu3MnR5q1bt/Dxxx/DwcEB1tbW+Mc//oFt27blqJeb9PR0jB07Fo6OjlCpVOjatStiY2NzravRaDB48GCUL18eSqUStWrVwooVKwrUzttcuXIFvXr1goODAywtLdGoUSNERkbq1ck+1XH48GGMHj0ajo6OsLe3x7/+9S9kZGQgPj4eAwYMQJkyZVCmTBlMmDABrz+wPvu9+O677zB//nx4enrCysoKbdq0wYULF3LEtG/fPrRq1Qo2Njawt7dHt27dcPnyZb06SUlJGDNmDCpWrAilUgknJyd06NABp0+ffusxHz58GI0bN4alpSW8vb2xdOnSPOuuWbMGDRs2hJWVFRwcHPDpp5/m+Ey+i6lTp8LExARRUVF65YGBgbCwsMDZs2d1ZX/99Rc6deoEOzs7WFtbo02bNjhy5EiOfWo0GgwZMgQuLi5QKpXw8vLCiBEjkJGRASDv+TvZ7/Obn/UdO3agTZs2UKlUUKvVaNy4MdauXatbf+DAAfTq1QseHh5QKpVwd3fH2LFjkZaWlqONgry3JB8caaES8dlnn+HLL7/E7t27MWzYsFzrXLx4ER999BHq1q2L6dOnQ6lU4saNG7r/JGvUqIHp06djypQpCAwMRKtWrQAAzZs31+3j6dOn8PPzw6effor+/fu/9VTEzJkzoVAo8MUXX+DRo0dYsGAB2rdvj+joaN2I0Ls4d+4cWrVqBXNzcwQGBqJixYq4efMmfv/9d8ycORPAqy+GY8eOoU+fPnBzc8Pt27cRFhaGtm3b4tKlS7C2tgYAPHz4EM2bN0dqaipGjx6NsmXLYvXq1ejatSs2btyIHj165BvL0KFDsWbNGvTt2xfNmzfHvn370KVLlxz1Hj58iH/84x9QKBQYNWoUHB0dsWPHDgwZMgSJiYkYM2ZMkfvj4sWLaNGiBVxdXTFx4kTY2Nhgw4YN6N69OzZt2pTjGP7973/D2dkZISEhOH78OJYtWwZ7e3scPXoUHh4emDVrFrZv3445c+agdu3aGDBggN72P//8M5KSkhAUFIQXL15g4cKF+PDDD3H+/HndZ2Pv3r3w8/NDpUqVMG3aNKSlpWHRokVo0aIFTp8+jYoVKwIAhg8fjo0bN2LUqFGoWbMmnj59isOHD+Py5cvw8fHJ85jPnz+Pjh07wtHREdOmTcPLly8xderUXD+bM2fOxOTJk9G7d28MHToUjx8/xqJFi9C6dWucOXMG9vb2b+3jhIQEPHnyRK9MoVCgbNmyAF6dlv39998xZMgQnD9/HiqVCrt27cLy5csxY8YM1KtXD8CrL3s/Pz80bNhQl+isXLkSH374IQ4dOoQmTZoAeHXKtkmTJoiPj0dgYCCqV68OjUaDjRs3IjU1tdCjnKtWrcLgwYNRq1YtTJo0Cfb29jhz5gx27tyJvn37AgA2bNiAtLQ0jBw5Eg4ODvj777+xaNEixMbG4rffftPtq6DvLcmIIDKAlStXCgDixIkTedaxs7MTDRo00L2eOnWqeP0jOH/+fAFAPH78OM99nDhxQgAQK1euzLGuTZs2AoAICwvLdV2bNm10r/fv3y8ACFdXV5GYmKgr37BhgwAgFi5cqCvz9PQUAQEBb93n7du3c8TWunVroVKpxN27d/W21Wq1un+npqbm2PexY8cEAPHzzz/rysaMGSMAiEOHDunKkpKShJeXl6hYsaLIysrKsZ9s0dHRAoAYOXKkXnnfvn0FADF16lRd2ZAhQ0SFChXEkydP9Op++umnws7OLtd4c/P48eMc+27Xrp2oU6eOePHiha5Mq9WK5s2biypVqujKsj9Pvr6+en3VrFkzoVAoxPDhw3VlL1++FG5ubrm+F1ZWViI2NlZX/tdffwkAYuzYsbqy+vXrCycnJ/H06VNd2dmzZ4WJiYkYMGCArszOzk4EBQUV6Nhf1717d2Fpaan3Gbh06ZIwNTXV+/zfuXNHmJqaipkzZ+ptf/78eWFmZpaj/E3ZfZbbolQqc+zTwsJCDB06VDx//ly4urqKRo0aiczMTCHEq/ekSpUqOfo/NTVVeHl5iQ4dOujKBgwYIExMTHL93c/e9s3f9Tdjvn37thBCiPj4eKFSqUTTpk1FWlparvsSQoiUlJQc+woNDRUKhUKvnwv63pJ88PQQlRhbW9t8ryLK/ity69at0Gq1RWpDqVRi0KBBBa4/YMAAqFQq3etevXqhQoUK2L59e5Haf93jx49x8OBBDB48GB4eHnrrXh8qf31EJzMzE0+fPkXlypVhb2+vd+ph+/btaNKkCVq2bKkrs7W1RWBgIO7cuYNLly7lGUv28YwePVqv/M1REyEENm3aBH9/fwgh8OTJE93i6+uLhISEAp0Oyc2zZ8+wb98+9O7dG0lJSbr9Pn36FL6+vrh+/To0Go3eNkOGDNHrq6ZNm0IIgSFDhujKTE1N0ahRI9y6dStHm927d4erq6vudZMmTdC0aVNdf8TFxSE6OhoDBw6Eg4ODrl7dunXRoUMHvc+Bvb09/vrrL9y/f7/Ax5yVlYVdu3ahe/fuep+BGjVqwNfXV69uREQEtFotevfurdfvzs7OqFKlCvbv31+gNhcvXow9e/boLTt27NCrU7t2bYSEhOCnn36Cr68vnjx5gtWrV+vmmEVHR+P69evo27cvnj59qoslJSUF7dq1w8GDB6HVaqHVarFlyxb4+/vnOo+msJd079mzB0lJSZg4cWKO+WGv7yt79BEAUlJS8OTJEzRv3hxCCJw5cwZA4d5bkg+eHqISk5ycDCcnpzzXf/LJJ/jpp58wdOhQTJw4Ee3atUPPnj3Rq1evAl8V4OrqWqjh6CpVqui9VigUqFy5cq7zSQor+0u0du3a+dZLS0tDaGgoVq5cCY1Gozc3IyEhQffvu3fvomnTpjm2r1Gjhm59Xm3dvXsXJiYm8Pb21iuvVq2a3uvHjx8jPj4ey5Ytw7Jly3Ld16NHj/I9nrzcuHEDQghMnjwZkydPznPfrycZbyZ7dnZ2AAB3d/cc5c+fP8+xvzffXwCoWrUqNmzYAOBVvwA5+wF41a+7du1CSkoKbGxsMHv2bAQEBMDd3R0NGzZE586dMWDAAFSqVCnPY378+DHS0tJyjaNatWp6X5zXr1+HECLXugAKfFVakyZNCjQRd/z48QgPD8fff/+NWbNmoWbNmnqxAEBAQECe2yckJCAjIwOJiYlv/YwX1M2bNwG8/XcmJiYGU6ZMQWRkZI73Pft3pjDvLckHkxYqEbGxsUhISMj3sksrKyscPHgQ+/fvx7Zt27Bz506sX78eH374IXbv3g1TU9O3tmOIeShvyuuvxaysrALF9Db//ve/sXLlSowZMwbNmjWDnZ0dFAoFPv300yKPOBVVdnv9+/fP8wurqPfCyN73uHHjcowyZHvz85FX/+ZW/nqyVxx69+6NVq1aYfPmzdi9ezfmzJmDb7/9FhEREfDz83vn/Wu1WigUCuzYsSPX47O1tX3nNl5369YtXXJy/vz5HLEAwJw5c/K8tYCtrS2ePXtWoLby+x0qrKysLHTo0AHPnj3DF198gerVq8PGxgYajQYDBw4s8d8ZKllMWqhE/PLLLwCQ55dVNhMTE7Rr1w7t2rXDvHnzMGvWLHz11VfYv38/2rdvb/A7iGb/p51NCIEbN27ofTGXKVMG8fHxOba9e/duvn9lZ6/L7WqV123cuBEBAQGYO3euruzFixc52vT09MTVq1dzbH/lyhXd+rx4enpCq9Xi5s2ben95vrm/7CuLsrKy0L59+3zjLqzs/jA3Nzf4vvPy5vsLANeuXdNNwMzus7z6tVy5cnp/iVeoUAEjR47EyJEj8ejRI/j4+GDmzJl5Ji3ZV4zlFsebbXp7e0MIAS8vL1StWrXAx1gUWq0WAwcOhFqtxpgxYzBr1iz06tVLdz+d7BE5tVqd73vl6OgItVr91s94mTJlAADx8fF6k4mzR0OyZbd74cKFPP/AOX/+PK5du4bVq1frTbzes2ePXr3CvrckD5zTQsVu3759mDFjBry8vNCvX7886+X2V1v2X3np6ekAoPtPJrckoiiyry7JtnHjRsTFxel9CXl7e+P48eO6yzcB4I8//njrZaiOjo5o3bo1VqxYgZiYGL11r48KmJqa5hglWLRoUY6/Qjt37oy///4bx44d05WlpKRg2bJlqFixot7w/puyj+f777/XK3/zUQGmpqb45z//iU2bNuX6RfT48eM823gbJycntG3bFkuXLkVcXJxB952XLVu26M2T+fvvv/HXX3/p+qNChQqoX78+Vq9erfeZunDhAnbv3o3OnTsDePXX/eun6rKPx8XFRffZzI2pqSl8fX2xZcsWvc/A5cuXsWvXLr26PXv2hKmpKUJCQnJ8HoQQePr0aeEOPh/z5s3D0aNHsWzZMsyYMQPNmzfHiBEjdFcdNWzYEN7e3vjuu++QnJycY/vs98rExATdu3fH77//jpMnT+aol30c2cnIwYMHdetSUlKwevVqvfodO3aESqVCaGgoXrx4keu+skehXu8jIQQWLlyoV7+g7y3JC0dayKB27NiBK1eu4OXLl3j48CH27duHPXv2wNPTE5GRkfnefG369Ok4ePAgunTpAk9PTzx69Ag//vgj3NzcdJNPvb29YW9vj7CwMKhUKtjY2KBp06a53gG0IBwcHNCyZUsMGjQIDx8+xIIFC1C5cmW9y7KHDh2KjRs3olOnTujduzdu3ryJNWvW5Jgfkpvvv/8eLVu2hI+PDwIDA+Hl5YU7d+5g27ZtukcRfPTRR/jll19gZ2eHmjVr4tixY9i7d6/uEtVsEydOxLp16+Dn54fRo0fDwcEBq1evxu3bt7Fp06Z85/3Ur18fffr0wY8//oiEhAQ0b94cUVFRuHHjRo6633zzDfbv34+mTZti2LBhqFmzJp49e4bTp09j7969BT4lkJvFixejZcuWqFOnDoYNG4ZKlSrh4cOHOHbsGGJjY/XuEWIIlStXRsuWLTFixAikp6djwYIFKFu2LCZMmKCrM2fOHPj5+aFZs2YYMmSI7rJYOzs73TOZkpKS4Obmhl69eqFevXqwtbXF3r17ceLECb0RstyEhIRg586daNWqFUaOHImXL19i0aJFqFWrFs6dO6er5+3tjf/7v//DpEmTcOfOHXTv3h0qlQq3b9/G5s2bERgYiHHjxr31mLN/B9/UvHlzVKpUCZcvX8bkyZMxcOBA+Pv7A3h1mXH9+vUxcuRIbNiwASYmJvjpp5/g5+eHWrVqYdCgQXB1dYVGo8H+/fuhVqvx+++/AwBmzZqF3bt3o02bNggMDESNGjUQFxeH3377DYcPH4a9vT06duwIDw8PDBkyBOPHj4epqSlWrFgBR0dHvWROrVZj/vz5GDp0KBo3boy+ffuiTJkyOHv2LFJTU7F69WpUr14d3t7eGDduHDQaDdRqNTZt2pTrnKaCvLckMyV9uRIZpzcvt7SwsBDOzs6iQ4cOYuHChXqXFWd78zLIqKgo0a1bN+Hi4iIsLCyEi4uL6NOnj7h27Zredlu3bhU1a9YUZmZmepcYt2nTRtSqVSvX+PK65HndunVi0qRJwsnJSVhZWYkuXbrkuDxZCCHmzp0rXF1dhVKpFC1atBAnT54s0CXPQghx4cIF0aNHD6FWqwUAUa1aNTF58mTd+ufPn4tBgwaJcuXKCVtbW+Hr6yuuXLmS66XWN2/eFL169RL29vbC0tJSNGnSRPzxxx+5HvOb0tLSxOjRo0XZsmWFjY2N8Pf3F/fu3ctxWbIQQjx8+FAEBQUJd3d3YW5uLpydnUW7du3EsmXLCtSWELlf8px9DAMGDBDOzs7C3NxcuLq6io8++khs3LhRVyevS+izPzNvXhYfEBAgbGxsdK+z34s5c+aIuXPnCnd3d6FUKkWrVq3E2bNnc8S6d+9e0aJFC2FlZSXUarXw9/cXly5d0q1PT08X48ePF/Xq1RMqlUrY2NiIevXqiR9//LFAffHnn3+Khg0bCgsLC1GpUiURFhaW52XAmzZtEi1bthQ2NjbCxsZGVK9eXQQFBYmrV6/m20Z+lzxnfy5fvnwpGjduLNzc3ER8fLze9gsXLhQAxPr163VlZ86cET179hRly5YVSqVSeHp6it69e4uoqCi9be/evSsGDBggHB0dhVKpFJUqVRJBQUEiPT1dV+fUqVOiadOmwsLCQnh4eIh58+bluOQ5W2RkpGjevLku9iZNmoh169bp1l+6dEm0b99e2NrainLlyolhw4aJs2fP5vr797b3luRFIUQxz14jIp327dtjwoQJ6Nixo9ShGLU7d+7Ay8sLc+bMKdDoBJVOSUlJurtclytXTupwqBTgnBaiEuTv76/3KAMiyptKpYKPj0+OxzzQ+4tzWohKwLp165CSkoLffvst33vVENEr3333HVQqFY4fP44PPvhA6nColGDSQlQCLl68iO+++w4VKlTA7NmzpQ6HqNT7448/cOzYMTRo0ED3zCEizmkhIiIiWeCcFiIiIpIFJi1EREQkC7Kf06LVanH//n2oVCqD3+KdiIiIiocQAklJSXBxcSnwQ3Fln7Tcv38/xxNfiYiISB7u3bsHNze3AtWVfdKiUqkAvDpotVotcTTSq/5DdcQlxaGCqgKujMp5K+/iD6A6EBcHVKgA5HIrcSIiIgBITEyEu7u77nu8IGSftGSfElKr1UxaAEzznYbkjGTYWthK0x/TpgHJyYCtLcD3g4iI3qIwUztkf8lzYmIi7OzskJCQwKSFiIhIJory/c2rh4iIiEgWmLQQERGRLMh+Tgvpi0uKQ5bIgqnCFBVUFSQIIA7IygJMTV9NxiWiUiErKwuZmZlSh0HvEXNzc5iamhp0n0xajEzj5Y2hSdLAVeWK2OBYCQJoDGg0gKsrECtB+0SkRwiBBw8eID4+XupQ6D1kb28PZ2dng91HjUkLEZERy05YnJycYG1tzZtwUokQQiA1NRWPHj0CAFQw0Mg7kxYiIiOVlZWlS1jKli0rdTj0nrGysgIAPHr0CE5OTgY5VcSJuERERip7Dou1tbXEkdD7KvuzZ6j5VExaiIiMHE8JkVQM/dlj0kJEREZn4cKFOHbsmNRhkIFJmrRUrFgRCoUixxIUFCRlWEREJGNz585FREQEfHx88q134MABKBQK3ZVVq1atgr29ffEHSEUmadJy4sQJxMXF6ZY9e/YAAD7++GMpwyIiIokNHDgQCoUCw4cPz7EuKCgICoUCAwcOzLHuyJEj+OWXX7B161YolcpCtfnJJ5/g2rVrRQ3ZYA4cOAAfHx8olUpUrlwZq1atKvC2N27cgEqlypF8RUREoFGjRrC3t4eNjQ3q16+PX375xbCBlwBJkxZHR0c4Ozvrlj/++APe3t5o06aNlGEREVEp4O7ujvDwcKSlpenKXrx4gbVr18LDwyPXbVq0aIHo6OgijZhYWVnBycmpqOEaxO3bt9GlSxd88MEHiI6OxpgxYzB06FDs2rXrrdtmZmaiT58+aNWqVY51Dg4O+Oqrr3Ds2DGcO3cOgwYNwqBBgwq039Kk1MxpycjIwJo1azB48GBOGiMiIvj4+MDd3R0RERG6soiICHh4eKBBgwZ6dbVaLUJDQ+Hl5QUrKyvUq1cPGzdu1Kuzfft2VK1aFVZWVvjggw9w584dvfVvnh66efMmunXrhvLly8PW1haNGzfG3r17DX6crwsLC4OXlxfmzp2LGjVqYNSoUejVqxfmz5//1m2//vprVK9eHb17986xrm3btujRowdq1KgBb29vfP7556hbty4OHz5cHIdRbEpN0rJlyxbEx8fnOtz3uvT0dCQmJuot9D9RA6JwYcQFRA2IkiiAKODChVc/iYje0eDBg7Fy5Urd6xUrVmDQoEE56oWGhuLnn39GWFgYLl68iLFjx6J///74888/AQD37t1Dz5494e/vj+joaAwdOhQTJ07Mt+3k5GR07twZUVFROHPmDDp16gR/f3/ExMTkuc2hQ4dga2ub7/Lrr7/muf2xY8fQvn17vTJfX9+3Tiret28ffvvtNyxevDjfesCrG79FRUXh6tWraN269Vvrlyal5uZy//3vf+Hn5wcXF5d864WGhiIkJKSEojKMadMMU6cgqpWrZpgdFTkAidsnooKZN+/V8jY+PkBkpH5Z167A6dNv3zY4+NXyDvr3749Jkybh7t27AF7NWQkPD8eBAwd0ddLT0zFr1izs3bsXzZo1AwBUqlQJhw8fxtKlS9GmTRssWbIE3t7emDt3LgCgWrVqOH/+PL799ts8265Xrx7q1aunez1jxgxs3rwZkZGRGDVqVK7bNGrUCNHR0fkeU/ny5fNc9+DBgxzry5cvj8TERKSlpelu2Pa6p0+fYuDAgVizZg3UanWe+05ISICrqyvS09NhamqKH3/8ER06dMg31tKmVCQtd+/exd69e/WGAPMyadIkBL/2S5CYmAh3d/fiDI+IyPgkJr56Ttjb5Pb/6+PHBdvWACPhjo6O6NKlC1atWgUhBLp06YJy5crp1blx4wZSU1NzfAFnZGToTiNdvnwZTZs21VufneDkJTk5GdOmTcO2bdsQFxeHly9fIi0tLd+RFisrK1SuXLkwh/jOhg0bhr59+7511ESlUiE6OhrJycmIiopCcHAwKlWqhLZt25ZMoAZQKpKWlStXwsnJCV26dHlrXaVSWegZ4URE9Aa1+tWDTd/G0TH3soJsm89f/YUxePBg3chGbqc/kpOTAQDbtm2D6xtxvcv3xbhx47Bnzx589913qFy5MqysrNCrVy9kZGTkuc2hQ4fg5+eX736XLl2Kfv365brO2dkZDx8+1Ct7+PAh1Gp1rqMswKtTQ5GRkfjuu+8AvDr9o9VqYWZmhmXLlmHw4MEAABMTE11CVb9+fVy+fBmhoaFMWgpDq9Vi5cqVCAgIgJmZ5OHI3trza5GamQprc2v0rdNXggDWAqmpgLU10FeC9omoYN7l1M2bp4uKWadOnZCRkQGFQgFfX98c62vWrAmlUomYmJg8rz6tUaMGIt+I+/jx4/m2e+TIEQwcOBA9evQA8Co5enPy7pve9fRQs2bNsH37dr2yPXv25DsqdOzYMWRlZeleb926Fd9++y2OHj2aI4l7nVarRXp6er6xljaSZwl79+5FTEyMLhOkdzNhzwRokjRwVblKk7RMmPBq2NjVlUkLERmEqakpLl++rPv3m1QqFcaNG4exY8dCq9WiZcuWSEhIwJEjR6BWqxEQEIDhw4dj7ty5GD9+PIYOHYpTp0699f4nVapUQUREBPz9/aFQKDB58mRotdp8t3nX00PDhw/HDz/8gAkTJmDw4MHYt28fNmzYgG3btunq/PDDD9i8eTOi/v8FDzVq1NDbx8mTJ2FiYoLatWvrykJDQ9GoUSN4e3sjPT0d27dvxy+//IIlS5YUOVYpSJ60dOzYEUIIqcMgIqJSLL8JpsCrSbKOjo4IDQ3FrVu3YG9vDx8fH3z55ZcAAA8PD2zatAljx47FokWL0KRJE8yaNSvfP5jnzZuHwYMHo3nz5ihXrhy++OKLYr9i1cvLC9u2bcPYsWOxcOFCuLm54aefftIbYXry5Alu3rxZqP2mpKRg5MiRiI2NhZWVFapXr441a9bgk08+MfQhFCuFkHnGkJiYCDs7OyQkJLz1Qy2Vkrx6yG2em26kJTY41jA7LVQAbv8baYmVoH0i0nnx4gVu374NLy8vWFpaSh0OvYfy+wwW5fu71NynhYiIiCg/TFqIiIhIFpi0EBERkSwwaSEiIiJZYNJCREREssCkhYiIiGRB8vu0kGE52zrr/Sz5AJz1fxIRERkIkxYjczLwpMQBSNw+EREZLZ4eIiIiIllg0kJERO+lAwcOQKFQID4+HgCwatUq2NvbSxoT5Y9JCxERlToDBw6EQqHA8OHDc6wLCgqCQqHAwIEDDdrmJ598gmvXrhl0n1J59uwZ+vXrB7VaDXt7ewwZMgTJyckF2lYIAT8/PygUCmzZskVXfvbsWfTp0wfu7u6wsrJCjRo1sHDhwmI6gtxxTouR+dfv/8KzF8/gYOmApf5LJQjgX8CzZ4CDA7BUgvaJyGi4u7sjPDwc8+fPh5WVFYBXz7JZu3YtPDw8DN6elZWVrh2569evH+Li4rBnzx5kZmZi0KBBCAwMxNq1a9+67YIFC6BQKHKUnzp1Ck5OTlizZg3c3d1x9OhRBAYGwtTUFKNGjSqOw8iBIy1GZtv1bdh4aSO2Xd/29srFEsA2YOPGVz+JiN6Bj48P3N3dERERoSuLiIiAh4cHGjRooFdXq9UiNDQUXl5esLKyQr169bBx40a9Otu3b0fVqlVhZWWFDz74AHfu3NFb/+bpoZs3b6Jbt24oX748bG1t0bhxY+zdu1dvm4oVK+qeFq1SqeDh4YFly5bp1fniiy9QtWpVWFtbo1KlSpg8eTIyMzPfoWfyd/nyZezcuRM//fQTmjZtipYtW2LRokUIDw/H/fv38902Ojoac+fOxYoVK3KsGzx4MBYuXIg2bdqgUqVK6N+/PwYNGqT3/hQ3Ji1ERFRqDR48GCtXrtS9XrFiBQYNGpSjXmhoKH7++WeEhYXh4sWLGDt2LPr3748///wTAHDv3j307NkT/v7+iI6OxtChQzFx4sR8205OTkbnzp0RFRWFM2fOoFOnTvD390dMTIxevblz56JRo0Y4c+YMRo4ciREjRuDq1au69SqVCqtWrcKlS5ewcOFCLF++HPPnz8+37Vq1asHW1jbPxc/PL89tjx07Bnt7ezRq1EhX1r59e5iYmOCvv/7Kc7vU1FT07dsXixcvhnMBb1uRkJAABweHAtU1BJ4eIiJ6D807Ng/zjs17az2fCj6I7BOpV9Z1XVecjjv91m2DmwUjuFlwkWMEgP79+2PSpEm4e/cuAODIkSMIDw/HgQMHdHXS09Mxa9Ys7N27F82aNQMAVKpUCYcPH8bSpUvRpk0bLFmyBN7e3pg7dy4AoFq1ajh//jy+/fbbPNuuV68e6tWrp3s9Y8YMbN68GZGRkXqnQzp37oyRI0cCeDWqMn/+fOzfvx/VqlUDAHz99de6uhUrVsS4ceMQHh6OCRMm5Nn29u3b8x2Nye801oMHD+Dk5KRXZmZmBgcHBzx48CDP7caOHYvmzZujW7duedZ53dGjR7F+/XpsK8GRdSYtRETvocT0RGiSNG+t527nnqPscerjAm2bmJ5YpNhe5+joiC5dumDVqlUQQqBLly4oV66cXp0bN24gNTUVHTp00CvPyMjQnUa6fPkymjZtqrc+O8HJS3JyMqZNm4Zt27YhLi4OL1++RFpaWo6Rlrp16+r+rVAo4OzsjEePHunK1q9fj++//x43b95EcnIyXr58CbVanW/bnp6e+a43tMjISOzbtw9nzpwpUP0LFy6gW7dumDp1Kjp27FjM0f0PkxYioveQWqmGq8r1rfUcrR1zLSvItmpl/l/MBTV48GDdyMbixYtzrM++Kmbbtm1wddWPS6lUFrndcePGYc+ePfjuu+9QuXJlWFlZoVevXsjIyNCrZ25urvdaoVBAq9UCeHWqpl+/fggJCYGvry/s7OwQHh6uG/HJS61atXSjS7lp1aoVduzYkeu6N5MmAHj58iWePXuW52mfffv24ebNmzku+f7nP/+JVq1a6Y1sXbp0Ce3atUNgYKDeKFJJYNJCRPQeepdTN2+eLipunTp1QkZGBhQKBXx9fXOsr1mzJpRKJWJiYtCmTZtc91GjRg1ERurHffz48XzbPXLkCAYOHIgePXoAeJUcvTl5922OHj0KT09PfPXVV7qy/JKRbO9yeqhZs2aIj4/HqVOn0LBhQwCvkhKtVptjtCnbxIkTMXToUL2yOnXqYP78+fD399eVXbx4ER9++CECAgIwc+bMtx6HoTFpISKiUs3U1BSXL1/W/ftNKpUK48aNw9ixY6HVatGyZUskJCTgyJEjUKvVCAgIwPDhwzF37lyMHz8eQ4cOxalTp7Bq1ap8261SpQoiIiLg7+8PhUKByZMn60ZQCqpKlSqIiYlBeHg4GjdujG3btmHz5s1v3e5dTg/VqFEDnTp1wrBhwxAWFobMzEyMGjUKn376KVxcXAAAGo0G7dq1w88//4wmTZrA2dk511EYDw8PeHl5AXh1SujDDz+Er68vgoODdfNjTE1N4eiYc0SuOPDqISIiKvXUanW+80BmzJiByZMnIzQ0VPelvW3bNt0XroeHBzZt2oQtW7agXr16CAsLw6xZs/Jtc968eShTpgyaN28Of39/+Pr6wsfHp1Bxd+3aFWPHjsWoUaNQv359HD16FJMnTy7UPori119/RfXq1dGuXTt07twZLVu21LsUOzMzE1evXkVqamqB97lx40Y8fvwYa9asQYUKFXRL48aNi+MQcqUQQogSa60YJCYmws7ODgkJCW+d2CSVadMMU6cg3Oa5QZOkgavKFbHBsYbZaaECcAM0GsDVFYiVoH0i0nnx4gVu374NLy8vWFpaSh0OvYfy+wwW5fubp4eMTJ/affD8xXOUsSwjUQB9gOfPgTIStU9EREaLSYuRmdNxjsQBSNw+EREZLc5pISIiIllg0kJERESywKSFiMjIyfx6C5IxQ3/2mLQYmeo/VIc6VI3qP1SXKIDqgFr96icRSSr7Tq2FuayVyJCyP3tv3jW4qDgR18gkZyQjKSMJyRnJEgWQDCQlvfpJRJIyNTWFvb297pbu1tbWUCgUEkdF7wMhBFJTU/Ho0SPY29vnelPAomDSQkRkxLLvcvrms2iISoK9vX2ezzsqCiYtRERGTKFQoEKFCnBycsr3WTZEhmZubm6wEZZsTFqIiN4DpqamBv8CISppnIhLREREssCkhYiIiGSBSQsRERHJApMWIiIikgUmLURERCQLvHrIyIR9FIa0zDRYmVtJFEAYkJYGWEnUPhERGS0mLUbmo6ofSRyAxO0TEZHR4ukhIiIikgXJkxaNRoP+/fujbNmysLKyQp06dXDy5EmpwyIiIqJSRtLTQ8+fP0eLFi3wwQcfYMeOHXB0dMT169dRpkwZKcOStVP3TyEjKwMWphZo6NJQggBOARkZgIUF0FCC9omIyGhJmrR8++23cHd3x8qVK3VlXl5eEkYkf93Cu0GTpIGryhWxwbESBNAN0GgAV1cgVoL2iYjIaEl6eigyMhKNGjXCxx9/DCcnJzRo0ADLly/Pd5v09HQkJibqLURERGT8JB1puXXrFpYsWYLg4GB8+eWXOHHiBEaPHg0LCwsEBATkuk1oaChCQkJKONK8TZsmdQRERETvB0lHWrRaLXx8fDBr1iw0aNAAgYGBGDZsGMLCwvLcZtKkSUhISNAt9+7dK8GIiYiISCqSJi0VKlRAzZo19cpq1KiBmJiYPLdRKpVQq9V6CxERERk/SZOWFi1a4OrVq3pl165dg6enp0QRERERUWkladIyduxYHD9+HLNmzcKNGzewdu1aLFu2DEFBQVKGRURERKWQpElL48aNsXnzZqxbtw61a9fGjBkzsGDBAvTr10/KsIiIiKgUkvzZQx999BE+4vNqiIiI6C0kv40/ERERUUFIPtJChnU56DIEBBRQSBTAZUAIQCFR+0REZLSYtBgZlVIlcQASt09EREaLp4eIiIhIFpi0EBERkSzw9JCRmXdsHhLTE6FWqhHcLFiCAOYBiYmAWg0ES9A+EREZLSYtRmbesXnQJGngqnKVLmnRaABXVyYtRERkUDw9RERERLLApIWIiIhkgUkLERERyQKTFiIiIpIFJi1EREQkC0xaiIiISBaYtBAREZEsMGkhIiIiWeDN5YyMTwUfuNu5w9HaUaIAfAB3d8BRovaJiMhoMWkxMpF9IiUOQOL2iYjIaPH0EBEREckCkxYiIiKSBSYtREREJAuc02Jkuq7risepj+Fo7SjN/JauXYHHj19NxOX8FiIiMiAmLUbmdNxpaJI0cFW5ShTAaUCjAVwlap+IiIwWTw8RERGRLDBpISIiIllg0kJERESywKSFiIiIZIFJCxEREckCkxYiIiKSBSYtREREJAtMWoiIiEgWeHM5IxPcLBiJ6YlQK9USBRAMJCYCaonaJyIio8WkxcgENwuWOACJ2yciIqPF00NEREQkC0xaiIiISBZ4esjIJKUnQUBAAQVUSpUEASQBQgAKBaCSoH0iIjJaHGkxMjUW14DdN3aosbiGRAHUAOzsXv0kIiIyICYtREREJAtMWoiIiEgWmLQQERGRLEiatEybNg0KhUJvqV69upQhERERUSkl+dVDtWrVwt69e3WvzcwkD4mIiIhKIckzBDMzMzg7O0sdBhEREZVyks9puX79OlxcXFCpUiX069cPMTEx+dZPT09HYmKi3kJERETGT9KRlqZNm2LVqlWoVq0a4uLiEBISglatWuHChQtQ5XFjstDQUISEhJRwpKXDtGkFqMTnFBIRkZGSdKTFz88PH3/8MerWrQtfX19s374d8fHx2LBhQ57bTJo0CQkJCbrl3r17JRgxERERSUXyOS2vs7e3R9WqVXHjxo086yiVSiiVyhKMSl62froVGVkZsDC1kCiArUBGBmAhUftERGS0SlXSkpycjJs3b+Kzzz6TOhTZaujSUOIAJG6fiIiMlqSnh8aNG4c///wTd+7cwdGjR9GjRw+YmpqiT58+UoZFREREpZCkIy2xsbHo06cPnj59CkdHR7Rs2RLHjx+Ho6OjlGERERFRKSRp0hIeHi5l80bpj2t/IC0zDVbmVvio6kcSBPAHkJYGWFkBH0nQPhERGa1SNaeF3t3wP4ZDk6SBq8oVscGxEgQwHNBoAFdXIFaC9omIyGhJfnM5IiIiooJg0kJERESywKSFiIiIZIFJCxEREckCkxYiIiKSBSYtREREJAtMWoiIiEgWmLQQERGRLDBpMTK2FrZQWahga2ErUQC2gEr16icREZEB8Y64RubKqCsSByBx+0REZLQ40kJERESywKSFiIiIZIFJCxEREckC57QYmfG7x+P5i+coY1kGczrOkSCA8cDz50CZMsAcCdonIiKjxaTFyKy7sA6aJA1cVa7SJC3r1gEaDeDqyqSFiIgMiqeHiIiISBaYtBAREZEsMGkhIiIiWWDSQkRERLLApIWIiIhkgUkLERERyQKTFiIiIpIFJi1EREQkC7y5nJHpUqULnr14BgdLB4kC6AI8ewY4SNQ+EREZLSYtRmap/1KJA5C4fSIiMlrvlLScPHkSGzZsQExMDDIyMvTWRUREvFNgRERERK8r8pyW8PBwNG/eHJcvX8bmzZuRmZmJixcvYt++fbCzszNkjERERERFT1pmzZqF+fPn4/fff4eFhQUWLlyIK1euoHfv3vDw8DBkjERERERFT1pu3ryJLl26AAAsLCyQkpIChUKBsWPHYtmyZQYLkAqn0bJGcJvnhkbLGkkUQCPAze3VTyIiIgMqctJSpkwZJCUlAQBcXV1x4cIFAEB8fDxSU1MNEx0V2oPkB9AkafAg+YFEATwANJpXP4mIiAyoyBNxW7dujT179qBOnTr4+OOP8fnnn2Pfvn3Ys2cP2rVrZ8gYiYiIiIqetPzwww948eIFAOCrr76Cubk5jh49in/+85/4+uuvDRYgEREREfAOSYvDazcPMzExwcSJEw0SEBEREVFuCpW0JCYmQq1W6/6dn+x6RERERIZQqKSlTJkyiIuLg5OTE+zt7aFQKHLUEUJAoVAgKyvLYEESERERFSpp2bdvn+600P79+4slICIiIqLcFCppadOmTa7/JiIiIipuRb5Py8qVK/Hbb7/lKP/tt9+wevXqdwqKiIiI6E1FTlpCQ0NRrly5HOVOTk6YNWtWkfb5zTffQKFQYMyYMUUN6703u8NsLPdfjtkdZksUwGxg+fJXP4mIiAyoyJc8x8TEwMvLK0e5p6cnYmJiCr2/EydOYOnSpahbt25RQyIAfev0lTgAidsnIiKjVeSRFicnJ5w7dy5H+dmzZ1G2bNlC7Ss5ORn9+vXD8uXLUaZMmaKGREREREasyElLnz59MHr0aOzfvx9ZWVnIysrCvn378Pnnn+PTTz8t1L6CgoLQpUsXtG/fvqjhEBERkZEr8umhGTNm4M6dO2jXrh3MzF7tRqvVYsCAAYWa0xIeHo7Tp0/jxIkTBaqfnp6O9PR03eu33eTufXP1yVW81L6EmYkZqpWrJkEAV4GXLwEzM6CaBO0TEZHRKnLSYmFhgfXr12PGjBk4e/YsrKysUKdOHXh6ehZ4H/fu3cPnn3+OPXv2wNLSskDbhIaGIiQkpKhhl1rTphlmP+1+bgdNkgauKlfEBscaZqeFCqDdq6c8u7oCsRK0T0RERkshhBBSNb5lyxb06NEDpqamurKsrCwoFAqYmJggPT1dbx2Q+0iLu7s7EhISJHl0gKGSDUP5Se0mbdLi5sakhYiI3ioxMRF2dnaF+v4u8khLVlYWVq1ahaioKDx69AharVZv/b59+966j3bt2uH8+fN6ZYMGDUL16tXxxRdf5EhYAECpVEKpVBY1bCIiIpKpIictn3/+OVatWoUuXbqgdu3auT6H6G1UKhVq166tV2ZjY4OyZcvmKCciIqL3W5GTlvDwcGzYsAGdO3c2ZDxEREREuXqnibiVK1c2ZCwAgAMHDhh8n0RERCR/Rb5Py3/+8x8sXLgQEs7jJSIiovdIkUdaDh8+jP3792PHjh2oVasWzM3N9dZHRES8c3BERERE2YqctNjb26NHjx6GjIWIiIgoT0VOWlauXGnIOIiIiIjyVeSkBQBevnyJAwcO4ObNm+jbty9UKhXu378PtVoNW1tbQ8VIhXBi2AlkiSyYKnLe46ZkAjgBZGUBudxjh4iI6F0UOmnRarUwMTHB3bt30alTJ8TExCA9PR0dOnSASqXCt99+i/T0dISFhRVHvPQWFVQVJA5A4vaJiMhoFerqofPnz6N169YAXt1crlGjRnj+/DmsrKx0dXr06IGoqCjDRklERETvvQKPtGzcuBHTp0/HmjVrAACHDh3C0aNHYWFhoVevYsWK0Gg0ho2SiIiI3nsFTlq0Wq3uYYavv35TbGwsVCqV4SKkQll2ahmSM5Jha2GLwIaBEgSwDEhOBmxtgUAJ2iciIqNVqKc8nz59GkFBQTh27Bh69+4Ne3t7LFu2DCqVCufOnYOjoyO6desGDw+PEru6qChPiTQkPuX5DXzKMxERFUCxP+XZx8cHhw4dAgDMmzcPvr6+qFmzJl68eIG+ffvi+vXrKFeuHNatW1f46ImIiIjyUeirh8zMXm3i5uaGs2fPIjw8HOfOnUNycjKGDBmCfv366U3MJSIiIjKEd7pPi5mZGfr372+oWIiIiIjyVOSk5eeff853/YABA4q6ayIiIqIcipy0fP7553qvMzMzkZqaCgsLC1hbWzNpISIiIoMq1M3lXvf8+XO9JTk5GVevXkXLli05EZeIiIgMrshJS26qVKmCb775JscoDBEREdG7MmjSAryanHv//n1D75aIiIjec0We0xIZGan3WgiBuLg4/PDDD2jRosU7B0ZFU7VsVdhZ2qG8TXmJAqgK2NkB5SVqn4iIjFaRk5bu3bvrvVYoFHB0dMSHH36IuXPnvmtcVET7AvZJHIDE7RMRkdEqctKi1WoNGQcRERFRvgw+p4WIiIioOBR5pCU4OLjAdefNm1fUZoiIiIgAvEPScubMGZw5cwaZmZmoVq0aAODatWswNTWFj4+Prp5CoXj3KKnA+kX0w5PUJyhnXQ6/9vxVggD6AU+eAOXKAb9K0D4RERmtIict/v7+UKlUWL16NcqUKQPg1Q3nBg0ahFatWuE///mPwYKkgvvzzp/QJGngqnKVKIA/AY0GcJWofSIiMlpFntMyd+5chIaG6hIWAChTpgz+7//+j1cPERERkcEVOWlJTEzE48ePc5Q/fvwYSUlJ7xQUERER0ZuKnLT06NEDgwYNQkREBGJjYxEbG4tNmzZhyJAh6NmzpyFjJCIiIir6nJawsDCMGzcOffv2RWZm5qudmZlhyJAhmDNnjsECJCIiIgLeIWmxtrbGjz/+iDlz5uDmzZsAAG9vb9jY2BgsOCIiIqJs73xzubi4OMTFxaFKlSqwsbGBEMIQcRERERHpKXLS8vTpU7Rr1w5Vq1ZF586dERcXBwAYMmQIL3cmIiIigyty0jJ27FiYm5sjJiYG1tbWuvJPPvkEO3fuNEhwRERERNmKPKdl9+7d2LVrF9zc3PTKq1Spgrt3775zYFQ0w3yGISE9AXZKO4kCGAYkJAB2ErVPRERGq8hJS0pKit4IS7Znz55BqVS+U1BUdFPbTpU4AInbJyIio1Xk00OtWrXCzz//rHutUCig1Woxe/ZsfPDBBwYJjoiIiChbkUdaZs+ejXbt2uHkyZPIyMjAhAkTcPHiRTx79gxHjhwxZIxERERERR9pqV27Nq5du4aWLVuiW7duSElJQc+ePXHmzBl4e3sbMkYiIiKioo20ZGZmolOnTggLC8NXX31l6JjoHbjNc9M95Tk2OFaCANz+95TnWAnaJyIio1WkkRZzc3OcO3fO0LEQERER5anIp4f69++P//73v+/U+JIlS1C3bl2o1Wqo1Wo0a9YMO3bseKd9EhERkXEq8kTcly9fYsWKFdi7dy8aNmyY45lD8+bNe+s+3Nzc8M0336BKlSoQQmD16tXo1q0bzpw5g1q1ahU1NCIiIjJChU5abt26hYoVK+LChQvw8fEBAFy7dk2vjkKhKNC+/P399V7PnDkTS5YswfHjx5m0EBERkZ5CJy1VqlRBXFwc9u/fD+DVbfu///57lC9f/p0CycrKwm+//YaUlBQ0a9Ysz3rp6elIT0/XvU5MTHyndomIiEgeCp20vPkU5x07diAlJaXIAZw/fx7NmjXDixcvYGtri82bN6NmzZp51g8NDUVISEiR26OSNW1aydUpCEPth4iISl6RJ+JmezOJKaxq1aohOjoaf/31F0aMGIGAgABcunQpz/qTJk1CQkKCbrl37947tU9ERETyUOiRFoVCkWPOSkHnsOTGwsIClStXBgA0bNgQJ06cwMKFC7F06dJc6yuVSj7biIiI6D1UpNNDAwcO1CUOL168wPDhw3NcPRQREVGkgLRard6cFSIiIiKgCElLQECA3uv+/fsXufFJkybBz88PHh4eSEpKwtq1a3HgwAHs2rWryPt8363puQbpL9OhNJNoNGrNGiA9HeBoGBERGVihk5aVK1carPFHjx5hwIABiIuLg52dHerWrYtdu3ahQ4cOBmvjfdO2YluJA5C4fSIiMlpFvrmcIbzrHXWJiIjo/fHOVw8RERERlQRJR1rI8A7cOaCb0yLJqaIDB/43p4WnioiIyICYtBiZ/hH9oUnSwFXlitjgWAkC6A9oNICrKxArQftERGS0eHqIiIiIZIFJCxEREckCkxYiIiKSBSYtREREJAtMWoiIiEgWmLQQERGRLDBpISIiIllg0kJERESywKSFiIiIZIF3xDUyktwFVy8A3gWXiIiKB0daiIiISBaYtBAREZEsMGkhIiIiWeCcFiMTciAECekJsFPaYWrbqRIEEAIkJAB2dsBUCdonIiKjxaTFyCw/vRyaJA1cVa7SJC3LlwMaDeDqyqSFiIgMiqeHiIiISBaYtBAREZEsMGkhIiIiWWDSQkRERLLApIWIiIhkgUkLERERyQKTFiIiIpIFJi1EREQkC7y5nJFpU7ENnqQ+QTnrchIF0AZ48gQoJ1H7RERktJi0GJlfe/4qcQASt09EREaLp4eIiIhIFpi0EBERkSwwaSEiIiJZ4JwWI/Ph6g/xMOUhytuUx76AfRIE8CHw8CFQvjywT4L2iYjIaDFpMTLXnl6DJkmDhBcJEgVwDdBogASJ2iciIqPF00NEREQkC0xaiIiISBaYtBAREZEsMGkhIiIiWWDSQkRERLLApIWIiIhkQdKkJTQ0FI0bN4ZKpYKTkxO6d++Oq1evShkSERERlVKSJi1//vkngoKCcPz4cezZsweZmZno2LEjUlJSpAyLiIiISiFJby63c+dOvderVq2Ck5MTTp06hdatW0sUlbxNaTMFyRnJsLWwlSiAKUByMmArUftERGS0StUdcRP+/11UHRwc8qyTnp6O9PR03evExMRij0tOAhsGShyAxO0TEZHRKjVJi1arxZgxY9CiRQvUrl07z3qhoaEICQkpkZimTSuRZgzKUDEXZD9y7J+CeJ+PvaSxr4moMErN1UNBQUG4cOECwsPD8603adIkJCQk6JZ79+6VUIREREQkpVIx0jJq1Cj88ccfOHjwINzc3PKtq1QqoVQqSygy+UlCHASyoIApVKhQ4u3bJsXBRGRBqzBFsqrk2yciIuMladIihMC///1vbN68GQcOHICXl5eU4RiF5WiMJIUGKuGKYMSWePuByxtDnaRBosoV84JLvn0iIjJekiYtQUFBWLt2LbZu3QqVSoUHDx4AAOzs7GBlZSVlaERERFTKSDqnZcmSJUhISEDbtm1RoUIF3bJ+/XopwyIiIqJSSPLTQ0REREQFUWquHiIiIiLKD5MWIiIikgUmLURERCQLTFqIiIhIFpi0EBERkSwwaSEiIiJZKBW38SfDGYAoaMVLmEj01q4eEAUT7UtoTfjRIiIiw+I3i5Eph2qStv+0nLTtExGR8eLpISIiIpIFJi1EREQkCzw9ZGTOYy0ykQpzWKMO+pZ4+3XOr4V5Zioyza1xvk7Jt09ERMaLSYuR2YMJSFJooBKukiQtHfZMgDpJg0SVK5MWIiIyKJ4eIiIiIllg0kJERESywKSFiIiIZIFJCxEREckCkxYiIiKSBSYtREREJAtMWoiIiEgWmLQQERGRLPDmckbGFs6A+P8/JZBs66z3k4iIyFCYtBiZQJyUtP1lgdK2T0RExounh4iIiEgWmLQQERGRLDBpISIiIlngnBYj8zv+hRd4Bks4wB9LS7z9j37/F6xePEOapQP+8C/59omIyHgxaTEy17ENSQoNVMJVkvarXt8GdZIGiSpp2iciIuPF00NEREQkC0xaiIiISBaYtBAREZEsMGkhIiIiWWDSQkRERLLApIWIiIhkgUkLERERyQKTFiIiIpIF3lzOyNRGH7wQz2GJMpK0f752H1i9eI40S2naJyIi48Wkxch0xBxJ29/TUdr2iYjIePH0EBEREckCkxYiIiKSBcmTloMHD8Lf3x8uLi5QKBTYsmWL1CERERFRKSR50pKSkoJ69eph8eLFUodiFH5AdYRCjR9QXZL2R/1QHZNC1Rj1gzTtExGR8ZJ8Iq6fnx/8/PykDsNoZCAZGYokZIhkSdq3yEiGMiMJ6RnStE9ERMZL8qSlsNLT05Genq57nZiYKGE0REREVFJkl7SEhoYiJCRE6jCM3rRpxtmWoRgyZjkef0EY6rgKsh9j7UOi4ia33y/J57QU1qRJk5CQkKBb7t27J3VIREREVAJkN9KiVCqhVCqlDoOIiIhKmOxGWoiIiOj9JPlIS3JyMm7cuKF7ffv2bURHR8PBwQEeHh4SRkZERESlieRJy8mTJ/HBBx/oXgcHBwMAAgICsGrVKomiIiIiotJG8qSlbdu2EEJIHQYRERGVcpInLWRYHyEMmSIN5rCSpP0/PgqDWWYaXppL0z4RERkvJi1Gpio+krT9a1WlbZ+IiIwXrx4iIiIiWWDSQkRERLLA00NG5j5OIQsZMIUFXNCwxNuvcP8UTLMykGVqgTiXkm+fiIiMF5MWIxOObkhSaKASrghGbIm33ye8G9RJGiSqXDEvuOTbJyIi48XTQ0RERCQLTFqIiIhIFpi0EBERkSwwaSEiIiJZYNJCREREssCkhYiIiGSBSQsRERHJApMWIiIikgUmLURERCQLvCOukQnCZUAIAApJ2v8h6DIUEBAStU9ERMaLSYuRUUIlafsZSmnbJyIi48XTQ0RERCQLTFqIiIhIFnh6yMgcwzykIxFKqNEMwSXefrNj86BMT0S6Uo1jzUq+fSIiMl5MWozMMcxDkkIDlXCVLGlRJ2mQqHJl0kJERAbF00NEREQkC0xaiIiISBaYtBAREZEsMGkhIiIiWWDSQkRERLLApIWIiIhkgUkLERERyQKTFiIiIpIF3lzOyFSAD+yEO6zhKEn7cRV8kGDnjlRradonIiLjxaTFyPRBpKTtr+sjbftERGS8eHqIiIiIZIFJCxEREckCkxYiIiKSBc5pMTLr0BWpeAxrOEoyv6XPuq6wTn2MVGtHzm8hIiKDYtJiZOJwGkkKDVTCVZL2K8SdhjpJg0SVNO0TEZHx4ukhIiIikgUmLURERCQLTFqIiIhIFkpF0rJ48WJUrFgRlpaWaNq0Kf7++2+pQyIiIqJSRvKkZf369QgODsbUqVNx+vRp1KtXD76+vnj06JHUoREREVEpInnSMm/ePAwbNgyDBg1CzZo1ERYWBmtra6xYsULq0IiIiKgUkTRpycjIwKlTp9C+fXtdmYmJCdq3b49jx45JGBkRERGVNpLep+XJkyfIyspC+fLl9crLly+PK1eu5LpNeno60tPTda8TEhIAAImJiQaP77VmZENAq/uZDsP3ydskCq3uZ3p6ybf/NgX5mJT0+14MH91SoST70Vj7kKi4FeT3tLh+v7K/t4UQBd9ISEij0QgA4ujRo3rl48ePF02aNMl1m6lTpwoAXLhw4cKFCxcjWO7du1fgvEHSkZZy5crB1NQUDx8+1Ct/+PAhnJ2dc91m0qRJCA4O1r3WarV49uwZypYtC4VCUazxliaJiYlwd3fHvXv3oFarpQ5H9tifhsX+NDz2qWGxPw2rKP0phEBSUhJcXFwK3I6kSYuFhQUaNmyIqKgodO/eHcCrJCQqKgqjRo3KdRulUgmlUqlXZm9vX8yRll5qtZq/cAbE/jQs9qfhsU8Ni/1pWIXtTzs7u0LtX/JnDwUHByMgIACNGjVCkyZNsGDBAqSkpGDQoEFSh0ZERESliORJyyeffILHjx9jypQpePDgAerXr4+dO3fmmJxLRERE7zfJkxYAGDVqVJ6ngyh3SqUSU6dOzXGqjIqG/WlY7E/DY58aFvvTsEqqPxVCFOZaIyIiIiJpSH5HXCIiIqKCYNJCREREssCkhYiIiGSBSQsRERHJApOWUmTx4sWoWLEiLC0t0bRpU/z9998F2i48PBwKhUJ3gz4AyMzMxBdffIE6derAxsYGLi4uGDBgAO7fv19M0Zc+huzPNw0fPhwKhQILFiwwTLAyUBz9efnyZXTt2hV2dnawsbFB48aNERMTY+DISydD92dycjJGjRoFNzc3WFlZoWbNmggLCyuGyEunwvTnqlWroFAo9BZLS0u9OkIITJkyBRUqVICVlRXat2+P69evF/dhlBqG7E+Dfh8V+cFBZFDh4eHCwsJCrFixQly8eFEMGzZM2Nvbi4cPH+a73e3bt4Wrq6to1aqV6Natm648Pj5etG/fXqxfv15cuXJFHDt2TDRp0kQ0bNiwmI+kdDB0f74uIiJC1KtXT7i4uIj58+cbPvhSqDj688aNG8LBwUGMHz9enD59Wty4cUNs3br1rfs0BsXRn8OGDRPe3t5i//794vbt22Lp0qXC1NRUbN26tRiPpHQobH+uXLlSqNVqERcXp1sePHigV+ebb74RdnZ2YsuWLeLs2bOia9euwsvLS6SlpZXEIUnK0P1pyO8jJi2lRJMmTURQUJDudVZWlnBxcRGhoaF5bvPy5UvRvHlz8dNPP4mAgIA8v2Sz/f333wKAuHv3rqHCLrWKqz9jY2OFq6uruHDhgvD09Hxvkpbi6M9PPvlE9O/fv7hCLtWKoz9r1aolpk+frlfm4+MjvvrqK4PGXhoVtj9Xrlwp7Ozs8tyfVqsVzs7OYs6cObqy+Ph4oVQqxbp16wwWd2ll6P7MTVG/j3h6qBTIyMjAqVOn0L59e12ZiYkJ2rdvj2PHjuW53fTp0+Hk5IQhQ4YUqJ2EhAQoFAqjf1ZTcfWnVqvFZ599hvHjx6NWrVoGj7u0Ko7+1Gq12LZtG6pWrQpfX184OTmhadOm2LJlS3EcQqlSXJ/P5s2bIzIyEhqNBkII7N+/H9euXUPHjh0NfgylSVH7Mzk5GZ6ennB3d0e3bt1w8eJF3brbt2/jwYMHevu0s7ND06ZN892nMSiO/sxNUb+PmLSUAk+ePEFWVlaORxeUL18eDx48yHWbw4cP47///S+WL19eoDZevHiBL774An369DH6h4MVV39+++23MDMzw+jRow0ab2lXHP356NEjJCcn45tvvkGnTp2we/du9OjRAz179sSff/5p8GMoTYrr87lo0SLUrFkTbm5usLCwQKdOnbB48WK0bt3aoPGXNkXpz2rVqmHFihXYunUr1qxZA61Wi+bNmyM2NhYAdNsVZp/Gojj6803v8n1UKm7jT4WTlJSEzz77DMuXL0e5cuXeWj8zMxO9e/eGEAJLliwpgQjlpSD9eerUKSxcuBCnT5+GQqEo4QjlpSD9qdVqAQDdunXD2LFjAQD169fH0aNHERYWhjZt2pRYvKVdQX/fFy1ahOPHjyMyMhKenp44ePAggoKC4OLiovdXMwHNmjVDs2bNdK+bN2+OGjVqYOnSpZgxY4aEkclTYfrzXb+PmLSUAuXKlYOpqSkePnyoV/7w4UM4OzvnqH/z5k3cuXMH/v7+urLsLwEzMzNcvXoV3t7eAP73Abl79y727dtn9KMsQPH056FDh/Do0SN4eHjo6mRlZeE///kPFixYgDt37hTPwZQCxdGf7u7uMDMzQ82aNfW2rVGjBg4fPlwMR1F6FEd/uri44Msvv8TmzZvRpUsXAEDdunURHR2N7777zqiTlsL2Z27Mzc3RoEED3LhxAwB02z18+BAVKlTQ22f9+vUNE3gpVRz9mc0Q30c8PVQKWFhYoGHDhoiKitKVabVaREVF6WWv2apXr47z588jOjpat3Tt2hUffPABoqOj4e7uDuB/H5Dr169j7969KFu2bIkdk5SKoz8/++wznDt3Tq+Oi4sLxo8fj127dpXk4ZW44uhPCwsLNG7cGFevXtXb9tq1a/D09Cz2Y5JScfRnZmYmMjMzYWKi/1+6qampLsExVoXtz9xkZWXh/PnzugTFy8sLzs7OevtMTEzEX3/9VeB9ylVx9CdgwO+jQk3bpWITHh4ulEqlWLVqlbh06ZIIDAwU9vb2usvGPvvsMzFx4sQ8t3/zaoKMjAzRtWtX4ebmJqKjo/UuRUtPTy/uw5GcofszN+/T1UPF0Z8RERHC3NxcLFu2TFy/fl0sWrRImJqaikOHDhXnoZQKxdGfbdq0EbVq1RL79+8Xt27dEitXrhSWlpbixx9/LM5DKRUK258hISFi165d4ubNm+LUqVPi008/FZaWluLixYu6Ot98842wt7cXW7duFefOnRPdunV7ry55NmR/GvL7iKeHSolPPvkEjx8/xpQpU/DgwQPUr18fO3fu1E2GiomJyfFXVH40Gg0iIyMBIMdw5v79+9G2bVtDhV4qGbo/33fF0Z89evRAWFgYQkNDMXr0aFSrVg2bNm1Cy5Yti+MQSpXi6M/w8HBMmjQJ/fr1w7Nnz+Dp6YmZM2di+PDhxXEIpUph+/P58+cYNmwYHjx4gDJlyqBhw4Y4evSo3unKCRMmICUlBYGBgYiPj0fLli2xc+fOHDehM0aG7k9Dfh8phBDi3Q6PiIiIqPjxT00iIiKSBSYtREREJAtMWoiIiEgWmLQQERGRLDBpISIiIllg0kJERESywKSFiIiIZIFJCxG9F65evQpnZ2ckJSW9te6lS5fg5uaGlJSUEoiMiAqKSQsR5UqhUOS7TJs2TeoQC2XSpEn497//DZVK9da6NWvWxD/+8Q/MmzevBCIjooLiHXGJKFcPHjzQ/Xv9+vWYMmWK3gMObW1tYWtrK0VohRYTE4PKlSvj9u3bcHV1LdA227Ztw7BhwxATEwMzMz7xhKg04EgLEeXK2dlZt9jZ2UGhUOiVhYeHo0aNGrC0tET16tXx448/6ra9c+cOFAoFNmzYgFatWsHKygqNGzfGtWvXcOLECTRq1Ai2trbw8/PD48ePddsNHDgQ3bt3R0hICBwdHaFWqzF8+HBkZGTo6qSnp2P06NFwcnKCpaUlWrZsiRMnTuR7LBs2bEC9evX0Epa7d+/C398fZcqUgY2NDWrVqoXt27fr1nfo0AHPnj3Dn3/+aYjuJCID4J8PRFRov/76K6ZMmYIffvgBDRo0wJkzZzBs2DDY2NggICBAV2/q1KlYsGABPDw8MHjwYPTt2xcqlQoLFy6EtbU1evfujSlTpmDJkiW6baKiomBpaYkDBw7gzp07GDRoEMqWLYuZM2cCePUgu02bNmH16tXw9PTE7Nmz4evrixs3bsDBwSHXeA8dOoRGjRrplQUFBSEjIwMHDx6EjY0NLl26pDdyZGFhgfr16+PQoUNo166dIbuPiIrKMA+yJiJjtnLlSmFnZ6d77e3tLdauXatXZ8aMGaJZs2ZCCCFu374tAIiffvpJt37dunUCgIiKitKVhYaGimrVquleBwQECAcHB5GSkqIrW7JkibC1tRVZWVkiOTlZmJubi19//VW3PiMjQ7i4uIjZs2fnGX+9evXE9OnT9crq1Kkjpk2blu9x9+jRQwwcODDfOkRUcjjSQkSFkpKSgps3b2LIkCEYNmyYrvzly5ews7PTq1u3bl3dv7Mfa1+nTh29skePHultU69ePVhbW+teN2vWDMnJybh37x4SEhKQmZmJFi1a6Nabm5ujSZMmuHz5cp4xp6WlwdLSUq9s9OjRGDFiBHbv3o327dvjn//8p168AGBlZYXU1NQ890tEJYtzWoioUJKTkwEAy5cvR3R0tG65cOECjh8/rlfX3Nxc92+FQpFrmVarLfaYy5Urh+fPn+uVDR06FLdu3cJnn32G8+fPo1GjRli0aJFenWfPnsHR0bHY4yOigmHSQkSFUr58ebi4uODWrVuoXLmy3uLl5fXO+z979izS0tJ0r48fPw5bW1u4u7vD29sbFhYWOHLkiG59ZmYmTpw4gZo1a+a5zwYNGuDSpUs5yt3d3TF8+HBERETgP//5D5YvX663/sKFC2jQoME7HxMRGQZPDxFRoYWEhGD06NGws7NDp06dkJ6ejpMnT+L58+cIDg5+p31nZGRgyJAh+Prrr3Hnzh1MnToVo0aNgomJCWxsbDBixAiMHz8eDg4O8PDwwOzZs5GamoohQ4bkuU9fX18MHToUWVlZMDU1BQCMGTMGfn5+qFq1Kp4/f479+/ejRo0aum3u3LkDjUaD9u3bv9PxEJHhMGkhokIbOnQorK2tMWfOHIwfPx42NjaoU6cOxowZ8877bteuHapUqYLWrVsjPT0dffr00buR3TfffAOtVovPPvsMSUlJaNSoEXbt2oUyZcrkuU8/Pz+YmZlh79698PX1BQBkZWUhKCgIsbGxUKvV6NSpE+bPn6/bZt26dejYsSM8PT3f+ZiIyDB4czkiKjUGDhyI+Ph4bNmyxeD7Xrx4MSIjI7Fr16631s3IyECVKlWwdu1avUm/RCQtjrQQ0XvhX//6F+Lj45GUlPTWW/nHxMTgyy+/ZMJCVMpwpIWISo3iHGkhIvlj0kJERESywEueiYiISBaYtBAREZEsMGkhIiIiWWDSQkRERLLApIWIiIhkgUkLERERyQKTFiIiIpIFJi1EREQkC0xaiIiISBb+H/oBTCCEtS8SAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculando média e mediana\n",
    "media = df_tempos['tempo_gasto'].mean()\n",
    "mediana = df_tempos['tempo_gasto'].median()\n",
    "\n",
    "# Plotando gráfico\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_tempos['tempo_gasto'], bins=50, color='blue', alpha=0.5)\n",
    "ax.axvline(media, color='red', linestyle='dashed', linewidth=2, label=f'Média = {media:.2f}')\n",
    "ax.axvline(mediana, color='green', linestyle='dashed', linewidth=2, label=f'Mediana = {mediana:.2f}')\n",
    "ax.set_xlabel('Tempo (s)')\n",
    "ax.set_ylabel('Frequência')\n",
    "ax.set_title('Distribuição de Tempos de Execução')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval = load(\"trec_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query  q0     docid  rank     score    system\n",
      "0     44  Q0  cenc2u15     1  9.595705  Pesquisa\n",
      "1     44  Q0  9zjtwp19     2  9.218919  Pesquisa\n",
      "2     44  Q0  vgtc0k3a     3  9.138041  Pesquisa\n",
      "3     44  Q0  1ogglaa3     4  8.957155  Pesquisa\n",
      "4     44  Q0  nd5xpk0t     5  8.828953  Pesquisa\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "NDCG@10: 0.5569450498054185\n",
      "Resultados: {'runid': 'Pesquisa', 'num_ret': 50000, 'num_rel': 24673, 'num_rel_ret': 8796, 'num_q': 50, 'map': 0.1607514087489604, 'gm_map': 0.09456175417922583, 'bpref': 0.31739196218798343, 'Rprec': 0.25396450726221553, 'recip_rank': 0.7945238095238095, 'P@5': 0.612, 'P@10': 0.588, 'P@15': 0.5613333333333334, 'P@20': 0.5489999999999999, 'P@30': 0.5293333333333333, 'P@100': 0.4232, 'P@200': 0.3518, 'P@500': 0.24971999999999997, 'P@1000': 0.17592, 'NDCG@5': 0.5866948850417006, 'NDCG@10': 0.5569450498054185, 'NDCG@15': 0.5301038903139429, 'NDCG@20': 0.5165360796843828, 'NDCG@30': 0.4968598935465589, 'NDCG@100': 0.40818913082774877, 'NDCG@200': 0.35099572011191393, 'NDCG@500': 0.32732576073842556, 'NDCG@1000': 0.37314846719870076}\n"
     ]
    }
   ],
   "source": [
    "### Calculando métricas\n",
    "run = pd.read_csv(f\"{CAMINHO_RUN}\", sep=\"\\s+\", \n",
    "                names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "print(run.head())\n",
    "run = run.to_dict(orient=\"list\")\n",
    "results = trec_eval.compute(predictions=[run], references=[qrel_dict])\n",
    "\n",
    "# salvando métricas    \n",
    "print(f\"NDCG@10: {results['NDCG@10']}\")\n",
    "print(f\"Resultados: {results}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLADE V2 (max e sem expansão para query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 queries completadas\n",
      "5 queries completadas\n",
      "10 queries completadas\n",
      "15 queries completadas\n",
      "20 queries completadas\n",
      "25 queries completadas\n",
      "30 queries completadas\n",
      "35 queries completadas\n",
      "40 queries completadas\n",
      "45 queries completadas\n",
      "CPU times: user 1min 50s, sys: 1.13 s, total: 1min 51s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tempo_gasto = run_all_queries_busca_exaustiva(parm_model=model,\n",
    "                                    parm_tokenizer=tokenizer, \n",
    "                                    parm_dict_queries=topics,\n",
    "                                    parm_corpus_expanded=corpus_expanded, \n",
    "                                    parm_lista_doc_id_passage=lista_doc_id_passage,\n",
    "                                    parm_num_max_hits=1000,\n",
    "                                    parm_se_query_expansion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.0000000\n",
       "mean      0.4535157\n",
       "std       0.0249504\n",
       "min       0.4248724\n",
       "25%       0.4307245\n",
       "50%       0.4407955\n",
       "75%       0.4805880\n",
       "max       0.4980636\n",
       "Name: tempo_gasto, Length: 8, dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tempos = pd.DataFrame({'tempo_gasto': tempo_gasto})\n",
    "df_tempos['tempo_gasto'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculando média e mediana\n",
    "media = df_tempos['tempo_gasto'].mean()\n",
    "mediana = df_tempos['tempo_gasto'].median()\n",
    "\n",
    "# Plotando gráfico\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_tempos['tempo_gasto'], bins=50, color='blue', alpha=0.5)\n",
    "ax.axvline(media, color='red', linestyle='dashed', linewidth=2, label=f'Média = {media:.2f}')\n",
    "ax.axvline(mediana, color='green', linestyle='dashed', linewidth=2, label=f'Mediana = {mediana:.2f}')\n",
    "ax.set_xlabel('Tempo (s)')\n",
    "ax.set_ylabel('Frequência')\n",
    "ax.set_title('Distribuição de Tempos de Execução')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_eval = load(\"trec_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query  q0     docid  rank      score    system\n",
      "0     44  Q0  dt2pew66     1  24.055389  Pesquisa\n",
      "1     44  Q0  xfjexm5b     2  23.631659  Pesquisa\n",
      "2     44  Q0  uc37poce     3  23.174759  Pesquisa\n",
      "3     44  Q0  qi1henyy     4  23.142399  Pesquisa\n",
      "4     44  Q0  28utunid     5  22.774845  Pesquisa\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "NDCG@10: 0.7273549342638757\n",
      "Resultados: {'runid': 'Pesquisa', 'num_ret': 50000, 'num_rel': 24673, 'num_rel_ret': 11272, 'num_q': 50, 'map': 0.24291340206093529, 'gm_map': 0.19607936040556906, 'bpref': 0.4018979572216081, 'Rprec': 0.33779669053277495, 'recip_rank': 0.9066666666666666, 'P@5': 0.8319999999999999, 'P@10': 0.7959999999999999, 'P@15': 0.7533333333333333, 'P@20': 0.738, 'P@30': 0.708, 'P@100': 0.5680000000000001, 'P@200': 0.47680000000000006, 'P@500': 0.33063999999999993, 'P@1000': 0.22543999999999997, 'NDCG@5': 0.7498872531565653, 'NDCG@10': 0.7273549342638757, 'NDCG@15': 0.7014248590932509, 'NDCG@20': 0.6874989230317395, 'NDCG@30': 0.6642314491344944, 'NDCG@100': 0.5457008110089955, 'NDCG@200': 0.47440394411020476, 'NDCG@500': 0.4339159178624668, 'NDCG@1000': 0.48621123137520206}\n"
     ]
    }
   ],
   "source": [
    "### Calculando métricas\n",
    "run = pd.read_csv(f\"{CAMINHO_RUN}\", sep=\"\\s+\", \n",
    "                names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "print(run.head())\n",
    "run = run.to_dict(orient=\"list\")\n",
    "results = trec_eval.compute(predictions=[run], references=[qrel_dict])\n",
    "\n",
    "# salvando métricas    \n",
    "print(f\"NDCG@10: {results['NDCG@10']}\")\n",
    "print(f\"Resultados: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treinapython39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
